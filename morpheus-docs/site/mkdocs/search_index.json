{
    "docs": [
        {
            "location": "/",
            "text": "Introduction\n\n\nThe Morpheus library is designed to facilitate the development of high performance analytical software involving large datasets for \nboth offline and real-time analysis on the \nJava Virtual Machine\n (JVM). The \nlibrary is written in Java 8 with extensive use of lambdas, but is accessible to all JVM languages.\n\n\nMotivation\n\n\nAt its core, Morpheus provides a versatile two-dimensional \nmemory efficient\n tabular data structure called a \nDataFrame\n, similar to\nthat first popularised in \nR\n. While\ndynamically typed scientific computing languages like \nR\n, \nPython\n & \nMatlab\n \nare great for doing research, they are not well suited for large scale production systems as they become extremely difficult to maintain, \nand dangerous to refactor. The Morpheus library attempts to retain the power and versatility of the \nDataFrame\n concept, while providing a \nmuch more \ntype safe\n and \nself describing\n set of interfaces, which should make developing, maintaining & scaling code complexity much \neasier. \n\n\nAnother advantage of the Morpheus library is that it is extremely good at \nscaling\n on \nmulti-core processor\n \narchitectures given the powerful \nthreading\n capabilities of the Java \nVirtual Machine. Many operations on a Morpheus \nDataFrame\n can seamlessly be run in \nparallel\n by simply calling \nparallel()\n on the entity \nyou wish to operate on, much like with \nJava 8 Streams\n. \nInternally, these parallel implementations are based on the Fork & Join framework, and near linear improvements in performance are observed \nfor certain types of operations as CPU cores are added.\n\n\nCapabilities\n\n\nA Morpheus \nDataFrame\n is a column store structure where each column is represented by a Morpheus \nArray\n of which there are many \nimplementations, including dense, sparse and \nmemory mapped\n versions. Morpheus arrays \nare optimized and wherever possible are backed by primitive native Java arrays (even for types such as \nLocalDate\n, \nLocalDateTime\n etc...) \nas these are far more efficient from a storage, access and garbage collection perspective. Memory mapped Morpheus \nArrays\n, while still \nexperimental, allow very large \nDataFrames\n to be created using off-heap storage that are backed by files.\n\n\nWhile the complete feature set of the Morpheus \nDataFrame\n is still evolving, there are already many powerful APIs to affect complex \ntransformations and analytical operations with ease. There are standard functions to compute summary statistics, perform various types \nof \nLinear Regressions\n, apply \nPrincipal Component Analysis\n \n(PCA) to mention just a few. The \nDataFrame\n is indexed in both the row and column dimension, allowing data to be efficiently \nsorted\n, \n\nsliced\n, \ngrouped\n, and \naggregated\n along either axis.\n\n\nData Access\n\n\nMorpheus also aims to provide a standard mechanism to load datasets from various data providers. The hope is that this API will \nbe embraced by the community in order to grow the catalogue of supported data sources. Currently, providers are implemented to enable \ndata to be loaded from \nQuandl\n, \nThe Federal Reserve\n, \n\nThe World Bank\n, \nYahoo Finance\n and \nGoogle Finance\n.\n\n\nMorpheus at a Glance\n\n\nA Simple Example\n\n\nConsider a dataset of motor vehicle characteristics accessible \nhere\n.\nThe code below loads this CSV data into a Morpheus \nDataFrame\n, filters the rows to only include those vehicles that have a power \nto weight ratio > 0.1 (where \nweight\n is converted into kilograms), then adds a column to record the relative efficiency between highway \nand city mileage (MPG), sorts the rows by this newly added column in descending order, and finally records this transformed result \nto a CSV file.\n\n\n\n\n\nDataFrame.read().csv(options -> {\n    options.setResource(\"http://zavtech.com/data/samples/cars93.csv\");\n    options.setExcludeColumnIndexes(0);\n}).rows().select(row -> {\n    double weightKG = row.getDouble(\"Weight\") * 0.453592d;\n    double horsepower = row.getDouble(\"Horsepower\");\n    return horsepower / weightKG > 0.1d;\n}).cols().add(\"MPG(Highway/City)\", Double.class, v -> {\n    double cityMpg = v.row().getDouble(\"MPG.city\");\n    double highwayMpg = v.row().getDouble(\"MPG.highway\");\n    return highwayMpg / cityMpg;\n}).rows().sort(false, \"MPG(Highway/City)\").write().csv(options -> {\n    options.setFile(\"/Users/witdxav/cars93m.csv\");\n    options.setTitle(\"DataFrame\");\n});\n\n\n\n\nThis example demonstrates the functional nature of the Morpheus API, where many method return types are in fact a \nDataFrame\n and \ntherefore allow this form of method chaining. In this example, the methods \ncsv()\n, \nselect()\n, \nadd()\n, and \nsort()\n all return\na frame. In some cases the same frame that the method operates on, or in other cases a filter or shallow copy of the frame being\noperated on. The first 10 rows of the transformed dataset in this example looks as follows, with the newly added column appearing\non the far right of the frame.\n\n\n\n Index  |  Manufacturer  |     Model      |   Type    |  Min.Price  |   Price   |  Max.Price  |  MPG.city  |  MPG.highway  |       AirBags        |  DriveTrain  |  Cylinders  |  EngineSize  |  Horsepower  |  RPM   |  Rev.per.mile  |  Man.trans.avail  |  Fuel.tank.capacity  |  Passengers  |  Length  |  Wheelbase  |  Width  |  Turn.circle  |  Rear.seat.room  |  Luggage.room  |  Weight  |  Origin   |           Make            |  MPG(Highway/City)  |\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     9  |      Cadillac  |       DeVille  |    Large  |    33.0000  |  34.7000  |    36.3000  |        16  |           25  |         Driver only  |       Front  |          8  |      4.9000  |         200  |  4100  |          1510  |               No  |             18.0000  |           6  |     206  |        114  |     73  |           43  |              35  |            18  |    3620  |      USA  |         Cadillac DeVille  |             1.5625  |\n    10  |      Cadillac  |       Seville  |  Midsize  |    37.5000  |  40.1000  |    42.7000  |        16  |           25  |  Driver & Passenger  |       Front  |          8  |      4.6000  |         295  |  6000  |          1985  |               No  |             20.0000  |           5  |     204  |        111  |     74  |           44  |              31  |            14  |    3935  |      USA  |         Cadillac Seville  |             1.5625  |\n    70  |    Oldsmobile  |  Eighty-Eight  |    Large  |    19.5000  |  20.7000  |    21.9000  |        19  |           28  |         Driver only  |       Front  |          6  |      3.8000  |         170  |  4800  |          1570  |               No  |             18.0000  |           6  |     201  |        111  |     74  |           42  |            31.5  |            17  |    3470  |      USA  |  Oldsmobile Eighty-Eight  |         1.47368421  |\n    74  |       Pontiac  |      Firebird  |   Sporty  |    14.0000  |  17.7000  |    21.4000  |        19  |           28  |  Driver & Passenger  |        Rear  |          6  |      3.4000  |         160  |  4600  |          1805  |              Yes  |             15.5000  |           4  |     196  |        101  |     75  |           43  |              25  |            13  |    3240  |      USA  |         Pontiac Firebird  |         1.47368421  |\n     6  |         Buick  |       LeSabre  |    Large  |    19.9000  |  20.8000  |    21.7000  |        19  |           28  |         Driver only  |       Front  |          6  |      3.8000  |         170  |  4800  |          1570  |               No  |             18.0000  |           6  |     200  |        111  |     74  |           42  |            30.5  |            17  |    3470  |      USA  |            Buick LeSabre  |         1.47368421  |\n    13  |     Chevrolet  |        Camaro  |   Sporty  |    13.4000  |  15.1000  |    16.8000  |        19  |           28  |  Driver & Passenger  |        Rear  |          6  |      3.4000  |         160  |  4600  |          1805  |              Yes  |             15.5000  |           4  |     193  |        101  |     74  |           43  |              25  |            13  |    3240  |      USA  |         Chevrolet Camaro  |         1.47368421  |\n    76  |       Pontiac  |    Bonneville  |    Large  |    19.4000  |  24.4000  |    29.4000  |        19  |           28  |  Driver & Passenger  |       Front  |          6  |      3.8000  |         170  |  4800  |          1565  |               No  |             18.0000  |           6  |     177  |        111  |     74  |           43  |            30.5  |            18  |    3495  |      USA  |       Pontiac Bonneville  |         1.47368421  |\n    56  |         Mazda  |          RX-7  |   Sporty  |    32.5000  |  32.5000  |    32.5000  |        17  |           25  |         Driver only  |        Rear  |     rotary  |      1.3000  |         255  |  6500  |          2325  |              Yes  |             20.0000  |           2  |     169  |         96  |     69  |           37  |              NA  |            NA  |    2895  |  non-USA  |               Mazda RX-7  |         1.47058824  |\n    18  |     Chevrolet  |      Corvette  |   Sporty  |    34.6000  |  38.0000  |    41.5000  |        17  |           25  |         Driver only  |        Rear  |          8  |      5.7000  |         300  |  5000  |          1450  |              Yes  |             20.0000  |           2  |     179  |         96  |     74  |           43  |              NA  |            NA  |    3380  |      USA  |       Chevrolet Corvette  |         1.47058824  |\n    51  |       Lincoln  |      Town_Car  |    Large  |    34.4000  |  36.1000  |    37.8000  |        18  |           26  |  Driver & Passenger  |        Rear  |          8  |      4.6000  |         210  |  4600  |          1840  |               No  |             20.0000  |           6  |     219  |        117  |     77  |           45  |            31.5  |            22  |    4055  |      USA  |         Lincoln Town_Car  |         1.44444444  |\n\n\n\n\nA Regression Example\n\n\nThe Morpheus API includes a regression interface in order to fit data to a linear model using either \nOLS\n, \n\nWLS\n or \nGLS\n. The code below uses the same car dataset introduced in the previous example, \nand regresses \nHorsepower\n on \nEngineSize\n. The code example prints the model results to standard out, which is shown below, \nand then creates a scatter chart with the regression line clearly displayed.\n\n\n\n\n\n//Load the data\nDataFrame<Integer,String> data = DataFrame.read().csv(options -> {\n    options.setResource(\"http://zavtech.com/data/samples/cars93.csv\");\n    options.setExcludeColumnIndexes(0);\n});\n\n//Run OLS regression and plot \nString regressand = \"Horsepower\";\nString regressor = \"EngineSize\";\ndata.regress().ols(regressand, regressor, true, model -> {\n    System.out.println(model);\n    DataFrame<Integer,String> xy = data.cols().select(regressand, regressor);\n    Chart.create().withScatterPlot(xy, false, regressor, chart -> {\n        chart.title().withText(regressand + \" regressed on \" + regressor);\n        chart.subtitle().withText(\"Single Variable Linear Regression\");\n        chart.plot().style(regressand).withColor(Color.RED).withPointsVisible(true);\n        chart.plot().trend(regressand).withColor(Color.BLACK);\n        chart.plot().axes().domain().label().withText(regressor);\n        chart.plot().axes().domain().format().withPattern(\"0.00;-0.00\");\n        chart.plot().axes().range(0).label().withText(regressand);\n        chart.plot().axes().range(0).format().withPattern(\"0;-0\");\n        chart.show();\n    });\n    return Optional.empty();\n});\n\n\n\n\n\n==============================================================================================\n                                   Linear Regression Results                                                            \n==============================================================================================\nModel:                                   OLS    R-Squared:                            0.5360\nObservations:                             93    R-Squared(adjusted):                  0.5309\nDF Model:                                  1    F-Statistic:                        105.1204\nDF Residuals:                             91    F-Statistic(Prob):                  1.11E-16\nStandard Error:                      35.8717    Runtime(millis)                           52\nDurbin-Watson:                        1.9591                                                \n==============================================================================================\n   Index     |  PARAMETER  |  STD_ERROR  |  T_STAT   |   P_VALUE   |  CI_LOWER  |  CI_UPPER  |\n----------------------------------------------------------------------------------------------\n  Intercept  |    45.2195  |    10.3119  |   4.3852  |   3.107E-5  |    24.736  |   65.7029  |\n EngineSize  |    36.9633  |     3.6052  |  10.2528  |  7.573E-17  |    29.802  |   44.1245  |\n==============================================================================================\n\n\n\n\n\n    \n\n\n\n\n\nUK House Price Trends\n\n\nIt is possible to access all UK residential \nreal-estate transaction records\n\nfrom 1995 through to current day via the \nUK Government Open Data\n initiative. The data is presented in CSV \nformat, and contains numerous \ncolumns\n, including such information as the \ntransaction date, price paid, fully qualified address (including postal code), property type, lease type and so on.\n\n\nLet us begin by writing a function to load these CSV files from Amazon S3 buckets, and since they are stored one file per year,\nwe provide a parameterized function accordingly. Given the requirements of our analysis, there is no need to load all the columns in the \nfile, so below we only choose to read columns at index 1, 2, 4, and 11. In addition, since the files do not include a header, we \nre-name columns to something more meaningful to make subsequent access a little clearer.\n\n\n\n\n\n/**\n * Loads UK house price from the Land Registry stored in an Amazon S3 bucket\n * Note the data does not have a header, so columns will be named Column-0, Column-1 etc...\n * @param year      the year for which to load prices\n * @return          the resulting DataFrame, with some columns renamed\n */\nprivate DataFrame<Integer,String> loadHousePrices(Year year) {\n    String resource = \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-%s.csv\";\n    return DataFrame.read().csv(options -> {\n        options.setResource(String.format(resource, year.getValue()));\n        options.setHeader(false);\n        options.setCharset(StandardCharsets.UTF_8);\n        options.setIncludeColumnIndexes(1, 2, 4, 11);\n        options.getFormats().setParser(\"TransactDate\", Parser.ofLocalDate(\"yyyy-MM-dd HH:mm\"));\n        options.setColumnNameMapping((colName, colOrdinal) -> {\n            switch (colOrdinal) {\n                case 0:     return \"PricePaid\";\n                case 1:     return \"TransactDate\";\n                case 2:     return \"PropertyType\";\n                case 3:     return \"City\";\n                default:    return colName;\n            }\n        });\n    });\n}\n\n\n\n\nBelow we use this data in order to compute the median nominal price (not inflation adjusted) of an \napartment\n for each year between \n1995 through 2014 for a subset of the largest cities in the UK. There are about 20 million records in the unfiltered dataset between \n1993 and 2014, and while it takes a fairly long time to load and parse (approximately 3.5GB of data), Morpheus executes the analytical \nportion of the code in about 5 seconds (not including load time) on a standard Apple Macbook Pro purchased in late 2013. Note how we use \nparallel processing to load and process the data by calling \nresults.rows().keys().parallel()\n. \n\n\n\n\n\n//Create a data frame to capture the median prices of Apartments in the UK'a largest cities\nDataFrame<Year,String> results = DataFrame.ofDoubles(\n    Range.of(1995, 2015).map(Year::of),\n    Array.of(\"LONDON\", \"BIRMINGHAM\", \"SHEFFIELD\", \"LEEDS\", \"LIVERPOOL\", \"MANCHESTER\")\n);\n\n//Process yearly data in parallel to leverage all CPU cores\nresults.rows().keys().parallel().forEach(year -> {\n    System.out.printf(\"Loading UK house prices for %s...\\n\", year);\n    DataFrame<Integer,String> prices = loadHousePrices(year);\n    prices.rows().select(row -> {\n        //Filter rows to include only apartments in the relevant cities\n        final String propType = row.getValue(\"PropertyType\");\n        final String city = row.getValue(\"City\");\n        final String cityUpperCase = city != null ? city.toUpperCase() : null;\n        return propType != null && propType.equals(\"F\") && results.cols().contains(cityUpperCase);\n    }).rows().groupBy(\"City\").forEach(0, (groupKey, group) -> {\n        //Group row filtered frame so we can compute median prices in selected cities\n        final String city = groupKey.item(0);\n        final double priceStat = group.colAt(\"PricePaid\").stats().median();\n        results.data().setDouble(year, city, priceStat);\n    });\n});\n\n//Map row keys to LocalDates, and map values to be percentage changes from start date\nfinal DataFrame<LocalDate,String> plotFrame = results.mapToDoubles(v -> {\n    final double firstValue = v.col().getDouble(0);\n    final double currentValue = v.getDouble();\n    return (currentValue / firstValue - 1d) * 100d;\n}).rows().mapKeys(row -> {\n    final Year year = row.key();\n    return LocalDate.of(year.getValue(), 12, 31);\n});\n\n//Create a plot, and display it\nChart.create().withLinePlot(plotFrame, chart -> {\n    chart.title().withText(\"Median Nominal House Price Changes\");\n    chart.title().withFont(new Font(\"Arial\", Font.BOLD, 14));\n    chart.subtitle().withText(\"Date Range: 1995 - 2014\");\n    chart.plot().axes().domain().label().withText(\"Year\");\n    chart.plot().axes().range(0).label().withText(\"Percent Change from 1995\");\n    chart.plot().axes().range(0).format().withPattern(\"0.##'%';-0.##'%'\");\n    chart.plot().style(\"LONDON\").withColor(Color.BLACK);\n    chart.legend().on().bottom();\n    chart.show();\n});\n\n\n\n\nThe percent change in nominal median prices for \napartments\n in the subset of chosen cities is shown in the plot below. It \nshows that London did not suffer any nominal house price decline as a result of the Global Financial Crisis (GFC), however not \nall cities in the UK proved as resilient. What is slightly surprising is that some of the less affluent northern cities saw a \nhigher rate of appreciation in the 2003 to 2006 period compared to London. One thing to note is that while London did not see \nany nominal price reduction, there was certainly a fairly severe correction in terms of EUR and USD since Pound Sterling \ndepreciated heavily against these currencies during the GFC.\n\n\n\n    \n\n\n\n\n\nVisualization\n\n\nVisualizing data in Morpheus \nDataFrames\n is made easy via a \nsimple chart abstraction API\n with adapters supporting both \n\nJFreeChart\n as well as \nGoogle Charts\n (with others\nto follow by popular demand). This design makes it possible to generate interactive \nJava Swing\n \ncharts as well as HTML5 browser based charts via the same programmatic interface. For more details on how to use this API, \nsee the section on visualization \nhere\n, and the code \nhere\n.\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\nMaven Artifacts\n\n\nMorpheus is published to Maven Central so it can be easily added as a dependency in your build tool of choice. The codebase is currently\ndivided into 5 repositories to allow each module to be evolved independently. The core module, which is aptly named \nmorpheus-core\n,\nis the foundational library on which all other modules depend. The various Maven artifacts are as follows: \n\n\nMorpheus Core\n\n\nThe \nfoundational\n library that contains Morpheus Arrays, DataFrames and other key interfaces & implementations.\n\n\n<dependency>\n    <groupId>com.zavtech</groupId>\n    <artifactId>morpheus-core</artifactId>\n    <version>${VERSION}</version>\n</dependency>\n\n\n\n\nMorpheus Visualization\n\n\nThe \nvisualization\n components to display \nDataFrames\n in charts and tables.\n\n\n<dependency>\n    <groupId>com.zavtech</groupId>\n    <artifactId>morpheus-viz</artifactId>\n    <version>${VERSION}</version>\n</dependency>\n\n\n\n\nMorpheus Quandl\n\n\nThe \nadapter\n to load data from \nQuandl\n\n\n<dependency>\n    <groupId>com.zavtech</groupId>\n    <artifactId>morpheus-quandl</artifactId>\n    <version>${VERSION}</version>\n</dependency>\n\n\n\n\nMorpheus Google\n\n\nThe \nadapter\n to load data from \nGoogle Finance\n\n\n<dependency>\n    <groupId>com.zavtech</groupId>\n    <artifactId>morpheus-google</artifactId>\n    <version>${VERSION}</version>\n</dependency>\n\n\n\n\nMorpheus Yahoo\n\n\nThe \nadapter\n to load data from \nYahoo Finance\n\n\n<dependency>\n    <groupId>com.zavtech</groupId>\n    <artifactId>morpheus-yahoo</artifactId>\n    <version>${VERSION}</version>\n</dependency>\n\n\n\n\nQ&A Forum\n\n\nA Questions & Answers forum has been setup using Google Groups and is accessible \nhere\n\n\nJavadocs\n\n\nMorpheus Javadocs can be accessed online \nhere\n.\n\n\nBuild Status\n\n\nA Continuous Integration build server can be accessed \nhere\n, which builds code after each merge.\n\n\nLicense\n\n\nMorpheus is released under the \nApache Software Foundation License Version 2\n.",
            "title": "Overview"
        },
        {
            "location": "/#introduction",
            "text": "The Morpheus library is designed to facilitate the development of high performance analytical software involving large datasets for \nboth offline and real-time analysis on the  Java Virtual Machine  (JVM). The \nlibrary is written in Java 8 with extensive use of lambdas, but is accessible to all JVM languages.",
            "title": "Introduction"
        },
        {
            "location": "/#motivation",
            "text": "At its core, Morpheus provides a versatile two-dimensional  memory efficient  tabular data structure called a  DataFrame , similar to\nthat first popularised in  R . While\ndynamically typed scientific computing languages like  R ,  Python  &  Matlab  \nare great for doing research, they are not well suited for large scale production systems as they become extremely difficult to maintain, \nand dangerous to refactor. The Morpheus library attempts to retain the power and versatility of the  DataFrame  concept, while providing a \nmuch more  type safe  and  self describing  set of interfaces, which should make developing, maintaining & scaling code complexity much \neasier.   Another advantage of the Morpheus library is that it is extremely good at  scaling  on  multi-core processor  \narchitectures given the powerful  threading  capabilities of the Java \nVirtual Machine. Many operations on a Morpheus  DataFrame  can seamlessly be run in  parallel  by simply calling  parallel()  on the entity \nyou wish to operate on, much like with  Java 8 Streams . \nInternally, these parallel implementations are based on the Fork & Join framework, and near linear improvements in performance are observed \nfor certain types of operations as CPU cores are added.",
            "title": "Motivation"
        },
        {
            "location": "/#capabilities",
            "text": "A Morpheus  DataFrame  is a column store structure where each column is represented by a Morpheus  Array  of which there are many \nimplementations, including dense, sparse and  memory mapped  versions. Morpheus arrays \nare optimized and wherever possible are backed by primitive native Java arrays (even for types such as  LocalDate ,  LocalDateTime  etc...) \nas these are far more efficient from a storage, access and garbage collection perspective. Memory mapped Morpheus  Arrays , while still \nexperimental, allow very large  DataFrames  to be created using off-heap storage that are backed by files.  While the complete feature set of the Morpheus  DataFrame  is still evolving, there are already many powerful APIs to affect complex \ntransformations and analytical operations with ease. There are standard functions to compute summary statistics, perform various types \nof  Linear Regressions , apply  Principal Component Analysis  \n(PCA) to mention just a few. The  DataFrame  is indexed in both the row and column dimension, allowing data to be efficiently  sorted ,  sliced ,  grouped , and  aggregated  along either axis.",
            "title": "Capabilities"
        },
        {
            "location": "/#data-access",
            "text": "Morpheus also aims to provide a standard mechanism to load datasets from various data providers. The hope is that this API will \nbe embraced by the community in order to grow the catalogue of supported data sources. Currently, providers are implemented to enable \ndata to be loaded from  Quandl ,  The Federal Reserve ,  The World Bank ,  Yahoo Finance  and  Google Finance .",
            "title": "Data Access"
        },
        {
            "location": "/#morpheus-at-a-glance",
            "text": "",
            "title": "Morpheus at a Glance"
        },
        {
            "location": "/#a-simple-example",
            "text": "Consider a dataset of motor vehicle characteristics accessible  here .\nThe code below loads this CSV data into a Morpheus  DataFrame , filters the rows to only include those vehicles that have a power \nto weight ratio > 0.1 (where  weight  is converted into kilograms), then adds a column to record the relative efficiency between highway \nand city mileage (MPG), sorts the rows by this newly added column in descending order, and finally records this transformed result \nto a CSV file.   DataFrame.read().csv(options -> {\n    options.setResource(\"http://zavtech.com/data/samples/cars93.csv\");\n    options.setExcludeColumnIndexes(0);\n}).rows().select(row -> {\n    double weightKG = row.getDouble(\"Weight\") * 0.453592d;\n    double horsepower = row.getDouble(\"Horsepower\");\n    return horsepower / weightKG > 0.1d;\n}).cols().add(\"MPG(Highway/City)\", Double.class, v -> {\n    double cityMpg = v.row().getDouble(\"MPG.city\");\n    double highwayMpg = v.row().getDouble(\"MPG.highway\");\n    return highwayMpg / cityMpg;\n}).rows().sort(false, \"MPG(Highway/City)\").write().csv(options -> {\n    options.setFile(\"/Users/witdxav/cars93m.csv\");\n    options.setTitle(\"DataFrame\");\n});  This example demonstrates the functional nature of the Morpheus API, where many method return types are in fact a  DataFrame  and \ntherefore allow this form of method chaining. In this example, the methods  csv() ,  select() ,  add() , and  sort()  all return\na frame. In some cases the same frame that the method operates on, or in other cases a filter or shallow copy of the frame being\noperated on. The first 10 rows of the transformed dataset in this example looks as follows, with the newly added column appearing\non the far right of the frame.  \n Index  |  Manufacturer  |     Model      |   Type    |  Min.Price  |   Price   |  Max.Price  |  MPG.city  |  MPG.highway  |       AirBags        |  DriveTrain  |  Cylinders  |  EngineSize  |  Horsepower  |  RPM   |  Rev.per.mile  |  Man.trans.avail  |  Fuel.tank.capacity  |  Passengers  |  Length  |  Wheelbase  |  Width  |  Turn.circle  |  Rear.seat.room  |  Luggage.room  |  Weight  |  Origin   |           Make            |  MPG(Highway/City)  |\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     9  |      Cadillac  |       DeVille  |    Large  |    33.0000  |  34.7000  |    36.3000  |        16  |           25  |         Driver only  |       Front  |          8  |      4.9000  |         200  |  4100  |          1510  |               No  |             18.0000  |           6  |     206  |        114  |     73  |           43  |              35  |            18  |    3620  |      USA  |         Cadillac DeVille  |             1.5625  |\n    10  |      Cadillac  |       Seville  |  Midsize  |    37.5000  |  40.1000  |    42.7000  |        16  |           25  |  Driver & Passenger  |       Front  |          8  |      4.6000  |         295  |  6000  |          1985  |               No  |             20.0000  |           5  |     204  |        111  |     74  |           44  |              31  |            14  |    3935  |      USA  |         Cadillac Seville  |             1.5625  |\n    70  |    Oldsmobile  |  Eighty-Eight  |    Large  |    19.5000  |  20.7000  |    21.9000  |        19  |           28  |         Driver only  |       Front  |          6  |      3.8000  |         170  |  4800  |          1570  |               No  |             18.0000  |           6  |     201  |        111  |     74  |           42  |            31.5  |            17  |    3470  |      USA  |  Oldsmobile Eighty-Eight  |         1.47368421  |\n    74  |       Pontiac  |      Firebird  |   Sporty  |    14.0000  |  17.7000  |    21.4000  |        19  |           28  |  Driver & Passenger  |        Rear  |          6  |      3.4000  |         160  |  4600  |          1805  |              Yes  |             15.5000  |           4  |     196  |        101  |     75  |           43  |              25  |            13  |    3240  |      USA  |         Pontiac Firebird  |         1.47368421  |\n     6  |         Buick  |       LeSabre  |    Large  |    19.9000  |  20.8000  |    21.7000  |        19  |           28  |         Driver only  |       Front  |          6  |      3.8000  |         170  |  4800  |          1570  |               No  |             18.0000  |           6  |     200  |        111  |     74  |           42  |            30.5  |            17  |    3470  |      USA  |            Buick LeSabre  |         1.47368421  |\n    13  |     Chevrolet  |        Camaro  |   Sporty  |    13.4000  |  15.1000  |    16.8000  |        19  |           28  |  Driver & Passenger  |        Rear  |          6  |      3.4000  |         160  |  4600  |          1805  |              Yes  |             15.5000  |           4  |     193  |        101  |     74  |           43  |              25  |            13  |    3240  |      USA  |         Chevrolet Camaro  |         1.47368421  |\n    76  |       Pontiac  |    Bonneville  |    Large  |    19.4000  |  24.4000  |    29.4000  |        19  |           28  |  Driver & Passenger  |       Front  |          6  |      3.8000  |         170  |  4800  |          1565  |               No  |             18.0000  |           6  |     177  |        111  |     74  |           43  |            30.5  |            18  |    3495  |      USA  |       Pontiac Bonneville  |         1.47368421  |\n    56  |         Mazda  |          RX-7  |   Sporty  |    32.5000  |  32.5000  |    32.5000  |        17  |           25  |         Driver only  |        Rear  |     rotary  |      1.3000  |         255  |  6500  |          2325  |              Yes  |             20.0000  |           2  |     169  |         96  |     69  |           37  |              NA  |            NA  |    2895  |  non-USA  |               Mazda RX-7  |         1.47058824  |\n    18  |     Chevrolet  |      Corvette  |   Sporty  |    34.6000  |  38.0000  |    41.5000  |        17  |           25  |         Driver only  |        Rear  |          8  |      5.7000  |         300  |  5000  |          1450  |              Yes  |             20.0000  |           2  |     179  |         96  |     74  |           43  |              NA  |            NA  |    3380  |      USA  |       Chevrolet Corvette  |         1.47058824  |\n    51  |       Lincoln  |      Town_Car  |    Large  |    34.4000  |  36.1000  |    37.8000  |        18  |           26  |  Driver & Passenger  |        Rear  |          8  |      4.6000  |         210  |  4600  |          1840  |               No  |             20.0000  |           6  |     219  |        117  |     77  |           45  |            31.5  |            22  |    4055  |      USA  |         Lincoln Town_Car  |         1.44444444  |",
            "title": "A Simple Example"
        },
        {
            "location": "/#a-regression-example",
            "text": "The Morpheus API includes a regression interface in order to fit data to a linear model using either  OLS ,  WLS  or  GLS . The code below uses the same car dataset introduced in the previous example, \nand regresses  Horsepower  on  EngineSize . The code example prints the model results to standard out, which is shown below, \nand then creates a scatter chart with the regression line clearly displayed.   //Load the data\nDataFrame<Integer,String> data = DataFrame.read().csv(options -> {\n    options.setResource(\"http://zavtech.com/data/samples/cars93.csv\");\n    options.setExcludeColumnIndexes(0);\n});\n\n//Run OLS regression and plot \nString regressand = \"Horsepower\";\nString regressor = \"EngineSize\";\ndata.regress().ols(regressand, regressor, true, model -> {\n    System.out.println(model);\n    DataFrame<Integer,String> xy = data.cols().select(regressand, regressor);\n    Chart.create().withScatterPlot(xy, false, regressor, chart -> {\n        chart.title().withText(regressand + \" regressed on \" + regressor);\n        chart.subtitle().withText(\"Single Variable Linear Regression\");\n        chart.plot().style(regressand).withColor(Color.RED).withPointsVisible(true);\n        chart.plot().trend(regressand).withColor(Color.BLACK);\n        chart.plot().axes().domain().label().withText(regressor);\n        chart.plot().axes().domain().format().withPattern(\"0.00;-0.00\");\n        chart.plot().axes().range(0).label().withText(regressand);\n        chart.plot().axes().range(0).format().withPattern(\"0;-0\");\n        chart.show();\n    });\n    return Optional.empty();\n});  \n==============================================================================================\n                                   Linear Regression Results                                                            \n==============================================================================================\nModel:                                   OLS    R-Squared:                            0.5360\nObservations:                             93    R-Squared(adjusted):                  0.5309\nDF Model:                                  1    F-Statistic:                        105.1204\nDF Residuals:                             91    F-Statistic(Prob):                  1.11E-16\nStandard Error:                      35.8717    Runtime(millis)                           52\nDurbin-Watson:                        1.9591                                                \n==============================================================================================\n   Index     |  PARAMETER  |  STD_ERROR  |  T_STAT   |   P_VALUE   |  CI_LOWER  |  CI_UPPER  |\n----------------------------------------------------------------------------------------------\n  Intercept  |    45.2195  |    10.3119  |   4.3852  |   3.107E-5  |    24.736  |   65.7029  |\n EngineSize  |    36.9633  |     3.6052  |  10.2528  |  7.573E-17  |    29.802  |   44.1245  |\n==============================================================================================",
            "title": "A Regression Example"
        },
        {
            "location": "/#uk-house-price-trends",
            "text": "It is possible to access all UK residential  real-estate transaction records \nfrom 1995 through to current day via the  UK Government Open Data  initiative. The data is presented in CSV \nformat, and contains numerous  columns , including such information as the \ntransaction date, price paid, fully qualified address (including postal code), property type, lease type and so on.  Let us begin by writing a function to load these CSV files from Amazon S3 buckets, and since they are stored one file per year,\nwe provide a parameterized function accordingly. Given the requirements of our analysis, there is no need to load all the columns in the \nfile, so below we only choose to read columns at index 1, 2, 4, and 11. In addition, since the files do not include a header, we \nre-name columns to something more meaningful to make subsequent access a little clearer.   /**\n * Loads UK house price from the Land Registry stored in an Amazon S3 bucket\n * Note the data does not have a header, so columns will be named Column-0, Column-1 etc...\n * @param year      the year for which to load prices\n * @return          the resulting DataFrame, with some columns renamed\n */\nprivate DataFrame<Integer,String> loadHousePrices(Year year) {\n    String resource = \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-%s.csv\";\n    return DataFrame.read().csv(options -> {\n        options.setResource(String.format(resource, year.getValue()));\n        options.setHeader(false);\n        options.setCharset(StandardCharsets.UTF_8);\n        options.setIncludeColumnIndexes(1, 2, 4, 11);\n        options.getFormats().setParser(\"TransactDate\", Parser.ofLocalDate(\"yyyy-MM-dd HH:mm\"));\n        options.setColumnNameMapping((colName, colOrdinal) -> {\n            switch (colOrdinal) {\n                case 0:     return \"PricePaid\";\n                case 1:     return \"TransactDate\";\n                case 2:     return \"PropertyType\";\n                case 3:     return \"City\";\n                default:    return colName;\n            }\n        });\n    });\n}  Below we use this data in order to compute the median nominal price (not inflation adjusted) of an  apartment  for each year between \n1995 through 2014 for a subset of the largest cities in the UK. There are about 20 million records in the unfiltered dataset between \n1993 and 2014, and while it takes a fairly long time to load and parse (approximately 3.5GB of data), Morpheus executes the analytical \nportion of the code in about 5 seconds (not including load time) on a standard Apple Macbook Pro purchased in late 2013. Note how we use \nparallel processing to load and process the data by calling  results.rows().keys().parallel() .    //Create a data frame to capture the median prices of Apartments in the UK'a largest cities\nDataFrame<Year,String> results = DataFrame.ofDoubles(\n    Range.of(1995, 2015).map(Year::of),\n    Array.of(\"LONDON\", \"BIRMINGHAM\", \"SHEFFIELD\", \"LEEDS\", \"LIVERPOOL\", \"MANCHESTER\")\n);\n\n//Process yearly data in parallel to leverage all CPU cores\nresults.rows().keys().parallel().forEach(year -> {\n    System.out.printf(\"Loading UK house prices for %s...\\n\", year);\n    DataFrame<Integer,String> prices = loadHousePrices(year);\n    prices.rows().select(row -> {\n        //Filter rows to include only apartments in the relevant cities\n        final String propType = row.getValue(\"PropertyType\");\n        final String city = row.getValue(\"City\");\n        final String cityUpperCase = city != null ? city.toUpperCase() : null;\n        return propType != null && propType.equals(\"F\") && results.cols().contains(cityUpperCase);\n    }).rows().groupBy(\"City\").forEach(0, (groupKey, group) -> {\n        //Group row filtered frame so we can compute median prices in selected cities\n        final String city = groupKey.item(0);\n        final double priceStat = group.colAt(\"PricePaid\").stats().median();\n        results.data().setDouble(year, city, priceStat);\n    });\n});\n\n//Map row keys to LocalDates, and map values to be percentage changes from start date\nfinal DataFrame<LocalDate,String> plotFrame = results.mapToDoubles(v -> {\n    final double firstValue = v.col().getDouble(0);\n    final double currentValue = v.getDouble();\n    return (currentValue / firstValue - 1d) * 100d;\n}).rows().mapKeys(row -> {\n    final Year year = row.key();\n    return LocalDate.of(year.getValue(), 12, 31);\n});\n\n//Create a plot, and display it\nChart.create().withLinePlot(plotFrame, chart -> {\n    chart.title().withText(\"Median Nominal House Price Changes\");\n    chart.title().withFont(new Font(\"Arial\", Font.BOLD, 14));\n    chart.subtitle().withText(\"Date Range: 1995 - 2014\");\n    chart.plot().axes().domain().label().withText(\"Year\");\n    chart.plot().axes().range(0).label().withText(\"Percent Change from 1995\");\n    chart.plot().axes().range(0).format().withPattern(\"0.##'%';-0.##'%'\");\n    chart.plot().style(\"LONDON\").withColor(Color.BLACK);\n    chart.legend().on().bottom();\n    chart.show();\n});  The percent change in nominal median prices for  apartments  in the subset of chosen cities is shown in the plot below. It \nshows that London did not suffer any nominal house price decline as a result of the Global Financial Crisis (GFC), however not \nall cities in the UK proved as resilient. What is slightly surprising is that some of the less affluent northern cities saw a \nhigher rate of appreciation in the 2003 to 2006 period compared to London. One thing to note is that while London did not see \nany nominal price reduction, there was certainly a fairly severe correction in terms of EUR and USD since Pound Sterling \ndepreciated heavily against these currencies during the GFC.",
            "title": "UK House Price Trends"
        },
        {
            "location": "/#visualization",
            "text": "Visualizing data in Morpheus  DataFrames  is made easy via a  simple chart abstraction API  with adapters supporting both  JFreeChart  as well as  Google Charts  (with others\nto follow by popular demand). This design makes it possible to generate interactive  Java Swing  \ncharts as well as HTML5 browser based charts via the same programmatic interface. For more details on how to use this API, \nsee the section on visualization  here , and the code  here .",
            "title": "Visualization"
        },
        {
            "location": "/#maven-artifacts",
            "text": "Morpheus is published to Maven Central so it can be easily added as a dependency in your build tool of choice. The codebase is currently\ndivided into 5 repositories to allow each module to be evolved independently. The core module, which is aptly named  morpheus-core ,\nis the foundational library on which all other modules depend. The various Maven artifacts are as follows:   Morpheus Core  The  foundational  library that contains Morpheus Arrays, DataFrames and other key interfaces & implementations.  <dependency>\n    <groupId>com.zavtech</groupId>\n    <artifactId>morpheus-core</artifactId>\n    <version>${VERSION}</version>\n</dependency>  Morpheus Visualization  The  visualization  components to display  DataFrames  in charts and tables.  <dependency>\n    <groupId>com.zavtech</groupId>\n    <artifactId>morpheus-viz</artifactId>\n    <version>${VERSION}</version>\n</dependency>  Morpheus Quandl  The  adapter  to load data from  Quandl  <dependency>\n    <groupId>com.zavtech</groupId>\n    <artifactId>morpheus-quandl</artifactId>\n    <version>${VERSION}</version>\n</dependency>  Morpheus Google  The  adapter  to load data from  Google Finance  <dependency>\n    <groupId>com.zavtech</groupId>\n    <artifactId>morpheus-google</artifactId>\n    <version>${VERSION}</version>\n</dependency>  Morpheus Yahoo  The  adapter  to load data from  Yahoo Finance  <dependency>\n    <groupId>com.zavtech</groupId>\n    <artifactId>morpheus-yahoo</artifactId>\n    <version>${VERSION}</version>\n</dependency>",
            "title": "Maven Artifacts"
        },
        {
            "location": "/#qa-forum",
            "text": "A Questions & Answers forum has been setup using Google Groups and is accessible  here",
            "title": "Q&amp;A Forum"
        },
        {
            "location": "/#javadocs",
            "text": "Morpheus Javadocs can be accessed online  here .",
            "title": "Javadocs"
        },
        {
            "location": "/#build-status",
            "text": "A Continuous Integration build server can be accessed  here , which builds code after each merge.",
            "title": "Build Status"
        },
        {
            "location": "/#license",
            "text": "Morpheus is released under the  Apache Software Foundation License Version 2 .",
            "title": "License"
        },
        {
            "location": "/array/overview/",
            "text": "Introduction\n\n\nIn order to scale large datasets on the \nJava Virtual Machine\n \n(JVM) it is necessary to limit large arrays to primitive types, as these are far more efficient from a \nmemory allocation & de-allocation perspective. The reason for this is that primitive arrays are represented \nas a single Object and a \ncontiguous block of memory\n. Object arrays on the other hand not only incur the \nmemory overhead of each object header in the array, but also impose a significant burden on the Garbage \nCollector. The section on \nperformance\n provides some hard numbers to demonstrate the comparative \ncosts of primitive arrays and their boxed counterparts. Future versions of Java which will likely \nintroduce support for value types as described \nhere\n,\nand improved array support as described \nhere\n, \nmay mitigate some of these concerns.\n\n\nIn order to address this circumstance, one of the fundamental building blocks of the Morpheus library \nis the \nArray\n interface, of which there exist many different implementations optimized to store various \ndata types. Where possible, Morpheus \nArrays\n map object types such as \nLocalDate\n to an appropriate\nprimitive value, namely a long in this case. In addition to this, support for dense, sparse and memory \nmapped (off heap) backing stores is included to enable flexible memory allocations for different \ncircumstances. Adding support for additional \nArray\n types is also fairly straightforward, and is \ndescribed in a later section.\n\n\nPrimitive collections libraries such as \nTrove\n and \nGoldman Sachs Collections\n \nprovide very effective high performance data structures, and the Morpheus \nArray\n interface is not intended \nto compete with these libraries. In fact, sparse Morpheus Array implementations leverage the Trove library \nunder the covers. A feature of Trove however is that each typed collection is represented by its own interface, \nsuch as \nTIntList\n for primitive integers and \nTDoubleList\n for primitive doubles, and they do not share a \ncommon interface. This makes it inconvenient to build generic APIs that can operate on multiple types of \nprimitive collections, without creating lots of overloaded methods.\n\n\nThe Morpheus Array class is somewhat similar in design to a \njava.sql.ResultSet\n in that there are type \nspecific read/write methods supporting primitives, as well as a generic object version that will box a value \nif its internal value is a primitive.  It is easy to interrogate an \nArray\n instance for its data type, and \ntherefore use the appropriate accessors or mutators that will avoid boxing of primitive types where possible.\n\n\nThe following sections describe how to use Morpheus \nArrays\n, and demonstrate some of the less obvious\nfeatures of the API.\n\n\nConstruction\n\n\nThere are a number of ways of creating Morpheus \nArray\n instances, but the most general way is using\nthe \nof\n method as illustrated below. In this example, we create a dense, sparse and memory mapped array\nby specifying the type, length and default value. To create a sparse \nArray\n, we simply provide a \nload\nfactor < 1f\n to indicate the array is not likely to be fully populated. Below, we signal that we expect\nthe sparse array to be half populated by declaring a load factor of 0.5f.\n\n\n\n\n\n//Create a dense array of double precision values with default value of NaN\nArray<Double> denseArray = Array.of(Double.class, 1000, Double.NaN);\n//Create a sparse array which we expect to be only half populated, default value = 0\nArray<Double> sparseArray = Array.of(Double.class, 1000, 0d, 0.5f);\n//Created a memory mapped array of double values using an anonymous file\nArray<Double> mappedArray1 = Array.mmap(Double.class, 1000, Double.NaN);\n//Created a memory mapped array of double values using a user specified file\nArray<Double> mappedArray2 = Array.mmap(Double.class, 1000, Double.NaN, \"test.dat\");\n\n//Assert that each array is of the type we expect\nAssert.assertTrue(denseArray.type() == Double.class);\nAssert.assertTrue(sparseArray.type() == Double.class);\nAssert.assertTrue(mappedArray1.type() == Double.class);\nAssert.assertTrue(mappedArray2.type() == Double.class);\n\n//Assert that each array is of the style we expect\nAssert.assertTrue(denseArray.style() == ArrayStyle.DENSE);\nAssert.assertTrue(sparseArray.style() == ArrayStyle.SPARSE);\nAssert.assertTrue(mappedArray1.style() == ArrayStyle.MAPPED);\nAssert.assertTrue(mappedArray2.style() == ArrayStyle.MAPPED);\n\n//Confirm all elements are initialized as expected for each Array\nIntStream.range(0, 1000).forEach(i -> {\n    Assert.assertTrue(Double.isNaN(denseArray.getDouble(i)));\n    Assert.assertTrue(sparseArray.getDouble(i) == 0d);\n    Assert.assertTrue(Double.isNaN(mappedArray1.getDouble(i)));\n    Assert.assertTrue(Double.isNaN(mappedArray2.getDouble(i)));\n});\n\n\n\n\nThere are also convenient methods for creating a \ndense\n \nArray\n given the values directly.\n\n\n\n\n\nArray<Boolean> booleans = Array.of(true, false, true, false, true, true);\nArray<Integer> integers = Array.of(0, 1, 2, 3, 4, 5);\nArray<Long> longs = Array.of(0L, 1L, 2L, 3L, 4L);\nArray<Double> doubles = Array.of(0d, 1d, 2d, 3d, 4d, 5d);\nArray<LocalDate> dates = Array.of(LocalDate.now(), LocalDate.now().plusDays(1));\n\n\n\n\nTo create a half populated sparse \nArray\n where even indices are non-zero, one could do something as follows:\n\n\n\n\n\nArray<Double> sparseArray = Array.of(Double.class, 1000, 0d, 0.5f).applyDoubles(v -> {\n    return v.index() % 2 == 0 ? Math.random() : 0d;\n});\n\n\n\n\nAccess\n\n\nGiven that a Morpheus \nArray\n is represented by an interface, the elements of the array need to be accessed \nvia methods, not via a [] operator. There exist getter and setter methods for \nboolean\n, \nint\n, \nlong\n, \n\ndouble\n and a generic \nobject\n type. Up casting is allowed in the sense that you can call \ngetDouble()\n on \n\nArray<Integer>\n and the internal \nint\n value will be cast to a double, however the reverse will result in an \n\nArrayException\n. That is, automatic down casting which can result in the loss of precision is now allowed.\n\n\n\n\n\n//Create a dense array of doubles\nArray<Double> array = Array.of(0d, 1d, 2d, 3d, 4d, 5d);\n//Set first element using a primitive\narray.setDouble(0, 22d);\n//Set second element using a boxed value\narray.setValue(1, 33d);\n//Read first element as primitive\nAssert.assertEquals(array.getDouble(0), 22d);\n//Read second element as generic boxed value\nAssert.assertEquals(array.getValue(1), new Double(33d));\n\n\n\n\nIteration\n\n\nThe Morpheus \nArray\n interface extends \nIterable\n and therefore iteration can be done via the \nforEach()\n \nmethod. For very large arrays that are backed by primitive values, this may not always be optimal as it \nresults in boxing each value. To that end, the \nArray\n interface exposes type specific \nforEachXXX()\n \nmethods to allow fast iteration without any boxing cost. Consider the example below\n\n\n\n\n\n//Create dense array of 20K random doubles\nArray<Double> array = Array.of(Double.class, 20000, Double.NaN).applyDoubles(v -> Math.random());\n//Iterate values by boxing doubles\narray.forEach(value -> Assert.assertTrue(value > 0d));\n//Iterate values and avoid boxing\narray.forEachDouble(v -> Assert.assertTrue(v > 0d));\n\n\n\n\nThis can also be performed using \nparallel processing\n which can provide a significant performance boost on \nmulti-core processor architectures that are mostly the norm these days. In fact, many functions on a Morpheus \n\nArray\n are parallel aware, and can result in significant boosts in performance as illustrated in the section \non \nperformance\n. The Fork & Join framework is used internally as a divide and conquer algorithm.\n\n\n\n\n\n//Parallel iterate values by boxing doubles\narray.parallel().forEach(value -> Assert.assertTrue(value > 0d));\n//Parallel iterate values and avoid boxing\narray.parallel().forEachDouble(v -> Assert.assertTrue(v > 0d));\n\n\n\n\nWhen iterating over an \nArray\n, it is sometimes not only useful to have access to the \nvalue\n but also the \n\nindex\n it is associated with. To facilitate this, the \nArray\n interface exposes a \nforEachValue()\n method\nwhich takes a consumer of \nArrayValue<?>\n objects, which itself declares various type specific primitive \naccessors, and also an \nindex()\n method that yields the current ordinal for the iteration. Consider\nthe example below where we iterate over all values printing only those values at index 0, 1000, 2000 and\nso on.\n\n\n\n\n\n//Print values at index 0, 1000, 2000 etc...\narray.forEachValue(v -> {\n    if (v.index() % 1000 == 0) {\n        System.out.printf(\"\\nValue = %s at index %s\", v.getDouble(), v.index());\n    }\n});\n\n\n\n\nAn important feature of the above iteration method is that the \nConsumer<ArrayValue<?>\n always \nreceives the \nsame instance\n of the \nArrayValue\n object, which between iterations is simply pointing at a different element \nin the underlying \nArray\n. For that reason, one must always treat \nArrayValue<?>\n objects as \nephemeral\n and \nonly valid for the life of the method they are passed to. That is, do not attempt to collect \nArrayValue<?>\n \ninstances in a collection, as they will in fact all be the same instance! Parallel iteration is also supported \nby the \nforEachValue()\n method, but in this case there will be one instance of an \nArrayValue<?>\n per thread to \navoid any collisions.\n\n\n\n\n\n//Parallel Print values at index 0, 1000, 2000 etc...\narray.parallel().forEachValue(v -> {\n    if (v.index() % 1000 == 0) {\n        System.out.printf(\"\\nv = %s at %s\", v.getDouble(), v.index());\n    }\n});\n\n\n\n\nUpdating\n\n\nModifying individual elements of a Morpheus \nArray\n via type specific setters has already been discussed.\nOften however it is useful to perform bulk updates on an \nArray\n, and to this end, various \napplyXXX()\n \nmethods exist to enable this to be done while avoiding boxing once again. Consider the example below\nwhere we create an \nArray<Double>\n of 1 million random doubles, and then proceed to cap the values at 50 using\n\napplyDoubles()\n. All the \napplyXXX()\n methods take functions which accept \nArrayValue<?>\n objects, \nwhich again should be treated as ephemeral and only valid for the life of the function they are passed\nto.\n\n\n\n\n\n//Create dense array of 1 million doubles\nArray<Double> array = Array.of(Double.class, 1000000, Double.NaN);\n//Update with random values\narray.applyDoubles(v -> Math.random() * 100d);\n//Cap Values to be no larger than 50\narray.applyDoubles(v -> Math.min(50d, v.getDouble()));\n//Assert values are capped\narray.forEachValue(v -> Assert.assertTrue(v.getDouble() <= 50d));\n\n\n\n\nThis can obviously be done in parallel since the order of operations in this case does not matter.\n\n\n\n\n\n//Parallel update with random values\narray.parallel().applyDoubles(v -> Math.random() * 100d);\n//Parallel Cap Values to be no larger than 50\narray.parallel().applyDoubles(v -> Math.min(50d, v.getDouble()));\n//Assert values are capped\narray.parallel().forEachValue(v -> Assert.assertTrue(v.getDouble() <= 50d));\n\n\n\n\nMapping\n\n\nWhile the \napplyXXX()\n methods discussed in the previous section are used to modify the contents of\nan existing \nArray\n, it is also useful to be able to map an array to some other representation. This\nis essentially the same as mapping with Java 8 Streams, and is a fundamental feature of functional\nprogramming. As with the \napplyXXX()\n methods, type specific \nmapToXXX()\n methods exist to enable\nmapping to various primitive types without any need for boxing.\n\n\n\n\n\n//Initial random generator\nRandom random = new Random();\n//Create Array of LocalDates with random offsets from today\nArray<LocalDate> dates = Array.of(LocalDate.class, 100, null).applyValues(v -> {\n    return LocalDate.now().minusDays(random.nextInt(1000));\n});\n//Map dates to date times with time set to 12:15\nArray<LocalDateTime> dateTimes = dates.map(v -> v.getValue().atTime(LocalTime.of(12, 15)));\n//Map dates to day count offsets from today\nArray<Long> dayCounts = dates.mapToLongs(v -> ChronoUnit.DAYS.between(v.getValue(), LocalDate.now()));\n//Check day counts resolve back to original dates\ndayCounts.forEachValue(v -> {\n    long dayCount = v.getLong();\n    LocalDate expected = dates.getValue(v.index());\n    LocalDate actual = LocalDate.now().minusDays(dayCount);\n    Assert.assertEquals(actual, expected);\n});\n\n\n\n\nThe mapping functions are also \nparallel\n aware.\n\n\n\n\n\n//Parallel map dates to day count offsets from today\nArray<Long> dayCounts = dates.parallel().mapToLongs(v -> {\n    LocalDate now = LocalDate.now();\n    LocalDate value = v.getValue();\n    return ChronoUnit.DAYS.between(value, now);\n});\n\n\n\n\nStatistics\n\n\nThe \nArray\n interface exposes a \nstats()\n method which makes it easy to compute summary statistics \non arrays that contain numerical data. Attempting to compute stats on non-numerical arrays will\nresult in an \nArrayException\n. The table below enumerates the supported statistics.\n\n\n\n\n\n\n\n\nMethod\n\n\nDescription\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\ncount()\n\n\nThe number of observations, ignoring nulls\n\n\n\n\n\n\n\n\nmin()\n\n\nThe minimum value, ignoring nulls\n\n\nDetails\n\n\n\n\n\n\nmax()\n\n\nThe maximim value, ignorning nulls\n\n\nDetails\n\n\n\n\n\n\nmean()\n\n\nThe first moment, or the arithmetic mean or average, ignoring nulls\n\n\nDetails\n\n\n\n\n\n\nvariance()\n\n\nThe un-biased variance or second moment, a measure of dispersion\n\n\nDetails\n\n\n\n\n\n\nstdDev()\n\n\nThe un-biased standard deviation, a measure of dispersion\n\n\nDetails\n\n\n\n\n\n\nskew()\n\n\nThe third moment, or skewness, a measure of the asymmetry in the distribution\n\n\nDetails\n\n\n\n\n\n\nkurtosis()\n\n\nThe fourth moment, or Kurtosis, a measure of the \"tailedness\" of the probability distribution\n\n\nDetails\n\n\n\n\n\n\nmedian()\n\n\nThe value separating the higher half of the data, or 50th percentile\n\n\nDetails\n\n\n\n\n\n\nmad()\n\n\nThe Mean Absolute Deviation from a central point, another measure of dispersion\n\n\nDetails\n\n\n\n\n\n\nsem()\n\n\nThe standard error of the mean\n\n\nDetails\n\n\n\n\n\n\ngeoMean()\n\n\nThe geometric mean, another measure of central tendency\n\n\nDetails\n\n\n\n\n\n\nsum()\n\n\nThe summation of all values, ignoring nulls\n\n\nDetails\n\n\n\n\n\n\nsumOfSquares()\n\n\nThe sum, over non-null observations, of the squared differences from the mean\n\n\nDetails\n\n\n\n\n\n\nautocorr(int lag)\n\n\nThe autocorrelation, which is the correlation of a signal with a delayed copy of itself\n\n\nDetails\n\n\n\n\n\n\npercentile(double nth)\n\n\nThe percentile value below which n% of values fall, ignoring nulls\n\n\nDetails\n\n\n\n\n\n\n\n\nBelow we initialise an \nArray\n of double precision values with elements equal to their index and the\nproceed to compute summary statistics.\n\n\n\n\n\n//Create dense array of 1 million doubles\nArray<Double> array = Array.of(Double.class, 1000, Double.NaN).applyDoubles(ArrayValue::index);\n\n//Compute stats\nAssert.assertEquals(array.stats().count(), 1000d);\nAssert.assertEquals(array.stats().min(), 0d);\nAssert.assertEquals(array.stats().max(), 999d);\nAssert.assertEquals(array.stats().mean(), 499.5d);\nAssert.assertEquals(array.stats().variance(), 83416.66666666667d);\nAssert.assertEquals(array.stats().stdDev(), 288.8194360957494d);\nAssert.assertEquals(array.stats().skew(), 0d);\nAssert.assertEquals(array.stats().kurtosis(), -1.2000000000000004d);\nAssert.assertEquals(array.stats().median(), 499.5d);\nAssert.assertEquals(array.stats().mad(), 250.00000000000003d);\nAssert.assertEquals(array.stats().sem(), 9.133272505880171d);\nAssert.assertEquals(array.stats().geoMean(), 0d);\nAssert.assertEquals(array.stats().sum(), 499500.0d);\nAssert.assertEquals(array.stats().sumSquares(), 3.328335E8);\nAssert.assertEquals(array.stats().autocorr(1), 1d);\nAssert.assertEquals(array.stats().percentile(0.5d), 499.5d);\n\n\n\n\nSearching\n\n\nThe Morpheus \nArray\n interface provides some useful functions to search for values given a user provided \npredicate. In addition, there are methods to perform binary searches to find a matching value, or to find \nthe next smallest / largest value given a user provided value that may not even exist in the array. \n\n\nThe first example below demonstrates how to find the first and last values in the array given some predicate.\nNote that the predicate again accepts an \nArrayValue<T>\n instance, which allows the index to be accessed as\nwell as the value in a way that can avoid boxing.\n\n\n\n\n\n//Create random with seed\nRandom random = new Random(3);\n//Create dense array double precision values\nArray<Double> array = Array.of(Double.class, 1000, Double.NaN).applyDoubles(v -> random.nextDouble() * 55d);\n\n//Find first value above 50\nAssert.assertTrue(array.first(v -> v.getDouble() > 50d).isPresent());\narray.first(v -> v.getDouble() > 50d).ifPresent(v -> {\n    Assert.assertEquals(v.getDouble(), 51.997892373318116d, 0.000001);\n    Assert.assertEquals(v.index(), 9);\n});\n\n//Find last value above 50\nAssert.assertTrue(array.last(v -> v.getDouble() > 50d).isPresent());\narray.last(v -> v.getDouble() > 50d).ifPresent(v -> {\n    Assert.assertEquals(v.getDouble(), 51.864302849037315d, 0.000001);\n    Assert.assertEquals(v.index(), 992);\n});\n\n\n\n\nThe next two examples are predicated on binary search, and for that to work the \nArray\n needs to be sorted, \nwhich can be done by calling one of the sort methods as shown below.\n\n\n\n\n\n//Sort the array for binary search\nArray<Double> sorted = array.sort(true);\n\n\n\n\nKnowing that the array is sorted, we can perform a binary search on a subset of the \nArray\n or the entire\n\nArray\n by providing the start and end index for the search space. The example below picks a number of\nindexes, selects the value for those indexes, and then proceeds to search for those values and assert\nthat we get a match at the expected location.\n\n\n\n\n\n//Perform binary search over entire array for various chosen values\nIntStream.of(27, 45, 145, 378, 945).forEach(index -> {\n    double value = sorted.getDouble(index);\n    int actual = sorted.binarySearch(0, 1000, value);\n    Assert.assertEquals(actual, index);\n});\n\n\n\n\nAnother useful search feature that leverages binary search and therefore performs well, is to find the\nclosest value before or after some value, even if that value does not exist in the \nArray\n. The examples\nbelow illustrate how to find the next and prior value. The result of this search is present as an \nArrayValue\n\nwrapped in an \nOptional\n, so the value and index are easily accessible.\n\n\n\n\n\n//Find next value given a value that does not actual exist in the array\nIntStream.of(27, 45, 145, 378, 945).forEach(index -> {\n    double value1 = sorted.getDouble(index);\n    double value2 = sorted.getDouble(index+1);\n    double mean = (value1 + value2) / 2d;\n    //Find next value given a value that does not exist in the array\n    Optional<ArrayValue<Double>> nextValue = sorted.next(mean);\n    Assert.assertTrue(nextValue.isPresent());\n    nextValue.ifPresent(v -> {\n        Assert.assertEquals(v.getDouble(), value2);\n        Assert.assertEquals(v.index(), index + 1);\n    });\n});\n\n//Find prior value given a value that does not actual exist in the array\nIntStream.of(27, 45, 145, 378, 945).forEach(index -> {\n    double value1 = sorted.getDouble(index);\n    double value2 = sorted.getDouble(index+1);\n    double mean = (value1 + value2) / 2d;\n    //Find prior value given a value that does not exist in the array\n    Optional<ArrayValue<Double>> priorValue = sorted.previous(mean);\n    Assert.assertTrue(priorValue.isPresent());\n    priorValue.ifPresent(v -> {\n        Assert.assertEquals(v.getDouble(), value1);\n        Assert.assertEquals(v.index(), index);\n    });\n});\n\n\n\n\nSorting\n\n\nThe \nArray\n interface provides various convenience methods to sort values in either ascending or descending\norder or in some bespoke order according to some user defined \nComparator\n. To demonstrate, let us first\ncreate an \nArray\n initialized with random double precision values, including both positive and negative values\nas follows:\n\n\n\n\n\n//Create random generator with seed\nRandom random = new Random(22);\n//Create dense array double precision values\nArray<Double> array = Array.of(Double.class, 1000, Double.NaN);\n//Initialise with random values\narray.applyDoubles(v -> {\n    final double sign = random.nextDouble() > 0.5d ? 1d : -1d;\n    return random.nextDouble() * 10 * sign;\n});\n\n\n\n\nSorting these values in ascending or descending order is as trivial:\n\n\n\n\n\n//Sort ascending\narray.sort(true);\n//Sort descending\narray.sort(false);\n\n\n\n\nIt is also possible to only sort a subset of the array in ascending or descending order:\n\n\n\n\n\n//Sort values between indexes 100 and 200 in ascending order\narray.sort(100, 200, true);\n//Sort values between indexes 100 and 200 in ascending order\narray.sort(100, 200, false);\n\n\n\n\nThe most general sort method allows the user to specify the range of values to operate on, and also\nprovide a \nComparator\n that accepts \nArrayValue\n instances. This enables the user to write a \nComparator\n\nimplementation that avoids boxing values, and also one that has access to the data values as well as their\nindex in the array. The code below shows how to sort our example \nArray\n by the \nabsolute value\n of the\nelements in the array, but only including items between index 100 (inclusive) and 200 (exclusive). We\nthen run a check to see the values are sorted as expected. Like with other functions that consume \nArrayValue\n\nobjects, they should be treated as ephemeral and only valid for the life of the method invocation.\n\n\n\n\n\n//Sort by absolute ascending value\narray.sort(100, 200, (v1, v2) -> {\n    final double d1 = Math.abs(v1.getDouble());\n    final double d2 = Math.abs(v2.getDouble());\n    return Double.compare(d1, d2);\n});\n\n//Check values in range are sorted as expected\nIntStream.range(101, 200).forEach(index -> {\n    double prior = Math.abs(array.getDouble(index-1));\n    double current = Math.abs(array.getDouble(index));\n    int compare = Double.compare(prior, current);\n    Assert.assertTrue(compare <= 0);\n});\n\n\n\n\nAll the sorting methods on a Morpheus \nArray\n support \nparallel\n execution, which can significantly \nimprove performance.\n\n\n\n\n\n//Parallel sort by absolute ascending value\narray.parallel().sort(100, 200, (v1, v2) -> {\n    final double d1 = Math.abs(v1.getDouble());\n    final double d2 = Math.abs(v2.getDouble());\n    return Double.compare(d1, d2);\n});\n\n\n\n\nCopying\n\n\nBeing able to efficiently create \ndeep copies\n of Morpheus \nArrays\n either in their entirety or only including\na subset of the elements is supported via three overloaded \ncopy()\n methods. The code examples below illustrate\nthese three cases, the first case copies the entire \nArray\n, the second creates a copy including only a range\nof values, and the third creates a copy given specific indexes.\n\n\n\n\n\n//Create random generator with seed\nRandom random = new Random(22);\n//Create dense array double precision values\nArray<Double> array = Array.of(Double.class, 1000, Double.NaN).applyDoubles(v -> random.nextDouble());\n\n//Deep copy of entire Array\nArray<Double> copy1 = array.copy();\n//Deep copy of subset of Array, start inclusive, end exclusive\nArray<Double> copy2 = array.copy(100, 200);\n//Deep copy of specific indexes\nArray<Double> copy3 = array.copy(new int[] {25, 304, 674, 485, 873});\n\n//Assert lengths as expected\nAssert.assertEquals(copy1.length(), array.length());\nAssert.assertEquals(copy2.length(), 100);\nAssert.assertEquals(copy3.length(), 5);\n\n//Asset values as expected\nIntStream.range(0, 1000).forEach(i -> Assert.assertEquals(copy1.getDouble(i), array.getDouble(i)));\nIntStream.range(0, 100).forEach(i -> Assert.assertEquals(copy2.getDouble(i), array.getDouble(i+100)));\nIntStream.of(0, 5).forEach(i -> {\n    switch (i) {\n        case 0: Assert.assertEquals(copy3.getDouble(i), array.getDouble(25));   break;\n        case 1: Assert.assertEquals(copy3.getDouble(i), array.getDouble(304));   break;\n        case 2: Assert.assertEquals(copy3.getDouble(i), array.getDouble(674));   break;\n        case 3: Assert.assertEquals(copy3.getDouble(i), array.getDouble(485));   break;\n        case 4: Assert.assertEquals(copy3.getDouble(i), array.getDouble(873));   break;\n        default:\n    }\n});\n\n\n\n\nStreams\n\n\nWhile Morpheus \nArrays\n offer many of the programmatic features available with Java 8 Streams, they by \nno means cover everything. Either way, being able to expose Morpheus \nArrays\n as Java 8 Streams will \nalways be useful for compatibility purposes with other libraries. To that end, the \nstream()\n method\nprovides access to type specific streams as shown in the code examples below.\n\n\n\n\n\n//Create Array of various types\nArray<Integer> integers = Array.of(0, 1, 2, 3, 4, 5);\nArray<Long> longs = Array.of(0L, 1L, 2L, 3L, 4L);\nArray<Double> doubles = Array.of(0d, 1d, 2d, 3d, 4d, 5d);\nArray<LocalDate> dates = Array.of(LocalDate.now(), LocalDate.now().plusDays(1));\n\n//Create Java 8 streams of these Arrays\nIntStream intStream = integers.stream().ints();\nLongStream longStream = longs.stream().longs();\nDoubleStream doubleStream = doubles.stream().doubles();\nStream<LocalDate> dateStream = dates.stream().values();\n\n\n\n\nExpanding\n\n\nJava native arrays cannot be expanded, however the Morpheus \nArray\n interface does support this. The\ninternal implementations have no choice but to re-create and re-populate internally, however this\nfeature does provide a convenient mechanism for growing \nArrays\n without having to do the heavy lifting\nyourself. In addition, given the support for dense, sparse and memory mapped backing stores, the\nexpansion behaviour differs across styles. The \nexpand()\n method takes the new length to grow the\n\nArray\n to, and the new elements will be initialized with the default value specified for the array\nupon creation.\n\n\n\n\n\n//Create array of random doubles, with defauly value of -1\nArray<Double> array = Array.of(Double.class, 10, -1d).applyDoubles(v -> Math.random());\n//Double the size of the array\narray.expand(20);\n//Confirm new length is as expected\nAssert.assertEquals(array.length(), 20);\n//Confirm new values initialized with default value\nIntStream.range(10, 20).forEach(i -> Assert.assertEquals(array.getDouble(i), -1d));\n\n\n\n\nFiltering\n\n\nCreating a filtered \nArray\n given some user defined predicate is a common programmatic requirement, and to\nsupport this, a \nfilter()\n method is provided that takes a \nPredicate\n which accepts \nArrayValue\n instances.\nThis design again allows boxing of primitive values to be avoided, and also makes the index of the value\naccessible should that factor into the filtering logic. The code below creates an \nArray\n of double\nprecision random values, and creates a filter which only includes values > 5. \n\n\n\n\n\n//Create random generator with seed\nRandom random = new Random(2);\n//Create array of random doubles, with default value NaN\nArray<Double> array = Array.of(Double.class, 1000, Double.NaN).applyDoubles(v -> random.nextDouble() * 10d);\n//Filter to include all values > 5\nArray<Double> filter = array.filter(v -> v.getDouble() > 5d);\n//Assert length as expected\nAssert.assertEquals(filter.length(), 486);\n//Assert all value are > 5\nfilter.forEachValue(v -> Assert.assertTrue(v.getDouble() > 5d));\n\n\n\n\nRead-Only\n\n\nNative Java arrays are inherently mutable, however since Morpheus \nArrays\n are represented by an interface\nwe can easily create a light-weight wrapper that only supports read operations on the underlying \nArray\n. This\nis useful when one needs to expose the \nArray\n to external code but in a way that guarantees that code\ncannot modify the array contents in any way. The \nreadOnly()\n method call shown below generates this \nlight-weight ready-only proxy.\n\n\n\n\n\n//Create array of random doubles, with default value NaN\nArray<Double> array = Array.of(Double.class, 1000, Double.NaN).applyDoubles(v -> Math.random());\n//Create a light-weight read only wrapper\nArray<Double> readOnly = array.readOnly();\n\n\n\n\nFilling\n\n\nFilling an \nArray\n either over a range of indexes or the entire array with a specific value is supported by\ntwo overloaded \nfill()\n methods. Below we create a random \nArray\n initialized with all \nNaN\n values, and\nthen proceed to fill indexes 10 through 20 with a fixed value.\n\n\n\n\n\n//Create array of random doubles, with default value NaN\nArray<Double> array = Array.of(Double.class, 1000, Double.NaN);\n//Fill indexes 10-20 (inclusive - exclusive) with value 25\narray.fill(25d, 10, 20);\n//Check results\nIntStream.range(10, 20).forEach(i -> {\n    Assert.assertEquals(array.getDouble(i), 25d);\n});\n\n\n\n\nDistinct\n\n\nFinding the distinct elements in an array is easy enough as you can simply collect the values in a \nSet<T>\n,\nhowever this would once again come with the cost of boxing when the \nArray\n is backed by a primitive type.\nIn order to avoid this, two overloaded \ndistinct()\n methods are provided, one which returns a new \nArray\n with\nall the distinct values, and the second returns a truncated array of distinct values based on a user specified \nlimit. The code below illustrates and example of how to use these.\n\n\n\n\n\n//Create a random with seed\nRandom random = new Random(10);\n//Create Array of random LocalDates, all elements initially null\nArray<LocalDate> dates = Array.of(LocalDate.class, 1000);\n//Populate with some random dates that will likely have duplicates\ndates.applyValues(v -> LocalDate.now().plusDays(random.nextInt(20)));\n//Generate distinct Array\nArray<LocalDate> distinct1 = dates.distinct();\n//Generate distinct limiting to first 5 matches\nArray<LocalDate> distinct2 = dates.distinct(5);\n//Check expected results\nAssert.assertEquals(distinct1.length(), 20);\nAssert.assertEquals(distinct2.length(), 5);\n\n\n\n\nBounds\n\n\nComputing the upper and lower bounds of an \nArray\n can be achieved in a number of ways, as shown by the code\nbelow. Firstly the \nbounds()\n method computes the min/max in one pass, and will work for any value that is \n\nComparable\n. Alternatively, the \nmin()\n and \nmax()\n methods perform the same logic however only return\nthe lower or upper bound respectively. Below we also confirm that the \nstats()\n interface to the \nArray\n\ngenerates the same results since in this example we are using an array of double precision values. If the \n\nArray\n was non-numeric, such as \nLocalDate\n for example, then the stats interface would not work and result \nin an \nArrayException\n.\n\n\n\n\n\n//Create a random with seed\nRandom random = new Random(21);\n//Create Array of random doubles, all elements initially null\nArray<Double> array = Array.of(Double.class, 1000).applyDoubles(v -> random.nextDouble() * 100);\n//Compute upper and lower bounds in one pass\nOptional<Bounds<Double>> bounds = array.bounds();\n//Confirm we have bounds\nAssert.assertTrue(bounds.isPresent());\n//Confirm expected results\nbounds.ifPresent(b -> {\n    Assert.assertEquals(b.lower(), 0.13021930271921445);\n    Assert.assertEquals(b.upper(), 99.9557586162974);\n    Assert.assertEquals(b.lower(), array.min().get());\n    Assert.assertEquals(b.upper(), array.max().get());\n    Assert.assertEquals(b.lower(), array.stats().min());\n    Assert.assertEquals(b.upper(), array.stats().max());\n});\n\n\n\n\nNull Check\n\n\nA convenience method named \nisNull()\n exists on the \nArray\n and \nArrayValue\n interface. In the former case,\nit takes the index of the array element to check for null, in the latter no arguments are required since the\n\nArrayValue\n is already pointing at some entry. This method is convenient as it can avoid accessing the value\nto check for null, which may again incur a boxing cost. In addition, \nDouble.NaN\n is considered to be null\nso one can avoid this special null check case as shown by the example below.\n\n\n\n\n\n//Create a random with seed\nRandom random = new Random(21);\n//Create Array of random doubles, all elements initially null\nArray<Double> array = Array.of(Double.class, 1000).applyDoubles(v -> random.nextDouble() * 100);\n//Set some values to NaN\narray.fill(Double.NaN, 10, 20);\n//Set some values to null, which is the same as NaN for double precision\narray.fill(null, 20, 30);\n//Filter out NaN values using is null\nArray<Double> filtered = array.filter(v -> !v.isNull());\n//Assert length\nAssert.assertEquals(filtered.length(), array.length() - 20);\n\n\n\n\nSwapping\n\n\nSwapping elements in an \nArray\n without having to access them directly is also supported, which means that optimized \nimplementations, such as for the \nLocalDate\n type for example, can avoid boxing the internal representation (long \nepoch day values in the case of LocalDate). The \nswap()\n method is used as part of the sort algorithm, and a simple \nexample of how to use it unrelated to sorting is shown below.\n\n\n\n\n\n//Create Array of doubles\nArray<Double> array = Array.of(10d, 20d, 30d, 40d);\n//Swap values\narray.swap(0, 3);\n//Assert values swapped\nAssert.assertEquals(array.getDouble(0), 40d);\nAssert.assertEquals(array.getDouble(3), 10d);",
            "title": "Overview"
        },
        {
            "location": "/array/overview/#introduction",
            "text": "In order to scale large datasets on the  Java Virtual Machine  \n(JVM) it is necessary to limit large arrays to primitive types, as these are far more efficient from a \nmemory allocation & de-allocation perspective. The reason for this is that primitive arrays are represented \nas a single Object and a  contiguous block of memory . Object arrays on the other hand not only incur the \nmemory overhead of each object header in the array, but also impose a significant burden on the Garbage \nCollector. The section on  performance  provides some hard numbers to demonstrate the comparative \ncosts of primitive arrays and their boxed counterparts. Future versions of Java which will likely \nintroduce support for value types as described  here ,\nand improved array support as described  here , \nmay mitigate some of these concerns.  In order to address this circumstance, one of the fundamental building blocks of the Morpheus library \nis the  Array  interface, of which there exist many different implementations optimized to store various \ndata types. Where possible, Morpheus  Arrays  map object types such as  LocalDate  to an appropriate\nprimitive value, namely a long in this case. In addition to this, support for dense, sparse and memory \nmapped (off heap) backing stores is included to enable flexible memory allocations for different \ncircumstances. Adding support for additional  Array  types is also fairly straightforward, and is \ndescribed in a later section.  Primitive collections libraries such as  Trove  and  Goldman Sachs Collections  \nprovide very effective high performance data structures, and the Morpheus  Array  interface is not intended \nto compete with these libraries. In fact, sparse Morpheus Array implementations leverage the Trove library \nunder the covers. A feature of Trove however is that each typed collection is represented by its own interface, \nsuch as  TIntList  for primitive integers and  TDoubleList  for primitive doubles, and they do not share a \ncommon interface. This makes it inconvenient to build generic APIs that can operate on multiple types of \nprimitive collections, without creating lots of overloaded methods.  The Morpheus Array class is somewhat similar in design to a  java.sql.ResultSet  in that there are type \nspecific read/write methods supporting primitives, as well as a generic object version that will box a value \nif its internal value is a primitive.  It is easy to interrogate an  Array  instance for its data type, and \ntherefore use the appropriate accessors or mutators that will avoid boxing of primitive types where possible.  The following sections describe how to use Morpheus  Arrays , and demonstrate some of the less obvious\nfeatures of the API.",
            "title": "Introduction"
        },
        {
            "location": "/array/overview/#construction",
            "text": "There are a number of ways of creating Morpheus  Array  instances, but the most general way is using\nthe  of  method as illustrated below. In this example, we create a dense, sparse and memory mapped array\nby specifying the type, length and default value. To create a sparse  Array , we simply provide a  load\nfactor < 1f  to indicate the array is not likely to be fully populated. Below, we signal that we expect\nthe sparse array to be half populated by declaring a load factor of 0.5f.   //Create a dense array of double precision values with default value of NaN\nArray<Double> denseArray = Array.of(Double.class, 1000, Double.NaN);\n//Create a sparse array which we expect to be only half populated, default value = 0\nArray<Double> sparseArray = Array.of(Double.class, 1000, 0d, 0.5f);\n//Created a memory mapped array of double values using an anonymous file\nArray<Double> mappedArray1 = Array.mmap(Double.class, 1000, Double.NaN);\n//Created a memory mapped array of double values using a user specified file\nArray<Double> mappedArray2 = Array.mmap(Double.class, 1000, Double.NaN, \"test.dat\");\n\n//Assert that each array is of the type we expect\nAssert.assertTrue(denseArray.type() == Double.class);\nAssert.assertTrue(sparseArray.type() == Double.class);\nAssert.assertTrue(mappedArray1.type() == Double.class);\nAssert.assertTrue(mappedArray2.type() == Double.class);\n\n//Assert that each array is of the style we expect\nAssert.assertTrue(denseArray.style() == ArrayStyle.DENSE);\nAssert.assertTrue(sparseArray.style() == ArrayStyle.SPARSE);\nAssert.assertTrue(mappedArray1.style() == ArrayStyle.MAPPED);\nAssert.assertTrue(mappedArray2.style() == ArrayStyle.MAPPED);\n\n//Confirm all elements are initialized as expected for each Array\nIntStream.range(0, 1000).forEach(i -> {\n    Assert.assertTrue(Double.isNaN(denseArray.getDouble(i)));\n    Assert.assertTrue(sparseArray.getDouble(i) == 0d);\n    Assert.assertTrue(Double.isNaN(mappedArray1.getDouble(i)));\n    Assert.assertTrue(Double.isNaN(mappedArray2.getDouble(i)));\n});  There are also convenient methods for creating a  dense   Array  given the values directly.   Array<Boolean> booleans = Array.of(true, false, true, false, true, true);\nArray<Integer> integers = Array.of(0, 1, 2, 3, 4, 5);\nArray<Long> longs = Array.of(0L, 1L, 2L, 3L, 4L);\nArray<Double> doubles = Array.of(0d, 1d, 2d, 3d, 4d, 5d);\nArray<LocalDate> dates = Array.of(LocalDate.now(), LocalDate.now().plusDays(1));  To create a half populated sparse  Array  where even indices are non-zero, one could do something as follows:   Array<Double> sparseArray = Array.of(Double.class, 1000, 0d, 0.5f).applyDoubles(v -> {\n    return v.index() % 2 == 0 ? Math.random() : 0d;\n});",
            "title": "Construction"
        },
        {
            "location": "/array/overview/#access",
            "text": "Given that a Morpheus  Array  is represented by an interface, the elements of the array need to be accessed \nvia methods, not via a [] operator. There exist getter and setter methods for  boolean ,  int ,  long ,  double  and a generic  object  type. Up casting is allowed in the sense that you can call  getDouble()  on  Array<Integer>  and the internal  int  value will be cast to a double, however the reverse will result in an  ArrayException . That is, automatic down casting which can result in the loss of precision is now allowed.   //Create a dense array of doubles\nArray<Double> array = Array.of(0d, 1d, 2d, 3d, 4d, 5d);\n//Set first element using a primitive\narray.setDouble(0, 22d);\n//Set second element using a boxed value\narray.setValue(1, 33d);\n//Read first element as primitive\nAssert.assertEquals(array.getDouble(0), 22d);\n//Read second element as generic boxed value\nAssert.assertEquals(array.getValue(1), new Double(33d));",
            "title": "Access"
        },
        {
            "location": "/array/overview/#iteration",
            "text": "The Morpheus  Array  interface extends  Iterable  and therefore iteration can be done via the  forEach()  \nmethod. For very large arrays that are backed by primitive values, this may not always be optimal as it \nresults in boxing each value. To that end, the  Array  interface exposes type specific  forEachXXX()  \nmethods to allow fast iteration without any boxing cost. Consider the example below   //Create dense array of 20K random doubles\nArray<Double> array = Array.of(Double.class, 20000, Double.NaN).applyDoubles(v -> Math.random());\n//Iterate values by boxing doubles\narray.forEach(value -> Assert.assertTrue(value > 0d));\n//Iterate values and avoid boxing\narray.forEachDouble(v -> Assert.assertTrue(v > 0d));  This can also be performed using  parallel processing  which can provide a significant performance boost on \nmulti-core processor architectures that are mostly the norm these days. In fact, many functions on a Morpheus  Array  are parallel aware, and can result in significant boosts in performance as illustrated in the section \non  performance . The Fork & Join framework is used internally as a divide and conquer algorithm.   //Parallel iterate values by boxing doubles\narray.parallel().forEach(value -> Assert.assertTrue(value > 0d));\n//Parallel iterate values and avoid boxing\narray.parallel().forEachDouble(v -> Assert.assertTrue(v > 0d));  When iterating over an  Array , it is sometimes not only useful to have access to the  value  but also the  index  it is associated with. To facilitate this, the  Array  interface exposes a  forEachValue()  method\nwhich takes a consumer of  ArrayValue<?>  objects, which itself declares various type specific primitive \naccessors, and also an  index()  method that yields the current ordinal for the iteration. Consider\nthe example below where we iterate over all values printing only those values at index 0, 1000, 2000 and\nso on.   //Print values at index 0, 1000, 2000 etc...\narray.forEachValue(v -> {\n    if (v.index() % 1000 == 0) {\n        System.out.printf(\"\\nValue = %s at index %s\", v.getDouble(), v.index());\n    }\n});  An important feature of the above iteration method is that the  Consumer<ArrayValue<?>  always  receives the \nsame instance  of the  ArrayValue  object, which between iterations is simply pointing at a different element \nin the underlying  Array . For that reason, one must always treat  ArrayValue<?>  objects as  ephemeral  and \nonly valid for the life of the method they are passed to. That is, do not attempt to collect  ArrayValue<?>  \ninstances in a collection, as they will in fact all be the same instance! Parallel iteration is also supported \nby the  forEachValue()  method, but in this case there will be one instance of an  ArrayValue<?>  per thread to \navoid any collisions.   //Parallel Print values at index 0, 1000, 2000 etc...\narray.parallel().forEachValue(v -> {\n    if (v.index() % 1000 == 0) {\n        System.out.printf(\"\\nv = %s at %s\", v.getDouble(), v.index());\n    }\n});",
            "title": "Iteration"
        },
        {
            "location": "/array/overview/#updating",
            "text": "Modifying individual elements of a Morpheus  Array  via type specific setters has already been discussed.\nOften however it is useful to perform bulk updates on an  Array , and to this end, various  applyXXX()  \nmethods exist to enable this to be done while avoiding boxing once again. Consider the example below\nwhere we create an  Array<Double>  of 1 million random doubles, and then proceed to cap the values at 50 using applyDoubles() . All the  applyXXX()  methods take functions which accept  ArrayValue<?>  objects, \nwhich again should be treated as ephemeral and only valid for the life of the function they are passed\nto.   //Create dense array of 1 million doubles\nArray<Double> array = Array.of(Double.class, 1000000, Double.NaN);\n//Update with random values\narray.applyDoubles(v -> Math.random() * 100d);\n//Cap Values to be no larger than 50\narray.applyDoubles(v -> Math.min(50d, v.getDouble()));\n//Assert values are capped\narray.forEachValue(v -> Assert.assertTrue(v.getDouble() <= 50d));  This can obviously be done in parallel since the order of operations in this case does not matter.   //Parallel update with random values\narray.parallel().applyDoubles(v -> Math.random() * 100d);\n//Parallel Cap Values to be no larger than 50\narray.parallel().applyDoubles(v -> Math.min(50d, v.getDouble()));\n//Assert values are capped\narray.parallel().forEachValue(v -> Assert.assertTrue(v.getDouble() <= 50d));",
            "title": "Updating"
        },
        {
            "location": "/array/overview/#mapping",
            "text": "While the  applyXXX()  methods discussed in the previous section are used to modify the contents of\nan existing  Array , it is also useful to be able to map an array to some other representation. This\nis essentially the same as mapping with Java 8 Streams, and is a fundamental feature of functional\nprogramming. As with the  applyXXX()  methods, type specific  mapToXXX()  methods exist to enable\nmapping to various primitive types without any need for boxing.   //Initial random generator\nRandom random = new Random();\n//Create Array of LocalDates with random offsets from today\nArray<LocalDate> dates = Array.of(LocalDate.class, 100, null).applyValues(v -> {\n    return LocalDate.now().minusDays(random.nextInt(1000));\n});\n//Map dates to date times with time set to 12:15\nArray<LocalDateTime> dateTimes = dates.map(v -> v.getValue().atTime(LocalTime.of(12, 15)));\n//Map dates to day count offsets from today\nArray<Long> dayCounts = dates.mapToLongs(v -> ChronoUnit.DAYS.between(v.getValue(), LocalDate.now()));\n//Check day counts resolve back to original dates\ndayCounts.forEachValue(v -> {\n    long dayCount = v.getLong();\n    LocalDate expected = dates.getValue(v.index());\n    LocalDate actual = LocalDate.now().minusDays(dayCount);\n    Assert.assertEquals(actual, expected);\n});  The mapping functions are also  parallel  aware.   //Parallel map dates to day count offsets from today\nArray<Long> dayCounts = dates.parallel().mapToLongs(v -> {\n    LocalDate now = LocalDate.now();\n    LocalDate value = v.getValue();\n    return ChronoUnit.DAYS.between(value, now);\n});",
            "title": "Mapping"
        },
        {
            "location": "/array/overview/#statistics",
            "text": "The  Array  interface exposes a  stats()  method which makes it easy to compute summary statistics \non arrays that contain numerical data. Attempting to compute stats on non-numerical arrays will\nresult in an  ArrayException . The table below enumerates the supported statistics.     Method  Description  Details      count()  The number of observations, ignoring nulls     min()  The minimum value, ignoring nulls  Details    max()  The maximim value, ignorning nulls  Details    mean()  The first moment, or the arithmetic mean or average, ignoring nulls  Details    variance()  The un-biased variance or second moment, a measure of dispersion  Details    stdDev()  The un-biased standard deviation, a measure of dispersion  Details    skew()  The third moment, or skewness, a measure of the asymmetry in the distribution  Details    kurtosis()  The fourth moment, or Kurtosis, a measure of the \"tailedness\" of the probability distribution  Details    median()  The value separating the higher half of the data, or 50th percentile  Details    mad()  The Mean Absolute Deviation from a central point, another measure of dispersion  Details    sem()  The standard error of the mean  Details    geoMean()  The geometric mean, another measure of central tendency  Details    sum()  The summation of all values, ignoring nulls  Details    sumOfSquares()  The sum, over non-null observations, of the squared differences from the mean  Details    autocorr(int lag)  The autocorrelation, which is the correlation of a signal with a delayed copy of itself  Details    percentile(double nth)  The percentile value below which n% of values fall, ignoring nulls  Details     Below we initialise an  Array  of double precision values with elements equal to their index and the\nproceed to compute summary statistics.   //Create dense array of 1 million doubles\nArray<Double> array = Array.of(Double.class, 1000, Double.NaN).applyDoubles(ArrayValue::index);\n\n//Compute stats\nAssert.assertEquals(array.stats().count(), 1000d);\nAssert.assertEquals(array.stats().min(), 0d);\nAssert.assertEquals(array.stats().max(), 999d);\nAssert.assertEquals(array.stats().mean(), 499.5d);\nAssert.assertEquals(array.stats().variance(), 83416.66666666667d);\nAssert.assertEquals(array.stats().stdDev(), 288.8194360957494d);\nAssert.assertEquals(array.stats().skew(), 0d);\nAssert.assertEquals(array.stats().kurtosis(), -1.2000000000000004d);\nAssert.assertEquals(array.stats().median(), 499.5d);\nAssert.assertEquals(array.stats().mad(), 250.00000000000003d);\nAssert.assertEquals(array.stats().sem(), 9.133272505880171d);\nAssert.assertEquals(array.stats().geoMean(), 0d);\nAssert.assertEquals(array.stats().sum(), 499500.0d);\nAssert.assertEquals(array.stats().sumSquares(), 3.328335E8);\nAssert.assertEquals(array.stats().autocorr(1), 1d);\nAssert.assertEquals(array.stats().percentile(0.5d), 499.5d);",
            "title": "Statistics"
        },
        {
            "location": "/array/overview/#searching",
            "text": "The Morpheus  Array  interface provides some useful functions to search for values given a user provided \npredicate. In addition, there are methods to perform binary searches to find a matching value, or to find \nthe next smallest / largest value given a user provided value that may not even exist in the array.   The first example below demonstrates how to find the first and last values in the array given some predicate.\nNote that the predicate again accepts an  ArrayValue<T>  instance, which allows the index to be accessed as\nwell as the value in a way that can avoid boxing.   //Create random with seed\nRandom random = new Random(3);\n//Create dense array double precision values\nArray<Double> array = Array.of(Double.class, 1000, Double.NaN).applyDoubles(v -> random.nextDouble() * 55d);\n\n//Find first value above 50\nAssert.assertTrue(array.first(v -> v.getDouble() > 50d).isPresent());\narray.first(v -> v.getDouble() > 50d).ifPresent(v -> {\n    Assert.assertEquals(v.getDouble(), 51.997892373318116d, 0.000001);\n    Assert.assertEquals(v.index(), 9);\n});\n\n//Find last value above 50\nAssert.assertTrue(array.last(v -> v.getDouble() > 50d).isPresent());\narray.last(v -> v.getDouble() > 50d).ifPresent(v -> {\n    Assert.assertEquals(v.getDouble(), 51.864302849037315d, 0.000001);\n    Assert.assertEquals(v.index(), 992);\n});  The next two examples are predicated on binary search, and for that to work the  Array  needs to be sorted, \nwhich can be done by calling one of the sort methods as shown below.   //Sort the array for binary search\nArray<Double> sorted = array.sort(true);  Knowing that the array is sorted, we can perform a binary search on a subset of the  Array  or the entire Array  by providing the start and end index for the search space. The example below picks a number of\nindexes, selects the value for those indexes, and then proceeds to search for those values and assert\nthat we get a match at the expected location.   //Perform binary search over entire array for various chosen values\nIntStream.of(27, 45, 145, 378, 945).forEach(index -> {\n    double value = sorted.getDouble(index);\n    int actual = sorted.binarySearch(0, 1000, value);\n    Assert.assertEquals(actual, index);\n});  Another useful search feature that leverages binary search and therefore performs well, is to find the\nclosest value before or after some value, even if that value does not exist in the  Array . The examples\nbelow illustrate how to find the next and prior value. The result of this search is present as an  ArrayValue \nwrapped in an  Optional , so the value and index are easily accessible.   //Find next value given a value that does not actual exist in the array\nIntStream.of(27, 45, 145, 378, 945).forEach(index -> {\n    double value1 = sorted.getDouble(index);\n    double value2 = sorted.getDouble(index+1);\n    double mean = (value1 + value2) / 2d;\n    //Find next value given a value that does not exist in the array\n    Optional<ArrayValue<Double>> nextValue = sorted.next(mean);\n    Assert.assertTrue(nextValue.isPresent());\n    nextValue.ifPresent(v -> {\n        Assert.assertEquals(v.getDouble(), value2);\n        Assert.assertEquals(v.index(), index + 1);\n    });\n});\n\n//Find prior value given a value that does not actual exist in the array\nIntStream.of(27, 45, 145, 378, 945).forEach(index -> {\n    double value1 = sorted.getDouble(index);\n    double value2 = sorted.getDouble(index+1);\n    double mean = (value1 + value2) / 2d;\n    //Find prior value given a value that does not exist in the array\n    Optional<ArrayValue<Double>> priorValue = sorted.previous(mean);\n    Assert.assertTrue(priorValue.isPresent());\n    priorValue.ifPresent(v -> {\n        Assert.assertEquals(v.getDouble(), value1);\n        Assert.assertEquals(v.index(), index);\n    });\n});",
            "title": "Searching"
        },
        {
            "location": "/array/overview/#sorting",
            "text": "The  Array  interface provides various convenience methods to sort values in either ascending or descending\norder or in some bespoke order according to some user defined  Comparator . To demonstrate, let us first\ncreate an  Array  initialized with random double precision values, including both positive and negative values\nas follows:   //Create random generator with seed\nRandom random = new Random(22);\n//Create dense array double precision values\nArray<Double> array = Array.of(Double.class, 1000, Double.NaN);\n//Initialise with random values\narray.applyDoubles(v -> {\n    final double sign = random.nextDouble() > 0.5d ? 1d : -1d;\n    return random.nextDouble() * 10 * sign;\n});  Sorting these values in ascending or descending order is as trivial:   //Sort ascending\narray.sort(true);\n//Sort descending\narray.sort(false);  It is also possible to only sort a subset of the array in ascending or descending order:   //Sort values between indexes 100 and 200 in ascending order\narray.sort(100, 200, true);\n//Sort values between indexes 100 and 200 in ascending order\narray.sort(100, 200, false);  The most general sort method allows the user to specify the range of values to operate on, and also\nprovide a  Comparator  that accepts  ArrayValue  instances. This enables the user to write a  Comparator \nimplementation that avoids boxing values, and also one that has access to the data values as well as their\nindex in the array. The code below shows how to sort our example  Array  by the  absolute value  of the\nelements in the array, but only including items between index 100 (inclusive) and 200 (exclusive). We\nthen run a check to see the values are sorted as expected. Like with other functions that consume  ArrayValue \nobjects, they should be treated as ephemeral and only valid for the life of the method invocation.   //Sort by absolute ascending value\narray.sort(100, 200, (v1, v2) -> {\n    final double d1 = Math.abs(v1.getDouble());\n    final double d2 = Math.abs(v2.getDouble());\n    return Double.compare(d1, d2);\n});\n\n//Check values in range are sorted as expected\nIntStream.range(101, 200).forEach(index -> {\n    double prior = Math.abs(array.getDouble(index-1));\n    double current = Math.abs(array.getDouble(index));\n    int compare = Double.compare(prior, current);\n    Assert.assertTrue(compare <= 0);\n});  All the sorting methods on a Morpheus  Array  support  parallel  execution, which can significantly \nimprove performance.   //Parallel sort by absolute ascending value\narray.parallel().sort(100, 200, (v1, v2) -> {\n    final double d1 = Math.abs(v1.getDouble());\n    final double d2 = Math.abs(v2.getDouble());\n    return Double.compare(d1, d2);\n});",
            "title": "Sorting"
        },
        {
            "location": "/array/overview/#copying",
            "text": "Being able to efficiently create  deep copies  of Morpheus  Arrays  either in their entirety or only including\na subset of the elements is supported via three overloaded  copy()  methods. The code examples below illustrate\nthese three cases, the first case copies the entire  Array , the second creates a copy including only a range\nof values, and the third creates a copy given specific indexes.   //Create random generator with seed\nRandom random = new Random(22);\n//Create dense array double precision values\nArray<Double> array = Array.of(Double.class, 1000, Double.NaN).applyDoubles(v -> random.nextDouble());\n\n//Deep copy of entire Array\nArray<Double> copy1 = array.copy();\n//Deep copy of subset of Array, start inclusive, end exclusive\nArray<Double> copy2 = array.copy(100, 200);\n//Deep copy of specific indexes\nArray<Double> copy3 = array.copy(new int[] {25, 304, 674, 485, 873});\n\n//Assert lengths as expected\nAssert.assertEquals(copy1.length(), array.length());\nAssert.assertEquals(copy2.length(), 100);\nAssert.assertEquals(copy3.length(), 5);\n\n//Asset values as expected\nIntStream.range(0, 1000).forEach(i -> Assert.assertEquals(copy1.getDouble(i), array.getDouble(i)));\nIntStream.range(0, 100).forEach(i -> Assert.assertEquals(copy2.getDouble(i), array.getDouble(i+100)));\nIntStream.of(0, 5).forEach(i -> {\n    switch (i) {\n        case 0: Assert.assertEquals(copy3.getDouble(i), array.getDouble(25));   break;\n        case 1: Assert.assertEquals(copy3.getDouble(i), array.getDouble(304));   break;\n        case 2: Assert.assertEquals(copy3.getDouble(i), array.getDouble(674));   break;\n        case 3: Assert.assertEquals(copy3.getDouble(i), array.getDouble(485));   break;\n        case 4: Assert.assertEquals(copy3.getDouble(i), array.getDouble(873));   break;\n        default:\n    }\n});",
            "title": "Copying"
        },
        {
            "location": "/array/overview/#streams",
            "text": "While Morpheus  Arrays  offer many of the programmatic features available with Java 8 Streams, they by \nno means cover everything. Either way, being able to expose Morpheus  Arrays  as Java 8 Streams will \nalways be useful for compatibility purposes with other libraries. To that end, the  stream()  method\nprovides access to type specific streams as shown in the code examples below.   //Create Array of various types\nArray<Integer> integers = Array.of(0, 1, 2, 3, 4, 5);\nArray<Long> longs = Array.of(0L, 1L, 2L, 3L, 4L);\nArray<Double> doubles = Array.of(0d, 1d, 2d, 3d, 4d, 5d);\nArray<LocalDate> dates = Array.of(LocalDate.now(), LocalDate.now().plusDays(1));\n\n//Create Java 8 streams of these Arrays\nIntStream intStream = integers.stream().ints();\nLongStream longStream = longs.stream().longs();\nDoubleStream doubleStream = doubles.stream().doubles();\nStream<LocalDate> dateStream = dates.stream().values();",
            "title": "Streams"
        },
        {
            "location": "/array/overview/#expanding",
            "text": "Java native arrays cannot be expanded, however the Morpheus  Array  interface does support this. The\ninternal implementations have no choice but to re-create and re-populate internally, however this\nfeature does provide a convenient mechanism for growing  Arrays  without having to do the heavy lifting\nyourself. In addition, given the support for dense, sparse and memory mapped backing stores, the\nexpansion behaviour differs across styles. The  expand()  method takes the new length to grow the Array  to, and the new elements will be initialized with the default value specified for the array\nupon creation.   //Create array of random doubles, with defauly value of -1\nArray<Double> array = Array.of(Double.class, 10, -1d).applyDoubles(v -> Math.random());\n//Double the size of the array\narray.expand(20);\n//Confirm new length is as expected\nAssert.assertEquals(array.length(), 20);\n//Confirm new values initialized with default value\nIntStream.range(10, 20).forEach(i -> Assert.assertEquals(array.getDouble(i), -1d));",
            "title": "Expanding"
        },
        {
            "location": "/array/overview/#filtering",
            "text": "Creating a filtered  Array  given some user defined predicate is a common programmatic requirement, and to\nsupport this, a  filter()  method is provided that takes a  Predicate  which accepts  ArrayValue  instances.\nThis design again allows boxing of primitive values to be avoided, and also makes the index of the value\naccessible should that factor into the filtering logic. The code below creates an  Array  of double\nprecision random values, and creates a filter which only includes values > 5.    //Create random generator with seed\nRandom random = new Random(2);\n//Create array of random doubles, with default value NaN\nArray<Double> array = Array.of(Double.class, 1000, Double.NaN).applyDoubles(v -> random.nextDouble() * 10d);\n//Filter to include all values > 5\nArray<Double> filter = array.filter(v -> v.getDouble() > 5d);\n//Assert length as expected\nAssert.assertEquals(filter.length(), 486);\n//Assert all value are > 5\nfilter.forEachValue(v -> Assert.assertTrue(v.getDouble() > 5d));",
            "title": "Filtering"
        },
        {
            "location": "/array/overview/#read-only",
            "text": "Native Java arrays are inherently mutable, however since Morpheus  Arrays  are represented by an interface\nwe can easily create a light-weight wrapper that only supports read operations on the underlying  Array . This\nis useful when one needs to expose the  Array  to external code but in a way that guarantees that code\ncannot modify the array contents in any way. The  readOnly()  method call shown below generates this \nlight-weight ready-only proxy.   //Create array of random doubles, with default value NaN\nArray<Double> array = Array.of(Double.class, 1000, Double.NaN).applyDoubles(v -> Math.random());\n//Create a light-weight read only wrapper\nArray<Double> readOnly = array.readOnly();",
            "title": "Read-Only"
        },
        {
            "location": "/array/overview/#filling",
            "text": "Filling an  Array  either over a range of indexes or the entire array with a specific value is supported by\ntwo overloaded  fill()  methods. Below we create a random  Array  initialized with all  NaN  values, and\nthen proceed to fill indexes 10 through 20 with a fixed value.   //Create array of random doubles, with default value NaN\nArray<Double> array = Array.of(Double.class, 1000, Double.NaN);\n//Fill indexes 10-20 (inclusive - exclusive) with value 25\narray.fill(25d, 10, 20);\n//Check results\nIntStream.range(10, 20).forEach(i -> {\n    Assert.assertEquals(array.getDouble(i), 25d);\n});",
            "title": "Filling"
        },
        {
            "location": "/array/overview/#distinct",
            "text": "Finding the distinct elements in an array is easy enough as you can simply collect the values in a  Set<T> ,\nhowever this would once again come with the cost of boxing when the  Array  is backed by a primitive type.\nIn order to avoid this, two overloaded  distinct()  methods are provided, one which returns a new  Array  with\nall the distinct values, and the second returns a truncated array of distinct values based on a user specified \nlimit. The code below illustrates and example of how to use these.   //Create a random with seed\nRandom random = new Random(10);\n//Create Array of random LocalDates, all elements initially null\nArray<LocalDate> dates = Array.of(LocalDate.class, 1000);\n//Populate with some random dates that will likely have duplicates\ndates.applyValues(v -> LocalDate.now().plusDays(random.nextInt(20)));\n//Generate distinct Array\nArray<LocalDate> distinct1 = dates.distinct();\n//Generate distinct limiting to first 5 matches\nArray<LocalDate> distinct2 = dates.distinct(5);\n//Check expected results\nAssert.assertEquals(distinct1.length(), 20);\nAssert.assertEquals(distinct2.length(), 5);",
            "title": "Distinct"
        },
        {
            "location": "/array/overview/#bounds",
            "text": "Computing the upper and lower bounds of an  Array  can be achieved in a number of ways, as shown by the code\nbelow. Firstly the  bounds()  method computes the min/max in one pass, and will work for any value that is  Comparable . Alternatively, the  min()  and  max()  methods perform the same logic however only return\nthe lower or upper bound respectively. Below we also confirm that the  stats()  interface to the  Array \ngenerates the same results since in this example we are using an array of double precision values. If the  Array  was non-numeric, such as  LocalDate  for example, then the stats interface would not work and result \nin an  ArrayException .   //Create a random with seed\nRandom random = new Random(21);\n//Create Array of random doubles, all elements initially null\nArray<Double> array = Array.of(Double.class, 1000).applyDoubles(v -> random.nextDouble() * 100);\n//Compute upper and lower bounds in one pass\nOptional<Bounds<Double>> bounds = array.bounds();\n//Confirm we have bounds\nAssert.assertTrue(bounds.isPresent());\n//Confirm expected results\nbounds.ifPresent(b -> {\n    Assert.assertEquals(b.lower(), 0.13021930271921445);\n    Assert.assertEquals(b.upper(), 99.9557586162974);\n    Assert.assertEquals(b.lower(), array.min().get());\n    Assert.assertEquals(b.upper(), array.max().get());\n    Assert.assertEquals(b.lower(), array.stats().min());\n    Assert.assertEquals(b.upper(), array.stats().max());\n});",
            "title": "Bounds"
        },
        {
            "location": "/array/overview/#null-check",
            "text": "A convenience method named  isNull()  exists on the  Array  and  ArrayValue  interface. In the former case,\nit takes the index of the array element to check for null, in the latter no arguments are required since the ArrayValue  is already pointing at some entry. This method is convenient as it can avoid accessing the value\nto check for null, which may again incur a boxing cost. In addition,  Double.NaN  is considered to be null\nso one can avoid this special null check case as shown by the example below.   //Create a random with seed\nRandom random = new Random(21);\n//Create Array of random doubles, all elements initially null\nArray<Double> array = Array.of(Double.class, 1000).applyDoubles(v -> random.nextDouble() * 100);\n//Set some values to NaN\narray.fill(Double.NaN, 10, 20);\n//Set some values to null, which is the same as NaN for double precision\narray.fill(null, 20, 30);\n//Filter out NaN values using is null\nArray<Double> filtered = array.filter(v -> !v.isNull());\n//Assert length\nAssert.assertEquals(filtered.length(), array.length() - 20);",
            "title": "Null Check"
        },
        {
            "location": "/array/overview/#swapping",
            "text": "Swapping elements in an  Array  without having to access them directly is also supported, which means that optimized \nimplementations, such as for the  LocalDate  type for example, can avoid boxing the internal representation (long \nepoch day values in the case of LocalDate). The  swap()  method is used as part of the sort algorithm, and a simple \nexample of how to use it unrelated to sorting is shown below.   //Create Array of doubles\nArray<Double> array = Array.of(10d, 20d, 30d, 40d);\n//Swap values\narray.swap(0, 3);\n//Assert values swapped\nAssert.assertEquals(array.getDouble(0), 40d);\nAssert.assertEquals(array.getDouble(3), 10d);",
            "title": "Swapping"
        },
        {
            "location": "/array/performance/",
            "text": "Introduction\n\n\nThe following sections provide some micro-benchmark timing and memory statistics for various \ntypes of Morpheus \nArrays\n. These benchmarks where performed on a 2013 MacBook Pro with a Core \ni7 2.6Ghz Quad-core CPU, and 16GB of memory. As with all benchmarks, these figures should be \ntaken with a pinch of salt as real-world performance could differ substantially for all sorts \nof reasons. They are however a reasonable set of observations to compare the relative cost of \ndifferent operations, and also to get a sense of how the sequential versus parallel execution \nof these operations compares.\n\n\nNative Arrays\n\n\nNon-primitive Java arrays do not scale particularly well, specifically in terms of their memory \nconsumption, and more importantly, they put a significant burden on the garbage collector. To \nillustrate this point, consider the plots below which compare the average initialization time \n(construction and population) of various types of array containing 5 million entries, followed \nby the subsequent garbage collection times. The lower case \nboolean\n, \nint\n, \nlong\n and \ndouble\n\nlabels represent primitive array types while the capitalized \nBoolean\n, \nInteger\n, \nLong\n and \nDouble\n\nrepresent the boxed versions.\n\n\n\n    \n\n\n\n\n\nFigure 1. Expect significant dispersion in initialization times for large non-primitive arrays\n\n\nWhile the variation in initialisation times appears to be fairly significant, the \nmagnitude\n of the \ngarbage collection (GC) times in figure 2 is potentially more concerning. The GC times for \nprimitive arrays barely registers on this chart which is dominated by the results for \nZonedDateTime\n. \nPrimitive arrays are represented by a single object and a contiguous block of memory, so the \ncollector only needs to keep track of one reference when performing its sweep. Clearly this makes\na profound difference in performance.\n\n\n\n    \n\n\n\n\n\nFigure 2. ZonedDateTime has a great API for time zone aware dates, but it comes at a price\n\n\nUsing the \nInstrumentation\n interface introduced in Java 5, we estimate the relative sizes\nof these various array instances, and once again, primitive arrays win hands down. Notice how \nthe \nDate\n array consumes more than twice the memory of a primitive \nlong\n array of the same \nlength, even though its internal state is no more than a single \nlong\n field representing epoch \nmilliseconds. Also, it appears that the boxed versions of the \nboolean\n, \nint\n, \nlong\n and \ndouble\n\narrays use at least 3 times more memory.\n\n\n\n    \n\n\n\n\n\nFigure 3. The boxed versions of the arrays appear to take more than 2-3 times the memory of their primitive counterparts\n\n\nThere is work underway at Oracle to address some of these issues, however it is not yet clear\nwhich future Java release, if any, will include some of the changes discussed \nhere\n.\n\n\n\n\nMorpheus Arrays\n\n\nMotivation\n\n\nThe moral of the story above is that to build high performance systems involving large data, \nprimitive arrays have to be used wherever possible, which is the fundamental design motivation \nbehind the Morpheus Array package. \n\n\nThis is by no means an attempt to compete with excellent primitive collections libraries such as \n\nTrove\n and \nGoldman Sachs Collections\n, \nand in fact, sparse implementations of Morpheus arrays use the Trove library internally. The \nMorpheus Array interface is a much more narrowly focused data structure, whose design was ultimately \nmotivated by the desire to build an efficient in-memory column store called a \nDataFrame\n.\n\n\nThe Morpheus library ships with a number of implementations of the \ncom.zavtech.morpheus.array.Array\n \ninterface, with support for many commonly used types. Each implementation is designed to unbox an object \ninto an appropriate primitive representation where possible, and box it when it is being read back from \nthe array. So for example, \nArray<Date>\n internally contains a primitive array of longs which store epoch \nmillisecond values. \nArray<String>\n represents its internal state with a large \nchar\narray, and unboxes \nstrings to store their contents at an appropriate location within this large \nchar\n array. Additional\n\nchar\n arrays are created if the size exceeds Integer.MAX_VALUE.\n\n\nDesign Compromise\n\n\nThis design choice is obviously a compromise in that it introduces a boxing/unboxing cost when reading\nand writing elements against a Morpheus array. While this is true, for the most part it turns out to be \na favorable compromise with more benefits than drawbacks, because it plays to the strengths\nof the Java Virtual Machine. Specifically, it is sympathetic to the fundamental design philosophy \nof generational garbage collection algorithms, which assume that most objects die young.\n\n\nThe plots below compare the same performance statistics discussed in the previous section in order to get \na sense of how Morpheus arrays compare to their native counterparts. As expected, the first chart suggests \nthat Morpheus arrays are somewhat slower to initialize, although not alarmingly so. This differential in\nperformance is not unexpected, even for the cases where no unboxing is required, as a method call \nis necessary to set the value unlike the [] operator for native arrays. \n\n\n\n    \n\n\n\n\n\nFigure 4. Morpheus arrays are slightly slower to initialise, but not alarmingly so - there is payback for the effort\n\n\nThere is a huge return on the investment made to keep everything as primitives however, and the subsequent \ngarbage collection times for Morpheus arrays is a small fraction compared to non-primitive native arrays.\nFigure 5 is the same plot as figure 4, but including the subsequent GC times to deallocate the array\ncreated in each test. So while you pay a small performance penalty with respect to initialisation, the \nmassive reduction in the GC load easily offsets this. Considering the \nmagnitude\n of the native object \narray GC times, it feels like a good trade.\n\n\n\n    \n\n\n\n\n\nFigure 5. Morpheus arrays which are based on primitives internally, are very friendly to the garbage collector\n\n\nThere is also a material reduction in the amount of memory required to store Morpheus arrays. Notice the huge \ndrop in memory required to represent a \nZonedDateTime\n array, or at least the equivalent information within a Morpheus \narray. Clearly it is not possible to improve on the allocated memory for primitive types, but in these cases\nMorpheus arrays match their native counterparts.\n\n\n\n    \n\n\n\n\n\nFigure 6. Morpheus arrays use significantly less memory than their object counterparts\n\n\nIteration & Boxing\n\n\nSignificantly reduced memory usage and much faster Garbage Collection times in return for slightly slower \ninitialization appears to be a reasonable compromise. Sadly, that does not fully represent the extent of the\ntrade-off here, as iteration times on some Morpheus arrays is likely to be slower, at least in cases where \neach element access requires boxing. For \nboolean\n, \nint\n, \nlong\n and \ndouble\narray types this is obviously \nnot an issue, but for other types there is likely to be a cost.\n\n\nThe results in figure 7 illustrate this performance deficit for a Morpheus \nLocalDateTime\n array versus its\nnative counterpart. In both sequential and parallel iteration, the boxing cost appears to roughly double \nthe iteration times in these examples. While that sounds bad, consider the results in figure 8, which are for \nthe same test, but in that example the subsequent Garbage Collection times incurred after each test is \nincluded. This presents a very different picture. In addition, iterating over a Morpheus array in parallel \nis trivial and a fluent extension of the API, and in this mode, it appears that you double the performance \nof native sequential execution on a Quad-Core machine. \n\n\n\n    \n\n\n\n\n\nFigure 7. Morpheus boxing & unboxing does come at a cost, but figure 8 shows the payoff\n\n\nIncluding the subsequent GC times after each test paints a very different picture. This also helps to \ndemonstrate the fact that the design compromise in the Morpheus library is sympathetic to the garbage collector, \nbecause the boxing of dates during array traversal results in very short lived objects that are stack based. In \nthe native examples, the objects survive far longer and are heap based, thus placing a much bigger burden on \nthe collector. So while the number of objects created in each test is roughly equivalent, the native examples \ntake far longer overall.\n\n\n\n    \n\n\n\n\n\nFigure 8. Including GC times completely changes the picture, and Morpheus proves much faster overall\n\n\nThe code to generate the plots in figure 7 and 8 is as follows:\n\n\n\n\n\nfinal int sample = 5;\nfinal boolean includeGC = false;\n\nRange<Integer> arrayLengths = Range.of(1, 11).map(i -> i * 100000);\nArray<String> labels = Array.ofStrings(\"Native(Seq)\", \"Morpheus(Seq)\", \"Native(Par)\", \"Morpheus(Par)\");\nDataFrame<String,String> results = DataFrame.ofDoubles(arrayLengths.map(String::valueOf), labels);\n\narrayLengths.forEach(arrayLength -> {\n\n    DataFrame<String,String> timing = PerfStat.run(sample, TimeUnit.MILLISECONDS, includeGC, tasks -> {\n\n        tasks.put(\"Native(Seq)\", () -> {\n            final AtomicInteger count = new AtomicInteger();\n            final LocalDateTime start = LocalDateTime.now().minusYears(5);\n            final LocalDateTime[] array = new LocalDateTime[arrayLength];\n            for (int i=0; i<array.length; ++i) {\n                array[i] = start.plusMinutes(i);\n            }\n            for (LocalDateTime value : array) {\n                if (value.getDayOfWeek() == DayOfWeek.MONDAY) {\n                    count.incrementAndGet();\n                }\n            }\n            return array;\n        });\n\n        tasks.put(\"Morpheus(Seq)\", () -> {\n            final AtomicInteger count = new AtomicInteger();\n            final LocalDateTime start = LocalDateTime.now().minusYears(5);\n            final Array<LocalDateTime> array = Array.of(LocalDateTime.class, arrayLength);\n            array.applyValues(v -> start.plusMinutes(v.index()));\n            array.forEach(value -> {\n                if (value.getDayOfWeek() == DayOfWeek.MONDAY) {\n                    count.incrementAndGet();\n                }\n            });\n            return array;\n        });\n\n        tasks.put(\"Native(Par)\", () -> {\n            final AtomicInteger count = new AtomicInteger();\n            final LocalDateTime start = LocalDateTime.now().minusYears(5);\n            final IntStream indexes = IntStream.range(0, arrayLength).parallel();\n            final Stream<LocalDateTime> dates = indexes.mapToObj(start::plusMinutes);\n            final LocalDateTime[] array = dates.toArray(LocalDateTime[]::new);\n            Stream.of(array).parallel().forEach(value -> {\n                if (value.getDayOfWeek() == DayOfWeek.MONDAY) {\n                    count.incrementAndGet();\n                }\n            });\n            return array;\n        });\n\n        tasks.put(\"Morpheus(Par)\", () -> {\n            final AtomicInteger count = new AtomicInteger();\n            final LocalDateTime start = LocalDateTime.now().minusYears(5);\n            final Array<LocalDateTime> array = Array.of(LocalDateTime.class, arrayLength);\n            array.parallel().applyValues(v -> start.plusMinutes(v.index()));\n            array.parallel().forEach(value -> {\n                if (value.getDayOfWeek() == DayOfWeek.MONDAY) {\n                    count.incrementAndGet();\n                }\n            });\n            return array;\n        });\n\n    });\n\n    String label = String.valueOf(arrayLength);\n    results.data().setDouble(label, \"Native(Seq)\", timing.data().getDouble(\"Mean\", \"Native(Seq)\"));\n    results.data().setDouble(label, \"Morpheus(Seq)\", timing.data().getDouble(\"Mean\", \"Morpheus(Seq)\"));\n    results.data().setDouble(label, \"Native(Par)\", timing.data().getDouble(\"Mean\", \"Native(Par)\"));\n    results.data().setDouble(label, \"Morpheus(Par)\", timing.data().getDouble(\"Mean\", \"Morpheus(Par)\"));\n\n});\n\n//Create title from template\nfinal String prefix = \"LocalDateTime Array Initialization + Traversal Times\";\nfinal String title = prefix + (includeGC ? \" (including-GC)\" : \" (excluding-GC)\");\n\n//Record chart to file\nfinal String fileSuffix = includeGC ? \"2.png\" : \"1.png\";\nfinal String filePrefix = \"./docs/images/native-vs-morpheus-array-sequential-vs-parallel\";\n\n//Plot results as a bar chart\nChart.create().withBarPlot(results, false, chart -> {\n    chart.title().withText(title);\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.plot().axes().domain().label().withText(\"Array Length\");\n    chart.plot().axes().range(0).label().withText(\"Time (Milliseconds)\");\n    chart.legend().on();\n    chart.writerPng(new File(filePrefix + fileSuffix), 845, 400, true);\n    chart.show();\n});\n\n\n\n\nSorting\n\n\nThis section compares some performance statistics of sorting large Morpheus arrays versus their \nnative counterparts. The Morpheus library is built around the highly versatile array sorting API \nin \nFastUtil\n developed by \nSebastiano Vigna\n.\nThe results below compare performance versus the JDK \njava.util.Arrays.sort()\n and \nparallelSort()\n \nfunctions, which it has to be said, is not an entirely fair benchmark.\n\n\nFirstly, the JDK array sorting algorithm is based on a dual-pivot quick sort whereas FastUtil is based\non a more traditional single-pivot quick sort. Secondly, the JDK implementation is presented with\nthe array directly, allowing everything to be inlined, and also enabling sliced work arrays to\nbe constructed as required. FastUtil on the other hand is not presented with the data directly, \nbut interacts with it via an \nIntComparator\n and \nSwapper\n, which does incur method call over head, \nbut also makes it far more versatile. The JDK sorting functionality is limited to only sorting arrays \nin their natural order, while FastUtil allows the sorting logic to be fully customized.\n\n\nThe results in Figure 9 and 10 demonstrate two very different outcomes, the former comparing performance\non a primitive \ndouble\n array, the latter on an \nLocalDateTime\n array (in both cases these arrays are\nrandomized before the tests are run). In the case of the double array, the JDK \nArrays\n implementation\nappears to offer roughly twice the performance of FastUtil as used in Morpheus. For the \nLocalDateTime\n\ntest, the tables are turned, and FastUtil vastly outperforms the native \nArrays\n call, and these times\ndo not include any garbage collection costs incurred after the each test is completed.\n\n\n\n    \n\n\n\n\n\nFigure 9. Sorting times for an array of 10 million random double precision values\n\n\nIn the \nLocalDatTime\n results below, Morpheus far outperforms the native array because it is only \noperating on the internal primitive array of longs. No boxing is required here, so hence the vast\nperformance gap. While this is not an entirely fair comparison, it demonstrates the benefits of\nstoring everything as primitives. \n\n\n\n    \n\n\n\n\n\nFigure 10. Same test as in figure 9, but using an array of randomly ordered LocalDateTimes\n\n\nThe code to generate the results in figure 9 is as follows:\n\n\n\n\n\nRange<Integer> arrayLengths = Range.of(1, 11).map(i -> i * 100000);\nArray<String> labels = Array.ofStrings(\"Native(Seq)\", \"Morpheus(Seq)\", \"Native(Par)\", \"Morpheus(Par)\");\nDataFrame<String,String> results = DataFrame.ofDoubles(arrayLengths.map(String::valueOf), labels);\n\narrayLengths.forEach(length -> {\n\n    System.out.println(\"Running sort test for array length \" + length);\n    double[] array1 = new double[length];\n    Array<Double> array2 = Array.of(Double.class, length);\n\n    DataFrame<String,String> timing = PerfStat.run(sample, TimeUnit.MILLISECONDS, false, tasks -> {\n        tasks.put(\"Native(Seq)\", () -> { Arrays.sort(array1); return array1; });\n        tasks.put(\"Morpheus(Seq)\", () -> array2.sort(true) );\n        tasks.put(\"Native(Par)\", () -> { Arrays.parallelSort(array1); return array1; });\n        tasks.put(\"Morpheus(Par)\", () -> array2.parallel().sort(true));\n        tasks.beforeEach(() -> {\n            array2.applyDoubles(v -> Math.random());\n            array2.forEachValue(v -> array1[v.index()] = v.getDouble());\n        });\n    });\n\n    String label = String.valueOf(length);\n    results.data().setDouble(label, \"Native(Seq)\", timing.data().getDouble(\"Mean\", \"Native(Seq)\"));\n    results.data().setDouble(label, \"Morpheus(Seq)\", timing.data().getDouble(\"Mean\", \"Morpheus(Seq)\"));\n    results.data().setDouble(label, \"Native(Par)\", timing.data().getDouble(\"Mean\", \"Native(Par)\"));\n    results.data().setDouble(label, \"Morpheus(Par)\", timing.data().getDouble(\"Mean\", \"Morpheus(Par)\"));\n});\n\nChart.create().withBarPlot(results, false, chart -> {\n    chart.title().withText(\"Sorting Performance for Array of Random LocalDateTimes (Sample \" + sample + \")\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 16));\n    chart.subtitle().withText(\"Dual-Pivot Quick Sort (Native) vs Single-Pivot Quick Sort (FastUtil)\");\n    chart.subtitle().withFont(new Font(\"Verdana\", Font.PLAIN, 14));\n    chart.plot().axes().domain().label().withText(\"Array Length\");\n    chart.plot().axes().range(0).label().withText(\"Total Time in Milliseconds\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\nSummary Statistics\n\n\nMorpheus provides an API to calculate various descriptive statistics on numerical arrays, such\nas min, max, variance, skew, kurtosis, auto correlation and so on. The chart below shows the\ncalculation times for these quantities on a Morpheus array of 10 million random double precision\nvalues. The code used to generate this plot is also included.\n\n\n\n    \n\n\n\n\n\nFigure 11. Calculation times for various summary statistics on a Morpheus array with 10 million elements\n\n\nThe code to generate these results is as follows:\n\n\n\n\n\nfinal int count = 10;\nfinal int size = 10000000;\nfinal Array<Double> array = Array.of(Double.class, size).applyDoubles(v -> Math.random() * 100);\n\nfinal DataFrame<String,String> times = PerfStat.run(count, TimeUnit.MILLISECONDS, true, tasks -> {\n    tasks.put(\"Min\", () -> array.stats().min());\n    tasks.put(\"Max\", () -> array.stats().max());\n    tasks.put(\"Mean\", () -> array.stats().mean());\n    tasks.put(\"Count\", () -> array.stats().count());\n    tasks.put(\"Variance\", () -> array.stats().variance());\n    tasks.put(\"StdDev\", () -> array.stats().stdDev());\n    tasks.put(\"Sum\", () -> array.stats().sum());\n    tasks.put(\"Skew\", () -> array.stats().skew());\n    tasks.put(\"Kurtosis\", () -> array.stats().kurtosis());\n    tasks.put(\"Median\", () -> array.stats().median());\n    tasks.put(\"95th Percentile\", () -> array.stats().percentile(95));\n    tasks.put(\"AutCorrelation(20)\", () -> array.stats().autocorr(20));\n});\n\nChart.create().withBarPlot(times.rows().select(\"Mean\").transpose(), false, chart -> {\n    chart.title().withText(\"Morpheus Array Statistic Calculation Times, 10 Million Entries (Sample 10)\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.plot().axes().domain().label().withText(\"Stat Type\");\n    chart.plot().axes().range(0).label().withText(\"Time (Milliseconds)\");\n    chart.plot().orient().horizontal();\n    chart.legend().off();\n    chart.show();\n});",
            "title": "Performance"
        },
        {
            "location": "/array/performance/#introduction",
            "text": "The following sections provide some micro-benchmark timing and memory statistics for various \ntypes of Morpheus  Arrays . These benchmarks where performed on a 2013 MacBook Pro with a Core \ni7 2.6Ghz Quad-core CPU, and 16GB of memory. As with all benchmarks, these figures should be \ntaken with a pinch of salt as real-world performance could differ substantially for all sorts \nof reasons. They are however a reasonable set of observations to compare the relative cost of \ndifferent operations, and also to get a sense of how the sequential versus parallel execution \nof these operations compares.",
            "title": "Introduction"
        },
        {
            "location": "/array/performance/#native-arrays",
            "text": "Non-primitive Java arrays do not scale particularly well, specifically in terms of their memory \nconsumption, and more importantly, they put a significant burden on the garbage collector. To \nillustrate this point, consider the plots below which compare the average initialization time \n(construction and population) of various types of array containing 5 million entries, followed \nby the subsequent garbage collection times. The lower case  boolean ,  int ,  long  and  double \nlabels represent primitive array types while the capitalized  Boolean ,  Integer ,  Long  and  Double \nrepresent the boxed versions.  \n       Figure 1. Expect significant dispersion in initialization times for large non-primitive arrays  While the variation in initialisation times appears to be fairly significant, the  magnitude  of the \ngarbage collection (GC) times in figure 2 is potentially more concerning. The GC times for \nprimitive arrays barely registers on this chart which is dominated by the results for  ZonedDateTime . \nPrimitive arrays are represented by a single object and a contiguous block of memory, so the \ncollector only needs to keep track of one reference when performing its sweep. Clearly this makes\na profound difference in performance.  \n       Figure 2. ZonedDateTime has a great API for time zone aware dates, but it comes at a price  Using the  Instrumentation  interface introduced in Java 5, we estimate the relative sizes\nof these various array instances, and once again, primitive arrays win hands down. Notice how \nthe  Date  array consumes more than twice the memory of a primitive  long  array of the same \nlength, even though its internal state is no more than a single  long  field representing epoch \nmilliseconds. Also, it appears that the boxed versions of the  boolean ,  int ,  long  and  double \narrays use at least 3 times more memory.  \n       Figure 3. The boxed versions of the arrays appear to take more than 2-3 times the memory of their primitive counterparts  There is work underway at Oracle to address some of these issues, however it is not yet clear\nwhich future Java release, if any, will include some of the changes discussed  here .",
            "title": "Native Arrays"
        },
        {
            "location": "/array/performance/#morpheus-arrays",
            "text": "",
            "title": "Morpheus Arrays"
        },
        {
            "location": "/array/performance/#motivation",
            "text": "The moral of the story above is that to build high performance systems involving large data, \nprimitive arrays have to be used wherever possible, which is the fundamental design motivation \nbehind the Morpheus Array package.   This is by no means an attempt to compete with excellent primitive collections libraries such as  Trove  and  Goldman Sachs Collections , \nand in fact, sparse implementations of Morpheus arrays use the Trove library internally. The \nMorpheus Array interface is a much more narrowly focused data structure, whose design was ultimately \nmotivated by the desire to build an efficient in-memory column store called a  DataFrame .  The Morpheus library ships with a number of implementations of the  com.zavtech.morpheus.array.Array  \ninterface, with support for many commonly used types. Each implementation is designed to unbox an object \ninto an appropriate primitive representation where possible, and box it when it is being read back from \nthe array. So for example,  Array<Date>  internally contains a primitive array of longs which store epoch \nmillisecond values.  Array<String>  represents its internal state with a large  char array, and unboxes \nstrings to store their contents at an appropriate location within this large  char  array. Additional char  arrays are created if the size exceeds Integer.MAX_VALUE.",
            "title": "Motivation"
        },
        {
            "location": "/array/performance/#design-compromise",
            "text": "This design choice is obviously a compromise in that it introduces a boxing/unboxing cost when reading\nand writing elements against a Morpheus array. While this is true, for the most part it turns out to be \na favorable compromise with more benefits than drawbacks, because it plays to the strengths\nof the Java Virtual Machine. Specifically, it is sympathetic to the fundamental design philosophy \nof generational garbage collection algorithms, which assume that most objects die young.  The plots below compare the same performance statistics discussed in the previous section in order to get \na sense of how Morpheus arrays compare to their native counterparts. As expected, the first chart suggests \nthat Morpheus arrays are somewhat slower to initialize, although not alarmingly so. This differential in\nperformance is not unexpected, even for the cases where no unboxing is required, as a method call \nis necessary to set the value unlike the [] operator for native arrays.   \n       Figure 4. Morpheus arrays are slightly slower to initialise, but not alarmingly so - there is payback for the effort  There is a huge return on the investment made to keep everything as primitives however, and the subsequent \ngarbage collection times for Morpheus arrays is a small fraction compared to non-primitive native arrays.\nFigure 5 is the same plot as figure 4, but including the subsequent GC times to deallocate the array\ncreated in each test. So while you pay a small performance penalty with respect to initialisation, the \nmassive reduction in the GC load easily offsets this. Considering the  magnitude  of the native object \narray GC times, it feels like a good trade.  \n       Figure 5. Morpheus arrays which are based on primitives internally, are very friendly to the garbage collector  There is also a material reduction in the amount of memory required to store Morpheus arrays. Notice the huge \ndrop in memory required to represent a  ZonedDateTime  array, or at least the equivalent information within a Morpheus \narray. Clearly it is not possible to improve on the allocated memory for primitive types, but in these cases\nMorpheus arrays match their native counterparts.  \n       Figure 6. Morpheus arrays use significantly less memory than their object counterparts",
            "title": "Design Compromise"
        },
        {
            "location": "/array/performance/#iteration-boxing",
            "text": "Significantly reduced memory usage and much faster Garbage Collection times in return for slightly slower \ninitialization appears to be a reasonable compromise. Sadly, that does not fully represent the extent of the\ntrade-off here, as iteration times on some Morpheus arrays is likely to be slower, at least in cases where \neach element access requires boxing. For  boolean ,  int ,  long  and  double array types this is obviously \nnot an issue, but for other types there is likely to be a cost.  The results in figure 7 illustrate this performance deficit for a Morpheus  LocalDateTime  array versus its\nnative counterpart. In both sequential and parallel iteration, the boxing cost appears to roughly double \nthe iteration times in these examples. While that sounds bad, consider the results in figure 8, which are for \nthe same test, but in that example the subsequent Garbage Collection times incurred after each test is \nincluded. This presents a very different picture. In addition, iterating over a Morpheus array in parallel \nis trivial and a fluent extension of the API, and in this mode, it appears that you double the performance \nof native sequential execution on a Quad-Core machine.   \n       Figure 7. Morpheus boxing & unboxing does come at a cost, but figure 8 shows the payoff  Including the subsequent GC times after each test paints a very different picture. This also helps to \ndemonstrate the fact that the design compromise in the Morpheus library is sympathetic to the garbage collector, \nbecause the boxing of dates during array traversal results in very short lived objects that are stack based. In \nthe native examples, the objects survive far longer and are heap based, thus placing a much bigger burden on \nthe collector. So while the number of objects created in each test is roughly equivalent, the native examples \ntake far longer overall.  \n       Figure 8. Including GC times completely changes the picture, and Morpheus proves much faster overall  The code to generate the plots in figure 7 and 8 is as follows:   final int sample = 5;\nfinal boolean includeGC = false;\n\nRange<Integer> arrayLengths = Range.of(1, 11).map(i -> i * 100000);\nArray<String> labels = Array.ofStrings(\"Native(Seq)\", \"Morpheus(Seq)\", \"Native(Par)\", \"Morpheus(Par)\");\nDataFrame<String,String> results = DataFrame.ofDoubles(arrayLengths.map(String::valueOf), labels);\n\narrayLengths.forEach(arrayLength -> {\n\n    DataFrame<String,String> timing = PerfStat.run(sample, TimeUnit.MILLISECONDS, includeGC, tasks -> {\n\n        tasks.put(\"Native(Seq)\", () -> {\n            final AtomicInteger count = new AtomicInteger();\n            final LocalDateTime start = LocalDateTime.now().minusYears(5);\n            final LocalDateTime[] array = new LocalDateTime[arrayLength];\n            for (int i=0; i<array.length; ++i) {\n                array[i] = start.plusMinutes(i);\n            }\n            for (LocalDateTime value : array) {\n                if (value.getDayOfWeek() == DayOfWeek.MONDAY) {\n                    count.incrementAndGet();\n                }\n            }\n            return array;\n        });\n\n        tasks.put(\"Morpheus(Seq)\", () -> {\n            final AtomicInteger count = new AtomicInteger();\n            final LocalDateTime start = LocalDateTime.now().minusYears(5);\n            final Array<LocalDateTime> array = Array.of(LocalDateTime.class, arrayLength);\n            array.applyValues(v -> start.plusMinutes(v.index()));\n            array.forEach(value -> {\n                if (value.getDayOfWeek() == DayOfWeek.MONDAY) {\n                    count.incrementAndGet();\n                }\n            });\n            return array;\n        });\n\n        tasks.put(\"Native(Par)\", () -> {\n            final AtomicInteger count = new AtomicInteger();\n            final LocalDateTime start = LocalDateTime.now().minusYears(5);\n            final IntStream indexes = IntStream.range(0, arrayLength).parallel();\n            final Stream<LocalDateTime> dates = indexes.mapToObj(start::plusMinutes);\n            final LocalDateTime[] array = dates.toArray(LocalDateTime[]::new);\n            Stream.of(array).parallel().forEach(value -> {\n                if (value.getDayOfWeek() == DayOfWeek.MONDAY) {\n                    count.incrementAndGet();\n                }\n            });\n            return array;\n        });\n\n        tasks.put(\"Morpheus(Par)\", () -> {\n            final AtomicInteger count = new AtomicInteger();\n            final LocalDateTime start = LocalDateTime.now().minusYears(5);\n            final Array<LocalDateTime> array = Array.of(LocalDateTime.class, arrayLength);\n            array.parallel().applyValues(v -> start.plusMinutes(v.index()));\n            array.parallel().forEach(value -> {\n                if (value.getDayOfWeek() == DayOfWeek.MONDAY) {\n                    count.incrementAndGet();\n                }\n            });\n            return array;\n        });\n\n    });\n\n    String label = String.valueOf(arrayLength);\n    results.data().setDouble(label, \"Native(Seq)\", timing.data().getDouble(\"Mean\", \"Native(Seq)\"));\n    results.data().setDouble(label, \"Morpheus(Seq)\", timing.data().getDouble(\"Mean\", \"Morpheus(Seq)\"));\n    results.data().setDouble(label, \"Native(Par)\", timing.data().getDouble(\"Mean\", \"Native(Par)\"));\n    results.data().setDouble(label, \"Morpheus(Par)\", timing.data().getDouble(\"Mean\", \"Morpheus(Par)\"));\n\n});\n\n//Create title from template\nfinal String prefix = \"LocalDateTime Array Initialization + Traversal Times\";\nfinal String title = prefix + (includeGC ? \" (including-GC)\" : \" (excluding-GC)\");\n\n//Record chart to file\nfinal String fileSuffix = includeGC ? \"2.png\" : \"1.png\";\nfinal String filePrefix = \"./docs/images/native-vs-morpheus-array-sequential-vs-parallel\";\n\n//Plot results as a bar chart\nChart.create().withBarPlot(results, false, chart -> {\n    chart.title().withText(title);\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.plot().axes().domain().label().withText(\"Array Length\");\n    chart.plot().axes().range(0).label().withText(\"Time (Milliseconds)\");\n    chart.legend().on();\n    chart.writerPng(new File(filePrefix + fileSuffix), 845, 400, true);\n    chart.show();\n});",
            "title": "Iteration &amp; Boxing"
        },
        {
            "location": "/array/performance/#sorting",
            "text": "This section compares some performance statistics of sorting large Morpheus arrays versus their \nnative counterparts. The Morpheus library is built around the highly versatile array sorting API \nin  FastUtil  developed by  Sebastiano Vigna .\nThe results below compare performance versus the JDK  java.util.Arrays.sort()  and  parallelSort()  \nfunctions, which it has to be said, is not an entirely fair benchmark.  Firstly, the JDK array sorting algorithm is based on a dual-pivot quick sort whereas FastUtil is based\non a more traditional single-pivot quick sort. Secondly, the JDK implementation is presented with\nthe array directly, allowing everything to be inlined, and also enabling sliced work arrays to\nbe constructed as required. FastUtil on the other hand is not presented with the data directly, \nbut interacts with it via an  IntComparator  and  Swapper , which does incur method call over head, \nbut also makes it far more versatile. The JDK sorting functionality is limited to only sorting arrays \nin their natural order, while FastUtil allows the sorting logic to be fully customized.  The results in Figure 9 and 10 demonstrate two very different outcomes, the former comparing performance\non a primitive  double  array, the latter on an  LocalDateTime  array (in both cases these arrays are\nrandomized before the tests are run). In the case of the double array, the JDK  Arrays  implementation\nappears to offer roughly twice the performance of FastUtil as used in Morpheus. For the  LocalDateTime \ntest, the tables are turned, and FastUtil vastly outperforms the native  Arrays  call, and these times\ndo not include any garbage collection costs incurred after the each test is completed.  \n       Figure 9. Sorting times for an array of 10 million random double precision values  In the  LocalDatTime  results below, Morpheus far outperforms the native array because it is only \noperating on the internal primitive array of longs. No boxing is required here, so hence the vast\nperformance gap. While this is not an entirely fair comparison, it demonstrates the benefits of\nstoring everything as primitives.   \n       Figure 10. Same test as in figure 9, but using an array of randomly ordered LocalDateTimes  The code to generate the results in figure 9 is as follows:   Range<Integer> arrayLengths = Range.of(1, 11).map(i -> i * 100000);\nArray<String> labels = Array.ofStrings(\"Native(Seq)\", \"Morpheus(Seq)\", \"Native(Par)\", \"Morpheus(Par)\");\nDataFrame<String,String> results = DataFrame.ofDoubles(arrayLengths.map(String::valueOf), labels);\n\narrayLengths.forEach(length -> {\n\n    System.out.println(\"Running sort test for array length \" + length);\n    double[] array1 = new double[length];\n    Array<Double> array2 = Array.of(Double.class, length);\n\n    DataFrame<String,String> timing = PerfStat.run(sample, TimeUnit.MILLISECONDS, false, tasks -> {\n        tasks.put(\"Native(Seq)\", () -> { Arrays.sort(array1); return array1; });\n        tasks.put(\"Morpheus(Seq)\", () -> array2.sort(true) );\n        tasks.put(\"Native(Par)\", () -> { Arrays.parallelSort(array1); return array1; });\n        tasks.put(\"Morpheus(Par)\", () -> array2.parallel().sort(true));\n        tasks.beforeEach(() -> {\n            array2.applyDoubles(v -> Math.random());\n            array2.forEachValue(v -> array1[v.index()] = v.getDouble());\n        });\n    });\n\n    String label = String.valueOf(length);\n    results.data().setDouble(label, \"Native(Seq)\", timing.data().getDouble(\"Mean\", \"Native(Seq)\"));\n    results.data().setDouble(label, \"Morpheus(Seq)\", timing.data().getDouble(\"Mean\", \"Morpheus(Seq)\"));\n    results.data().setDouble(label, \"Native(Par)\", timing.data().getDouble(\"Mean\", \"Native(Par)\"));\n    results.data().setDouble(label, \"Morpheus(Par)\", timing.data().getDouble(\"Mean\", \"Morpheus(Par)\"));\n});\n\nChart.create().withBarPlot(results, false, chart -> {\n    chart.title().withText(\"Sorting Performance for Array of Random LocalDateTimes (Sample \" + sample + \")\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 16));\n    chart.subtitle().withText(\"Dual-Pivot Quick Sort (Native) vs Single-Pivot Quick Sort (FastUtil)\");\n    chart.subtitle().withFont(new Font(\"Verdana\", Font.PLAIN, 14));\n    chart.plot().axes().domain().label().withText(\"Array Length\");\n    chart.plot().axes().range(0).label().withText(\"Total Time in Milliseconds\");\n    chart.legend().on();\n    chart.show();\n});",
            "title": "Sorting"
        },
        {
            "location": "/array/performance/#summary-statistics",
            "text": "Morpheus provides an API to calculate various descriptive statistics on numerical arrays, such\nas min, max, variance, skew, kurtosis, auto correlation and so on. The chart below shows the\ncalculation times for these quantities on a Morpheus array of 10 million random double precision\nvalues. The code used to generate this plot is also included.  \n       Figure 11. Calculation times for various summary statistics on a Morpheus array with 10 million elements  The code to generate these results is as follows:   final int count = 10;\nfinal int size = 10000000;\nfinal Array<Double> array = Array.of(Double.class, size).applyDoubles(v -> Math.random() * 100);\n\nfinal DataFrame<String,String> times = PerfStat.run(count, TimeUnit.MILLISECONDS, true, tasks -> {\n    tasks.put(\"Min\", () -> array.stats().min());\n    tasks.put(\"Max\", () -> array.stats().max());\n    tasks.put(\"Mean\", () -> array.stats().mean());\n    tasks.put(\"Count\", () -> array.stats().count());\n    tasks.put(\"Variance\", () -> array.stats().variance());\n    tasks.put(\"StdDev\", () -> array.stats().stdDev());\n    tasks.put(\"Sum\", () -> array.stats().sum());\n    tasks.put(\"Skew\", () -> array.stats().skew());\n    tasks.put(\"Kurtosis\", () -> array.stats().kurtosis());\n    tasks.put(\"Median\", () -> array.stats().median());\n    tasks.put(\"95th Percentile\", () -> array.stats().percentile(95));\n    tasks.put(\"AutCorrelation(20)\", () -> array.stats().autocorr(20));\n});\n\nChart.create().withBarPlot(times.rows().select(\"Mean\").transpose(), false, chart -> {\n    chart.title().withText(\"Morpheus Array Statistic Calculation Times, 10 Million Entries (Sample 10)\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.plot().axes().domain().label().withText(\"Stat Type\");\n    chart.plot().axes().range(0).label().withText(\"Time (Milliseconds)\");\n    chart.plot().orient().horizontal();\n    chart.legend().off();\n    chart.show();\n});",
            "title": "Summary Statistics"
        },
        {
            "location": "/frame/construction/",
            "text": "Introduction\n\n\nA \nDataFrame\n is a column storage optimized structure where each column is configured to hold a specific data type,\nand can either be densely or sparsely populated. Once created, a \nDataFrame\n can be reshaped by adding additional\nrows and columns, with restrictions in some cases, which are discussed later.\n\n\nMore often than not, a \nDataFrame\n will likely be initialized from the contents of a file or a database, but it\nis also possible to build one incrementally through the API. The following sections describe various ways in which\na DataFrame can be created.\n\n\nConstruction\n\n\nProgrammatically\n\n\nThere are a number of static methods on the DataFrame interface that can be used create frames that are optimized\nto hold specific types of data. For example, a commonly used frame for numerical analysis would be one optimized\nto hold double precision values. The code below illustrates various common construction calls for the supported\ndata types.\n\n\n\n\n\nimport com.zavtech.morpheus.array.Array;\nimport com.zavtech.morpheus.frame.DataFrame;\nimport com.zavtech.morpheus.range.Range;\n\n//Create a list of column keys\nIterable<Month> months = Array.of(Month.class, Month.values());\n//Create a list of row keys\nIterable<Year> years = Range.of(1995, 2000).map(Year::of);\n\n//Create frames optimized to hold various types of data.\nDataFrame<Year,Month> booleans = DataFrame.ofBooleans(years, months);\nDataFrame<Year,Month> integers = DataFrame.ofInts(years, months);\nDataFrame<Year,Month> longs = DataFrame.ofLongs(years, months);\nDataFrame<Year,Month> doubles = DataFrame.ofDoubles(years, months);\nDataFrame<Year,Month> objects = DataFrame.ofValues(years, months);\n\n\n\n\nSingle Column\n\n\nFactory methods are provided to easily create a frame with one column.\n\n\n\n\n\n//Create a frame with a single column initially\nDataFrame<Year,Month> booleans = DataFrame.ofBooleans(years, Month.JANUARY);\nDataFrame<Year,Month> integers = DataFrame.ofInts(years, Month.JANUARY);\nDataFrame<Year,Month> longs = DataFrame.ofLongs(years, Month.JANUARY);\nDataFrame<Year,Month> doubles = DataFrame.ofDoubles(years, Month.JANUARY);\nDataFrame<Year,Month> objects = DataFrame.ofValues(years, Month.JANUARY);\n\n\n\n\nSingle Row\n\n\nFactory methods are provided to easily create a frame with one row.\n\n\n\n\n\n//Create a frame with a single row initially\nDataFrame<Year,Month> booleans = DataFrame.ofBooleans(Year.of(2014), months);\nDataFrame<Year,Month> integers = DataFrame.ofInts(Year.of(2014), months);\nDataFrame<Year,Month> longs = DataFrame.ofLongs(Year.of(2014), months);\nDataFrame<Year,Month> doubles = DataFrame.ofDoubles(Year.of(2014), months);\nDataFrame<Year,Month> objects = DataFrame.ofValues(Year.of(2014), months);\n\n\n\n\nMixed Column Types\n\n\nThe example below demonstrates how to create a \nDataFrame\n where each column is configured to hold\na different data type, similar to the way a SQL table might be structured. In this case, 5 columns \nare created via calls to \ncolumns.add()\n which takes the column key and the data type for the column. \nIn addition, the column values in this example are initialized with random values via the type specific \n\napplyXXX()\n methods.\n\n\n\n\n\n//Create a frame with 5 columns each optimized for a different data type and randomly initialized values\nRandom rand = new java.util.Random();\nDataFrame<Year,Month> randomFrame = DataFrame.of(years, Month.class, columns -> {\n    columns.add(Month.JANUARY, Boolean.class).applyBooleans(v -> rand.nextBoolean()); \n    columns.add(Month.MARCH, Integer.class).applyInts(v -> rand.nextInt());       \n    columns.add(Month.JUNE, Long.class).applyLongs(v -> rand.nextLong());           \n    columns.add(Month.SEPTEMBER, Double.class).applyDoubles(v -> rand.nextDouble());    \n    columns.add(Month.DECEMBER, Object.class).applyValues(v -> String.valueOf(rand.nextDouble()));\n});\n\n\n\n\nOn occasion, it might be desirable to create an empty DataFrame which can be expanded over time, by adding\nkeys to the frame's row and column axis. Methods are provided to add keys individually or in bulk via \n\nIterable\n as shown below. When adding columns, the class representing the data type for that column must\nbe specified.\n\n\n\n\n\n//Create an empty frame with initial capacity, then add rows and columns\nDataFrame<Year,Month> frame = DataFrame.empty(Year.class, Month.class);\nframe.rows().add(Year.of(1975));\nframe.rows().add(Year.of(1980));\nframe.rows().addAll(Range.of(1995, 2014).map(Year::of));\nframe.cols().add(Month.JANUARY, Double.class);\nframe.cols().add(Month.MARCH, Double.class);\nframe.cols().addAll(Array.of(Month.APRIL, Month.JULY), Double.class);\n\n\n\n\nFrom CSV\n\n\nThe Morpheus library includes an API to initialise a \nDataFrame\n from a CSV data source. The parser \nwill attempt to guess the column types from the first N rows in the sample (which is configurable), \nin the absence of explicitly configured column types. It supports CSV formats with and without a header,\nand can optionally utilize a user provided function to generate the keys for the row axis. If a row key\ngeneration function is not provided, a sequence of integers will be used by default. The following section \nillustrates various parsing examples, beginning with simple cases and progressing to more elaborate \nexamples.\n\n\n\n\n\n//Parse file or classpath resource, with first row as header\nDataFrame<Integer,String> frame1 = DataFrame.readCsv(\"/temp/data.csv\");\n\n//Parse URL, with first row as header\nDataFrame<Integer,String> frame2 = DataFrame.readCsv(\"http://www.domain.com/data?file.csv\");\n\n//Parse file, with first row as header, and row keys parsed as LocalDates from the first column, index=0\nDataFrame<LocalDate,String> frame3 = DataFrame.readCsv(options -> {\n    options.withResource(\"/temp/data.csv\");\n    options.withRowKeyParser(LocalDate.class, row -> LocalDate.parse(row[0]));\n});\n\n\n\n\nThe third case illustrated above demonstrates the \nDataFrame.readCsv()\n method that uses a lambda expression\nwhich accepts a \nCSVRequest\n object that can be used to specify all sorts of customizations regarding how the\nCSV data is parsed into a \nDataFrame\n. The features supported by the request descriptor are as follows:\n\n\n\n\nColumn specific parsers\n\n\nFunction to create row keys from raw tokens in a row\n\n\nPredicate to select a subset of rows\n\n\nPredicate to select a subset of columns by index\n\n\nPredicate to select a subset of columns by column name\n\n\nSpecific character encoding, e.g. UTF-16\n\n\nRow count batch size which can influence performance\n\n\nParallel processing of CSV content to improve performance for large files.\n\n\n\n\nLet's consider a real-world example where we wish to parse a CSV file from Yahoo Finance which contains historical\nprices for the S&P 500 index. A sample of the file can be downloaded \nhere\n and the first 10 rows in the file\nare shown below.\n\n\n\nDate,Open,High,Low,Close,Volume,Adj Close\n2014-06-06,194.869995,195.429993,194.779999,195.380005,78696000,185.713099\n2014-06-05,193.410004,194.649994,192.699997,194.449997,92103000,184.829105\n2014-06-04,192.470001,193.300003,192.270004,193.190002,55529000,183.631452\n2014-06-03,192.429993,192.899994,192.25,192.800003,65047000,183.260749\n2014-06-02,192.949997,192.990005,191.970001,192.899994,64656000,183.355793\n2014-05-30,192.190002,192.800003,192.029999,192.679993,76316000,183.146676\n2014-05-29,191.820007,192.399994,191.330002,192.369995,64377000,182.852017\n2014-05-28,191.520004,191.820007,191.059998,191.380005,66723000,181.911009\n2014-05-27,191.059998,191.580002,190.949997,191.520004,72010000,182.044081\n2014-05-23,189.759995,190.479996,189.589996,190.350006,61092800,180.931972\n\n\n\n\nThe code below will parse this into a \nDataFrame\n with a row axis made up of \nLocalDate\n objects as per\nthe row key function. A specific parser is defined for the \nVolume\n column in order to force the type\nto a \nlong\n, and the rest of the columns are left to the default behaviour, which in this case will\nall resolve to \ndouble\n type.\n\n\n\n\n\nDateTimeFormatter dateFormat = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\");\nString url = \"http://chart.finance.yahoo.com/table.csv?s=SPY&a=0&b=1&c=2013&d=5&e=6&f=2014&g=d&ignore=.csv\";\nDataFrame<LocalDate,String> frame = DataFrame.readCsv(options -> {\n    options.withResource(url);\n    options.withRowKeyParser(LocalDate.class, values -> LocalDate.parse(values[0], dateFormat));\n    options.withParser(\"Volume\", v -> v == null ? 0L : Long.parseLong(v));\n});\n\n\n\n\n\n   Index     |     Date     |     Open     |     High     |     Low      |    Close     |   Volume   |  Adj Close   |\n---------------------------------------------------------------------------------------------------------------------\n 2014-06-06  |  2014-06-06  |  194.869995  |  195.429993  |  194.779999  |  195.380005  |  78696000  |  184.624915  |\n 2014-06-05  |  2014-06-05  |  193.410004  |  194.649994  |  192.699997  |  194.449997  |  92103000  |  183.746101  |\n 2014-06-04  |  2014-06-04  |  192.470001  |  193.300003  |  192.270004  |  193.190002  |  55529000  |  182.555466  |\n 2014-06-03  |  2014-06-03  |  192.429993  |  192.899994  |    192.2500  |  192.800003  |  65047000  |  182.186934  |\n 2014-06-02  |  2014-06-02  |  192.949997  |  192.990005  |  191.970001  |  192.899994  |  64656000  |  182.281421  |\n 2014-05-30  |  2014-05-30  |  192.190002  |  192.800003  |  192.029999  |  192.679993  |  76316000  |   182.07353  |\n 2014-05-29  |  2014-05-29  |  191.820007  |  192.399994  |  191.330002  |  192.369995  |  64377000  |  181.780597  |\n 2014-05-28  |  2014-05-28  |  191.520004  |  191.820007  |  191.059998  |  191.380005  |  66723000  |  180.845103  |\n 2014-05-27  |  2014-05-27  |  191.059998  |  191.580002  |  190.949997  |  191.520004  |  72010000  |  180.977396  |\n 2014-05-23  |  2014-05-23  |  189.759995  |  190.479996  |  189.589996  |  190.350006  |  61092800  |  179.871803  |\n\n\n\n\nParallel Loading\n\n\nOther features of the \nCsvRequest\n discussed below can improve performance when reading very large files. The\nfirst is to turn on parallel processing and the second is to vary the batch size. The batch size can be\ninfluential, especially when the parser which converts a raw string from the CSV content to another type is an\nexpensive operation. \n\n\nThe chart below shows some performance statistics comparing parallel versus sequential loading of a 40MB file \ncontaining roughly 760,000 rows of CSV content.  While the absolute figures are very machine specific, the relative \ndifference does suggest that parallel loading can make a material improvement on a multi-core machine, which\nis pretty standard issue these days.\n\n\n\n    \n\n\n\n\n\nThe code to produce this plot is as follows:\n\n\n\n\n\nimport com.zavtech.morpheus.frame.DataFrame;\nimport com.zavtech.morpheus.viz.chart.Chart;\n\nfinal String path = \"/Users/witdxav/Dropbox/data/fxcm/AUDUSD/2012/AUDUSD-2012.csv\";\nfinal DateTimeFormatter dateFormat = DateTimeFormatter.ofPattern(\"yyyy.MM.dd\");\nfinal DateTimeFormatter timeFormat = DateTimeFormatter.ofPattern(\"HH:mm\");\n\nDataFrame<String,String> timingStats = PerfStat.run(5, TimeUnit.MILLISECONDS, false, tasks -> {\n\n    tasks.put(\"Sequential\", () -> DataFrame.read().<LocalDateTime>csv(options -> {\n        options.setHeader(false);\n        options.setParallel(false);\n        options.setResource(path);\n        options.setExcludeColumnIndexes(1);\n        options.setRowKeyParser(LocalDateTime.class, row -> {\n            final LocalDate date = LocalDate.parse(row[0], dateFormat);\n            final LocalTime time = LocalTime.parse(row[1], timeFormat);\n            return LocalDateTime.of(date, time);\n        });\n    }));\n\n    tasks.put(\"Parallel\", () -> DataFrame.read().<LocalDateTime>csv(options -> {\n        options.setHeader(false);\n        options.setParallel(true);\n        options.setResource(path);\n        options.setExcludeColumnIndexes(1);\n        options.setRowKeyParser(LocalDateTime.class, row -> {\n            final LocalDate date = LocalDate.parse(row[0], dateFormat);\n            final LocalTime time = LocalTime.parse(row[1], timeFormat);\n            return LocalDateTime.of(date, time);\n        });\n    }));\n\n});\n\nChart.create().withBarPlot(timingStats, false, chart -> {\n    chart.title().withText(\"CSV Parsing Performance (Sequential vs Parallel)\");\n    chart.subtitle().withText(\"File Size: 40MB, 760,000 lines, 6 columns\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 16));\n    chart.plot().axes().domain().label().withText(\"Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time in Milliseconds\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\nRow / Column Filtering\n\n\nMorpheus supports both CSV row and column filtering at load time, which can be convenient if you only\nneed to analyze a subsection of a very large file. The alternative would be to simply load the entire file,\nand then filter the \nDataFrame\n, but this could be very inefficient from a memory perspective. The example\nbelow demonstrates both row and column filtering of the \nYahoo Finance\n quote data set described above, \nwhere we limit the extraction to the \nOpen\n, \nClose\n and \nAdj Close\n columns, and also to only include \nrows that fall on a \nMonday\n.\n\n\n\n\n\nDateTimeFormatter dateFormat = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\");\nSet<String> columnSet = Collect.asSet(\"Open\", \"Close\", \"Adj Close\");\nString url = \"http://chart.finance.yahoo.com/table.csv?s=SPY&a=0&b=1&c=2013&d=5&e=6&f=2014&g=d&ignore=.csv\";\nDataFrame<LocalDate,String> frame = DataFrame.readCsv(options -> {\n    options.withResource(url);\n    options.withColNamePredicate(columnSet::contains);\n    options.withRowKeyParser(LocalDate.class, values -> LocalDate.parse(values[0], dateFormat));\n    options.withParser(\"Volume\", v -> v == null ? 0L : Long.parseLong(v));\n    options.withRowPredicate(row -> {\n        LocalDate date = LocalDate.parse(row[0], dateFormat);\n        return date.getDayOfWeek() == DayOfWeek.MONDAY;\n    });\n});\n\n\n\n\n\n   Index     |     Open     |    Close     |  Adj Close   |\n-----------------------------------------------------------\n 2014-06-02  |  192.949997  |  192.899994  |  182.281421  |\n 2014-05-19  |  187.690002  |  188.740005  |  178.350428  |\n 2014-05-12  |  188.800003  |  189.789993  |  179.342617  |\n 2014-05-05  |  187.139999  |  188.419998  |  178.048036  |\n 2014-04-28  |  187.050003  |  186.880005  |  176.592815  |\n 2014-04-21  |  186.440002  |  187.039993  |  176.743996  |\n 2014-04-14  |  182.929993  |  182.940002  |  172.869698  |\n 2014-04-07  |  185.949997  |  184.339996  |  174.192626  |\n 2014-03-31  |  186.669998  |  187.009995  |  176.715649  |\n 2014-03-24  |  186.839996  |  185.429993  |  175.222621  |\n\n\n\n\nFrom JSON\n\n\nThe Morpheus library has the ability to read and write a \nDataFrame\n to a JSON format, which makes it easy\nto integrate with a web application. The parser leverages the \nGoogle GSON\n\nlibrary, and specifically the stream based API for both reading and writing, which allows it to handle very large\nJSON files. An example of the format is shown below, which depicts a frame with 3 rows and 2 columns, a row axis with\nkeys of type \nLocalDate\n, and a column of booleans and a column of integers.\n\n\n\n\n\n{\n  \"DataFrame\": {\n    \"rowCount\": 3,\n    \"colCount\": 2,\n    \"rowKeys\": {\n      \"type\": \"LocalDate\",\n      \"values\": [\n        \"2015-12-07\",\n        \"2015-12-08\",\n        \"2015-12-09\",\n      ]\n    },\n    \"columns\": [\n      {\n        \"key\": \"ColumnName1\",\n        \"keyType\": \"String\",\n        \"dataType\": \"boolean\",\n        \"nullValue\": \"false\",\n        \"values\": [\n          \"true\",\n          \"false\",\n          \"true\",\n        ]\n      },\n      {\n        \"key\": \"ColumnName2\",\n        \"keyType\": \"String\",\n        \"dataType\": \"int\",\n        \"nullValue\": \"0\",\n        \"values\": [\n          \"884841579\",\n          \"72476677\",\n          \"478622964\",\n        ]\n      }\n   ]\n  }\n}\n\n\n\n\nTwo static \nDataFrame.fromJson()\n functions exist to load a frame from a file, URL or classpath resource,\nin much the same way as the CSV functions. Row and column predicates can be used to select a subset of the\ndata as required. The following code demonstrates some basic examples:\n\n\n\n\n\n//Parse a file, or classpath resource from Morpheus JSON format\nDataFrame<LocalDate,String> frame = DataFrame.readJson(\"/temp/data.json\");\n\n\n\n\nTo select a subset of rows and columns, apply predicates to the request as below.\n\n\n\n\n\n//Parse a file, or classpath resource from Morpheus JSON format, selecting only a subset of rows & columns\nfinal Set<String> columns = Stream.of(\"Date\", \"PostCode\", \"Street\", \"County\").collect(Collectors.toSet());\nDataFrame<LocalDate,String> frame = DataFrame.readJson(options -> {\n    options.withResource(\"/temp/data.json\");\n    options.withCharset(StandardCharsets.UTF_16);\n    options.withRowPredicate(rowKey -> rowKey.getDayOfWeek() == DayOfWeek.MONDAY);\n    options.withColPredicate(columns::contains);\n});\n\n\n\n\nFrom SQL\n\n\nThe Morpheus library ships with a \nDataFrameSource\n implementation designed to initialize \nDataFrames\n from\nthe results of a SQL query. The following section illustrates various examples of how the API can be used\nin this regard. \n\n\nAs a convenience, a static \nreadDb()\n method exists on the \nDataFrame\n class which takes as an argument a \nfunction that configures the details of the request. At a minimum, two inputs are required on the request \ndescriptor, namely the database connection (or the details of how to establish a connection), and a SQL \nexpression. Many other aspects of the request can be tailored, but the most basic example is as follows:\n\n\n\n\n\nimport com.zavtech.morpheus.frame.DataFrame;\n\n//Ensure the JDBC driver is loaded \nClass.forName(\"org.h2.Driver\");\n\n//Create a frame from a select statement\nDataFrame<Integer,String> frame = DataFrame.readDb(options -> {\n    options.withConnection(\"jdbc:h2://databases/testDb\", \"sa\", null);\n    options.withSql(\"select * from Customer where city = 'London'\");\n});\n\n\n\n\nBy default, the first column in the SQL \nResultSet\n is used to initialize the row keys of the \nDataFrame\n,\nand in the example above, this amounts to the unique id of the Customer record. The API does provide\nfor greater control by supporting a user defined function that can be applied to generate the key for \neach row in the SQL \nResultSet\n. A good example of where this would be necessary is the case of a \ncomposite key.\n\n\n\n\n\nimport javax.sql.DataSource;\nimport com.zavtech.morpheus.util.Tuple;\nimport com.zavtech.morpheus.frame.DataFrame;\n\n// Join products and inventory to see what we have where\njavax.sql.DataSource dataSource = getDataSource();\nDataFrame<Tuple,String> frame = DataFrame.readDb(options -> {\n    options.withConnection(dataSource);\n    options.withSql(\"select * from Product t1 inner join Inventory t2 on t1.productId = t2.productId\");\n    options.withExcludeColumns(\"productId\", \"warehouseId\");  //not need as part of the key\n    options.withRowCapacity(1000);\n    options.withRowKeyFunction(rs -> {\n        String productId = rs.getString(\"productId\");\n        String warehouseId = rs.getString(\"warehouseId\");\n        return Tuple.of(warehouseId, productId);\n    });\n});\n\n\n\n\nIn the above example, because we have supplied our own function to generate a row key from a record in the \n\nResultSet\n, we know the resulting frame with be parameterized as \n<Tuple,String>\n. By default, frames\ngenerated from a database query will always have a \nString\n based column axis representing the column\nnames in the \nResultSet\n, but these can obviously be mapped to some other type post construction.\n\n\nIt is often desirable to transform data read from a SQL \nResultSet\n so that it can be typed more appropriately\nor modified in some way. The Morpheus API provides the capability to configure column specific extractors\nto perform such transformations. Consider the example below, where we query Customer records which have a\n\nVARCHAR\n column used to capture a user's time zone, and we wish to convert this to a \nZoneId\n within \nthe resulting \nDataFrame\n. This example also demonstrates a parameterized query where arguments are\napplied using the \nwithParameters()\n method on the request descriptor.\n\n\n\n\n\nimport java.time.LocalDate;\nimport java.time.ZoneId;\nimport javax.sql.DataSource;\nimport com.zavtech.morpheus.frame.DataFrame;\nimport com.zavtech.morpheus.util.sql.SQLExtractor;\n\n//Convert the user's time zone into a ZoneId \njavax.sql.DataSource dataSource = getDataSource();\nDataFrame<Integer,String> frame = DataFrame.readDb(options -> {\n    options.withConnection(dataSource);\n    options.withSql(\"select * from Customer where city = ? and dob > ?\");\n    options.withParameters(\"London\", LocalDate.of(1970, 1, 1));\n    options.withExtractor(\"UserZone\", SQLExtractor.with(ZoneId.class, (rs, colIndex) -> {\n        final String tzName = rs.getString(colIndex);\n        return tzName != null ? ZoneId.of(tzName) : null;\n    }));\n});",
            "title": "Construction"
        },
        {
            "location": "/frame/construction/#introduction",
            "text": "A  DataFrame  is a column storage optimized structure where each column is configured to hold a specific data type,\nand can either be densely or sparsely populated. Once created, a  DataFrame  can be reshaped by adding additional\nrows and columns, with restrictions in some cases, which are discussed later.  More often than not, a  DataFrame  will likely be initialized from the contents of a file or a database, but it\nis also possible to build one incrementally through the API. The following sections describe various ways in which\na DataFrame can be created.",
            "title": "Introduction"
        },
        {
            "location": "/frame/construction/#construction",
            "text": "",
            "title": "Construction"
        },
        {
            "location": "/frame/construction/#programmatically",
            "text": "There are a number of static methods on the DataFrame interface that can be used create frames that are optimized\nto hold specific types of data. For example, a commonly used frame for numerical analysis would be one optimized\nto hold double precision values. The code below illustrates various common construction calls for the supported\ndata types.   import com.zavtech.morpheus.array.Array;\nimport com.zavtech.morpheus.frame.DataFrame;\nimport com.zavtech.morpheus.range.Range;\n\n//Create a list of column keys\nIterable<Month> months = Array.of(Month.class, Month.values());\n//Create a list of row keys\nIterable<Year> years = Range.of(1995, 2000).map(Year::of);\n\n//Create frames optimized to hold various types of data.\nDataFrame<Year,Month> booleans = DataFrame.ofBooleans(years, months);\nDataFrame<Year,Month> integers = DataFrame.ofInts(years, months);\nDataFrame<Year,Month> longs = DataFrame.ofLongs(years, months);\nDataFrame<Year,Month> doubles = DataFrame.ofDoubles(years, months);\nDataFrame<Year,Month> objects = DataFrame.ofValues(years, months);",
            "title": "Programmatically"
        },
        {
            "location": "/frame/construction/#single-column",
            "text": "Factory methods are provided to easily create a frame with one column.   //Create a frame with a single column initially\nDataFrame<Year,Month> booleans = DataFrame.ofBooleans(years, Month.JANUARY);\nDataFrame<Year,Month> integers = DataFrame.ofInts(years, Month.JANUARY);\nDataFrame<Year,Month> longs = DataFrame.ofLongs(years, Month.JANUARY);\nDataFrame<Year,Month> doubles = DataFrame.ofDoubles(years, Month.JANUARY);\nDataFrame<Year,Month> objects = DataFrame.ofValues(years, Month.JANUARY);",
            "title": "Single Column"
        },
        {
            "location": "/frame/construction/#single-row",
            "text": "Factory methods are provided to easily create a frame with one row.   //Create a frame with a single row initially\nDataFrame<Year,Month> booleans = DataFrame.ofBooleans(Year.of(2014), months);\nDataFrame<Year,Month> integers = DataFrame.ofInts(Year.of(2014), months);\nDataFrame<Year,Month> longs = DataFrame.ofLongs(Year.of(2014), months);\nDataFrame<Year,Month> doubles = DataFrame.ofDoubles(Year.of(2014), months);\nDataFrame<Year,Month> objects = DataFrame.ofValues(Year.of(2014), months);",
            "title": "Single Row"
        },
        {
            "location": "/frame/construction/#mixed-column-types",
            "text": "The example below demonstrates how to create a  DataFrame  where each column is configured to hold\na different data type, similar to the way a SQL table might be structured. In this case, 5 columns \nare created via calls to  columns.add()  which takes the column key and the data type for the column. \nIn addition, the column values in this example are initialized with random values via the type specific  applyXXX()  methods.   //Create a frame with 5 columns each optimized for a different data type and randomly initialized values\nRandom rand = new java.util.Random();\nDataFrame<Year,Month> randomFrame = DataFrame.of(years, Month.class, columns -> {\n    columns.add(Month.JANUARY, Boolean.class).applyBooleans(v -> rand.nextBoolean()); \n    columns.add(Month.MARCH, Integer.class).applyInts(v -> rand.nextInt());       \n    columns.add(Month.JUNE, Long.class).applyLongs(v -> rand.nextLong());           \n    columns.add(Month.SEPTEMBER, Double.class).applyDoubles(v -> rand.nextDouble());    \n    columns.add(Month.DECEMBER, Object.class).applyValues(v -> String.valueOf(rand.nextDouble()));\n});  On occasion, it might be desirable to create an empty DataFrame which can be expanded over time, by adding\nkeys to the frame's row and column axis. Methods are provided to add keys individually or in bulk via  Iterable  as shown below. When adding columns, the class representing the data type for that column must\nbe specified.   //Create an empty frame with initial capacity, then add rows and columns\nDataFrame<Year,Month> frame = DataFrame.empty(Year.class, Month.class);\nframe.rows().add(Year.of(1975));\nframe.rows().add(Year.of(1980));\nframe.rows().addAll(Range.of(1995, 2014).map(Year::of));\nframe.cols().add(Month.JANUARY, Double.class);\nframe.cols().add(Month.MARCH, Double.class);\nframe.cols().addAll(Array.of(Month.APRIL, Month.JULY), Double.class);",
            "title": "Mixed Column Types"
        },
        {
            "location": "/frame/construction/#from-csv",
            "text": "The Morpheus library includes an API to initialise a  DataFrame  from a CSV data source. The parser \nwill attempt to guess the column types from the first N rows in the sample (which is configurable), \nin the absence of explicitly configured column types. It supports CSV formats with and without a header,\nand can optionally utilize a user provided function to generate the keys for the row axis. If a row key\ngeneration function is not provided, a sequence of integers will be used by default. The following section \nillustrates various parsing examples, beginning with simple cases and progressing to more elaborate \nexamples.   //Parse file or classpath resource, with first row as header\nDataFrame<Integer,String> frame1 = DataFrame.readCsv(\"/temp/data.csv\");\n\n//Parse URL, with first row as header\nDataFrame<Integer,String> frame2 = DataFrame.readCsv(\"http://www.domain.com/data?file.csv\");\n\n//Parse file, with first row as header, and row keys parsed as LocalDates from the first column, index=0\nDataFrame<LocalDate,String> frame3 = DataFrame.readCsv(options -> {\n    options.withResource(\"/temp/data.csv\");\n    options.withRowKeyParser(LocalDate.class, row -> LocalDate.parse(row[0]));\n});  The third case illustrated above demonstrates the  DataFrame.readCsv()  method that uses a lambda expression\nwhich accepts a  CSVRequest  object that can be used to specify all sorts of customizations regarding how the\nCSV data is parsed into a  DataFrame . The features supported by the request descriptor are as follows:   Column specific parsers  Function to create row keys from raw tokens in a row  Predicate to select a subset of rows  Predicate to select a subset of columns by index  Predicate to select a subset of columns by column name  Specific character encoding, e.g. UTF-16  Row count batch size which can influence performance  Parallel processing of CSV content to improve performance for large files.   Let's consider a real-world example where we wish to parse a CSV file from Yahoo Finance which contains historical\nprices for the S&P 500 index. A sample of the file can be downloaded  here  and the first 10 rows in the file\nare shown below.  \nDate,Open,High,Low,Close,Volume,Adj Close\n2014-06-06,194.869995,195.429993,194.779999,195.380005,78696000,185.713099\n2014-06-05,193.410004,194.649994,192.699997,194.449997,92103000,184.829105\n2014-06-04,192.470001,193.300003,192.270004,193.190002,55529000,183.631452\n2014-06-03,192.429993,192.899994,192.25,192.800003,65047000,183.260749\n2014-06-02,192.949997,192.990005,191.970001,192.899994,64656000,183.355793\n2014-05-30,192.190002,192.800003,192.029999,192.679993,76316000,183.146676\n2014-05-29,191.820007,192.399994,191.330002,192.369995,64377000,182.852017\n2014-05-28,191.520004,191.820007,191.059998,191.380005,66723000,181.911009\n2014-05-27,191.059998,191.580002,190.949997,191.520004,72010000,182.044081\n2014-05-23,189.759995,190.479996,189.589996,190.350006,61092800,180.931972  The code below will parse this into a  DataFrame  with a row axis made up of  LocalDate  objects as per\nthe row key function. A specific parser is defined for the  Volume  column in order to force the type\nto a  long , and the rest of the columns are left to the default behaviour, which in this case will\nall resolve to  double  type.   DateTimeFormatter dateFormat = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\");\nString url = \"http://chart.finance.yahoo.com/table.csv?s=SPY&a=0&b=1&c=2013&d=5&e=6&f=2014&g=d&ignore=.csv\";\nDataFrame<LocalDate,String> frame = DataFrame.readCsv(options -> {\n    options.withResource(url);\n    options.withRowKeyParser(LocalDate.class, values -> LocalDate.parse(values[0], dateFormat));\n    options.withParser(\"Volume\", v -> v == null ? 0L : Long.parseLong(v));\n});  \n   Index     |     Date     |     Open     |     High     |     Low      |    Close     |   Volume   |  Adj Close   |\n---------------------------------------------------------------------------------------------------------------------\n 2014-06-06  |  2014-06-06  |  194.869995  |  195.429993  |  194.779999  |  195.380005  |  78696000  |  184.624915  |\n 2014-06-05  |  2014-06-05  |  193.410004  |  194.649994  |  192.699997  |  194.449997  |  92103000  |  183.746101  |\n 2014-06-04  |  2014-06-04  |  192.470001  |  193.300003  |  192.270004  |  193.190002  |  55529000  |  182.555466  |\n 2014-06-03  |  2014-06-03  |  192.429993  |  192.899994  |    192.2500  |  192.800003  |  65047000  |  182.186934  |\n 2014-06-02  |  2014-06-02  |  192.949997  |  192.990005  |  191.970001  |  192.899994  |  64656000  |  182.281421  |\n 2014-05-30  |  2014-05-30  |  192.190002  |  192.800003  |  192.029999  |  192.679993  |  76316000  |   182.07353  |\n 2014-05-29  |  2014-05-29  |  191.820007  |  192.399994  |  191.330002  |  192.369995  |  64377000  |  181.780597  |\n 2014-05-28  |  2014-05-28  |  191.520004  |  191.820007  |  191.059998  |  191.380005  |  66723000  |  180.845103  |\n 2014-05-27  |  2014-05-27  |  191.059998  |  191.580002  |  190.949997  |  191.520004  |  72010000  |  180.977396  |\n 2014-05-23  |  2014-05-23  |  189.759995  |  190.479996  |  189.589996  |  190.350006  |  61092800  |  179.871803  |",
            "title": "From CSV"
        },
        {
            "location": "/frame/construction/#parallel-loading",
            "text": "Other features of the  CsvRequest  discussed below can improve performance when reading very large files. The\nfirst is to turn on parallel processing and the second is to vary the batch size. The batch size can be\ninfluential, especially when the parser which converts a raw string from the CSV content to another type is an\nexpensive operation.   The chart below shows some performance statistics comparing parallel versus sequential loading of a 40MB file \ncontaining roughly 760,000 rows of CSV content.  While the absolute figures are very machine specific, the relative \ndifference does suggest that parallel loading can make a material improvement on a multi-core machine, which\nis pretty standard issue these days.  \n       The code to produce this plot is as follows:   import com.zavtech.morpheus.frame.DataFrame;\nimport com.zavtech.morpheus.viz.chart.Chart;\n\nfinal String path = \"/Users/witdxav/Dropbox/data/fxcm/AUDUSD/2012/AUDUSD-2012.csv\";\nfinal DateTimeFormatter dateFormat = DateTimeFormatter.ofPattern(\"yyyy.MM.dd\");\nfinal DateTimeFormatter timeFormat = DateTimeFormatter.ofPattern(\"HH:mm\");\n\nDataFrame<String,String> timingStats = PerfStat.run(5, TimeUnit.MILLISECONDS, false, tasks -> {\n\n    tasks.put(\"Sequential\", () -> DataFrame.read().<LocalDateTime>csv(options -> {\n        options.setHeader(false);\n        options.setParallel(false);\n        options.setResource(path);\n        options.setExcludeColumnIndexes(1);\n        options.setRowKeyParser(LocalDateTime.class, row -> {\n            final LocalDate date = LocalDate.parse(row[0], dateFormat);\n            final LocalTime time = LocalTime.parse(row[1], timeFormat);\n            return LocalDateTime.of(date, time);\n        });\n    }));\n\n    tasks.put(\"Parallel\", () -> DataFrame.read().<LocalDateTime>csv(options -> {\n        options.setHeader(false);\n        options.setParallel(true);\n        options.setResource(path);\n        options.setExcludeColumnIndexes(1);\n        options.setRowKeyParser(LocalDateTime.class, row -> {\n            final LocalDate date = LocalDate.parse(row[0], dateFormat);\n            final LocalTime time = LocalTime.parse(row[1], timeFormat);\n            return LocalDateTime.of(date, time);\n        });\n    }));\n\n});\n\nChart.create().withBarPlot(timingStats, false, chart -> {\n    chart.title().withText(\"CSV Parsing Performance (Sequential vs Parallel)\");\n    chart.subtitle().withText(\"File Size: 40MB, 760,000 lines, 6 columns\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 16));\n    chart.plot().axes().domain().label().withText(\"Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time in Milliseconds\");\n    chart.legend().on();\n    chart.show();\n});",
            "title": "Parallel Loading"
        },
        {
            "location": "/frame/construction/#row-column-filtering",
            "text": "Morpheus supports both CSV row and column filtering at load time, which can be convenient if you only\nneed to analyze a subsection of a very large file. The alternative would be to simply load the entire file,\nand then filter the  DataFrame , but this could be very inefficient from a memory perspective. The example\nbelow demonstrates both row and column filtering of the  Yahoo Finance  quote data set described above, \nwhere we limit the extraction to the  Open ,  Close  and  Adj Close  columns, and also to only include \nrows that fall on a  Monday .   DateTimeFormatter dateFormat = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\");\nSet<String> columnSet = Collect.asSet(\"Open\", \"Close\", \"Adj Close\");\nString url = \"http://chart.finance.yahoo.com/table.csv?s=SPY&a=0&b=1&c=2013&d=5&e=6&f=2014&g=d&ignore=.csv\";\nDataFrame<LocalDate,String> frame = DataFrame.readCsv(options -> {\n    options.withResource(url);\n    options.withColNamePredicate(columnSet::contains);\n    options.withRowKeyParser(LocalDate.class, values -> LocalDate.parse(values[0], dateFormat));\n    options.withParser(\"Volume\", v -> v == null ? 0L : Long.parseLong(v));\n    options.withRowPredicate(row -> {\n        LocalDate date = LocalDate.parse(row[0], dateFormat);\n        return date.getDayOfWeek() == DayOfWeek.MONDAY;\n    });\n});  \n   Index     |     Open     |    Close     |  Adj Close   |\n-----------------------------------------------------------\n 2014-06-02  |  192.949997  |  192.899994  |  182.281421  |\n 2014-05-19  |  187.690002  |  188.740005  |  178.350428  |\n 2014-05-12  |  188.800003  |  189.789993  |  179.342617  |\n 2014-05-05  |  187.139999  |  188.419998  |  178.048036  |\n 2014-04-28  |  187.050003  |  186.880005  |  176.592815  |\n 2014-04-21  |  186.440002  |  187.039993  |  176.743996  |\n 2014-04-14  |  182.929993  |  182.940002  |  172.869698  |\n 2014-04-07  |  185.949997  |  184.339996  |  174.192626  |\n 2014-03-31  |  186.669998  |  187.009995  |  176.715649  |\n 2014-03-24  |  186.839996  |  185.429993  |  175.222621  |",
            "title": "Row / Column Filtering"
        },
        {
            "location": "/frame/construction/#from-json",
            "text": "The Morpheus library has the ability to read and write a  DataFrame  to a JSON format, which makes it easy\nto integrate with a web application. The parser leverages the  Google GSON \nlibrary, and specifically the stream based API for both reading and writing, which allows it to handle very large\nJSON files. An example of the format is shown below, which depicts a frame with 3 rows and 2 columns, a row axis with\nkeys of type  LocalDate , and a column of booleans and a column of integers.   {\n  \"DataFrame\": {\n    \"rowCount\": 3,\n    \"colCount\": 2,\n    \"rowKeys\": {\n      \"type\": \"LocalDate\",\n      \"values\": [\n        \"2015-12-07\",\n        \"2015-12-08\",\n        \"2015-12-09\",\n      ]\n    },\n    \"columns\": [\n      {\n        \"key\": \"ColumnName1\",\n        \"keyType\": \"String\",\n        \"dataType\": \"boolean\",\n        \"nullValue\": \"false\",\n        \"values\": [\n          \"true\",\n          \"false\",\n          \"true\",\n        ]\n      },\n      {\n        \"key\": \"ColumnName2\",\n        \"keyType\": \"String\",\n        \"dataType\": \"int\",\n        \"nullValue\": \"0\",\n        \"values\": [\n          \"884841579\",\n          \"72476677\",\n          \"478622964\",\n        ]\n      }\n   ]\n  }\n}  Two static  DataFrame.fromJson()  functions exist to load a frame from a file, URL or classpath resource,\nin much the same way as the CSV functions. Row and column predicates can be used to select a subset of the\ndata as required. The following code demonstrates some basic examples:   //Parse a file, or classpath resource from Morpheus JSON format\nDataFrame<LocalDate,String> frame = DataFrame.readJson(\"/temp/data.json\");  To select a subset of rows and columns, apply predicates to the request as below.   //Parse a file, or classpath resource from Morpheus JSON format, selecting only a subset of rows & columns\nfinal Set<String> columns = Stream.of(\"Date\", \"PostCode\", \"Street\", \"County\").collect(Collectors.toSet());\nDataFrame<LocalDate,String> frame = DataFrame.readJson(options -> {\n    options.withResource(\"/temp/data.json\");\n    options.withCharset(StandardCharsets.UTF_16);\n    options.withRowPredicate(rowKey -> rowKey.getDayOfWeek() == DayOfWeek.MONDAY);\n    options.withColPredicate(columns::contains);\n});",
            "title": "From JSON"
        },
        {
            "location": "/frame/construction/#from-sql",
            "text": "The Morpheus library ships with a  DataFrameSource  implementation designed to initialize  DataFrames  from\nthe results of a SQL query. The following section illustrates various examples of how the API can be used\nin this regard.   As a convenience, a static  readDb()  method exists on the  DataFrame  class which takes as an argument a \nfunction that configures the details of the request. At a minimum, two inputs are required on the request \ndescriptor, namely the database connection (or the details of how to establish a connection), and a SQL \nexpression. Many other aspects of the request can be tailored, but the most basic example is as follows:   import com.zavtech.morpheus.frame.DataFrame;\n\n//Ensure the JDBC driver is loaded \nClass.forName(\"org.h2.Driver\");\n\n//Create a frame from a select statement\nDataFrame<Integer,String> frame = DataFrame.readDb(options -> {\n    options.withConnection(\"jdbc:h2://databases/testDb\", \"sa\", null);\n    options.withSql(\"select * from Customer where city = 'London'\");\n});  By default, the first column in the SQL  ResultSet  is used to initialize the row keys of the  DataFrame ,\nand in the example above, this amounts to the unique id of the Customer record. The API does provide\nfor greater control by supporting a user defined function that can be applied to generate the key for \neach row in the SQL  ResultSet . A good example of where this would be necessary is the case of a \ncomposite key.   import javax.sql.DataSource;\nimport com.zavtech.morpheus.util.Tuple;\nimport com.zavtech.morpheus.frame.DataFrame;\n\n// Join products and inventory to see what we have where\njavax.sql.DataSource dataSource = getDataSource();\nDataFrame<Tuple,String> frame = DataFrame.readDb(options -> {\n    options.withConnection(dataSource);\n    options.withSql(\"select * from Product t1 inner join Inventory t2 on t1.productId = t2.productId\");\n    options.withExcludeColumns(\"productId\", \"warehouseId\");  //not need as part of the key\n    options.withRowCapacity(1000);\n    options.withRowKeyFunction(rs -> {\n        String productId = rs.getString(\"productId\");\n        String warehouseId = rs.getString(\"warehouseId\");\n        return Tuple.of(warehouseId, productId);\n    });\n});  In the above example, because we have supplied our own function to generate a row key from a record in the  ResultSet , we know the resulting frame with be parameterized as  <Tuple,String> . By default, frames\ngenerated from a database query will always have a  String  based column axis representing the column\nnames in the  ResultSet , but these can obviously be mapped to some other type post construction.  It is often desirable to transform data read from a SQL  ResultSet  so that it can be typed more appropriately\nor modified in some way. The Morpheus API provides the capability to configure column specific extractors\nto perform such transformations. Consider the example below, where we query Customer records which have a VARCHAR  column used to capture a user's time zone, and we wish to convert this to a  ZoneId  within \nthe resulting  DataFrame . This example also demonstrates a parameterized query where arguments are\napplied using the  withParameters()  method on the request descriptor.   import java.time.LocalDate;\nimport java.time.ZoneId;\nimport javax.sql.DataSource;\nimport com.zavtech.morpheus.frame.DataFrame;\nimport com.zavtech.morpheus.util.sql.SQLExtractor;\n\n//Convert the user's time zone into a ZoneId \njavax.sql.DataSource dataSource = getDataSource();\nDataFrame<Integer,String> frame = DataFrame.readDb(options -> {\n    options.withConnection(dataSource);\n    options.withSql(\"select * from Customer where city = ? and dob > ?\");\n    options.withParameters(\"London\", LocalDate.of(1970, 1, 1));\n    options.withExtractor(\"UserZone\", SQLExtractor.with(ZoneId.class, (rs, colIndex) -> {\n        final String tzName = rs.getString(colIndex);\n        return tzName != null ? ZoneId.of(tzName) : null;\n    }));\n});",
            "title": "From SQL"
        },
        {
            "location": "/frame/access/",
            "text": "DataFrame Access\n\n\nIntroduction\n\n\nA \nDataFrame\n is composed of a collection of column vectors represented by Morpheus Arrays, and the frame API provides\nvarious mechanisms for reading and writing data in a type safe and efficient manner (for example, avoiding boxing of\nprimitive types). Elements can be read or written using a random access API, or they can be consumed iteratively, including \nvia Java 8 streams. It is also possible to process rows and columns in an efficient manner that places very little burden on \nthe Garbage Collector. The following sections illustrates various \nDataFrame\n access scenarios.\n\n\nOne of the design goals of Morpheus was to formulate a flexible API that is both intuitive and consistent, so that\noperations that apply at the frame level look and feel similar to operations that work on either the row or column\ndimension of the \nDataFrame\n. There is also a strong emphasis on type safety, so most of the peripheral interfaces in \nMorpheus are paramterised by both the row and column key types.\n\n\nONS Population Dataset\n\n\nThe examples in this section mostly operate on a population dataset made available by the \nOffice for National Statistics\n \n(ONS) in the UK, which is described \nhere\n\nand accessible \nhere\n. (unless the example creates a custom \nDataFrame\n for illustration). \nThe ONS dataset provides a population decomposition by gender and age across various boroughs and wards in the UK, between 1999 \nand 2014. The data can be loaded as follows, and the first 10 rows are shown below. The row axis in this example is a composite k\ney made up of two items from each row, namely the year and the borough name.\n\n\n\n\n\n/**\n * Returns the ONS population dataset for UK boroughs\n * @return  the ONS population dataser\n */\nstatic DataFrame<Tuple,String> loadPopulationDataset() {\n    return DataFrame.read().csv(options -> {\n        options.setResource(\"http://tinyurl.com/ons-population-year\");\n        options.setRowKeyParser(Tuple.class, row -> Tuple.of(Integer.parseInt(row[1]), row[2]));\n        options.setExcludeColumns(\"Code\");\n        options.getFormats().setNullValues(\"-\");\n        options.setColumnType(\"All Males\", Double.class);\n        options.setColumnType(\"All Females\", Double.class);\n        options.setColumnType(\"All Persons\", Double.class);\n        options.setColumnType(\"[MF]\\\\s+\\\\d+\", Double.class);\n    });\n}\n\n\n\n\nIn the parsing code above, we expressly coerce the various counts into a double type as the default CSV parsing logic would\nassume these are integers. The reason we parse them into doubles is to allow us to perform computations on these values, such \nas converting them to percentages, and therefore a double precision representation makes sense.\n\n\n\n         Index          |  Year  |     Borough      |  All Persons  |  All Males  |    M 0    |    M 1    |    M 2    |    M 3    |    M 4    |    M 5    |    M 6    |    M 7    |    M 8    |    M 9    |   M 10    |   M 11    |   M 12    |   M 13    |   M 14    |   M 15    |   M 16    |   M 17    |   M 18    |   M 19    |   M 20    |   M 21    |   M 22    |   M 23    |   M 24    |    M 25    |    M 26    |    M 27    |    M 28    |    M 29    |    M 30    |    M 31    |    M 32    |    M 33    |    M 34    |    M 35    |    M 36    |    M 37    |    M 38    |    M 39    |    M 40    |   M 41    |   M 42    |   M 43    |   M 44    |   M 45    |   M 46    |   M 47    |   M 48    |   M 49    |    M 50    |    M 51    |    M 52    |   M 53    |   M 54    |   M 55    |   M 56    |   M 57    |   M 58    |   M 59    |   M 60    |   M 61    |   M 62    |   M 63    |   M 64    |   M 65    |   M 66    |   M 67    |   M 68    |   M 69    |   M 70    |   M 71    |   M 72    |   M 73    |   M 74    |   M 75    |   M 76    |   M 77    |   M 78    |   M 79    |   M 80    |   M 81    |   M 82    |   M 83    |   M 84    |   M 85    |   M 86    |   M 87    |   M 88    |   M 89   |  M 90+  |  All Females  |    F 0    |    F 1    |    F 2    |    F 3    |    F 4    |    F 5    |    F 6    |    F 7    |    F 8    |    F 9    |   F 10    |   F 11    |   F 12    |   F 13    |   F 14    |   F 15    |   F 16    |   F 17    |   F 18    |   F 19    |   F 20    |   F 21    |   F 22    |   F 23    |   F 24    |   F 25    |    F 26    |   F 27    |    F 28    |    F 29    |    F 30    |   F 31    |    F 32    |    F 33    |   F 34    |   F 35    |   F 36    |   F 37    |   F 38    |   F 39    |   F 40    |   F 41    |   F 42    |   F 43    |   F 44    |   F 45    |   F 46    |   F 47    |   F 48    |   F 49    |   F 50    |   F 51    |   F 52    |   F 53    |   F 54    |   F 55    |   F 56    |   F 57    |   F 58    |   F 59    |   F 60    |   F 61    |   F 62    |   F 63    |   F 64    |   F 65    |   F 66    |   F 67    |   F 68    |   F 69    |   F 70    |   F 71    |   F 72    |   F 73    |   F 74    |   F 75    |   F 76    |   F 77    |   F 78    |   F 79    |   F 80    |   F 81    |   F 82    |   F 83    |   F 84    |   F 85    |   F 86    |   F 87    |   F 88    |   F 89    |  F 90+  |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n (1999,City of London)  |  1999  |  City of London  |    6581.0000  |  3519.0000  |  24.0000  |  23.0000  |  22.0000  |  21.0000  |  20.0000  |  21.0000  |  20.0000  |  21.0000  |  20.0000  |  22.0000  |  22.0000  |  21.0000  |  19.0000  |  18.0000  |  17.0000  |  17.0000  |  17.0000  |  21.0000  |  26.0000  |  32.0000  |  40.0000  |  47.0000  |  52.0000  |  60.0000  |  65.0000  |   70.0000  |   73.0000  |   73.0000  |   73.0000  |   78.0000  |   78.0000  |   77.0000  |   75.0000  |   72.0000  |   71.0000  |   71.0000  |   65.0000  |   60.0000  |   58.0000  |   58.0000  |   57.0000  |  55.0000  |  51.0000  |  53.0000  |  53.0000  |  59.0000  |  59.0000  |  60.0000  |  65.0000  |  68.0000  |   68.0000  |   70.0000  |   63.0000  |  62.0000  |  61.0000  |  54.0000  |  49.0000  |  47.0000  |  43.0000  |  38.0000  |  35.0000  |  34.0000  |  34.0000  |  33.0000  |  32.0000  |  31.0000  |  32.0000  |  31.0000  |  30.0000  |  30.0000  |  29.0000  |  27.0000  |  26.0000  |  24.0000  |  23.0000  |  20.0000  |  20.0000  |  18.0000  |  17.0000  |  15.0000  |  14.0000  |  12.0000  |  12.0000  |  11.0000  |  11.0000  |  43.0000  |      NaN  |      NaN  |      NaN  |     NaN  |      0  |    3062.0000  |  24.0000  |  24.0000  |  22.0000  |  22.0000  |  21.0000  |  21.0000  |  19.0000  |  19.0000  |  18.0000  |  18.0000  |  18.0000  |  16.0000  |  17.0000  |  16.0000  |  16.0000  |  16.0000  |  20.0000  |  25.0000  |  26.0000  |  30.0000  |  39.0000  |  47.0000  |  50.0000  |  54.0000  |  61.0000  |  68.0000  |   71.0000  |  72.0000  |   71.0000  |   73.0000  |   71.0000  |  68.0000  |   66.0000  |   62.0000  |  55.0000  |  51.0000  |  47.0000  |  43.0000  |  42.0000  |  39.0000  |  39.0000  |  41.0000  |  42.0000  |  43.0000  |  44.0000  |  40.0000  |  44.0000  |  45.0000  |  47.0000  |  48.0000  |  49.0000  |  49.0000  |  50.0000  |  46.0000  |  44.0000  |  39.0000  |  36.0000  |  34.0000  |  33.0000  |  31.0000  |  30.0000  |  29.0000  |  29.0000  |  30.0000  |  31.0000  |  30.0000  |  30.0000  |  29.0000  |  29.0000  |  28.0000  |  26.0000  |  26.0000  |  25.0000  |  23.0000  |  21.0000  |  20.0000  |  20.0000  |  19.0000  |  17.0000  |  17.0000  |  17.0000  |  16.0000  |  15.0000  |  14.0000  |  13.0000  |  76.0000  |      NaN  |      NaN  |      NaN  |      NaN  |      0  |\n (2000,City of London)  |  2000  |  City of London  |    7014.0000  |  3775.0000  |  25.0000  |  25.0000  |  24.0000  |  23.0000  |  22.0000  |  21.0000  |  22.0000  |  21.0000  |  22.0000  |  21.0000  |  22.0000  |  20.0000  |  18.0000  |  16.0000  |  16.0000  |  15.0000  |  17.0000  |  19.0000  |  24.0000  |  30.0000  |  39.0000  |  48.0000  |  57.0000  |  64.0000  |  72.0000  |   81.0000  |   88.0000  |   90.0000  |   90.0000  |   91.0000  |   94.0000  |   94.0000  |   90.0000  |   86.0000  |   81.0000  |   80.0000  |   76.0000  |   70.0000  |   65.0000  |   62.0000  |   62.0000  |  62.0000  |  59.0000  |  56.0000  |  56.0000  |  56.0000  |  62.0000  |  62.0000  |  62.0000  |  67.0000  |   71.0000  |   71.0000  |   74.0000  |  66.0000  |  64.0000  |  63.0000  |  56.0000  |  50.0000  |  47.0000  |  43.0000  |  39.0000  |  34.0000  |  32.0000  |  31.0000  |  30.0000  |  29.0000  |  28.0000  |  28.0000  |  27.0000  |  28.0000  |  27.0000  |  26.0000  |  25.0000  |  23.0000  |  22.0000  |  22.0000  |  19.0000  |  19.0000  |  17.0000  |  16.0000  |  15.0000  |  14.0000  |  12.0000  |  12.0000  |  11.0000  |  39.0000  |      NaN  |      NaN  |      NaN  |     NaN  |      0  |    3239.0000  |  26.0000  |  24.0000  |  24.0000  |  22.0000  |  21.0000  |  21.0000  |  20.0000  |  19.0000  |  19.0000  |  19.0000  |  19.0000  |  18.0000  |  17.0000  |  18.0000  |  17.0000  |  18.0000  |  20.0000  |  23.0000  |  28.0000  |  32.0000  |  38.0000  |  49.0000  |  58.0000  |  61.0000  |  69.0000  |  75.0000  |   82.0000  |  83.0000  |   84.0000  |   81.0000  |   80.0000  |  76.0000  |   73.0000  |   69.0000  |  64.0000  |  57.0000  |  53.0000  |  48.0000  |  44.0000  |  43.0000  |  41.0000  |  42.0000  |  41.0000  |  43.0000  |  43.0000  |  44.0000  |  41.0000  |  45.0000  |  45.0000  |  47.0000  |  49.0000  |  50.0000  |  51.0000  |  51.0000  |  47.0000  |  45.0000  |  41.0000  |  37.0000  |  35.0000  |  32.0000  |  30.0000  |  29.0000  |  28.0000  |  27.0000  |  28.0000  |  29.0000  |  28.0000  |  28.0000  |  28.0000  |  28.0000  |  27.0000  |  25.0000  |  25.0000  |  24.0000  |  22.0000  |  21.0000  |  19.0000  |  20.0000  |  19.0000  |  17.0000  |  16.0000  |  15.0000  |  15.0000  |  15.0000  |  13.0000  |  81.0000  |      NaN  |      NaN  |      NaN  |      NaN  |      0  |\n (2001,City of London)  |  2001  |  City of London  |    7359.0000  |  3984.0000  |  17.0000  |  30.0000  |  25.0000  |  19.0000  |  21.0000  |  25.0000  |  20.0000  |  16.0000  |  19.0000  |  27.0000  |  26.0000  |  16.0000  |  29.0000  |  19.0000  |  11.0000  |  14.0000  |  12.0000  |  13.0000  |  12.0000  |  27.0000  |  38.0000  |  39.0000  |  53.0000  |  63.0000  |  80.0000  |  110.0000  |  110.0000  |  100.0000  |  108.0000  |  125.0000  |  114.0000  |   95.0000  |   93.0000  |  105.0000  |   99.0000  |   84.0000  |   89.0000  |   71.0000  |   63.0000  |   75.0000  |   65.0000  |  52.0000  |  47.0000  |  76.0000  |  73.0000  |  54.0000  |  49.0000  |  45.0000  |  60.0000  |  49.0000  |  111.0000  |   73.0000  |   55.0000  |  81.0000  |  80.0000  |  61.0000  |  63.0000  |  56.0000  |  64.0000  |  48.0000  |  28.0000  |  34.0000  |  45.0000  |  36.0000  |  25.0000  |  28.0000  |  21.0000  |  28.0000  |  21.0000  |  27.0000  |  27.0000  |  21.0000  |  24.0000  |  29.0000  |  21.0000  |  19.0000  |  23.0000  |  20.0000  |  11.0000  |  19.0000  |  15.0000  |  14.0000  |   8.0000  |  15.0000  |  10.0000  |   6.0000  |  10.0000  |   5.0000  |   4.0000  |  3.0000  |     13  |    3375.0000  |  32.0000  |  30.0000  |  23.0000  |  21.0000  |  26.0000  |  16.0000  |  21.0000  |  15.0000  |  18.0000  |  24.0000  |  20.0000  |  20.0000  |  10.0000  |  26.0000  |  13.0000  |  13.0000  |  14.0000  |  39.0000  |  28.0000  |  35.0000  |  47.0000  |  39.0000  |  49.0000  |  54.0000  |  74.0000  |  93.0000  |   89.0000  |  80.0000  |   84.0000  |  107.0000  |   85.0000  |  83.0000  |  101.0000  |   66.0000  |  60.0000  |  54.0000  |  66.0000  |  60.0000  |  48.0000  |  45.0000  |  37.0000  |  28.0000  |  31.0000  |  63.0000  |  43.0000  |  53.0000  |  50.0000  |  47.0000  |  35.0000  |  42.0000  |  37.0000  |  65.0000  |  48.0000  |  57.0000  |  57.0000  |  38.0000  |  45.0000  |  38.0000  |  38.0000  |  33.0000  |  32.0000  |  32.0000  |  26.0000  |  25.0000  |  24.0000  |  29.0000  |  28.0000  |  27.0000  |  34.0000  |  27.0000  |  20.0000  |  26.0000  |  24.0000  |  31.0000  |  18.0000  |  24.0000  |  27.0000  |  13.0000  |  13.0000  |  15.0000  |  18.0000  |  22.0000  |  18.0000  |  11.0000  |  10.0000  |  10.0000  |  14.0000  |  26.0000  |  16.0000  |   7.0000  |     15  |\n (2002,City of London)  |  2002  |  City of London  |    7280.0000  |  3968.0000  |  29.0000  |  18.0000  |  28.0000  |  27.0000  |  20.0000  |  20.0000  |  24.0000  |  20.0000  |  17.0000  |  20.0000  |  25.0000  |  25.0000  |  14.0000  |  27.0000  |   9.0000  |   5.0000  |   8.0000  |  10.0000  |  12.0000  |  10.0000  |  20.0000  |  39.0000  |  50.0000  |  61.0000  |  68.0000  |   99.0000  |  113.0000  |  122.0000  |  102.0000  |  114.0000  |  114.0000  |  107.0000  |   99.0000  |   89.0000  |  101.0000  |   93.0000  |   84.0000  |   77.0000  |   73.0000  |   61.0000  |   78.0000  |  65.0000  |  57.0000  |  50.0000  |  75.0000  |  72.0000  |  56.0000  |  50.0000  |  45.0000  |  61.0000  |   46.0000  |  113.0000  |   70.0000  |  49.0000  |  85.0000  |  76.0000  |  59.0000  |  61.0000  |  55.0000  |  60.0000  |  45.0000  |  28.0000  |  31.0000  |  46.0000  |  40.0000  |  23.0000  |  26.0000  |  20.0000  |  25.0000  |  21.0000  |  24.0000  |  30.0000  |  21.0000  |  25.0000  |  29.0000  |  20.0000  |  18.0000  |  21.0000  |  17.0000  |  11.0000  |  17.0000  |  14.0000  |  13.0000  |   8.0000  |  15.0000  |   9.0000  |   6.0000  |   8.0000  |   4.0000  |  2.0000  |     14  |    3312.0000  |  28.0000  |  35.0000  |  30.0000  |  19.0000  |  21.0000  |  26.0000  |  15.0000  |  20.0000  |  13.0000  |  18.0000  |  17.0000  |  15.0000  |  17.0000  |   6.0000  |  19.0000  |  13.0000  |   5.0000  |  15.0000  |  35.0000  |  29.0000  |  35.0000  |  48.0000  |  52.0000  |  47.0000  |  67.0000  |  74.0000  |   84.0000  |  84.0000  |   83.0000  |   70.0000  |  115.0000  |  84.0000  |   94.0000  |  101.0000  |  63.0000  |  61.0000  |  57.0000  |  58.0000  |  62.0000  |  44.0000  |  47.0000  |  37.0000  |  28.0000  |  33.0000  |  60.0000  |  36.0000  |  47.0000  |  52.0000  |  45.0000  |  36.0000  |  38.0000  |  41.0000  |  63.0000  |  51.0000  |  54.0000  |  55.0000  |  37.0000  |  41.0000  |  40.0000  |  36.0000  |  30.0000  |  29.0000  |  30.0000  |  27.0000  |  23.0000  |  23.0000  |  28.0000  |  28.0000  |  25.0000  |  34.0000  |  25.0000  |  19.0000  |  24.0000  |  23.0000  |  31.0000  |  19.0000  |  25.0000  |  26.0000  |  10.0000  |  12.0000  |  14.0000  |  17.0000  |  22.0000  |  15.0000  |   9.0000  |  10.0000  |   7.0000  |  11.0000  |  26.0000  |  14.0000  |     20  |\n (2003,City of London)  |  2003  |  City of London  |    7115.0000  |  3892.0000  |  35.0000  |  24.0000  |  16.0000  |  22.0000  |  26.0000  |  17.0000  |  19.0000  |  24.0000  |  21.0000  |  21.0000  |  24.0000  |  25.0000  |  21.0000  |  10.0000  |  16.0000  |  10.0000  |   4.0000  |   5.0000  |  12.0000  |  12.0000  |  11.0000  |  19.0000  |  49.0000  |  60.0000  |  66.0000  |   80.0000  |  100.0000  |  118.0000  |  114.0000  |  105.0000  |  108.0000  |  105.0000  |   99.0000  |   94.0000  |   85.0000  |   97.0000  |   89.0000  |   82.0000  |   76.0000  |   69.0000  |   63.0000  |  74.0000  |  64.0000  |  57.0000  |  51.0000  |  75.0000  |  74.0000  |  56.0000  |  51.0000  |  39.0000  |   64.0000  |   44.0000  |  108.0000  |  65.0000  |  47.0000  |  80.0000  |  74.0000  |  56.0000  |  61.0000  |  55.0000  |  55.0000  |  44.0000  |  26.0000  |  30.0000  |  42.0000  |  36.0000  |  23.0000  |  24.0000  |  22.0000  |  22.0000  |  18.0000  |  23.0000  |  31.0000  |  18.0000  |  24.0000  |  27.0000  |  19.0000  |  17.0000  |  22.0000  |  15.0000  |  10.0000  |  16.0000  |  12.0000  |  10.0000  |   8.0000  |  14.0000  |   8.0000  |   6.0000  |   7.0000  |  2.0000  |     13  |    3223.0000  |  18.0000  |  25.0000  |  31.0000  |  29.0000  |  18.0000  |  21.0000  |  26.0000  |  14.0000  |  19.0000  |  12.0000  |  19.0000  |  13.0000  |  11.0000  |  12.0000  |   2.0000  |  16.0000  |   7.0000  |  12.0000  |  22.0000  |  31.0000  |  34.0000  |  37.0000  |  52.0000  |  61.0000  |  58.0000  |  85.0000  |   73.0000  |  86.0000  |   82.0000  |   85.0000  |   75.0000  |  99.0000  |   85.0000  |   89.0000  |  92.0000  |  60.0000  |  58.0000  |  45.0000  |  51.0000  |  53.0000  |  42.0000  |  47.0000  |  44.0000  |  29.0000  |  35.0000  |  59.0000  |  38.0000  |  41.0000  |  49.0000  |  39.0000  |  35.0000  |  40.0000  |  37.0000  |  61.0000  |  49.0000  |  53.0000  |  50.0000  |  35.0000  |  40.0000  |  34.0000  |  36.0000  |  30.0000  |  29.0000  |  30.0000  |  27.0000  |  24.0000  |  23.0000  |  28.0000  |  30.0000  |  25.0000  |  32.0000  |  24.0000  |  17.0000  |  23.0000  |  21.0000  |  30.0000  |  16.0000  |  23.0000  |  24.0000  |  10.0000  |  11.0000  |  14.0000  |  15.0000  |  16.0000  |  16.0000  |   9.0000  |   2.0000  |   5.0000  |  10.0000  |  22.0000  |     26  |\n (2004,City of London)  |  2004  |  City of London  |    7118.0000  |  3875.0000  |  28.0000  |  27.0000  |  25.0000  |  16.0000  |  20.0000  |  27.0000  |  17.0000  |  20.0000  |  31.0000  |  18.0000  |  22.0000  |  21.0000  |  22.0000  |  16.0000  |   4.0000  |  13.0000  |   8.0000  |   0.0000  |   2.0000  |  13.0000  |   9.0000  |  25.0000  |  26.0000  |  63.0000  |  70.0000  |   69.0000  |   89.0000  |  112.0000  |  115.0000  |  116.0000  |  100.0000  |   93.0000  |  102.0000  |   93.0000  |   86.0000  |   79.0000  |   99.0000  |   92.0000  |   88.0000  |   81.0000  |   73.0000  |  59.0000  |  65.0000  |  67.0000  |  58.0000  |  49.0000  |  76.0000  |  70.0000  |  55.0000  |  50.0000  |   44.0000  |   61.0000  |   45.0000  |  99.0000  |  65.0000  |  46.0000  |  77.0000  |  76.0000  |  55.0000  |  57.0000  |  53.0000  |  57.0000  |  43.0000  |  28.0000  |  32.0000  |  39.0000  |  34.0000  |  22.0000  |  25.0000  |  21.0000  |  22.0000  |  18.0000  |  20.0000  |  30.0000  |  16.0000  |  24.0000  |  28.0000  |  19.0000  |  14.0000  |  20.0000  |   9.0000  |  12.0000  |  14.0000  |  10.0000  |   8.0000  |   8.0000  |  15.0000  |   8.0000  |   5.0000  |  5.0000  |     12  |    3243.0000  |  31.0000  |  18.0000  |  20.0000  |  26.0000  |  25.0000  |  20.0000  |  25.0000  |  28.0000  |  14.0000  |  19.0000  |   7.0000  |  19.0000  |  10.0000  |   9.0000  |   9.0000  |   1.0000  |  13.0000  |  17.0000  |   9.0000  |  22.0000  |  38.0000  |  41.0000  |  47.0000  |  59.0000  |  79.0000  |  80.0000  |   91.0000  |  86.0000  |   79.0000  |   81.0000  |   79.0000  |  78.0000  |   91.0000  |   80.0000  |  87.0000  |  83.0000  |  56.0000  |  61.0000  |  47.0000  |  48.0000  |  55.0000  |  40.0000  |  45.0000  |  39.0000  |  30.0000  |  34.0000  |  59.0000  |  37.0000  |  39.0000  |  56.0000  |  38.0000  |  33.0000  |  38.0000  |  36.0000  |  60.0000  |  51.0000  |  52.0000  |  51.0000  |  34.0000  |  40.0000  |  35.0000  |  34.0000  |  28.0000  |  32.0000  |  28.0000  |  27.0000  |  25.0000  |  20.0000  |  29.0000  |  28.0000  |  24.0000  |  30.0000  |  24.0000  |  15.0000  |  21.0000  |  22.0000  |  29.0000  |  15.0000  |  23.0000  |  22.0000  |  12.0000  |  11.0000  |  12.0000  |  11.0000  |  12.0000  |  12.0000  |   7.0000  |   1.0000  |   3.0000  |   9.0000  |     42  |\n (2005,City of London)  |  2005  |  City of London  |    7131.0000  |  3869.0000  |  34.0000  |  25.0000  |  22.0000  |  23.0000  |  12.0000  |  19.0000  |  22.0000  |  19.0000  |  19.0000  |  31.0000  |  20.0000  |  21.0000  |  20.0000  |  19.0000  |   4.0000  |   3.0000  |  12.0000  |  10.0000  |   0.0000  |   7.0000  |   9.0000  |  16.0000  |  43.0000  |  40.0000  |  72.0000  |   93.0000  |   74.0000  |   89.0000  |  117.0000  |  113.0000  |  104.0000  |   88.0000  |   92.0000  |  109.0000  |   98.0000  |   77.0000  |   76.0000  |  103.0000  |   89.0000  |   78.0000  |   80.0000  |  68.0000  |  61.0000  |  69.0000  |  62.0000  |  58.0000  |  51.0000  |  82.0000  |  67.0000  |  58.0000  |   52.0000  |   42.0000  |   59.0000  |  50.0000  |  89.0000  |  67.0000  |  47.0000  |  73.0000  |  70.0000  |  55.0000  |  58.0000  |  50.0000  |  53.0000  |  42.0000  |  27.0000  |  30.0000  |  42.0000  |  31.0000  |  23.0000  |  24.0000  |  20.0000  |  17.0000  |  17.0000  |  19.0000  |  30.0000  |  15.0000  |  21.0000  |  27.0000  |  17.0000  |  14.0000  |  21.0000  |   8.0000  |  14.0000  |  12.0000  |   7.0000  |   7.0000  |   6.0000  |  14.0000  |   7.0000  |  2.0000  |     12  |    3262.0000  |  29.0000  |  29.0000  |  15.0000  |  15.0000  |  23.0000  |  25.0000  |  17.0000  |  23.0000  |  25.0000  |  14.0000  |  18.0000  |   7.0000  |  20.0000  |   7.0000  |   7.0000  |   6.0000  |   2.0000  |   7.0000  |  19.0000  |  15.0000  |  31.0000  |  43.0000  |  49.0000  |  50.0000  |  82.0000  |  76.0000  |   89.0000  |  89.0000  |   85.0000  |   76.0000  |   90.0000  |  76.0000  |   85.0000  |   91.0000  |  74.0000  |  80.0000  |  74.0000  |  60.0000  |  55.0000  |  52.0000  |  45.0000  |  52.0000  |  43.0000  |  47.0000  |  38.0000  |  34.0000  |  34.0000  |  55.0000  |  33.0000  |  38.0000  |  57.0000  |  38.0000  |  36.0000  |  39.0000  |  37.0000  |  58.0000  |  52.0000  |  55.0000  |  50.0000  |  34.0000  |  37.0000  |  35.0000  |  37.0000  |  26.0000  |  33.0000  |  30.0000  |  25.0000  |  24.0000  |  18.0000  |  27.0000  |  26.0000  |  20.0000  |  28.0000  |  22.0000  |  18.0000  |  21.0000  |  24.0000  |  26.0000  |  16.0000  |  21.0000  |  22.0000  |  10.0000  |  12.0000  |  11.0000  |  10.0000  |  12.0000  |   9.0000  |   2.0000  |   2.0000  |   3.0000  |     50  |\n (2006,City of London)  |  2006  |  City of London  |    7254.0000  |  3959.0000  |  26.0000  |  25.0000  |  26.0000  |  20.0000  |  18.0000  |  13.0000  |  16.0000  |  23.0000  |  22.0000  |  18.0000  |  30.0000  |  18.0000  |  20.0000  |  17.0000  |   7.0000  |   5.0000  |   5.0000  |  15.0000  |  12.0000  |   0.0000  |  12.0000  |  19.0000  |  32.0000  |  63.0000  |  63.0000  |   82.0000  |  109.0000  |   83.0000  |   99.0000  |  124.0000  |  114.0000  |  103.0000  |   83.0000  |   85.0000  |  116.0000  |   98.0000  |   77.0000  |   74.0000  |  107.0000  |   87.0000  |   75.0000  |  82.0000  |  68.0000  |  63.0000  |  69.0000  |  59.0000  |  62.0000  |  53.0000  |  83.0000  |  66.0000  |   60.0000  |   52.0000  |   48.0000  |  63.0000  |  48.0000  |  85.0000  |  67.0000  |  47.0000  |  68.0000  |  62.0000  |  56.0000  |  56.0000  |  48.0000  |  54.0000  |  40.0000  |  28.0000  |  26.0000  |  41.0000  |  31.0000  |  22.0000  |  23.0000  |  20.0000  |  14.0000  |  16.0000  |  19.0000  |  29.0000  |  17.0000  |  20.0000  |  28.0000  |  17.0000  |  14.0000  |  19.0000  |   6.0000  |  10.0000  |  12.0000  |   5.0000  |   4.0000  |   6.0000  |  12.0000  |  7.0000  |     13  |    3295.0000  |  28.0000  |  26.0000  |  31.0000  |  12.0000  |  17.0000  |  25.0000  |  23.0000  |  20.0000  |  22.0000  |  25.0000  |  14.0000  |  18.0000  |   5.0000  |  19.0000  |   4.0000  |   7.0000  |   4.0000  |  12.0000  |   7.0000  |  27.0000  |  24.0000  |  33.0000  |  51.0000  |  58.0000  |  71.0000  |  88.0000  |   80.0000  |  86.0000  |   86.0000  |   92.0000  |   84.0000  |  86.0000  |   78.0000  |   84.0000  |  80.0000  |  77.0000  |  74.0000  |  68.0000  |  62.0000  |  45.0000  |  53.0000  |  45.0000  |  51.0000  |  45.0000  |  46.0000  |  39.0000  |  38.0000  |  33.0000  |  53.0000  |  31.0000  |  37.0000  |  57.0000  |  39.0000  |  36.0000  |  38.0000  |  36.0000  |  56.0000  |  50.0000  |  53.0000  |  47.0000  |  30.0000  |  41.0000  |  31.0000  |  34.0000  |  24.0000  |  30.0000  |  29.0000  |  23.0000  |  26.0000  |  17.0000  |  27.0000  |  26.0000  |  20.0000  |  26.0000  |  24.0000  |  18.0000  |  20.0000  |  22.0000  |  27.0000  |  17.0000  |  20.0000  |  19.0000  |  11.0000  |  11.0000  |  10.0000  |   9.0000  |  11.0000  |   9.0000  |   2.0000  |   0.0000  |     45  |\n (2007,City of London)  |  2007  |  City of London  |    7607.0000  |  4197.0000  |  31.0000  |  25.0000  |  24.0000  |  22.0000  |  23.0000  |  16.0000  |  13.0000  |  14.0000  |  27.0000  |  23.0000  |  15.0000  |  33.0000  |  16.0000  |  19.0000  |   7.0000  |   7.0000  |   3.0000  |   9.0000  |  16.0000  |  27.0000  |   0.0000  |  30.0000  |  35.0000  |  64.0000  |  91.0000  |   90.0000  |  113.0000  |  123.0000  |   99.0000  |  107.0000  |  123.0000  |  102.0000  |  106.0000  |   84.0000  |   87.0000  |  114.0000  |  104.0000  |   86.0000  |   83.0000  |  117.0000  |   88.0000  |  79.0000  |  74.0000  |  70.0000  |  75.0000  |  74.0000  |  63.0000  |  64.0000  |  57.0000  |  80.0000  |   65.0000  |   62.0000  |   55.0000  |  51.0000  |  64.0000  |  53.0000  |  78.0000  |  64.0000  |  48.0000  |  63.0000  |  59.0000  |  54.0000  |  58.0000  |  46.0000  |  54.0000  |  36.0000  |  26.0000  |  30.0000  |  39.0000  |  30.0000  |  22.0000  |  23.0000  |  17.0000  |  16.0000  |  16.0000  |  19.0000  |  27.0000  |  15.0000  |  15.0000  |  29.0000  |  14.0000  |  14.0000  |  18.0000  |   6.0000  |   6.0000  |  10.0000  |   4.0000  |   4.0000  |   6.0000  |  9.0000  |     20  |    3410.0000  |  24.0000  |  24.0000  |  30.0000  |  25.0000  |  17.0000  |  17.0000  |  17.0000  |  21.0000  |  16.0000  |  21.0000  |  25.0000  |  15.0000  |  18.0000  |   4.0000  |  19.0000  |   3.0000  |  11.0000  |  24.0000  |  15.0000  |  24.0000  |  39.0000  |  34.0000  |  56.0000  |  70.0000  |  75.0000  |  91.0000  |  100.0000  |  85.0000  |  101.0000  |   83.0000  |   80.0000  |  76.0000  |   80.0000  |   72.0000  |  72.0000  |  72.0000  |  76.0000  |  73.0000  |  62.0000  |  65.0000  |  47.0000  |  56.0000  |  48.0000  |  55.0000  |  50.0000  |  49.0000  |  43.0000  |  36.0000  |  30.0000  |  48.0000  |  29.0000  |  38.0000  |  54.0000  |  41.0000  |  39.0000  |  39.0000  |  39.0000  |  56.0000  |  52.0000  |  53.0000  |  46.0000  |  27.0000  |  41.0000  |  35.0000  |  32.0000  |  27.0000  |  30.0000  |  32.0000  |  22.0000  |  24.0000  |  16.0000  |  27.0000  |  26.0000  |  18.0000  |  25.0000  |  23.0000  |  17.0000  |  18.0000  |  22.0000  |  23.0000  |  17.0000  |  17.0000  |  19.0000  |  11.0000  |  10.0000  |  10.0000  |   7.0000  |   9.0000  |   8.0000  |   1.0000  |     36  |\n (2008,City of London)  |  2008  |  City of London  |    7429.0000  |  4131.0000  |  29.0000  |  26.0000  |  22.0000  |  24.0000  |  18.0000  |  22.0000  |  13.0000  |  12.0000  |  17.0000  |  30.0000  |  20.0000  |  15.0000  |  30.0000  |  17.0000  |  10.0000  |   8.0000  |   6.0000  |   7.0000  |  11.0000  |  18.0000  |  28.0000  |  11.0000  |  43.0000  |  39.0000  |  75.0000  |  100.0000  |   85.0000  |  123.0000  |  122.0000  |  102.0000  |   93.0000  |  111.0000  |   96.0000  |  101.0000  |   75.0000  |   80.0000  |  103.0000  |   92.0000  |   83.0000  |   79.0000  |  121.0000  |  86.0000  |  79.0000  |  69.0000  |  70.0000  |  66.0000  |  77.0000  |  57.0000  |  67.0000  |  67.0000  |   81.0000  |   64.0000  |   61.0000  |  51.0000  |  52.0000  |  61.0000  |  55.0000  |  73.0000  |  61.0000  |  46.0000  |  60.0000  |  61.0000  |  53.0000  |  55.0000  |  42.0000  |  53.0000  |  34.0000  |  26.0000  |  29.0000  |  39.0000  |  29.0000  |  24.0000  |  22.0000  |  17.0000  |  15.0000  |  14.0000  |  19.0000  |  24.0000  |  14.0000  |  16.0000  |  27.0000  |  13.0000  |  12.0000  |  17.0000  |   6.0000  |   6.0000  |   7.0000  |   4.0000  |   3.0000  |  5.0000  |     25  |    3298.0000  |  12.0000  |  17.0000  |  23.0000  |  23.0000  |  24.0000  |  16.0000  |  17.0000  |  16.0000  |  19.0000  |  15.0000  |  21.0000  |  24.0000  |  15.0000  |  19.0000  |   1.0000  |  19.0000  |   0.0000  |  22.0000  |  26.0000  |  25.0000  |  35.0000  |  43.0000  |  35.0000  |  57.0000  |  73.0000  |  82.0000  |   89.0000  |  95.0000  |   88.0000  |   94.0000  |   76.0000  |  77.0000  |   68.0000  |   69.0000  |  57.0000  |  60.0000  |  61.0000  |  64.0000  |  68.0000  |  57.0000  |  63.0000  |  41.0000  |  52.0000  |  46.0000  |  52.0000  |  50.0000  |  50.0000  |  37.0000  |  35.0000  |  32.0000  |  42.0000  |  29.0000  |  38.0000  |  53.0000  |  40.0000  |  42.0000  |  42.0000  |  38.0000  |  52.0000  |  51.0000  |  51.0000  |  48.0000  |  27.0000  |  41.0000  |  34.0000  |  29.0000  |  30.0000  |  31.0000  |  29.0000  |  23.0000  |  22.0000  |  16.0000  |  28.0000  |  27.0000  |  20.0000  |  24.0000  |  22.0000  |  16.0000  |  15.0000  |  22.0000  |  24.0000  |  15.0000  |  17.0000  |  18.0000  |  10.0000  |   9.0000  |   9.0000  |   7.0000  |   9.0000  |   8.0000  |     30  |\n\n\n\n\nRow / Column Access\n\n\nRandom Access\n\n\nRandom access to specific rows or columns can be achieved in O(1) time using either the ordinal or the key for\nthe row or column in question. Convenience functions to access the first and last row / column vectors is also provided\nvia a \nfirst()\n and \nlast()\n method. Each call to \nrowAt()\n or \ncolAt()\n creates a new row or column vector respectively, \nso iterating over the frame in this way is unlikely to be as fast as the  \nforEach()\n technique described above.\n\n\n\n\n\n//Load ONS dataset\nfinal DataFrame<Tuple,String> frame = loadPopulationDataset();\n\n//Random access to a row by ordinal or key\nDataFrameRow<Tuple,String> row1 = frame.rowAt(4);\nDataFrameRow<Tuple,String> row2 = frame.rowAt(Tuple.of(2003, \"City of London\"));\n\n//Random access to a column by ordinal or key\nDataFrameColumn<Tuple,String> column1 = frame.colAt(2);\nDataFrameColumn<Tuple,String> column2 = frame.colAt(\"All Persons\");\n\n//Access first and last rows\nOptional<DataFrameRow<Tuple,String>> firstRow = frame.rows().first();\nOptional<DataFrameRow<Tuple,String>> lastRow = frame.rows().last();\n\n//Access first and last columns\nOptional<DataFrameColumn<Tuple,String>> firstColumn = frame.cols().first();\nOptional<DataFrameColumn<Tuple,String>> lastColumn = frame.cols().last();\n\n\n\n\nThe example below is slightly more elaborate where we access the \"All Persons\" column in the ONS \nDataFrame\n in order to \nlocate the row which contains the largest population value by using the \nmax()\n function. Before we do this however,\nwe access the largest value using the \nstats().max()\n function so we can assert the \nmax()\n points us to a row with\na matching value - a nice unit test.\n\n\n\n\n\n// Find row with max value for \"All Persons: column using column key\nfinal double expectedMax = frame.colAt(\"All Persons\").stats().max();\nframe.colAt(\"All Persons\").max().ifPresent(value -> {\n    final DataFrameRow<Tuple,String> row = frame.rowAt(value.rowKey());\n    final double actualMax = row.getDouble(\"All Persons\");\n    Asserts.assertEquals(actualMax, expectedMax, \"The max values match\");\n});\n\n\n\n\nIteration\n\n\nThe \nDataFrame\n API is entirely symmetric in the row and column axis, so everything that you can do in the row dimension\nyou can also do in the column dimension. Some of the examples below operate on either the row or column axis simply for brevity, \nbut you can assume there is a direct analogue in the other dimension.\n\n\nIterating over the rows or columns of a \nDataFrame\n is a common operation, so Morpehus provides a simple API to do this \nefficiently, both via \nsequential\n and \nparallel\n execution. Rather than creating a new row or column object for each \nitem in the iteration, Morpheus presents the same proxy object which points at the appropriate row or column, thus reducing\nthe burden on the garbage collector. This means you cannot collect rows or columns, unless you call \ncopy()\n on them to create \na detached view on the row or column vector. This is generally discouraged, especially for large frames as it would likely be \ninefficient both from a memory consumption and garbage collection perspective.\n\n\nThe code below uses the ONS dataset described in the introduction and shows one of several ways we could convert the male \nand female population counts into percentages by gender (for example, in the City of London in 1999, 24 out of 3519 males \nwere 0 years old (column M 0), making it 24 / 3519 = 0.6820%). The first code example illustrates how to iterate\nover each row sequentially, then for each row we iterate over all values and modify those that represent a male or\nfemale count with a percentage value. Note how we avoid any boxing / unboxing of primitives by using type specific\ngetters and setters when accessing what we know to be double precision values (since the CSV parsing was setup that way).\n\n\n\n\n\n//Sequential: Convert male & female population counts into weights\nframe.rows().forEach(row -> row.forEach(value -> {\n    if (value.colKey().matches(\"M\\\\s+\\\\d+\")) {\n        double totalMales = value.row().getDouble(\"All Males\");\n        double count = value.getDouble();\n        value.setDouble(count / totalMales);\n    } else if (value.colKey().matches(\"F\\\\s+\\\\d+\")) {\n        double totalFemales = value.row().getDouble(\"All Females\");\n        double count = value.getDouble();\n        value.setDouble(count / totalFemales);\n    }\n}));\n\n//Print frame to std out with custom formatting\nframe.out().print(formats -> {\n    formats.withDecimalFormat(\"All Persons\", \"0;-0\", 1);\n    formats.withDecimalFormat(\"All Males\", \"0;-0\", 1);\n    formats.withDecimalFormat(\"All Females\", \"0;-0\", 1);\n    formats.withDecimalFormat(Double.class, \"0.00'%';-0.00'%'\", 100);\n});\n\n\n\n\n\n         Index          |  Year  |     Borough      |  All Persons  |  All Males  |   M 0   |   M 1   |   M 2   |   M 3   |   M 4   |   M 5   |   M 6   |   M 7   |   M 8   |   M 9   |  M 10   |  M 11   |  M 12   |  M 13   |  M 14   |  M 15   |  M 16   |  M 17   |  M 18   |  M 19   |  M 20   |  M 21   |  M 22   |  M 23   |  M 24   |  M 25   |  M 26   |  M 27   |  M 28   |  M 29   |  M 30   |  M 31   |  M 32   |  M 33   |  M 34   |  M 35   |  M 36   |  M 37   |  M 38   |  M 39   |  M 40   |  M 41   |  M 42   |  M 43   |  M 44   |  M 45   |  M 46   |  M 47   |  M 48   |  M 49   |  M 50   |  M 51   |  M 52   |  M 53   |  M 54   |  M 55   |  M 56   |  M 57   |  M 58   |  M 59   |  M 60   |  M 61   |  M 62   |  M 63   |  M 64   |  M 65   |  M 66   |  M 67   |  M 68   |  M 69   |  M 70   |  M 71   |  M 72   |  M 73   |  M 74   |  M 75   |  M 76   |  M 77   |  M 78   |  M 79   |  M 80   |  M 81   |  M 82   |  M 83   |  M 84   |  M 85   |  M 86   |  M 87   |  M 88   |  M 89   |  M 90+  |  All Females  |   F 0   |   F 1   |   F 2   |   F 3   |   F 4   |   F 5   |   F 6   |   F 7   |   F 8   |   F 9   |  F 10   |  F 11   |  F 12   |  F 13   |  F 14   |  F 15   |  F 16   |  F 17   |  F 18   |  F 19   |  F 20   |  F 21   |  F 22   |  F 23   |  F 24   |  F 25   |  F 26   |  F 27   |  F 28   |  F 29   |  F 30   |  F 31   |  F 32   |  F 33   |  F 34   |  F 35   |  F 36   |  F 37   |  F 38   |  F 39   |  F 40   |  F 41   |  F 42   |  F 43   |  F 44   |  F 45   |  F 46   |  F 47   |  F 48   |  F 49   |  F 50   |  F 51   |  F 52   |  F 53   |  F 54   |  F 55   |  F 56   |  F 57   |  F 58   |  F 59   |  F 60   |  F 61   |  F 62   |  F 63   |  F 64   |  F 65   |  F 66   |  F 67   |  F 68   |  F 69   |  F 70   |  F 71   |  F 72   |  F 73   |  F 74   |  F 75   |  F 76   |  F 77   |  F 78   |  F 79   |  F 80   |  F 81   |  F 82   |  F 83   |  F 84   |  F 85   |  F 86   |  F 87   |  F 88   |  F 89   |  F 90+  |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n (1999,City of London)  |  1999  |  City of London  |         6581  |       3519  |  0.68%  |  0.65%  |  0.63%  |  0.60%  |  0.57%  |  0.60%  |  0.57%  |  0.60%  |  0.57%  |  0.63%  |  0.63%  |  0.60%  |  0.54%  |  0.51%  |  0.48%  |  0.48%  |  0.48%  |  0.60%  |  0.74%  |  0.91%  |  1.14%  |  1.34%  |  1.48%  |  1.71%  |  1.85%  |  1.99%  |  2.07%  |  2.07%  |  2.07%  |  2.22%  |  2.22%  |  2.19%  |  2.13%  |  2.05%  |  2.02%  |  2.02%  |  1.85%  |  1.71%  |  1.65%  |  1.65%  |  1.62%  |  1.56%  |  1.45%  |  1.51%  |  1.51%  |  1.68%  |  1.68%  |  1.71%  |  1.85%  |  1.93%  |  1.93%  |  1.99%  |  1.79%  |  1.76%  |  1.73%  |  1.53%  |  1.39%  |  1.34%  |  1.22%  |  1.08%  |  0.99%  |  0.97%  |  0.97%  |  0.94%  |  0.91%  |  0.88%  |  0.91%  |  0.88%  |  0.85%  |  0.85%  |  0.82%  |  0.77%  |  0.74%  |  0.68%  |  0.65%  |  0.57%  |  0.57%  |  0.51%  |  0.48%  |  0.43%  |  0.40%  |  0.34%  |  0.34%  |  0.31%  |  0.31%  |  1.22%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |         3062  |  0.78%  |  0.78%  |  0.72%  |  0.72%  |  0.69%  |  0.69%  |  0.62%  |  0.62%  |  0.59%  |  0.59%  |  0.59%  |  0.52%  |  0.56%  |  0.52%  |  0.52%  |  0.52%  |  0.65%  |  0.82%  |  0.85%  |  0.98%  |  1.27%  |  1.53%  |  1.63%  |  1.76%  |  1.99%  |  2.22%  |  2.32%  |  2.35%  |  2.32%  |  2.38%  |  2.32%  |  2.22%  |  2.16%  |  2.02%  |  1.80%  |  1.67%  |  1.53%  |  1.40%  |  1.37%  |  1.27%  |  1.27%  |  1.34%  |  1.37%  |  1.40%  |  1.44%  |  1.31%  |  1.44%  |  1.47%  |  1.53%  |  1.57%  |  1.60%  |  1.60%  |  1.63%  |  1.50%  |  1.44%  |  1.27%  |  1.18%  |  1.11%  |  1.08%  |  1.01%  |  0.98%  |  0.95%  |  0.95%  |  0.98%  |  1.01%  |  0.98%  |  0.98%  |  0.95%  |  0.95%  |  0.91%  |  0.85%  |  0.85%  |  0.82%  |  0.75%  |  0.69%  |  0.65%  |  0.65%  |  0.62%  |  0.56%  |  0.56%  |  0.56%  |  0.52%  |  0.49%  |  0.46%  |  0.42%  |  2.48%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |\n (2000,City of London)  |  2000  |  City of London  |         7014  |       3775  |  0.66%  |  0.66%  |  0.64%  |  0.61%  |  0.58%  |  0.56%  |  0.58%  |  0.56%  |  0.58%  |  0.56%  |  0.58%  |  0.53%  |  0.48%  |  0.42%  |  0.42%  |  0.40%  |  0.45%  |  0.50%  |  0.64%  |  0.79%  |  1.03%  |  1.27%  |  1.51%  |  1.70%  |  1.91%  |  2.15%  |  2.33%  |  2.38%  |  2.38%  |  2.41%  |  2.49%  |  2.49%  |  2.38%  |  2.28%  |  2.15%  |  2.12%  |  2.01%  |  1.85%  |  1.72%  |  1.64%  |  1.64%  |  1.64%  |  1.56%  |  1.48%  |  1.48%  |  1.48%  |  1.64%  |  1.64%  |  1.64%  |  1.77%  |  1.88%  |  1.88%  |  1.96%  |  1.75%  |  1.70%  |  1.67%  |  1.48%  |  1.32%  |  1.25%  |  1.14%  |  1.03%  |  0.90%  |  0.85%  |  0.82%  |  0.79%  |  0.77%  |  0.74%  |  0.74%  |  0.72%  |  0.74%  |  0.72%  |  0.69%  |  0.66%  |  0.61%  |  0.58%  |  0.58%  |  0.50%  |  0.50%  |  0.45%  |  0.42%  |  0.40%  |  0.37%  |  0.32%  |  0.32%  |  0.29%  |  1.03%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |         3239  |  0.80%  |  0.74%  |  0.74%  |  0.68%  |  0.65%  |  0.65%  |  0.62%  |  0.59%  |  0.59%  |  0.59%  |  0.59%  |  0.56%  |  0.52%  |  0.56%  |  0.52%  |  0.56%  |  0.62%  |  0.71%  |  0.86%  |  0.99%  |  1.17%  |  1.51%  |  1.79%  |  1.88%  |  2.13%  |  2.32%  |  2.53%  |  2.56%  |  2.59%  |  2.50%  |  2.47%  |  2.35%  |  2.25%  |  2.13%  |  1.98%  |  1.76%  |  1.64%  |  1.48%  |  1.36%  |  1.33%  |  1.27%  |  1.30%  |  1.27%  |  1.33%  |  1.33%  |  1.36%  |  1.27%  |  1.39%  |  1.39%  |  1.45%  |  1.51%  |  1.54%  |  1.57%  |  1.57%  |  1.45%  |  1.39%  |  1.27%  |  1.14%  |  1.08%  |  0.99%  |  0.93%  |  0.90%  |  0.86%  |  0.83%  |  0.86%  |  0.90%  |  0.86%  |  0.86%  |  0.86%  |  0.86%  |  0.83%  |  0.77%  |  0.77%  |  0.74%  |  0.68%  |  0.65%  |  0.59%  |  0.62%  |  0.59%  |  0.52%  |  0.49%  |  0.46%  |  0.46%  |  0.46%  |  0.40%  |  2.50%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |\n (2001,City of London)  |  2001  |  City of London  |         7359  |       3984  |  0.43%  |  0.75%  |  0.63%  |  0.48%  |  0.53%  |  0.63%  |  0.50%  |  0.40%  |  0.48%  |  0.68%  |  0.65%  |  0.40%  |  0.73%  |  0.48%  |  0.28%  |  0.35%  |  0.30%  |  0.33%  |  0.30%  |  0.68%  |  0.95%  |  0.98%  |  1.33%  |  1.58%  |  2.01%  |  2.76%  |  2.76%  |  2.51%  |  2.71%  |  3.14%  |  2.86%  |  2.38%  |  2.33%  |  2.64%  |  2.48%  |  2.11%  |  2.23%  |  1.78%  |  1.58%  |  1.88%  |  1.63%  |  1.31%  |  1.18%  |  1.91%  |  1.83%  |  1.36%  |  1.23%  |  1.13%  |  1.51%  |  1.23%  |  2.79%  |  1.83%  |  1.38%  |  2.03%  |  2.01%  |  1.53%  |  1.58%  |  1.41%  |  1.61%  |  1.20%  |  0.70%  |  0.85%  |  1.13%  |  0.90%  |  0.63%  |  0.70%  |  0.53%  |  0.70%  |  0.53%  |  0.68%  |  0.68%  |  0.53%  |  0.60%  |  0.73%  |  0.53%  |  0.48%  |  0.58%  |  0.50%  |  0.28%  |  0.48%  |  0.38%  |  0.35%  |  0.20%  |  0.38%  |  0.25%  |  0.15%  |  0.25%  |  0.13%  |  0.10%  |  0.08%  |     13  |         3375  |  0.95%  |  0.89%  |  0.68%  |  0.62%  |  0.77%  |  0.47%  |  0.62%  |  0.44%  |  0.53%  |  0.71%  |  0.59%  |  0.59%  |  0.30%  |  0.77%  |  0.39%  |  0.39%  |  0.41%  |  1.16%  |  0.83%  |  1.04%  |  1.39%  |  1.16%  |  1.45%  |  1.60%  |  2.19%  |  2.76%  |  2.64%  |  2.37%  |  2.49%  |  3.17%  |  2.52%  |  2.46%  |  2.99%  |  1.96%  |  1.78%  |  1.60%  |  1.96%  |  1.78%  |  1.42%  |  1.33%  |  1.10%  |  0.83%  |  0.92%  |  1.87%  |  1.27%  |  1.57%  |  1.48%  |  1.39%  |  1.04%  |  1.24%  |  1.10%  |  1.93%  |  1.42%  |  1.69%  |  1.69%  |  1.13%  |  1.33%  |  1.13%  |  1.13%  |  0.98%  |  0.95%  |  0.95%  |  0.77%  |  0.74%  |  0.71%  |  0.86%  |  0.83%  |  0.80%  |  1.01%  |  0.80%  |  0.59%  |  0.77%  |  0.71%  |  0.92%  |  0.53%  |  0.71%  |  0.80%  |  0.39%  |  0.39%  |  0.44%  |  0.53%  |  0.65%  |  0.53%  |  0.33%  |  0.30%  |  0.30%  |  0.41%  |  0.77%  |  0.47%  |  0.21%  |     15  |\n (2002,City of London)  |  2002  |  City of London  |         7280  |       3968  |  0.73%  |  0.45%  |  0.71%  |  0.68%  |  0.50%  |  0.50%  |  0.60%  |  0.50%  |  0.43%  |  0.50%  |  0.63%  |  0.63%  |  0.35%  |  0.68%  |  0.23%  |  0.13%  |  0.20%  |  0.25%  |  0.30%  |  0.25%  |  0.50%  |  0.98%  |  1.26%  |  1.54%  |  1.71%  |  2.49%  |  2.85%  |  3.07%  |  2.57%  |  2.87%  |  2.87%  |  2.70%  |  2.49%  |  2.24%  |  2.55%  |  2.34%  |  2.12%  |  1.94%  |  1.84%  |  1.54%  |  1.97%  |  1.64%  |  1.44%  |  1.26%  |  1.89%  |  1.81%  |  1.41%  |  1.26%  |  1.13%  |  1.54%  |  1.16%  |  2.85%  |  1.76%  |  1.23%  |  2.14%  |  1.92%  |  1.49%  |  1.54%  |  1.39%  |  1.51%  |  1.13%  |  0.71%  |  0.78%  |  1.16%  |  1.01%  |  0.58%  |  0.66%  |  0.50%  |  0.63%  |  0.53%  |  0.60%  |  0.76%  |  0.53%  |  0.63%  |  0.73%  |  0.50%  |  0.45%  |  0.53%  |  0.43%  |  0.28%  |  0.43%  |  0.35%  |  0.33%  |  0.20%  |  0.38%  |  0.23%  |  0.15%  |  0.20%  |  0.10%  |  0.05%  |     14  |         3312  |  0.85%  |  1.06%  |  0.91%  |  0.57%  |  0.63%  |  0.79%  |  0.45%  |  0.60%  |  0.39%  |  0.54%  |  0.51%  |  0.45%  |  0.51%  |  0.18%  |  0.57%  |  0.39%  |  0.15%  |  0.45%  |  1.06%  |  0.88%  |  1.06%  |  1.45%  |  1.57%  |  1.42%  |  2.02%  |  2.23%  |  2.54%  |  2.54%  |  2.51%  |  2.11%  |  3.47%  |  2.54%  |  2.84%  |  3.05%  |  1.90%  |  1.84%  |  1.72%  |  1.75%  |  1.87%  |  1.33%  |  1.42%  |  1.12%  |  0.85%  |  1.00%  |  1.81%  |  1.09%  |  1.42%  |  1.57%  |  1.36%  |  1.09%  |  1.15%  |  1.24%  |  1.90%  |  1.54%  |  1.63%  |  1.66%  |  1.12%  |  1.24%  |  1.21%  |  1.09%  |  0.91%  |  0.88%  |  0.91%  |  0.82%  |  0.69%  |  0.69%  |  0.85%  |  0.85%  |  0.75%  |  1.03%  |  0.75%  |  0.57%  |  0.72%  |  0.69%  |  0.94%  |  0.57%  |  0.75%  |  0.79%  |  0.30%  |  0.36%  |  0.42%  |  0.51%  |  0.66%  |  0.45%  |  0.27%  |  0.30%  |  0.21%  |  0.33%  |  0.79%  |  0.42%  |     20  |\n (2003,City of London)  |  2003  |  City of London  |         7115  |       3892  |  0.90%  |  0.62%  |  0.41%  |  0.57%  |  0.67%  |  0.44%  |  0.49%  |  0.62%  |  0.54%  |  0.54%  |  0.62%  |  0.64%  |  0.54%  |  0.26%  |  0.41%  |  0.26%  |  0.10%  |  0.13%  |  0.31%  |  0.31%  |  0.28%  |  0.49%  |  1.26%  |  1.54%  |  1.70%  |  2.06%  |  2.57%  |  3.03%  |  2.93%  |  2.70%  |  2.77%  |  2.70%  |  2.54%  |  2.42%  |  2.18%  |  2.49%  |  2.29%  |  2.11%  |  1.95%  |  1.77%  |  1.62%  |  1.90%  |  1.64%  |  1.46%  |  1.31%  |  1.93%  |  1.90%  |  1.44%  |  1.31%  |  1.00%  |  1.64%  |  1.13%  |  2.77%  |  1.67%  |  1.21%  |  2.06%  |  1.90%  |  1.44%  |  1.57%  |  1.41%  |  1.41%  |  1.13%  |  0.67%  |  0.77%  |  1.08%  |  0.92%  |  0.59%  |  0.62%  |  0.57%  |  0.57%  |  0.46%  |  0.59%  |  0.80%  |  0.46%  |  0.62%  |  0.69%  |  0.49%  |  0.44%  |  0.57%  |  0.39%  |  0.26%  |  0.41%  |  0.31%  |  0.26%  |  0.21%  |  0.36%  |  0.21%  |  0.15%  |  0.18%  |  0.05%  |     13  |         3223  |  0.56%  |  0.78%  |  0.96%  |  0.90%  |  0.56%  |  0.65%  |  0.81%  |  0.43%  |  0.59%  |  0.37%  |  0.59%  |  0.40%  |  0.34%  |  0.37%  |  0.06%  |  0.50%  |  0.22%  |  0.37%  |  0.68%  |  0.96%  |  1.05%  |  1.15%  |  1.61%  |  1.89%  |  1.80%  |  2.64%  |  2.26%  |  2.67%  |  2.54%  |  2.64%  |  2.33%  |  3.07%  |  2.64%  |  2.76%  |  2.85%  |  1.86%  |  1.80%  |  1.40%  |  1.58%  |  1.64%  |  1.30%  |  1.46%  |  1.37%  |  0.90%  |  1.09%  |  1.83%  |  1.18%  |  1.27%  |  1.52%  |  1.21%  |  1.09%  |  1.24%  |  1.15%  |  1.89%  |  1.52%  |  1.64%  |  1.55%  |  1.09%  |  1.24%  |  1.05%  |  1.12%  |  0.93%  |  0.90%  |  0.93%  |  0.84%  |  0.74%  |  0.71%  |  0.87%  |  0.93%  |  0.78%  |  0.99%  |  0.74%  |  0.53%  |  0.71%  |  0.65%  |  0.93%  |  0.50%  |  0.71%  |  0.74%  |  0.31%  |  0.34%  |  0.43%  |  0.47%  |  0.50%  |  0.50%  |  0.28%  |  0.06%  |  0.16%  |  0.31%  |  0.68%  |     26  |\n (2004,City of London)  |  2004  |  City of London  |         7118  |       3875  |  0.72%  |  0.70%  |  0.65%  |  0.41%  |  0.52%  |  0.70%  |  0.44%  |  0.52%  |  0.80%  |  0.46%  |  0.57%  |  0.54%  |  0.57%  |  0.41%  |  0.10%  |  0.34%  |  0.21%  |  0.00%  |  0.05%  |  0.34%  |  0.23%  |  0.65%  |  0.67%  |  1.63%  |  1.81%  |  1.78%  |  2.30%  |  2.89%  |  2.97%  |  2.99%  |  2.58%  |  2.40%  |  2.63%  |  2.40%  |  2.22%  |  2.04%  |  2.55%  |  2.37%  |  2.27%  |  2.09%  |  1.88%  |  1.52%  |  1.68%  |  1.73%  |  1.50%  |  1.26%  |  1.96%  |  1.81%  |  1.42%  |  1.29%  |  1.14%  |  1.57%  |  1.16%  |  2.55%  |  1.68%  |  1.19%  |  1.99%  |  1.96%  |  1.42%  |  1.47%  |  1.37%  |  1.47%  |  1.11%  |  0.72%  |  0.83%  |  1.01%  |  0.88%  |  0.57%  |  0.65%  |  0.54%  |  0.57%  |  0.46%  |  0.52%  |  0.77%  |  0.41%  |  0.62%  |  0.72%  |  0.49%  |  0.36%  |  0.52%  |  0.23%  |  0.31%  |  0.36%  |  0.26%  |  0.21%  |  0.21%  |  0.39%  |  0.21%  |  0.13%  |  0.13%  |     12  |         3243  |  0.96%  |  0.56%  |  0.62%  |  0.80%  |  0.77%  |  0.62%  |  0.77%  |  0.86%  |  0.43%  |  0.59%  |  0.22%  |  0.59%  |  0.31%  |  0.28%  |  0.28%  |  0.03%  |  0.40%  |  0.52%  |  0.28%  |  0.68%  |  1.17%  |  1.26%  |  1.45%  |  1.82%  |  2.44%  |  2.47%  |  2.81%  |  2.65%  |  2.44%  |  2.50%  |  2.44%  |  2.41%  |  2.81%  |  2.47%  |  2.68%  |  2.56%  |  1.73%  |  1.88%  |  1.45%  |  1.48%  |  1.70%  |  1.23%  |  1.39%  |  1.20%  |  0.93%  |  1.05%  |  1.82%  |  1.14%  |  1.20%  |  1.73%  |  1.17%  |  1.02%  |  1.17%  |  1.11%  |  1.85%  |  1.57%  |  1.60%  |  1.57%  |  1.05%  |  1.23%  |  1.08%  |  1.05%  |  0.86%  |  0.99%  |  0.86%  |  0.83%  |  0.77%  |  0.62%  |  0.89%  |  0.86%  |  0.74%  |  0.93%  |  0.74%  |  0.46%  |  0.65%  |  0.68%  |  0.89%  |  0.46%  |  0.71%  |  0.68%  |  0.37%  |  0.34%  |  0.37%  |  0.34%  |  0.37%  |  0.37%  |  0.22%  |  0.03%  |  0.09%  |  0.28%  |     42  |\n (2005,City of London)  |  2005  |  City of London  |         7131  |       3869  |  0.88%  |  0.65%  |  0.57%  |  0.59%  |  0.31%  |  0.49%  |  0.57%  |  0.49%  |  0.49%  |  0.80%  |  0.52%  |  0.54%  |  0.52%  |  0.49%  |  0.10%  |  0.08%  |  0.31%  |  0.26%  |  0.00%  |  0.18%  |  0.23%  |  0.41%  |  1.11%  |  1.03%  |  1.86%  |  2.40%  |  1.91%  |  2.30%  |  3.02%  |  2.92%  |  2.69%  |  2.27%  |  2.38%  |  2.82%  |  2.53%  |  1.99%  |  1.96%  |  2.66%  |  2.30%  |  2.02%  |  2.07%  |  1.76%  |  1.58%  |  1.78%  |  1.60%  |  1.50%  |  1.32%  |  2.12%  |  1.73%  |  1.50%  |  1.34%  |  1.09%  |  1.52%  |  1.29%  |  2.30%  |  1.73%  |  1.21%  |  1.89%  |  1.81%  |  1.42%  |  1.50%  |  1.29%  |  1.37%  |  1.09%  |  0.70%  |  0.78%  |  1.09%  |  0.80%  |  0.59%  |  0.62%  |  0.52%  |  0.44%  |  0.44%  |  0.49%  |  0.78%  |  0.39%  |  0.54%  |  0.70%  |  0.44%  |  0.36%  |  0.54%  |  0.21%  |  0.36%  |  0.31%  |  0.18%  |  0.18%  |  0.16%  |  0.36%  |  0.18%  |  0.05%  |     12  |         3262  |  0.89%  |  0.89%  |  0.46%  |  0.46%  |  0.71%  |  0.77%  |  0.52%  |  0.71%  |  0.77%  |  0.43%  |  0.55%  |  0.21%  |  0.61%  |  0.21%  |  0.21%  |  0.18%  |  0.06%  |  0.21%  |  0.58%  |  0.46%  |  0.95%  |  1.32%  |  1.50%  |  1.53%  |  2.51%  |  2.33%  |  2.73%  |  2.73%  |  2.61%  |  2.33%  |  2.76%  |  2.33%  |  2.61%  |  2.79%  |  2.27%  |  2.45%  |  2.27%  |  1.84%  |  1.69%  |  1.59%  |  1.38%  |  1.59%  |  1.32%  |  1.44%  |  1.16%  |  1.04%  |  1.04%  |  1.69%  |  1.01%  |  1.16%  |  1.75%  |  1.16%  |  1.10%  |  1.20%  |  1.13%  |  1.78%  |  1.59%  |  1.69%  |  1.53%  |  1.04%  |  1.13%  |  1.07%  |  1.13%  |  0.80%  |  1.01%  |  0.92%  |  0.77%  |  0.74%  |  0.55%  |  0.83%  |  0.80%  |  0.61%  |  0.86%  |  0.67%  |  0.55%  |  0.64%  |  0.74%  |  0.80%  |  0.49%  |  0.64%  |  0.67%  |  0.31%  |  0.37%  |  0.34%  |  0.31%  |  0.37%  |  0.28%  |  0.06%  |  0.06%  |  0.09%  |     50  |\n (2006,City of London)  |  2006  |  City of London  |         7254  |       3959  |  0.66%  |  0.63%  |  0.66%  |  0.51%  |  0.45%  |  0.33%  |  0.40%  |  0.58%  |  0.56%  |  0.45%  |  0.76%  |  0.45%  |  0.51%  |  0.43%  |  0.18%  |  0.13%  |  0.13%  |  0.38%  |  0.30%  |  0.00%  |  0.30%  |  0.48%  |  0.81%  |  1.59%  |  1.59%  |  2.07%  |  2.75%  |  2.10%  |  2.50%  |  3.13%  |  2.88%  |  2.60%  |  2.10%  |  2.15%  |  2.93%  |  2.48%  |  1.94%  |  1.87%  |  2.70%  |  2.20%  |  1.89%  |  2.07%  |  1.72%  |  1.59%  |  1.74%  |  1.49%  |  1.57%  |  1.34%  |  2.10%  |  1.67%  |  1.52%  |  1.31%  |  1.21%  |  1.59%  |  1.21%  |  2.15%  |  1.69%  |  1.19%  |  1.72%  |  1.57%  |  1.41%  |  1.41%  |  1.21%  |  1.36%  |  1.01%  |  0.71%  |  0.66%  |  1.04%  |  0.78%  |  0.56%  |  0.58%  |  0.51%  |  0.35%  |  0.40%  |  0.48%  |  0.73%  |  0.43%  |  0.51%  |  0.71%  |  0.43%  |  0.35%  |  0.48%  |  0.15%  |  0.25%  |  0.30%  |  0.13%  |  0.10%  |  0.15%  |  0.30%  |  0.18%  |     13  |         3295  |  0.85%  |  0.79%  |  0.94%  |  0.36%  |  0.52%  |  0.76%  |  0.70%  |  0.61%  |  0.67%  |  0.76%  |  0.42%  |  0.55%  |  0.15%  |  0.58%  |  0.12%  |  0.21%  |  0.12%  |  0.36%  |  0.21%  |  0.82%  |  0.73%  |  1.00%  |  1.55%  |  1.76%  |  2.15%  |  2.67%  |  2.43%  |  2.61%  |  2.61%  |  2.79%  |  2.55%  |  2.61%  |  2.37%  |  2.55%  |  2.43%  |  2.34%  |  2.25%  |  2.06%  |  1.88%  |  1.37%  |  1.61%  |  1.37%  |  1.55%  |  1.37%  |  1.40%  |  1.18%  |  1.15%  |  1.00%  |  1.61%  |  0.94%  |  1.12%  |  1.73%  |  1.18%  |  1.09%  |  1.15%  |  1.09%  |  1.70%  |  1.52%  |  1.61%  |  1.43%  |  0.91%  |  1.24%  |  0.94%  |  1.03%  |  0.73%  |  0.91%  |  0.88%  |  0.70%  |  0.79%  |  0.52%  |  0.82%  |  0.79%  |  0.61%  |  0.79%  |  0.73%  |  0.55%  |  0.61%  |  0.67%  |  0.82%  |  0.52%  |  0.61%  |  0.58%  |  0.33%  |  0.33%  |  0.30%  |  0.27%  |  0.33%  |  0.27%  |  0.06%  |  0.00%  |     45  |\n (2007,City of London)  |  2007  |  City of London  |         7607  |       4197  |  0.74%  |  0.60%  |  0.57%  |  0.52%  |  0.55%  |  0.38%  |  0.31%  |  0.33%  |  0.64%  |  0.55%  |  0.36%  |  0.79%  |  0.38%  |  0.45%  |  0.17%  |  0.17%  |  0.07%  |  0.21%  |  0.38%  |  0.64%  |  0.00%  |  0.71%  |  0.83%  |  1.52%  |  2.17%  |  2.14%  |  2.69%  |  2.93%  |  2.36%  |  2.55%  |  2.93%  |  2.43%  |  2.53%  |  2.00%  |  2.07%  |  2.72%  |  2.48%  |  2.05%  |  1.98%  |  2.79%  |  2.10%  |  1.88%  |  1.76%  |  1.67%  |  1.79%  |  1.76%  |  1.50%  |  1.52%  |  1.36%  |  1.91%  |  1.55%  |  1.48%  |  1.31%  |  1.22%  |  1.52%  |  1.26%  |  1.86%  |  1.52%  |  1.14%  |  1.50%  |  1.41%  |  1.29%  |  1.38%  |  1.10%  |  1.29%  |  0.86%  |  0.62%  |  0.71%  |  0.93%  |  0.71%  |  0.52%  |  0.55%  |  0.41%  |  0.38%  |  0.38%  |  0.45%  |  0.64%  |  0.36%  |  0.36%  |  0.69%  |  0.33%  |  0.33%  |  0.43%  |  0.14%  |  0.14%  |  0.24%  |  0.10%  |  0.10%  |  0.14%  |  0.21%  |     20  |         3410  |  0.70%  |  0.70%  |  0.88%  |  0.73%  |  0.50%  |  0.50%  |  0.50%  |  0.62%  |  0.47%  |  0.62%  |  0.73%  |  0.44%  |  0.53%  |  0.12%  |  0.56%  |  0.09%  |  0.32%  |  0.70%  |  0.44%  |  0.70%  |  1.14%  |  1.00%  |  1.64%  |  2.05%  |  2.20%  |  2.67%  |  2.93%  |  2.49%  |  2.96%  |  2.43%  |  2.35%  |  2.23%  |  2.35%  |  2.11%  |  2.11%  |  2.11%  |  2.23%  |  2.14%  |  1.82%  |  1.91%  |  1.38%  |  1.64%  |  1.41%  |  1.61%  |  1.47%  |  1.44%  |  1.26%  |  1.06%  |  0.88%  |  1.41%  |  0.85%  |  1.11%  |  1.58%  |  1.20%  |  1.14%  |  1.14%  |  1.14%  |  1.64%  |  1.52%  |  1.55%  |  1.35%  |  0.79%  |  1.20%  |  1.03%  |  0.94%  |  0.79%  |  0.88%  |  0.94%  |  0.65%  |  0.70%  |  0.47%  |  0.79%  |  0.76%  |  0.53%  |  0.73%  |  0.67%  |  0.50%  |  0.53%  |  0.65%  |  0.67%  |  0.50%  |  0.50%  |  0.56%  |  0.32%  |  0.29%  |  0.29%  |  0.21%  |  0.26%  |  0.23%  |  0.03%  |     36  |\n (2008,City of London)  |  2008  |  City of London  |         7429  |       4131  |  0.70%  |  0.63%  |  0.53%  |  0.58%  |  0.44%  |  0.53%  |  0.31%  |  0.29%  |  0.41%  |  0.73%  |  0.48%  |  0.36%  |  0.73%  |  0.41%  |  0.24%  |  0.19%  |  0.15%  |  0.17%  |  0.27%  |  0.44%  |  0.68%  |  0.27%  |  1.04%  |  0.94%  |  1.82%  |  2.42%  |  2.06%  |  2.98%  |  2.95%  |  2.47%  |  2.25%  |  2.69%  |  2.32%  |  2.44%  |  1.82%  |  1.94%  |  2.49%  |  2.23%  |  2.01%  |  1.91%  |  2.93%  |  2.08%  |  1.91%  |  1.67%  |  1.69%  |  1.60%  |  1.86%  |  1.38%  |  1.62%  |  1.62%  |  1.96%  |  1.55%  |  1.48%  |  1.23%  |  1.26%  |  1.48%  |  1.33%  |  1.77%  |  1.48%  |  1.11%  |  1.45%  |  1.48%  |  1.28%  |  1.33%  |  1.02%  |  1.28%  |  0.82%  |  0.63%  |  0.70%  |  0.94%  |  0.70%  |  0.58%  |  0.53%  |  0.41%  |  0.36%  |  0.34%  |  0.46%  |  0.58%  |  0.34%  |  0.39%  |  0.65%  |  0.31%  |  0.29%  |  0.41%  |  0.15%  |  0.15%  |  0.17%  |  0.10%  |  0.07%  |  0.12%  |     25  |         3298  |  0.36%  |  0.52%  |  0.70%  |  0.70%  |  0.73%  |  0.49%  |  0.52%  |  0.49%  |  0.58%  |  0.45%  |  0.64%  |  0.73%  |  0.45%  |  0.58%  |  0.03%  |  0.58%  |  0.00%  |  0.67%  |  0.79%  |  0.76%  |  1.06%  |  1.30%  |  1.06%  |  1.73%  |  2.21%  |  2.49%  |  2.70%  |  2.88%  |  2.67%  |  2.85%  |  2.30%  |  2.33%  |  2.06%  |  2.09%  |  1.73%  |  1.82%  |  1.85%  |  1.94%  |  2.06%  |  1.73%  |  1.91%  |  1.24%  |  1.58%  |  1.39%  |  1.58%  |  1.52%  |  1.52%  |  1.12%  |  1.06%  |  0.97%  |  1.27%  |  0.88%  |  1.15%  |  1.61%  |  1.21%  |  1.27%  |  1.27%  |  1.15%  |  1.58%  |  1.55%  |  1.55%  |  1.46%  |  0.82%  |  1.24%  |  1.03%  |  0.88%  |  0.91%  |  0.94%  |  0.88%  |  0.70%  |  0.67%  |  0.49%  |  0.85%  |  0.82%  |  0.61%  |  0.73%  |  0.67%  |  0.49%  |  0.45%  |  0.67%  |  0.73%  |  0.45%  |  0.52%  |  0.55%  |  0.30%  |  0.27%  |  0.27%  |  0.21%  |  0.27%  |  0.24%  |     30  |\n\n\n\n\nCalling \nparallel()\n as per the example below enables concurrent iteration over the \nDataFrame\n rows which internally is \nimplemented using the \nFork and Join\n framework introduced in \nJava 7. This is the exact same procedure as above, but would likely be significantly faster for a very large frame on a multi-core \nmachine. It has to be said that parallel execution will not necessarily always be faster due to the overhead of context switching, \nhowever if the consumer function being executed is expensive or the frame is very large, you will likely see a significant \nperformance boost.\n\n\n\n\n\n//Parallel: Convert male & female population counts into weights\nframe.rows().parallel().forEach(row -> row.forEach(value -> {\n    if (value.colKey().matches(\"M\\\\s+\\\\d+\")) {\n        double totalMales = value.row().getDouble(\"All Males\");\n        double count = value.getDouble();\n        value.setDouble(count / totalMales);\n    } else if (value.colKey().matches(\"F\\\\s+\\\\d+\")) {\n        double totalFemales = value.row().getDouble(\"All Females\");\n        double count = value.getDouble();\n        value.setDouble(count / totalFemales);\n    }\n}));\n\n\n\n\nValue Access\n\n\nRandom Access\n\n\nForgetting about the ONS example for a moment, let's construct a new 5x5 \nDataFrame\n programmatically with\nmixed types as per the code below. The first 4 columns are of a homogeneous type, while the fifth column contains\na mixture of types.\n\n\n\n\n\nimport java.time.LocalDate;\nimport java.time.Month;\nimport java.time.Year;\n\nimport com.zavtech.morpheus.util.Range;\nimport com.zavtech.morpheus.array.Array;\nimport com.zavtech.morpheus.frame.DataFrame;\n\n//Create 5x5 frame with columns of different types.\nfinal Range<Year> years = Range.of(2000 ,2005).map(Year::of);\nfinal DataFrame<Year,String> frame = DataFrame.of(years, String.class, columns -> {\n    columns.add(\"Column-0\", Array.of(true, false, false, true, true));\n    columns.add(\"Column-1\", Array.of(1, 2, 3, 4, 5));\n    columns.add(\"Column-2\", Array.of(10L, 11L, 12L, 13L, 14L));\n    columns.add(\"Column-3\", Array.of(20d, 21d, 22d, 23d, 24d));\n    columns.add(\"Column-4\", Array.of(\"Hello\", LocalDate.of(1998, 1, 1), Month.JANUARY, 56.45d, true));\n});\n\n\n\n\n\n Index  |  Column-0  |  Column-1  |  Column-2  |  Column-3  |   Column-4   |\n----------------------------------------------------------------------------\n  2000  |      true  |         1  |        10  |   20.0000  |       Hello  |\n  2001  |     false  |         2  |        11  |   21.0000  |  1998-01-01  |\n  2002  |     false  |         3  |        12  |   22.0000  |     JANUARY  |\n  2003  |      true  |         4  |        13  |   23.0000  |      56.450  |\n  2004  |      true  |         5  |        14  |   24.0000  |        true  |\n\n\n\n\nRandom access to \nDataFrame\n values is possible via the function calls illustrated below. There are type specific\naccess methods for primitives, specifically with support for \nboolean\n, \nint\n, \nlong\nand \ndouble\n. Type agnostic \nmethods are also available, namely \ngetValue()\n and \nsetValue()\n, however when accessing primitive values in this \nmanner, boxing will take place with a small but inevitable performance hit. The random access API allows values to \nbe accessed by ordinals, keys or a combination of ordinals and keys for ultimate flexibility.\n\n\nType specific random access methods on this frame using a combination of ordinals and keys are shown below. Note \nthat there are corresponding setter methods analogous to these getters, but are omitted for brevity. If a type \nincompatible call is made, for example calling \ngetBoolean()\n on a cell that contains a \ndouble\n, a \nDataFrameException\n\nwill be thrown.\n\n\n\n\n\n//Random access to primitive boolean values via ordinal or keys or any combination thereof\nboolean b1 = frame.data().getBoolean(0, 0);\nboolean b2 = frame.data().getBoolean(Year.of(2003), 0);\nboolean b3 = frame.data().getBoolean(0, \"Column-0\");\nboolean b4 = frame.data().getBoolean(Year.of(2001), \"Column-0\");\n\n//Random access to primitive int values via ordinal or keys or any combination thereof\nint i1 = frame.data().getInt(4, 1);\nint i2 = frame.data().getInt(Year.of(2003), 1);\nint i3 = frame.data().getInt(0, \"Column-1\");\nint i4 = frame.data().getInt(Year.of(2001), \"Column-1\");\n\n//Random access to primitive long values via ordinal or keys or any combination thereof\nlong l1 = frame.data().getLong(4, 2);\nlong l2 = frame.data().getLong(Year.of(2003), 2);\nlong l3 = frame.data().getLong(0, \"Column-2\");\nlong l4 = frame.data().getLong(Year.of(2001), \"Column-2\");\n\n//Random access to primitive double values via ordinal or keys or any combination thereof\ndouble d1 = frame.data().getDouble(4, 3);\ndouble d2 = frame.data().getDouble(Year.of(2003), 3);\ndouble d3 = frame.data().getDouble(0, \"Column-3\");\ndouble d4 = frame.data().getDouble(Year.of(2001), \"Column-3\");\n\n//Random access to any values via ordinal or keys or any combination thereof\nString o1 = frame.data().getValue(0, 4);\nLocalDate o2 = frame.data().getValue(Year.of(2001), 4);\nMonth o3 = frame.data().getValue(2, \"Column-4\");\nDouble o4 = frame.data().getValue(Year.of(2004), \"Column-4\");\n\n\n\n\n\nIteration\n\n\nTo iterate over all values in a \nDataFrame\n, use the \nforEachValue()\n method or the \nvalues()\n method\nto access a Java 8 stream of \nDataFrameValue\n objects. The API makes it easy to iterate over all\nvalues in parallel much the same way as the \njava.util.stream.Stream\n interface, simply by calling \n\nparallel()\n on the frame. The example below first creates a 8x4 frame of doubles initialized with random \nvalues between 0 and 1, and then counts the number of values > 0.5, first sequentially, then in parallel.\n\n\n\n\n\n//Create DataFrame of random doubles\nDataFrame<Integer,String> frame = DataFrame.ofDoubles(\n    Array.of(0, 1, 2, 3, 4, 5, 6, 7),\n    Array.of(\"A\", \"B\", \"C\", \"D\"), \n    value -> Math.random()\n);\n\n//Count number of values > 0.5d first sequentially, then in parallel\nlong count1 = frame.values().filter(v -> v.getDouble() > 0.5d).count();\nlong count2 = frame.values().parallel().filter(v -> v.getDouble() > 0.5d).count();\nAssert.assertEquals(count1, count2);\n\n\n\n\nIt should be noted that using the parallel API will not always speed up execution, and in some cases will \nactually result in worse performance. If one or more of the stream pipeline operations (such as \nfilter()\n)\nis slow, then parallel execution should significantly improve performance over sequential execution.\n\n\nDataFrameValue\n\n\nThe \nforEachValue()\n and the \nvalues()\n method operate on a class called \nDataFrameValue\n, which provides a\nversatile API that wraps the actual datum value. For example, it provides an \nisNull()\n method to indicate if\na value is null, which avoids the consumer from having to code type specific checks, such as for \nDouble.isNaN\n.\nThe \nDataFrameValue\n also provides access to the row and column ordinals and keys associated with the datum,\nvia \nrowOrdinal()\n, \ncolOrdinal()\n, \nrowKey()\n and \ncolKey()\n methods, in addition to the frame itself via\n\nframe()\n. This makes it possible to write more reusable consumers and predicates for example.\n\n\nFirst & Last\n\n\nSelecting the first and last row or column returns an \nOptional<DataFrameRow>\n and \nOptional<DataFrameColumn>\n \nrespectively, as shown in the following examples: \n\n\n\n\n\n//Load the Wimbledon dataset\nDataFrame<Integer,String> frame = getWimbledonData(2013);\n\n//Select the first row, and assert match date is 24-06-2013\nframe.rows().first().ifPresent(row -> {\n    LocalDate matchDate = row.getValue(\"Date\");\n    assert(matchDate.equals(LocalDate.of(2013, 6, 24)));\n});\n\n//Select last row, and assert the winner is A Murray.\nframe.rows().last().ifPresent(row -> {\n    String winner = row.getValue(\"Winner\");\n    assert(winner.equals(\"Murray A.\"));\n});\n\n//Select first column, namely the date column, and find max date\nframe.cols().first().ifPresent(column -> {\n    Optional<LocalDate> matchDate = column.max();\n    LocalDate finalDate = LocalDate.of(2013, 7, 7);\n    assert(matchDate.isPresent() && matchDate.get().equals(finalDate));\n});\n\n//Select last column and find the max AvgL betting odds\nframe.cols().last().ifPresent(column -> {\n    double matchDate = column.stats().max();\n    assert(matchDate == 23.26d);\n});\n\n\n\n\nExamples\n\n\nDemean\n\n\nLet's say we wish to demean the male and female population counts in the ONS \nDataFrame\n introduced earlier. That \nis to say, for each row, we want to compute the average male and female population value, and them subtract the \nmale average from all male counts, and subtract the female average from all female counts.\n\n\n\n\n\n//Demean male and female population counts\nfinal DataFrame<Tuple,String> onsFrame = loadPopulationDataset();\n\n//Iterate over male then female column set\nArray.of(\"M\\\\s+\\\\d++\", \"F\\\\s+\\\\d++\").forEach(regex -> {\n    onsFrame.cols().select(c -> c.key().matches(regex)).rows().parallel().forEach(row -> {\n        final double mean = row.stats().mean();\n        row.applyDoubles(v -> v.getDouble() - mean);\n    });\n});\n\n//Print frame to std out with custom formatting\nframe.out().print(formats -> {\n    formats.withDecimalFormat(\"All Persons\", \"0;-0\", 1);\n    formats.withDecimalFormat(\"All Males\", \"0;-0\", 1);\n    formats.withDecimalFormat(\"All Females\", \"0;-0\", 1);\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});\n\n\n\n\n\n         Index          |  Year  |     Borough      |  All Persons  |  All Males  |   M 0    |   M 1    |   M 2    |   M 3    |   M 4    |   M 5    |   M 6    |   M 7    |   M 8    |   M 9    |   M 10   |   M 11   |   M 12   |   M 13   |   M 14   |   M 15   |   M 16   |   M 17   |   M 18   |   M 19   |   M 20   |   M 21   |   M 22   |  M 23   |  M 24   |  M 25   |  M 26   |  M 27   |  M 28   |  M 29   |  M 30   |  M 31   |  M 32   |  M 33   |  M 34   |  M 35   |  M 36   |  M 37   |  M 38   |  M 39   |  M 40   |  M 41   |  M 42   |  M 43   |  M 44   |  M 45   |  M 46   |  M 47   |  M 48   |  M 49   |  M 50   |  M 51   |  M 52   |  M 53   |  M 54   |  M 55   |  M 56   |  M 57   |  M 58   |  M 59   |   M 60   |   M 61   |   M 62   |   M 63   |   M 64   |   M 65   |   M 66   |   M 67   |   M 68   |   M 69   |   M 70   |   M 71   |   M 72   |   M 73   |   M 74   |   M 75   |   M 76   |   M 77   |   M 78   |   M 79   |   M 80   |   M 81   |   M 82   |   M 83   |   M 84   |   M 85   |   M 86   |   M 87   |   M 88   |   M 89   |  M 90+  |  All Females  |   F 0    |   F 1    |   F 2    |   F 3    |   F 4    |   F 5    |   F 6    |   F 7    |   F 8    |   F 9    |   F 10   |   F 11   |   F 12   |   F 13   |   F 14   |   F 15   |   F 16   |   F 17   |   F 18   |   F 19   |   F 20   |  F 21   |  F 22   |  F 23   |  F 24   |  F 25   |  F 26   |  F 27   |  F 28   |  F 29   |  F 30   |  F 31   |  F 32   |  F 33   |  F 34   |  F 35   |  F 36   |  F 37   |  F 38   |  F 39   |  F 40   |  F 41   |  F 42   |  F 43   |  F 44   |  F 45   |  F 46   |  F 47   |  F 48   |  F 49   |  F 50   |  F 51   |  F 52   |  F 53   |  F 54   |  F 55   |  F 56   |  F 57   |  F 58   |  F 59   |  F 60   |   F 61   |   F 62   |   F 63   |   F 64   |   F 65   |   F 66   |   F 67   |   F 68   |   F 69   |   F 70   |   F 71   |   F 72   |   F 73   |   F 74   |   F 75   |   F 76   |   F 77   |   F 78   |   F 79   |   F 80   |   F 81   |   F 82   |   F 83   |   F 84   |   F 85   |   F 86   |   F 87   |   F 88   |   F 89   |  F 90+  |\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n (1999,City of London)  |  1999  |  City of London  |         6581  |       3519  |  -16.92  |  -17.92  |  -18.92  |  -19.92  |  -20.92  |  -19.92  |  -20.92  |  -19.92  |  -20.92  |  -18.92  |  -18.92  |  -19.92  |  -21.92  |  -22.92  |  -23.92  |  -23.92  |  -23.92  |  -19.92  |  -14.92  |   -8.92  |   -0.92  |    6.08  |   11.08  |  19.08  |  24.08  |  29.08  |  32.08  |  32.08  |  32.08  |  37.08  |  37.08  |  36.08  |  34.08  |  31.08  |  30.08  |  30.08  |  24.08  |  19.08  |  17.08  |  17.08  |  16.08  |  14.08  |  10.08  |  12.08  |  12.08  |  18.08  |  18.08  |  19.08  |  24.08  |  27.08  |  27.08  |  29.08  |  22.08  |  21.08  |  20.08  |  13.08  |   8.08  |   6.08  |   2.08  |  -2.92  |   -5.92  |   -6.92  |   -6.92  |   -7.92  |   -8.92  |   -9.92  |   -8.92  |   -9.92  |  -10.92  |  -10.92  |  -11.92  |  -13.92  |  -14.92  |  -16.92  |  -17.92  |  -20.92  |  -20.92  |  -22.92  |  -23.92  |  -25.92  |  -26.92  |  -28.92  |  -28.92  |  -29.92  |  -29.92  |    2.08  |     NaN  |     NaN  |     NaN  |     NaN  |      0  |         3062  |  -11.60  |  -11.60  |  -13.60  |  -13.60  |  -14.60  |  -14.60  |  -16.60  |  -16.60  |  -17.60  |  -17.60  |  -17.60  |  -19.60  |  -18.60  |  -19.60  |  -19.60  |  -19.60  |  -15.60  |  -10.60  |   -9.60  |   -5.60  |    3.40  |  11.40  |  14.40  |  18.40  |  25.40  |  32.40  |  35.40  |  36.40  |  35.40  |  37.40  |  35.40  |  32.40  |  30.40  |  26.40  |  19.40  |  15.40  |  11.40  |   7.40  |   6.40  |   3.40  |   3.40  |   5.40  |   6.40  |   7.40  |   8.40  |   4.40  |   8.40  |   9.40  |  11.40  |  12.40  |  13.40  |  13.40  |  14.40  |  10.40  |   8.40  |   3.40  |   0.40  |  -1.60  |  -2.60  |  -4.60  |  -5.60  |   -6.60  |   -6.60  |   -5.60  |   -4.60  |   -5.60  |   -5.60  |   -6.60  |   -6.60  |   -7.60  |   -9.60  |   -9.60  |  -10.60  |  -12.60  |  -14.60  |  -15.60  |  -15.60  |  -16.60  |  -18.60  |  -18.60  |  -18.60  |  -19.60  |  -20.60  |  -21.60  |  -22.60  |   40.40  |     NaN  |     NaN  |     NaN  |     NaN  |      0  |\n (2000,City of London)  |  2000  |  City of London  |         7014  |       3775  |  -18.90  |  -18.90  |  -19.90  |  -20.90  |  -21.90  |  -22.90  |  -21.90  |  -22.90  |  -21.90  |  -22.90  |  -21.90  |  -23.90  |  -25.90  |  -27.90  |  -27.90  |  -28.90  |  -26.90  |  -24.90  |  -19.90  |  -13.90  |   -4.90  |    4.10  |   13.10  |  20.10  |  28.10  |  37.10  |  44.10  |  46.10  |  46.10  |  47.10  |  50.10  |  50.10  |  46.10  |  42.10  |  37.10  |  36.10  |  32.10  |  26.10  |  21.10  |  18.10  |  18.10  |  18.10  |  15.10  |  12.10  |  12.10  |  12.10  |  18.10  |  18.10  |  18.10  |  23.10  |  27.10  |  27.10  |  30.10  |  22.10  |  20.10  |  19.10  |  12.10  |   6.10  |   3.10  |  -0.90  |   -4.90  |   -9.90  |  -11.90  |  -12.90  |  -13.90  |  -14.90  |  -15.90  |  -15.90  |  -16.90  |  -15.90  |  -16.90  |  -17.90  |  -18.90  |  -20.90  |  -21.90  |  -21.90  |  -24.90  |  -24.90  |  -26.90  |  -27.90  |  -28.90  |  -29.90  |  -31.90  |  -31.90  |  -32.90  |   -4.90  |     NaN  |     NaN  |     NaN  |     NaN  |      0  |         3239  |  -11.66  |  -13.66  |  -13.66  |  -15.66  |  -16.66  |  -16.66  |  -17.66  |  -18.66  |  -18.66  |  -18.66  |  -18.66  |  -19.66  |  -20.66  |  -19.66  |  -20.66  |  -19.66  |  -17.66  |  -14.66  |   -9.66  |   -5.66  |    0.34  |  11.34  |  20.34  |  23.34  |  31.34  |  37.34  |  44.34  |  45.34  |  46.34  |  43.34  |  42.34  |  38.34  |  35.34  |  31.34  |  26.34  |  19.34  |  15.34  |  10.34  |   6.34  |   5.34  |   3.34  |   4.34  |   3.34  |   5.34  |   5.34  |   6.34  |   3.34  |   7.34  |   7.34  |   9.34  |  11.34  |  12.34  |  13.34  |  13.34  |   9.34  |   7.34  |   3.34  |  -0.66  |  -2.66  |  -5.66  |  -7.66  |   -8.66  |   -9.66  |  -10.66  |   -9.66  |   -8.66  |   -9.66  |   -9.66  |   -9.66  |   -9.66  |  -10.66  |  -12.66  |  -12.66  |  -13.66  |  -15.66  |  -16.66  |  -18.66  |  -17.66  |  -18.66  |  -20.66  |  -21.66  |  -22.66  |  -22.66  |  -22.66  |  -24.66  |   43.34  |     NaN  |     NaN  |     NaN  |     NaN  |      0  |\n (2001,City of London)  |  2001  |  City of London  |         7359  |       3984  |  -27.12  |  -14.12  |  -19.12  |  -25.12  |  -23.12  |  -19.12  |  -24.12  |  -28.12  |  -25.12  |  -17.12  |  -18.12  |  -28.12  |  -15.12  |  -25.12  |  -33.12  |  -30.12  |  -32.12  |  -31.12  |  -32.12  |  -17.12  |   -6.12  |   -5.12  |    8.88  |  18.88  |  35.88  |  65.88  |  65.88  |  55.88  |  63.88  |  80.88  |  69.88  |  50.88  |  48.88  |  60.88  |  54.88  |  39.88  |  44.88  |  26.88  |  18.88  |  30.88  |  20.88  |   7.88  |   2.88  |  31.88  |  28.88  |   9.88  |   4.88  |   0.88  |  15.88  |   4.88  |  66.88  |  28.88  |  10.88  |  36.88  |  35.88  |  16.88  |  18.88  |  11.88  |  19.88  |   3.88  |  -16.12  |  -10.12  |    0.88  |   -8.12  |  -19.12  |  -16.12  |  -23.12  |  -16.12  |  -23.12  |  -17.12  |  -17.12  |  -23.12  |  -20.12  |  -15.12  |  -23.12  |  -25.12  |  -21.12  |  -24.12  |  -33.12  |  -25.12  |  -29.12  |  -30.12  |  -36.12  |  -29.12  |  -34.12  |  -38.12  |  -34.12  |  -39.12  |  -40.12  |  -41.12  |     13  |         3375  |   -5.33  |   -7.33  |  -14.33  |  -16.33  |  -11.33  |  -21.33  |  -16.33  |  -22.33  |  -19.33  |  -13.33  |  -17.33  |  -17.33  |  -27.33  |  -11.33  |  -24.33  |  -24.33  |  -23.33  |    1.67  |   -9.33  |   -2.33  |    9.67  |   1.67  |  11.67  |  16.67  |  36.67  |  55.67  |  51.67  |  42.67  |  46.67  |  69.67  |  47.67  |  45.67  |  63.67  |  28.67  |  22.67  |  16.67  |  28.67  |  22.67  |  10.67  |   7.67  |  -0.33  |  -9.33  |  -6.33  |  25.67  |   5.67  |  15.67  |  12.67  |   9.67  |  -2.33  |   4.67  |  -0.33  |  27.67  |  10.67  |  19.67  |  19.67  |   0.67  |   7.67  |   0.67  |   0.67  |  -4.33  |  -5.33  |   -5.33  |  -11.33  |  -12.33  |  -13.33  |   -8.33  |   -9.33  |  -10.33  |   -3.33  |  -10.33  |  -17.33  |  -11.33  |  -13.33  |   -6.33  |  -19.33  |  -13.33  |  -10.33  |  -24.33  |  -24.33  |  -22.33  |  -19.33  |  -15.33  |  -19.33  |  -26.33  |  -27.33  |  -27.33  |  -23.33  |  -11.33  |  -21.33  |  -30.33  |     15  |\n (2002,City of London)  |  2002  |  City of London  |         7280  |       3968  |  -14.93  |  -25.93  |  -15.93  |  -16.93  |  -23.93  |  -23.93  |  -19.93  |  -23.93  |  -26.93  |  -23.93  |  -18.93  |  -18.93  |  -29.93  |  -16.93  |  -34.93  |  -38.93  |  -35.93  |  -33.93  |  -31.93  |  -33.93  |  -23.93  |   -4.93  |    6.07  |  17.07  |  24.07  |  55.07  |  69.07  |  78.07  |  58.07  |  70.07  |  70.07  |  63.07  |  55.07  |  45.07  |  57.07  |  49.07  |  40.07  |  33.07  |  29.07  |  17.07  |  34.07  |  21.07  |  13.07  |   6.07  |  31.07  |  28.07  |  12.07  |   6.07  |   1.07  |  17.07  |   2.07  |  69.07  |  26.07  |   5.07  |  41.07  |  32.07  |  15.07  |  17.07  |  11.07  |  16.07  |    1.07  |  -15.93  |  -12.93  |    2.07  |   -3.93  |  -20.93  |  -17.93  |  -23.93  |  -18.93  |  -22.93  |  -19.93  |  -13.93  |  -22.93  |  -18.93  |  -14.93  |  -23.93  |  -25.93  |  -22.93  |  -26.93  |  -32.93  |  -26.93  |  -29.93  |  -30.93  |  -35.93  |  -28.93  |  -34.93  |  -37.93  |  -35.93  |  -39.93  |  -41.93  |     14  |         3312  |   -8.58  |   -1.58  |   -6.58  |  -17.58  |  -15.58  |  -10.58  |  -21.58  |  -16.58  |  -23.58  |  -18.58  |  -19.58  |  -21.58  |  -19.58  |  -30.58  |  -17.58  |  -23.58  |  -31.58  |  -21.58  |   -1.58  |   -7.58  |   -1.58  |  11.42  |  15.42  |  10.42  |  30.42  |  37.42  |  47.42  |  47.42  |  46.42  |  33.42  |  78.42  |  47.42  |  57.42  |  64.42  |  26.42  |  24.42  |  20.42  |  21.42  |  25.42  |   7.42  |  10.42  |   0.42  |  -8.58  |  -3.58  |  23.42  |  -0.58  |  10.42  |  15.42  |   8.42  |  -0.58  |   1.42  |   4.42  |  26.42  |  14.42  |  17.42  |  18.42  |   0.42  |   4.42  |   3.42  |  -0.58  |  -6.58  |   -7.58  |   -6.58  |   -9.58  |  -13.58  |  -13.58  |   -8.58  |   -8.58  |  -11.58  |   -2.58  |  -11.58  |  -17.58  |  -12.58  |  -13.58  |   -5.58  |  -17.58  |  -11.58  |  -10.58  |  -26.58  |  -24.58  |  -22.58  |  -19.58  |  -14.58  |  -21.58  |  -27.58  |  -26.58  |  -29.58  |  -25.58  |  -10.58  |  -22.58  |     20  |\n (2003,City of London)  |  2003  |  City of London  |         7115  |       3892  |   -8.10  |  -19.10  |  -27.10  |  -21.10  |  -17.10  |  -26.10  |  -24.10  |  -19.10  |  -22.10  |  -22.10  |  -19.10  |  -18.10  |  -22.10  |  -33.10  |  -27.10  |  -33.10  |  -39.10  |  -38.10  |  -31.10  |  -31.10  |  -32.10  |  -24.10  |    5.90  |  16.90  |  22.90  |  36.90  |  56.90  |  74.90  |  70.90  |  61.90  |  64.90  |  61.90  |  55.90  |  50.90  |  41.90  |  53.90  |  45.90  |  38.90  |  32.90  |  25.90  |  19.90  |  30.90  |  20.90  |  13.90  |   7.90  |  31.90  |  30.90  |  12.90  |   7.90  |  -4.10  |  20.90  |   0.90  |  64.90  |  21.90  |   3.90  |  36.90  |  30.90  |  12.90  |  17.90  |  11.90  |   11.90  |    0.90  |  -17.10  |  -13.10  |   -1.10  |   -7.10  |  -20.10  |  -19.10  |  -21.10  |  -21.10  |  -25.10  |  -20.10  |  -12.10  |  -25.10  |  -19.10  |  -16.10  |  -24.10  |  -26.10  |  -21.10  |  -28.10  |  -33.10  |  -27.10  |  -31.10  |  -33.10  |  -35.10  |  -29.10  |  -35.10  |  -37.10  |  -36.10  |  -41.10  |     13  |         3223  |  -17.52  |  -10.52  |   -4.52  |   -6.52  |  -17.52  |  -14.52  |   -9.52  |  -21.52  |  -16.52  |  -23.52  |  -16.52  |  -22.52  |  -24.52  |  -23.52  |  -33.52  |  -19.52  |  -28.52  |  -23.52  |  -13.52  |   -4.52  |   -1.52  |   1.48  |  16.48  |  25.48  |  22.48  |  49.48  |  37.48  |  50.48  |  46.48  |  49.48  |  39.48  |  63.48  |  49.48  |  53.48  |  56.48  |  24.48  |  22.48  |   9.48  |  15.48  |  17.48  |   6.48  |  11.48  |   8.48  |  -6.52  |  -0.52  |  23.48  |   2.48  |   5.48  |  13.48  |   3.48  |  -0.52  |   4.48  |   1.48  |  25.48  |  13.48  |  17.48  |  14.48  |  -0.52  |   4.48  |  -1.52  |   0.48  |   -5.52  |   -6.52  |   -5.52  |   -8.52  |  -11.52  |  -12.52  |   -7.52  |   -5.52  |  -10.52  |   -3.52  |  -11.52  |  -18.52  |  -12.52  |  -14.52  |   -5.52  |  -19.52  |  -12.52  |  -11.52  |  -25.52  |  -24.52  |  -21.52  |  -20.52  |  -19.52  |  -19.52  |  -26.52  |  -33.52  |  -30.52  |  -25.52  |  -13.52  |     26  |\n (2004,City of London)  |  2004  |  City of London  |         7118  |       3875  |  -14.92  |  -15.92  |  -17.92  |  -26.92  |  -22.92  |  -15.92  |  -25.92  |  -22.92  |  -11.92  |  -24.92  |  -20.92  |  -21.92  |  -20.92  |  -26.92  |  -38.92  |  -29.92  |  -34.92  |  -42.92  |  -40.92  |  -29.92  |  -33.92  |  -17.92  |  -16.92  |  20.08  |  27.08  |  26.08  |  46.08  |  69.08  |  72.08  |  73.08  |  57.08  |  50.08  |  59.08  |  50.08  |  43.08  |  36.08  |  56.08  |  49.08  |  45.08  |  38.08  |  30.08  |  16.08  |  22.08  |  24.08  |  15.08  |   6.08  |  33.08  |  27.08  |  12.08  |   7.08  |   1.08  |  18.08  |   2.08  |  56.08  |  22.08  |   3.08  |  34.08  |  33.08  |  12.08  |  14.08  |   10.08  |   14.08  |    0.08  |  -14.92  |  -10.92  |   -3.92  |   -8.92  |  -20.92  |  -17.92  |  -21.92  |  -20.92  |  -24.92  |  -22.92  |  -12.92  |  -26.92  |  -18.92  |  -14.92  |  -23.92  |  -28.92  |  -22.92  |  -33.92  |  -30.92  |  -28.92  |  -32.92  |  -34.92  |  -34.92  |  -27.92  |  -34.92  |  -37.92  |  -37.92  |     12  |         3243  |   -4.57  |  -17.57  |  -15.57  |   -9.57  |  -10.57  |  -15.57  |  -10.57  |   -7.57  |  -21.57  |  -16.57  |  -28.57  |  -16.57  |  -25.57  |  -26.57  |  -26.57  |  -34.57  |  -22.57  |  -18.57  |  -26.57  |  -13.57  |    2.43  |   5.43  |  11.43  |  23.43  |  43.43  |  44.43  |  55.43  |  50.43  |  43.43  |  45.43  |  43.43  |  42.43  |  55.43  |  44.43  |  51.43  |  47.43  |  20.43  |  25.43  |  11.43  |  12.43  |  19.43  |   4.43  |   9.43  |   3.43  |  -5.57  |  -1.57  |  23.43  |   1.43  |   3.43  |  20.43  |   2.43  |  -2.57  |   2.43  |   0.43  |  24.43  |  15.43  |  16.43  |  15.43  |  -1.57  |   4.43  |  -0.57  |   -1.57  |   -7.57  |   -3.57  |   -7.57  |   -8.57  |  -10.57  |  -15.57  |   -6.57  |   -7.57  |  -11.57  |   -5.57  |  -11.57  |  -20.57  |  -14.57  |  -13.57  |   -6.57  |  -20.57  |  -12.57  |  -13.57  |  -23.57  |  -24.57  |  -23.57  |  -24.57  |  -23.57  |  -23.57  |  -28.57  |  -34.57  |  -32.57  |  -26.57  |     42  |\n (2005,City of London)  |  2005  |  City of London  |         7131  |       3869  |   -8.86  |  -17.86  |  -20.86  |  -19.86  |  -30.86  |  -23.86  |  -20.86  |  -23.86  |  -23.86  |  -11.86  |  -22.86  |  -21.86  |  -22.86  |  -23.86  |  -38.86  |  -39.86  |  -30.86  |  -32.86  |  -42.86  |  -35.86  |  -33.86  |  -26.86  |    0.14  |  -2.86  |  29.14  |  50.14  |  31.14  |  46.14  |  74.14  |  70.14  |  61.14  |  45.14  |  49.14  |  66.14  |  55.14  |  34.14  |  33.14  |  60.14  |  46.14  |  35.14  |  37.14  |  25.14  |  18.14  |  26.14  |  19.14  |  15.14  |   8.14  |  39.14  |  24.14  |  15.14  |   9.14  |  -0.86  |  16.14  |   7.14  |  46.14  |  24.14  |   4.14  |  30.14  |  27.14  |  12.14  |   15.14  |    7.14  |   10.14  |   -0.86  |  -15.86  |  -12.86  |   -0.86  |  -11.86  |  -19.86  |  -18.86  |  -22.86  |  -25.86  |  -25.86  |  -23.86  |  -12.86  |  -27.86  |  -21.86  |  -15.86  |  -25.86  |  -28.86  |  -21.86  |  -34.86  |  -28.86  |  -30.86  |  -35.86  |  -35.86  |  -36.86  |  -28.86  |  -35.86  |  -40.86  |     12  |         3262  |   -6.69  |   -6.69  |  -20.69  |  -20.69  |  -12.69  |  -10.69  |  -18.69  |  -12.69  |  -10.69  |  -21.69  |  -17.69  |  -28.69  |  -15.69  |  -28.69  |  -28.69  |  -29.69  |  -33.69  |  -28.69  |  -16.69  |  -20.69  |   -4.69  |   7.31  |  13.31  |  14.31  |  46.31  |  40.31  |  53.31  |  53.31  |  49.31  |  40.31  |  54.31  |  40.31  |  49.31  |  55.31  |  38.31  |  44.31  |  38.31  |  24.31  |  19.31  |  16.31  |   9.31  |  16.31  |   7.31  |  11.31  |   2.31  |  -1.69  |  -1.69  |  19.31  |  -2.69  |   2.31  |  21.31  |   2.31  |   0.31  |   3.31  |   1.31  |  22.31  |  16.31  |  19.31  |  14.31  |  -1.69  |   1.31  |   -0.69  |    1.31  |   -9.69  |   -2.69  |   -5.69  |  -10.69  |  -11.69  |  -17.69  |   -8.69  |   -9.69  |  -15.69  |   -7.69  |  -13.69  |  -17.69  |  -14.69  |  -11.69  |   -9.69  |  -19.69  |  -14.69  |  -13.69  |  -25.69  |  -23.69  |  -24.69  |  -25.69  |  -23.69  |  -26.69  |  -33.69  |  -33.69  |  -32.69  |     50  |\n (2006,City of London)  |  2006  |  City of London  |         7254  |       3959  |  -17.84  |  -18.84  |  -17.84  |  -23.84  |  -25.84  |  -30.84  |  -27.84  |  -20.84  |  -21.84  |  -25.84  |  -13.84  |  -25.84  |  -23.84  |  -26.84  |  -36.84  |  -38.84  |  -38.84  |  -28.84  |  -31.84  |  -43.84  |  -31.84  |  -24.84  |  -11.84  |  19.16  |  19.16  |  38.16  |  65.16  |  39.16  |  55.16  |  80.16  |  70.16  |  59.16  |  39.16  |  41.16  |  72.16  |  54.16  |  33.16  |  30.16  |  63.16  |  43.16  |  31.16  |  38.16  |  24.16  |  19.16  |  25.16  |  15.16  |  18.16  |   9.16  |  39.16  |  22.16  |  16.16  |   8.16  |   4.16  |  19.16  |   4.16  |  41.16  |  23.16  |   3.16  |  24.16  |  18.16  |   12.16  |   12.16  |    4.16  |   10.16  |   -3.84  |  -15.84  |  -17.84  |   -2.84  |  -12.84  |  -21.84  |  -20.84  |  -23.84  |  -29.84  |  -27.84  |  -24.84  |  -14.84  |  -26.84  |  -23.84  |  -15.84  |  -26.84  |  -29.84  |  -24.84  |  -37.84  |  -33.84  |  -31.84  |  -38.84  |  -39.84  |  -37.84  |  -31.84  |  -36.84  |     13  |         3295  |   -8.11  |  -10.11  |   -5.11  |  -24.11  |  -19.11  |  -11.11  |  -13.11  |  -16.11  |  -14.11  |  -11.11  |  -22.11  |  -18.11  |  -31.11  |  -17.11  |  -32.11  |  -29.11  |  -32.11  |  -24.11  |  -29.11  |   -9.11  |  -12.11  |  -3.11  |  14.89  |  21.89  |  34.89  |  51.89  |  43.89  |  49.89  |  49.89  |  55.89  |  47.89  |  49.89  |  41.89  |  47.89  |  43.89  |  40.89  |  37.89  |  31.89  |  25.89  |   8.89  |  16.89  |   8.89  |  14.89  |   8.89  |   9.89  |   2.89  |   1.89  |  -3.11  |  16.89  |  -5.11  |   0.89  |  20.89  |   2.89  |  -0.11  |   1.89  |  -0.11  |  19.89  |  13.89  |  16.89  |  10.89  |  -6.11  |    4.89  |   -5.11  |   -2.11  |  -12.11  |   -6.11  |   -7.11  |  -13.11  |  -10.11  |  -19.11  |   -9.11  |  -10.11  |  -16.11  |  -10.11  |  -12.11  |  -18.11  |  -16.11  |  -14.11  |   -9.11  |  -19.11  |  -16.11  |  -17.11  |  -25.11  |  -25.11  |  -26.11  |  -27.11  |  -25.11  |  -27.11  |  -34.11  |  -36.11  |     45  |\n (2007,City of London)  |  2007  |  City of London  |         7607  |       4197  |  -15.41  |  -21.41  |  -22.41  |  -24.41  |  -23.41  |  -30.41  |  -33.41  |  -32.41  |  -19.41  |  -23.41  |  -31.41  |  -13.41  |  -30.41  |  -27.41  |  -39.41  |  -39.41  |  -43.41  |  -37.41  |  -30.41  |  -19.41  |  -46.41  |  -16.41  |  -11.41  |  17.59  |  44.59  |  43.59  |  66.59  |  76.59  |  52.59  |  60.59  |  76.59  |  55.59  |  59.59  |  37.59  |  40.59  |  67.59  |  57.59  |  39.59  |  36.59  |  70.59  |  41.59  |  32.59  |  27.59  |  23.59  |  28.59  |  27.59  |  16.59  |  17.59  |  10.59  |  33.59  |  18.59  |  15.59  |   8.59  |   4.59  |  17.59  |   6.59  |  31.59  |  17.59  |   1.59  |  16.59  |   12.59  |    7.59  |   11.59  |   -0.41  |    7.59  |  -10.41  |  -20.41  |  -16.41  |   -7.41  |  -16.41  |  -24.41  |  -23.41  |  -29.41  |  -30.41  |  -30.41  |  -27.41  |  -19.41  |  -31.41  |  -31.41  |  -17.41  |  -32.41  |  -32.41  |  -28.41  |  -40.41  |  -40.41  |  -36.41  |  -42.41  |  -42.41  |  -40.41  |  -37.41  |     20  |         3410  |  -13.49  |  -13.49  |   -7.49  |  -12.49  |  -20.49  |  -20.49  |  -20.49  |  -16.49  |  -21.49  |  -16.49  |  -12.49  |  -22.49  |  -19.49  |  -33.49  |  -18.49  |  -34.49  |  -26.49  |  -13.49  |  -22.49  |  -13.49  |    1.51  |  -3.49  |  18.51  |  32.51  |  37.51  |  53.51  |  62.51  |  47.51  |  63.51  |  45.51  |  42.51  |  38.51  |  42.51  |  34.51  |  34.51  |  34.51  |  38.51  |  35.51  |  24.51  |  27.51  |   9.51  |  18.51  |  10.51  |  17.51  |  12.51  |  11.51  |   5.51  |  -1.49  |  -7.49  |  10.51  |  -8.49  |   0.51  |  16.51  |   3.51  |   1.51  |   1.51  |   1.51  |  18.51  |  14.51  |  15.51  |   8.51  |  -10.49  |    3.51  |   -2.49  |   -5.49  |  -10.49  |   -7.49  |   -5.49  |  -15.49  |  -13.49  |  -21.49  |  -10.49  |  -11.49  |  -19.49  |  -12.49  |  -14.49  |  -20.49  |  -19.49  |  -15.49  |  -14.49  |  -20.49  |  -20.49  |  -18.49  |  -26.49  |  -27.49  |  -27.49  |  -30.49  |  -28.49  |  -29.49  |  -36.49  |     36  |\n (2008,City of London)  |  2008  |  City of London  |         7429  |       4131  |  -16.62  |  -19.62  |  -23.62  |  -21.62  |  -27.62  |  -23.62  |  -32.62  |  -33.62  |  -28.62  |  -15.62  |  -25.62  |  -30.62  |  -15.62  |  -28.62  |  -35.62  |  -37.62  |  -39.62  |  -38.62  |  -34.62  |  -27.62  |  -17.62  |  -34.62  |   -2.62  |  -6.62  |  29.38  |  54.38  |  39.38  |  77.38  |  76.38  |  56.38  |  47.38  |  65.38  |  50.38  |  55.38  |  29.38  |  34.38  |  57.38  |  46.38  |  37.38  |  33.38  |  75.38  |  40.38  |  33.38  |  23.38  |  24.38  |  20.38  |  31.38  |  11.38  |  21.38  |  21.38  |  35.38  |  18.38  |  15.38  |   5.38  |   6.38  |  15.38  |   9.38  |  27.38  |  15.38  |   0.38  |   14.38  |   15.38  |    7.38  |    9.38  |   -3.62  |    7.38  |  -11.62  |  -19.62  |  -16.62  |   -6.62  |  -16.62  |  -21.62  |  -23.62  |  -28.62  |  -30.62  |  -31.62  |  -26.62  |  -21.62  |  -31.62  |  -29.62  |  -18.62  |  -32.62  |  -33.62  |  -28.62  |  -39.62  |  -39.62  |  -38.62  |  -41.62  |  -42.62  |  -40.62  |     25  |         3298  |  -24.31  |  -19.31  |  -13.31  |  -13.31  |  -12.31  |  -20.31  |  -19.31  |  -20.31  |  -17.31  |  -21.31  |  -15.31  |  -12.31  |  -21.31  |  -17.31  |  -35.31  |  -17.31  |  -36.31  |  -14.31  |  -10.31  |  -11.31  |   -1.31  |   6.69  |  -1.31  |  20.69  |  36.69  |  45.69  |  52.69  |  58.69  |  51.69  |  57.69  |  39.69  |  40.69  |  31.69  |  32.69  |  20.69  |  23.69  |  24.69  |  27.69  |  31.69  |  20.69  |  26.69  |   4.69  |  15.69  |   9.69  |  15.69  |  13.69  |  13.69  |   0.69  |  -1.31  |  -4.31  |   5.69  |  -7.31  |   1.69  |  16.69  |   3.69  |   5.69  |   5.69  |   1.69  |  15.69  |  14.69  |  14.69  |   11.69  |   -9.31  |    4.69  |   -2.31  |   -7.31  |   -6.31  |   -5.31  |   -7.31  |  -13.31  |  -14.31  |  -20.31  |   -8.31  |   -9.31  |  -16.31  |  -12.31  |  -14.31  |  -20.31  |  -21.31  |  -14.31  |  -12.31  |  -21.31  |  -19.31  |  -18.31  |  -26.31  |  -27.31  |  -27.31  |  -29.31  |  -27.31  |  -28.31  |     30  |\n\n\n\n\nWeights\n\n\nThis example demonstrates a scenario where we wish to convert all the male and female counts to be a percentage of\ntotal count per that Borough in 2007. For example, in the City of London in 2007, the total population was 7607 people.\nWe would like to divide all the age specific male and female counts for the City of London by this number so we can see values \nas a percentage of the baseline in 2007. The code below shows how a reusable \nToDoubleFunction\n can be used to apply this\nlogic to different parts of the frame incrementally. Note this is done all in place, so if you would want to preserve the\noriginal frame, you ought to create a copy of the frame first using \nDataFrame.copy()\n.\n\n\n\n\n\n//Load ONS population dataset\nfinal DataFrame<Tuple,String> onsFrame = loadPopulationDataset();\n\n//Define function to compute population weight as a percentage of 2007 value per borough\nfinal ToDoubleFunction<DataFrameValue<Tuple,String>> compute = value -> {\n    final String borough = value.rowKey().item(1);\n    final Tuple rowKey2014 = Tuple.of(2007, borough);\n    final double boroughCountIn2014 = onsFrame.data().getDouble(rowKey2014, \"All Persons\");\n    return value.getDouble() / boroughCountIn2014;\n};\n\n//Apply function to various columns in order\nonsFrame.cols().select(c -> c.key().matches(\"[MF]\\\\s+\\\\d+\")).applyDoubles(compute);\nonsFrame.colAt(\"All Males\").applyDoubles(compute);\nonsFrame.colAt(\"All Females\").applyDoubles(compute);\n\n//Print frame to std out\nonsFrame.out().print(formats -> {\n    formats.withDecimalFormat(\"All Persons\", \"0.0;-0.0\", 1);\n    formats.withDecimalFormat(Double.class, \"0.00'%';-0.00'%'\", 100);\n});\n\n\n\n\n\n         Index          |  Year  |     Borough      |  All Persons  |  All Males  |   M 0   |   M 1   |   M 2   |   M 3   |   M 4   |   M 5   |   M 6   |   M 7   |   M 8   |   M 9   |  M 10   |  M 11   |  M 12   |  M 13   |  M 14   |  M 15   |  M 16   |  M 17   |  M 18   |  M 19   |  M 20   |  M 21   |  M 22   |  M 23   |  M 24   |  M 25   |  M 26   |  M 27   |  M 28   |  M 29   |  M 30   |  M 31   |  M 32   |  M 33   |  M 34   |  M 35   |  M 36   |  M 37   |  M 38   |  M 39   |  M 40   |  M 41   |  M 42   |  M 43   |  M 44   |  M 45   |  M 46   |  M 47   |  M 48   |  M 49   |  M 50   |  M 51   |  M 52   |  M 53   |  M 54   |  M 55   |  M 56   |  M 57   |  M 58   |  M 59   |  M 60   |  M 61   |  M 62   |  M 63   |  M 64   |  M 65   |  M 66   |  M 67   |  M 68   |  M 69   |  M 70   |  M 71   |  M 72   |  M 73   |  M 74   |  M 75   |  M 76   |  M 77   |  M 78   |  M 79   |  M 80   |  M 81   |  M 82   |  M 83   |  M 84   |  M 85   |  M 86   |  M 87   |  M 88   |  M 89   |  M 90+  |  All Females  |   F 0   |   F 1   |   F 2   |   F 3   |   F 4   |   F 5   |   F 6   |   F 7   |   F 8   |   F 9   |  F 10   |  F 11   |  F 12   |  F 13   |  F 14   |  F 15   |  F 16   |  F 17   |  F 18   |  F 19   |  F 20   |  F 21   |  F 22   |  F 23   |  F 24   |  F 25   |  F 26   |  F 27   |  F 28   |  F 29   |  F 30   |  F 31   |  F 32   |  F 33   |  F 34   |  F 35   |  F 36   |  F 37   |  F 38   |  F 39   |  F 40   |  F 41   |  F 42   |  F 43   |  F 44   |  F 45   |  F 46   |  F 47   |  F 48   |  F 49   |  F 50   |  F 51   |  F 52   |  F 53   |  F 54   |  F 55   |  F 56   |  F 57   |  F 58   |  F 59   |  F 60   |  F 61   |  F 62   |  F 63   |  F 64   |  F 65   |  F 66   |  F 67   |  F 68   |  F 69   |  F 70   |  F 71   |  F 72   |  F 73   |  F 74   |  F 75   |  F 76   |  F 77   |  F 78   |  F 79   |  F 80   |  F 81   |  F 82   |  F 83   |  F 84   |  F 85   |  F 86   |  F 87   |  F 88   |  F 89   |  F 90+  |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n (1999,City of London)  |  1999  |  City of London  |       6581.0  |     46.26%  |  0.32%  |  0.30%  |  0.29%  |  0.28%  |  0.26%  |  0.28%  |  0.26%  |  0.28%  |  0.26%  |  0.29%  |  0.29%  |  0.28%  |  0.25%  |  0.24%  |  0.22%  |  0.22%  |  0.22%  |  0.28%  |  0.34%  |  0.42%  |  0.53%  |  0.62%  |  0.68%  |  0.79%  |  0.85%  |  0.92%  |  0.96%  |  0.96%  |  0.96%  |  1.03%  |  1.03%  |  1.01%  |  0.99%  |  0.95%  |  0.93%  |  0.93%  |  0.85%  |  0.79%  |  0.76%  |  0.76%  |  0.75%  |  0.72%  |  0.67%  |  0.70%  |  0.70%  |  0.78%  |  0.78%  |  0.79%  |  0.85%  |  0.89%  |  0.89%  |  0.92%  |  0.83%  |  0.82%  |  0.80%  |  0.71%  |  0.64%  |  0.62%  |  0.57%  |  0.50%  |  0.46%  |  0.45%  |  0.45%  |  0.43%  |  0.42%  |  0.41%  |  0.42%  |  0.41%  |  0.39%  |  0.39%  |  0.38%  |  0.35%  |  0.34%  |  0.32%  |  0.30%  |  0.26%  |  0.26%  |  0.24%  |  0.22%  |  0.20%  |  0.18%  |  0.16%  |  0.16%  |  0.14%  |  0.14%  |  0.57%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |       40.25%  |  0.32%  |  0.32%  |  0.29%  |  0.29%  |  0.28%  |  0.28%  |  0.25%  |  0.25%  |  0.24%  |  0.24%  |  0.24%  |  0.21%  |  0.22%  |  0.21%  |  0.21%  |  0.21%  |  0.26%  |  0.33%  |  0.34%  |  0.39%  |  0.51%  |  0.62%  |  0.66%  |  0.71%  |  0.80%  |  0.89%  |  0.93%  |  0.95%  |  0.93%  |  0.96%  |  0.93%  |  0.89%  |  0.87%  |  0.82%  |  0.72%  |  0.67%  |  0.62%  |  0.57%  |  0.55%  |  0.51%  |  0.51%  |  0.54%  |  0.55%  |  0.57%  |  0.58%  |  0.53%  |  0.58%  |  0.59%  |  0.62%  |  0.63%  |  0.64%  |  0.64%  |  0.66%  |  0.60%  |  0.58%  |  0.51%  |  0.47%  |  0.45%  |  0.43%  |  0.41%  |  0.39%  |  0.38%  |  0.38%  |  0.39%  |  0.41%  |  0.39%  |  0.39%  |  0.38%  |  0.38%  |  0.37%  |  0.34%  |  0.34%  |  0.33%  |  0.30%  |  0.28%  |  0.26%  |  0.26%  |  0.25%  |  0.22%  |  0.22%  |  0.22%  |  0.21%  |  0.20%  |  0.18%  |  0.17%  |  1.00%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |\n (2000,City of London)  |  2000  |  City of London  |       7014.0  |     49.63%  |  0.33%  |  0.33%  |  0.32%  |  0.30%  |  0.29%  |  0.28%  |  0.29%  |  0.28%  |  0.29%  |  0.28%  |  0.29%  |  0.26%  |  0.24%  |  0.21%  |  0.21%  |  0.20%  |  0.22%  |  0.25%  |  0.32%  |  0.39%  |  0.51%  |  0.63%  |  0.75%  |  0.84%  |  0.95%  |  1.06%  |  1.16%  |  1.18%  |  1.18%  |  1.20%  |  1.24%  |  1.24%  |  1.18%  |  1.13%  |  1.06%  |  1.05%  |  1.00%  |  0.92%  |  0.85%  |  0.82%  |  0.82%  |  0.82%  |  0.78%  |  0.74%  |  0.74%  |  0.74%  |  0.82%  |  0.82%  |  0.82%  |  0.88%  |  0.93%  |  0.93%  |  0.97%  |  0.87%  |  0.84%  |  0.83%  |  0.74%  |  0.66%  |  0.62%  |  0.57%  |  0.51%  |  0.45%  |  0.42%  |  0.41%  |  0.39%  |  0.38%  |  0.37%  |  0.37%  |  0.35%  |  0.37%  |  0.35%  |  0.34%  |  0.33%  |  0.30%  |  0.29%  |  0.29%  |  0.25%  |  0.25%  |  0.22%  |  0.21%  |  0.20%  |  0.18%  |  0.16%  |  0.16%  |  0.14%  |  0.51%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |       42.58%  |  0.34%  |  0.32%  |  0.32%  |  0.29%  |  0.28%  |  0.28%  |  0.26%  |  0.25%  |  0.25%  |  0.25%  |  0.25%  |  0.24%  |  0.22%  |  0.24%  |  0.22%  |  0.24%  |  0.26%  |  0.30%  |  0.37%  |  0.42%  |  0.50%  |  0.64%  |  0.76%  |  0.80%  |  0.91%  |  0.99%  |  1.08%  |  1.09%  |  1.10%  |  1.06%  |  1.05%  |  1.00%  |  0.96%  |  0.91%  |  0.84%  |  0.75%  |  0.70%  |  0.63%  |  0.58%  |  0.57%  |  0.54%  |  0.55%  |  0.54%  |  0.57%  |  0.57%  |  0.58%  |  0.54%  |  0.59%  |  0.59%  |  0.62%  |  0.64%  |  0.66%  |  0.67%  |  0.67%  |  0.62%  |  0.59%  |  0.54%  |  0.49%  |  0.46%  |  0.42%  |  0.39%  |  0.38%  |  0.37%  |  0.35%  |  0.37%  |  0.38%  |  0.37%  |  0.37%  |  0.37%  |  0.37%  |  0.35%  |  0.33%  |  0.33%  |  0.32%  |  0.29%  |  0.28%  |  0.25%  |  0.26%  |  0.25%  |  0.22%  |  0.21%  |  0.20%  |  0.20%  |  0.20%  |  0.17%  |  1.06%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |\n (2001,City of London)  |  2001  |  City of London  |       7359.0  |     52.37%  |  0.22%  |  0.39%  |  0.33%  |  0.25%  |  0.28%  |  0.33%  |  0.26%  |  0.21%  |  0.25%  |  0.35%  |  0.34%  |  0.21%  |  0.38%  |  0.25%  |  0.14%  |  0.18%  |  0.16%  |  0.17%  |  0.16%  |  0.35%  |  0.50%  |  0.51%  |  0.70%  |  0.83%  |  1.05%  |  1.45%  |  1.45%  |  1.31%  |  1.42%  |  1.64%  |  1.50%  |  1.25%  |  1.22%  |  1.38%  |  1.30%  |  1.10%  |  1.17%  |  0.93%  |  0.83%  |  0.99%  |  0.85%  |  0.68%  |  0.62%  |  1.00%  |  0.96%  |  0.71%  |  0.64%  |  0.59%  |  0.79%  |  0.64%  |  1.46%  |  0.96%  |  0.72%  |  1.06%  |  1.05%  |  0.80%  |  0.83%  |  0.74%  |  0.84%  |  0.63%  |  0.37%  |  0.45%  |  0.59%  |  0.47%  |  0.33%  |  0.37%  |  0.28%  |  0.37%  |  0.28%  |  0.35%  |  0.35%  |  0.28%  |  0.32%  |  0.38%  |  0.28%  |  0.25%  |  0.30%  |  0.26%  |  0.14%  |  0.25%  |  0.20%  |  0.18%  |  0.11%  |  0.20%  |  0.13%  |  0.08%  |  0.13%  |  0.07%  |  0.05%  |  0.04%  |     13  |       44.37%  |  0.42%  |  0.39%  |  0.30%  |  0.28%  |  0.34%  |  0.21%  |  0.28%  |  0.20%  |  0.24%  |  0.32%  |  0.26%  |  0.26%  |  0.13%  |  0.34%  |  0.17%  |  0.17%  |  0.18%  |  0.51%  |  0.37%  |  0.46%  |  0.62%  |  0.51%  |  0.64%  |  0.71%  |  0.97%  |  1.22%  |  1.17%  |  1.05%  |  1.10%  |  1.41%  |  1.12%  |  1.09%  |  1.33%  |  0.87%  |  0.79%  |  0.71%  |  0.87%  |  0.79%  |  0.63%  |  0.59%  |  0.49%  |  0.37%  |  0.41%  |  0.83%  |  0.57%  |  0.70%  |  0.66%  |  0.62%  |  0.46%  |  0.55%  |  0.49%  |  0.85%  |  0.63%  |  0.75%  |  0.75%  |  0.50%  |  0.59%  |  0.50%  |  0.50%  |  0.43%  |  0.42%  |  0.42%  |  0.34%  |  0.33%  |  0.32%  |  0.38%  |  0.37%  |  0.35%  |  0.45%  |  0.35%  |  0.26%  |  0.34%  |  0.32%  |  0.41%  |  0.24%  |  0.32%  |  0.35%  |  0.17%  |  0.17%  |  0.20%  |  0.24%  |  0.29%  |  0.24%  |  0.14%  |  0.13%  |  0.13%  |  0.18%  |  0.34%  |  0.21%  |  0.09%  |     15  |\n (2002,City of London)  |  2002  |  City of London  |       7280.0  |     52.16%  |  0.38%  |  0.24%  |  0.37%  |  0.35%  |  0.26%  |  0.26%  |  0.32%  |  0.26%  |  0.22%  |  0.26%  |  0.33%  |  0.33%  |  0.18%  |  0.35%  |  0.12%  |  0.07%  |  0.11%  |  0.13%  |  0.16%  |  0.13%  |  0.26%  |  0.51%  |  0.66%  |  0.80%  |  0.89%  |  1.30%  |  1.49%  |  1.60%  |  1.34%  |  1.50%  |  1.50%  |  1.41%  |  1.30%  |  1.17%  |  1.33%  |  1.22%  |  1.10%  |  1.01%  |  0.96%  |  0.80%  |  1.03%  |  0.85%  |  0.75%  |  0.66%  |  0.99%  |  0.95%  |  0.74%  |  0.66%  |  0.59%  |  0.80%  |  0.60%  |  1.49%  |  0.92%  |  0.64%  |  1.12%  |  1.00%  |  0.78%  |  0.80%  |  0.72%  |  0.79%  |  0.59%  |  0.37%  |  0.41%  |  0.60%  |  0.53%  |  0.30%  |  0.34%  |  0.26%  |  0.33%  |  0.28%  |  0.32%  |  0.39%  |  0.28%  |  0.33%  |  0.38%  |  0.26%  |  0.24%  |  0.28%  |  0.22%  |  0.14%  |  0.22%  |  0.18%  |  0.17%  |  0.11%  |  0.20%  |  0.12%  |  0.08%  |  0.11%  |  0.05%  |  0.03%  |     14  |       43.54%  |  0.37%  |  0.46%  |  0.39%  |  0.25%  |  0.28%  |  0.34%  |  0.20%  |  0.26%  |  0.17%  |  0.24%  |  0.22%  |  0.20%  |  0.22%  |  0.08%  |  0.25%  |  0.17%  |  0.07%  |  0.20%  |  0.46%  |  0.38%  |  0.46%  |  0.63%  |  0.68%  |  0.62%  |  0.88%  |  0.97%  |  1.10%  |  1.10%  |  1.09%  |  0.92%  |  1.51%  |  1.10%  |  1.24%  |  1.33%  |  0.83%  |  0.80%  |  0.75%  |  0.76%  |  0.82%  |  0.58%  |  0.62%  |  0.49%  |  0.37%  |  0.43%  |  0.79%  |  0.47%  |  0.62%  |  0.68%  |  0.59%  |  0.47%  |  0.50%  |  0.54%  |  0.83%  |  0.67%  |  0.71%  |  0.72%  |  0.49%  |  0.54%  |  0.53%  |  0.47%  |  0.39%  |  0.38%  |  0.39%  |  0.35%  |  0.30%  |  0.30%  |  0.37%  |  0.37%  |  0.33%  |  0.45%  |  0.33%  |  0.25%  |  0.32%  |  0.30%  |  0.41%  |  0.25%  |  0.33%  |  0.34%  |  0.13%  |  0.16%  |  0.18%  |  0.22%  |  0.29%  |  0.20%  |  0.12%  |  0.13%  |  0.09%  |  0.14%  |  0.34%  |  0.18%  |     20  |\n (2003,City of London)  |  2003  |  City of London  |       7115.0  |     51.16%  |  0.46%  |  0.32%  |  0.21%  |  0.29%  |  0.34%  |  0.22%  |  0.25%  |  0.32%  |  0.28%  |  0.28%  |  0.32%  |  0.33%  |  0.28%  |  0.13%  |  0.21%  |  0.13%  |  0.05%  |  0.07%  |  0.16%  |  0.16%  |  0.14%  |  0.25%  |  0.64%  |  0.79%  |  0.87%  |  1.05%  |  1.31%  |  1.55%  |  1.50%  |  1.38%  |  1.42%  |  1.38%  |  1.30%  |  1.24%  |  1.12%  |  1.28%  |  1.17%  |  1.08%  |  1.00%  |  0.91%  |  0.83%  |  0.97%  |  0.84%  |  0.75%  |  0.67%  |  0.99%  |  0.97%  |  0.74%  |  0.67%  |  0.51%  |  0.84%  |  0.58%  |  1.42%  |  0.85%  |  0.62%  |  1.05%  |  0.97%  |  0.74%  |  0.80%  |  0.72%  |  0.72%  |  0.58%  |  0.34%  |  0.39%  |  0.55%  |  0.47%  |  0.30%  |  0.32%  |  0.29%  |  0.29%  |  0.24%  |  0.30%  |  0.41%  |  0.24%  |  0.32%  |  0.35%  |  0.25%  |  0.22%  |  0.29%  |  0.20%  |  0.13%  |  0.21%  |  0.16%  |  0.13%  |  0.11%  |  0.18%  |  0.11%  |  0.08%  |  0.09%  |  0.03%  |     13  |       42.37%  |  0.24%  |  0.33%  |  0.41%  |  0.38%  |  0.24%  |  0.28%  |  0.34%  |  0.18%  |  0.25%  |  0.16%  |  0.25%  |  0.17%  |  0.14%  |  0.16%  |  0.03%  |  0.21%  |  0.09%  |  0.16%  |  0.29%  |  0.41%  |  0.45%  |  0.49%  |  0.68%  |  0.80%  |  0.76%  |  1.12%  |  0.96%  |  1.13%  |  1.08%  |  1.12%  |  0.99%  |  1.30%  |  1.12%  |  1.17%  |  1.21%  |  0.79%  |  0.76%  |  0.59%  |  0.67%  |  0.70%  |  0.55%  |  0.62%  |  0.58%  |  0.38%  |  0.46%  |  0.78%  |  0.50%  |  0.54%  |  0.64%  |  0.51%  |  0.46%  |  0.53%  |  0.49%  |  0.80%  |  0.64%  |  0.70%  |  0.66%  |  0.46%  |  0.53%  |  0.45%  |  0.47%  |  0.39%  |  0.38%  |  0.39%  |  0.35%  |  0.32%  |  0.30%  |  0.37%  |  0.39%  |  0.33%  |  0.42%  |  0.32%  |  0.22%  |  0.30%  |  0.28%  |  0.39%  |  0.21%  |  0.30%  |  0.32%  |  0.13%  |  0.14%  |  0.18%  |  0.20%  |  0.21%  |  0.21%  |  0.12%  |  0.03%  |  0.07%  |  0.13%  |  0.29%  |     26  |\n (2004,City of London)  |  2004  |  City of London  |       7118.0  |     50.94%  |  0.37%  |  0.35%  |  0.33%  |  0.21%  |  0.26%  |  0.35%  |  0.22%  |  0.26%  |  0.41%  |  0.24%  |  0.29%  |  0.28%  |  0.29%  |  0.21%  |  0.05%  |  0.17%  |  0.11%  |  0.00%  |  0.03%  |  0.17%  |  0.12%  |  0.33%  |  0.34%  |  0.83%  |  0.92%  |  0.91%  |  1.17%  |  1.47%  |  1.51%  |  1.52%  |  1.31%  |  1.22%  |  1.34%  |  1.22%  |  1.13%  |  1.04%  |  1.30%  |  1.21%  |  1.16%  |  1.06%  |  0.96%  |  0.78%  |  0.85%  |  0.88%  |  0.76%  |  0.64%  |  1.00%  |  0.92%  |  0.72%  |  0.66%  |  0.58%  |  0.80%  |  0.59%  |  1.30%  |  0.85%  |  0.60%  |  1.01%  |  1.00%  |  0.72%  |  0.75%  |  0.70%  |  0.75%  |  0.57%  |  0.37%  |  0.42%  |  0.51%  |  0.45%  |  0.29%  |  0.33%  |  0.28%  |  0.29%  |  0.24%  |  0.26%  |  0.39%  |  0.21%  |  0.32%  |  0.37%  |  0.25%  |  0.18%  |  0.26%  |  0.12%  |  0.16%  |  0.18%  |  0.13%  |  0.11%  |  0.11%  |  0.20%  |  0.11%  |  0.07%  |  0.07%  |     12  |       42.63%  |  0.41%  |  0.24%  |  0.26%  |  0.34%  |  0.33%  |  0.26%  |  0.33%  |  0.37%  |  0.18%  |  0.25%  |  0.09%  |  0.25%  |  0.13%  |  0.12%  |  0.12%  |  0.01%  |  0.17%  |  0.22%  |  0.12%  |  0.29%  |  0.50%  |  0.54%  |  0.62%  |  0.78%  |  1.04%  |  1.05%  |  1.20%  |  1.13%  |  1.04%  |  1.06%  |  1.04%  |  1.03%  |  1.20%  |  1.05%  |  1.14%  |  1.09%  |  0.74%  |  0.80%  |  0.62%  |  0.63%  |  0.72%  |  0.53%  |  0.59%  |  0.51%  |  0.39%  |  0.45%  |  0.78%  |  0.49%  |  0.51%  |  0.74%  |  0.50%  |  0.43%  |  0.50%  |  0.47%  |  0.79%  |  0.67%  |  0.68%  |  0.67%  |  0.45%  |  0.53%  |  0.46%  |  0.45%  |  0.37%  |  0.42%  |  0.37%  |  0.35%  |  0.33%  |  0.26%  |  0.38%  |  0.37%  |  0.32%  |  0.39%  |  0.32%  |  0.20%  |  0.28%  |  0.29%  |  0.38%  |  0.20%  |  0.30%  |  0.29%  |  0.16%  |  0.14%  |  0.16%  |  0.14%  |  0.16%  |  0.16%  |  0.09%  |  0.01%  |  0.04%  |  0.12%  |     42  |\n (2005,City of London)  |  2005  |  City of London  |       7131.0  |     50.86%  |  0.45%  |  0.33%  |  0.29%  |  0.30%  |  0.16%  |  0.25%  |  0.29%  |  0.25%  |  0.25%  |  0.41%  |  0.26%  |  0.28%  |  0.26%  |  0.25%  |  0.05%  |  0.04%  |  0.16%  |  0.13%  |  0.00%  |  0.09%  |  0.12%  |  0.21%  |  0.57%  |  0.53%  |  0.95%  |  1.22%  |  0.97%  |  1.17%  |  1.54%  |  1.49%  |  1.37%  |  1.16%  |  1.21%  |  1.43%  |  1.29%  |  1.01%  |  1.00%  |  1.35%  |  1.17%  |  1.03%  |  1.05%  |  0.89%  |  0.80%  |  0.91%  |  0.82%  |  0.76%  |  0.67%  |  1.08%  |  0.88%  |  0.76%  |  0.68%  |  0.55%  |  0.78%  |  0.66%  |  1.17%  |  0.88%  |  0.62%  |  0.96%  |  0.92%  |  0.72%  |  0.76%  |  0.66%  |  0.70%  |  0.55%  |  0.35%  |  0.39%  |  0.55%  |  0.41%  |  0.30%  |  0.32%  |  0.26%  |  0.22%  |  0.22%  |  0.25%  |  0.39%  |  0.20%  |  0.28%  |  0.35%  |  0.22%  |  0.18%  |  0.28%  |  0.11%  |  0.18%  |  0.16%  |  0.09%  |  0.09%  |  0.08%  |  0.18%  |  0.09%  |  0.03%  |     12  |       42.88%  |  0.38%  |  0.38%  |  0.20%  |  0.20%  |  0.30%  |  0.33%  |  0.22%  |  0.30%  |  0.33%  |  0.18%  |  0.24%  |  0.09%  |  0.26%  |  0.09%  |  0.09%  |  0.08%  |  0.03%  |  0.09%  |  0.25%  |  0.20%  |  0.41%  |  0.57%  |  0.64%  |  0.66%  |  1.08%  |  1.00%  |  1.17%  |  1.17%  |  1.12%  |  1.00%  |  1.18%  |  1.00%  |  1.12%  |  1.20%  |  0.97%  |  1.05%  |  0.97%  |  0.79%  |  0.72%  |  0.68%  |  0.59%  |  0.68%  |  0.57%  |  0.62%  |  0.50%  |  0.45%  |  0.45%  |  0.72%  |  0.43%  |  0.50%  |  0.75%  |  0.50%  |  0.47%  |  0.51%  |  0.49%  |  0.76%  |  0.68%  |  0.72%  |  0.66%  |  0.45%  |  0.49%  |  0.46%  |  0.49%  |  0.34%  |  0.43%  |  0.39%  |  0.33%  |  0.32%  |  0.24%  |  0.35%  |  0.34%  |  0.26%  |  0.37%  |  0.29%  |  0.24%  |  0.28%  |  0.32%  |  0.34%  |  0.21%  |  0.28%  |  0.29%  |  0.13%  |  0.16%  |  0.14%  |  0.13%  |  0.16%  |  0.12%  |  0.03%  |  0.03%  |  0.04%  |     50  |\n (2006,City of London)  |  2006  |  City of London  |       7254.0  |     52.04%  |  0.34%  |  0.33%  |  0.34%  |  0.26%  |  0.24%  |  0.17%  |  0.21%  |  0.30%  |  0.29%  |  0.24%  |  0.39%  |  0.24%  |  0.26%  |  0.22%  |  0.09%  |  0.07%  |  0.07%  |  0.20%  |  0.16%  |  0.00%  |  0.16%  |  0.25%  |  0.42%  |  0.83%  |  0.83%  |  1.08%  |  1.43%  |  1.09%  |  1.30%  |  1.63%  |  1.50%  |  1.35%  |  1.09%  |  1.12%  |  1.52%  |  1.29%  |  1.01%  |  0.97%  |  1.41%  |  1.14%  |  0.99%  |  1.08%  |  0.89%  |  0.83%  |  0.91%  |  0.78%  |  0.82%  |  0.70%  |  1.09%  |  0.87%  |  0.79%  |  0.68%  |  0.63%  |  0.83%  |  0.63%  |  1.12%  |  0.88%  |  0.62%  |  0.89%  |  0.82%  |  0.74%  |  0.74%  |  0.63%  |  0.71%  |  0.53%  |  0.37%  |  0.34%  |  0.54%  |  0.41%  |  0.29%  |  0.30%  |  0.26%  |  0.18%  |  0.21%  |  0.25%  |  0.38%  |  0.22%  |  0.26%  |  0.37%  |  0.22%  |  0.18%  |  0.25%  |  0.08%  |  0.13%  |  0.16%  |  0.07%  |  0.05%  |  0.08%  |  0.16%  |  0.09%  |     13  |       43.32%  |  0.37%  |  0.34%  |  0.41%  |  0.16%  |  0.22%  |  0.33%  |  0.30%  |  0.26%  |  0.29%  |  0.33%  |  0.18%  |  0.24%  |  0.07%  |  0.25%  |  0.05%  |  0.09%  |  0.05%  |  0.16%  |  0.09%  |  0.35%  |  0.32%  |  0.43%  |  0.67%  |  0.76%  |  0.93%  |  1.16%  |  1.05%  |  1.13%  |  1.13%  |  1.21%  |  1.10%  |  1.13%  |  1.03%  |  1.10%  |  1.05%  |  1.01%  |  0.97%  |  0.89%  |  0.82%  |  0.59%  |  0.70%  |  0.59%  |  0.67%  |  0.59%  |  0.60%  |  0.51%  |  0.50%  |  0.43%  |  0.70%  |  0.41%  |  0.49%  |  0.75%  |  0.51%  |  0.47%  |  0.50%  |  0.47%  |  0.74%  |  0.66%  |  0.70%  |  0.62%  |  0.39%  |  0.54%  |  0.41%  |  0.45%  |  0.32%  |  0.39%  |  0.38%  |  0.30%  |  0.34%  |  0.22%  |  0.35%  |  0.34%  |  0.26%  |  0.34%  |  0.32%  |  0.24%  |  0.26%  |  0.29%  |  0.35%  |  0.22%  |  0.26%  |  0.25%  |  0.14%  |  0.14%  |  0.13%  |  0.12%  |  0.14%  |  0.12%  |  0.03%  |  0.00%  |     45  |\n (2007,City of London)  |  2007  |  City of London  |       7607.0  |     55.17%  |  0.41%  |  0.33%  |  0.32%  |  0.29%  |  0.30%  |  0.21%  |  0.17%  |  0.18%  |  0.35%  |  0.30%  |  0.20%  |  0.43%  |  0.21%  |  0.25%  |  0.09%  |  0.09%  |  0.04%  |  0.12%  |  0.21%  |  0.35%  |  0.00%  |  0.39%  |  0.46%  |  0.84%  |  1.20%  |  1.18%  |  1.49%  |  1.62%  |  1.30%  |  1.41%  |  1.62%  |  1.34%  |  1.39%  |  1.10%  |  1.14%  |  1.50%  |  1.37%  |  1.13%  |  1.09%  |  1.54%  |  1.16%  |  1.04%  |  0.97%  |  0.92%  |  0.99%  |  0.97%  |  0.83%  |  0.84%  |  0.75%  |  1.05%  |  0.85%  |  0.82%  |  0.72%  |  0.67%  |  0.84%  |  0.70%  |  1.03%  |  0.84%  |  0.63%  |  0.83%  |  0.78%  |  0.71%  |  0.76%  |  0.60%  |  0.71%  |  0.47%  |  0.34%  |  0.39%  |  0.51%  |  0.39%  |  0.29%  |  0.30%  |  0.22%  |  0.21%  |  0.21%  |  0.25%  |  0.35%  |  0.20%  |  0.20%  |  0.38%  |  0.18%  |  0.18%  |  0.24%  |  0.08%  |  0.08%  |  0.13%  |  0.05%  |  0.05%  |  0.08%  |  0.12%  |     20  |       44.83%  |  0.32%  |  0.32%  |  0.39%  |  0.33%  |  0.22%  |  0.22%  |  0.22%  |  0.28%  |  0.21%  |  0.28%  |  0.33%  |  0.20%  |  0.24%  |  0.05%  |  0.25%  |  0.04%  |  0.14%  |  0.32%  |  0.20%  |  0.32%  |  0.51%  |  0.45%  |  0.74%  |  0.92%  |  0.99%  |  1.20%  |  1.31%  |  1.12%  |  1.33%  |  1.09%  |  1.05%  |  1.00%  |  1.05%  |  0.95%  |  0.95%  |  0.95%  |  1.00%  |  0.96%  |  0.82%  |  0.85%  |  0.62%  |  0.74%  |  0.63%  |  0.72%  |  0.66%  |  0.64%  |  0.57%  |  0.47%  |  0.39%  |  0.63%  |  0.38%  |  0.50%  |  0.71%  |  0.54%  |  0.51%  |  0.51%  |  0.51%  |  0.74%  |  0.68%  |  0.70%  |  0.60%  |  0.35%  |  0.54%  |  0.46%  |  0.42%  |  0.35%  |  0.39%  |  0.42%  |  0.29%  |  0.32%  |  0.21%  |  0.35%  |  0.34%  |  0.24%  |  0.33%  |  0.30%  |  0.22%  |  0.24%  |  0.29%  |  0.30%  |  0.22%  |  0.22%  |  0.25%  |  0.14%  |  0.13%  |  0.13%  |  0.09%  |  0.12%  |  0.11%  |  0.01%  |     36  |\n (2008,City of London)  |  2008  |  City of London  |       7429.0  |     54.31%  |  0.38%  |  0.34%  |  0.29%  |  0.32%  |  0.24%  |  0.29%  |  0.17%  |  0.16%  |  0.22%  |  0.39%  |  0.26%  |  0.20%  |  0.39%  |  0.22%  |  0.13%  |  0.11%  |  0.08%  |  0.09%  |  0.14%  |  0.24%  |  0.37%  |  0.14%  |  0.57%  |  0.51%  |  0.99%  |  1.31%  |  1.12%  |  1.62%  |  1.60%  |  1.34%  |  1.22%  |  1.46%  |  1.26%  |  1.33%  |  0.99%  |  1.05%  |  1.35%  |  1.21%  |  1.09%  |  1.04%  |  1.59%  |  1.13%  |  1.04%  |  0.91%  |  0.92%  |  0.87%  |  1.01%  |  0.75%  |  0.88%  |  0.88%  |  1.06%  |  0.84%  |  0.80%  |  0.67%  |  0.68%  |  0.80%  |  0.72%  |  0.96%  |  0.80%  |  0.60%  |  0.79%  |  0.80%  |  0.70%  |  0.72%  |  0.55%  |  0.70%  |  0.45%  |  0.34%  |  0.38%  |  0.51%  |  0.38%  |  0.32%  |  0.29%  |  0.22%  |  0.20%  |  0.18%  |  0.25%  |  0.32%  |  0.18%  |  0.21%  |  0.35%  |  0.17%  |  0.16%  |  0.22%  |  0.08%  |  0.08%  |  0.09%  |  0.05%  |  0.04%  |  0.07%  |     25  |       43.35%  |  0.16%  |  0.22%  |  0.30%  |  0.30%  |  0.32%  |  0.21%  |  0.22%  |  0.21%  |  0.25%  |  0.20%  |  0.28%  |  0.32%  |  0.20%  |  0.25%  |  0.01%  |  0.25%  |  0.00%  |  0.29%  |  0.34%  |  0.33%  |  0.46%  |  0.57%  |  0.46%  |  0.75%  |  0.96%  |  1.08%  |  1.17%  |  1.25%  |  1.16%  |  1.24%  |  1.00%  |  1.01%  |  0.89%  |  0.91%  |  0.75%  |  0.79%  |  0.80%  |  0.84%  |  0.89%  |  0.75%  |  0.83%  |  0.54%  |  0.68%  |  0.60%  |  0.68%  |  0.66%  |  0.66%  |  0.49%  |  0.46%  |  0.42%  |  0.55%  |  0.38%  |  0.50%  |  0.70%  |  0.53%  |  0.55%  |  0.55%  |  0.50%  |  0.68%  |  0.67%  |  0.67%  |  0.63%  |  0.35%  |  0.54%  |  0.45%  |  0.38%  |  0.39%  |  0.41%  |  0.38%  |  0.30%  |  0.29%  |  0.21%  |  0.37%  |  0.35%  |  0.26%  |  0.32%  |  0.29%  |  0.21%  |  0.20%  |  0.29%  |  0.32%  |  0.20%  |  0.22%  |  0.24%  |  0.13%  |  0.12%  |  0.12%  |  0.09%  |  0.12%  |  0.11%  |     30  |",
            "title": "Accessing"
        },
        {
            "location": "/frame/access/#dataframe-access",
            "text": "",
            "title": "DataFrame Access"
        },
        {
            "location": "/frame/access/#introduction",
            "text": "A  DataFrame  is composed of a collection of column vectors represented by Morpheus Arrays, and the frame API provides\nvarious mechanisms for reading and writing data in a type safe and efficient manner (for example, avoiding boxing of\nprimitive types). Elements can be read or written using a random access API, or they can be consumed iteratively, including \nvia Java 8 streams. It is also possible to process rows and columns in an efficient manner that places very little burden on \nthe Garbage Collector. The following sections illustrates various  DataFrame  access scenarios.  One of the design goals of Morpheus was to formulate a flexible API that is both intuitive and consistent, so that\noperations that apply at the frame level look and feel similar to operations that work on either the row or column\ndimension of the  DataFrame . There is also a strong emphasis on type safety, so most of the peripheral interfaces in \nMorpheus are paramterised by both the row and column key types.",
            "title": "Introduction"
        },
        {
            "location": "/frame/access/#ons-population-dataset",
            "text": "The examples in this section mostly operate on a population dataset made available by the  Office for National Statistics  \n(ONS) in the UK, which is described  here \nand accessible  here . (unless the example creates a custom  DataFrame  for illustration). \nThe ONS dataset provides a population decomposition by gender and age across various boroughs and wards in the UK, between 1999 \nand 2014. The data can be loaded as follows, and the first 10 rows are shown below. The row axis in this example is a composite k\ney made up of two items from each row, namely the year and the borough name.   /**\n * Returns the ONS population dataset for UK boroughs\n * @return  the ONS population dataser\n */\nstatic DataFrame<Tuple,String> loadPopulationDataset() {\n    return DataFrame.read().csv(options -> {\n        options.setResource(\"http://tinyurl.com/ons-population-year\");\n        options.setRowKeyParser(Tuple.class, row -> Tuple.of(Integer.parseInt(row[1]), row[2]));\n        options.setExcludeColumns(\"Code\");\n        options.getFormats().setNullValues(\"-\");\n        options.setColumnType(\"All Males\", Double.class);\n        options.setColumnType(\"All Females\", Double.class);\n        options.setColumnType(\"All Persons\", Double.class);\n        options.setColumnType(\"[MF]\\\\s+\\\\d+\", Double.class);\n    });\n}  In the parsing code above, we expressly coerce the various counts into a double type as the default CSV parsing logic would\nassume these are integers. The reason we parse them into doubles is to allow us to perform computations on these values, such \nas converting them to percentages, and therefore a double precision representation makes sense.  \n         Index          |  Year  |     Borough      |  All Persons  |  All Males  |    M 0    |    M 1    |    M 2    |    M 3    |    M 4    |    M 5    |    M 6    |    M 7    |    M 8    |    M 9    |   M 10    |   M 11    |   M 12    |   M 13    |   M 14    |   M 15    |   M 16    |   M 17    |   M 18    |   M 19    |   M 20    |   M 21    |   M 22    |   M 23    |   M 24    |    M 25    |    M 26    |    M 27    |    M 28    |    M 29    |    M 30    |    M 31    |    M 32    |    M 33    |    M 34    |    M 35    |    M 36    |    M 37    |    M 38    |    M 39    |    M 40    |   M 41    |   M 42    |   M 43    |   M 44    |   M 45    |   M 46    |   M 47    |   M 48    |   M 49    |    M 50    |    M 51    |    M 52    |   M 53    |   M 54    |   M 55    |   M 56    |   M 57    |   M 58    |   M 59    |   M 60    |   M 61    |   M 62    |   M 63    |   M 64    |   M 65    |   M 66    |   M 67    |   M 68    |   M 69    |   M 70    |   M 71    |   M 72    |   M 73    |   M 74    |   M 75    |   M 76    |   M 77    |   M 78    |   M 79    |   M 80    |   M 81    |   M 82    |   M 83    |   M 84    |   M 85    |   M 86    |   M 87    |   M 88    |   M 89   |  M 90+  |  All Females  |    F 0    |    F 1    |    F 2    |    F 3    |    F 4    |    F 5    |    F 6    |    F 7    |    F 8    |    F 9    |   F 10    |   F 11    |   F 12    |   F 13    |   F 14    |   F 15    |   F 16    |   F 17    |   F 18    |   F 19    |   F 20    |   F 21    |   F 22    |   F 23    |   F 24    |   F 25    |    F 26    |   F 27    |    F 28    |    F 29    |    F 30    |   F 31    |    F 32    |    F 33    |   F 34    |   F 35    |   F 36    |   F 37    |   F 38    |   F 39    |   F 40    |   F 41    |   F 42    |   F 43    |   F 44    |   F 45    |   F 46    |   F 47    |   F 48    |   F 49    |   F 50    |   F 51    |   F 52    |   F 53    |   F 54    |   F 55    |   F 56    |   F 57    |   F 58    |   F 59    |   F 60    |   F 61    |   F 62    |   F 63    |   F 64    |   F 65    |   F 66    |   F 67    |   F 68    |   F 69    |   F 70    |   F 71    |   F 72    |   F 73    |   F 74    |   F 75    |   F 76    |   F 77    |   F 78    |   F 79    |   F 80    |   F 81    |   F 82    |   F 83    |   F 84    |   F 85    |   F 86    |   F 87    |   F 88    |   F 89    |  F 90+  |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n (1999,City of London)  |  1999  |  City of London  |    6581.0000  |  3519.0000  |  24.0000  |  23.0000  |  22.0000  |  21.0000  |  20.0000  |  21.0000  |  20.0000  |  21.0000  |  20.0000  |  22.0000  |  22.0000  |  21.0000  |  19.0000  |  18.0000  |  17.0000  |  17.0000  |  17.0000  |  21.0000  |  26.0000  |  32.0000  |  40.0000  |  47.0000  |  52.0000  |  60.0000  |  65.0000  |   70.0000  |   73.0000  |   73.0000  |   73.0000  |   78.0000  |   78.0000  |   77.0000  |   75.0000  |   72.0000  |   71.0000  |   71.0000  |   65.0000  |   60.0000  |   58.0000  |   58.0000  |   57.0000  |  55.0000  |  51.0000  |  53.0000  |  53.0000  |  59.0000  |  59.0000  |  60.0000  |  65.0000  |  68.0000  |   68.0000  |   70.0000  |   63.0000  |  62.0000  |  61.0000  |  54.0000  |  49.0000  |  47.0000  |  43.0000  |  38.0000  |  35.0000  |  34.0000  |  34.0000  |  33.0000  |  32.0000  |  31.0000  |  32.0000  |  31.0000  |  30.0000  |  30.0000  |  29.0000  |  27.0000  |  26.0000  |  24.0000  |  23.0000  |  20.0000  |  20.0000  |  18.0000  |  17.0000  |  15.0000  |  14.0000  |  12.0000  |  12.0000  |  11.0000  |  11.0000  |  43.0000  |      NaN  |      NaN  |      NaN  |     NaN  |      0  |    3062.0000  |  24.0000  |  24.0000  |  22.0000  |  22.0000  |  21.0000  |  21.0000  |  19.0000  |  19.0000  |  18.0000  |  18.0000  |  18.0000  |  16.0000  |  17.0000  |  16.0000  |  16.0000  |  16.0000  |  20.0000  |  25.0000  |  26.0000  |  30.0000  |  39.0000  |  47.0000  |  50.0000  |  54.0000  |  61.0000  |  68.0000  |   71.0000  |  72.0000  |   71.0000  |   73.0000  |   71.0000  |  68.0000  |   66.0000  |   62.0000  |  55.0000  |  51.0000  |  47.0000  |  43.0000  |  42.0000  |  39.0000  |  39.0000  |  41.0000  |  42.0000  |  43.0000  |  44.0000  |  40.0000  |  44.0000  |  45.0000  |  47.0000  |  48.0000  |  49.0000  |  49.0000  |  50.0000  |  46.0000  |  44.0000  |  39.0000  |  36.0000  |  34.0000  |  33.0000  |  31.0000  |  30.0000  |  29.0000  |  29.0000  |  30.0000  |  31.0000  |  30.0000  |  30.0000  |  29.0000  |  29.0000  |  28.0000  |  26.0000  |  26.0000  |  25.0000  |  23.0000  |  21.0000  |  20.0000  |  20.0000  |  19.0000  |  17.0000  |  17.0000  |  17.0000  |  16.0000  |  15.0000  |  14.0000  |  13.0000  |  76.0000  |      NaN  |      NaN  |      NaN  |      NaN  |      0  |\n (2000,City of London)  |  2000  |  City of London  |    7014.0000  |  3775.0000  |  25.0000  |  25.0000  |  24.0000  |  23.0000  |  22.0000  |  21.0000  |  22.0000  |  21.0000  |  22.0000  |  21.0000  |  22.0000  |  20.0000  |  18.0000  |  16.0000  |  16.0000  |  15.0000  |  17.0000  |  19.0000  |  24.0000  |  30.0000  |  39.0000  |  48.0000  |  57.0000  |  64.0000  |  72.0000  |   81.0000  |   88.0000  |   90.0000  |   90.0000  |   91.0000  |   94.0000  |   94.0000  |   90.0000  |   86.0000  |   81.0000  |   80.0000  |   76.0000  |   70.0000  |   65.0000  |   62.0000  |   62.0000  |  62.0000  |  59.0000  |  56.0000  |  56.0000  |  56.0000  |  62.0000  |  62.0000  |  62.0000  |  67.0000  |   71.0000  |   71.0000  |   74.0000  |  66.0000  |  64.0000  |  63.0000  |  56.0000  |  50.0000  |  47.0000  |  43.0000  |  39.0000  |  34.0000  |  32.0000  |  31.0000  |  30.0000  |  29.0000  |  28.0000  |  28.0000  |  27.0000  |  28.0000  |  27.0000  |  26.0000  |  25.0000  |  23.0000  |  22.0000  |  22.0000  |  19.0000  |  19.0000  |  17.0000  |  16.0000  |  15.0000  |  14.0000  |  12.0000  |  12.0000  |  11.0000  |  39.0000  |      NaN  |      NaN  |      NaN  |     NaN  |      0  |    3239.0000  |  26.0000  |  24.0000  |  24.0000  |  22.0000  |  21.0000  |  21.0000  |  20.0000  |  19.0000  |  19.0000  |  19.0000  |  19.0000  |  18.0000  |  17.0000  |  18.0000  |  17.0000  |  18.0000  |  20.0000  |  23.0000  |  28.0000  |  32.0000  |  38.0000  |  49.0000  |  58.0000  |  61.0000  |  69.0000  |  75.0000  |   82.0000  |  83.0000  |   84.0000  |   81.0000  |   80.0000  |  76.0000  |   73.0000  |   69.0000  |  64.0000  |  57.0000  |  53.0000  |  48.0000  |  44.0000  |  43.0000  |  41.0000  |  42.0000  |  41.0000  |  43.0000  |  43.0000  |  44.0000  |  41.0000  |  45.0000  |  45.0000  |  47.0000  |  49.0000  |  50.0000  |  51.0000  |  51.0000  |  47.0000  |  45.0000  |  41.0000  |  37.0000  |  35.0000  |  32.0000  |  30.0000  |  29.0000  |  28.0000  |  27.0000  |  28.0000  |  29.0000  |  28.0000  |  28.0000  |  28.0000  |  28.0000  |  27.0000  |  25.0000  |  25.0000  |  24.0000  |  22.0000  |  21.0000  |  19.0000  |  20.0000  |  19.0000  |  17.0000  |  16.0000  |  15.0000  |  15.0000  |  15.0000  |  13.0000  |  81.0000  |      NaN  |      NaN  |      NaN  |      NaN  |      0  |\n (2001,City of London)  |  2001  |  City of London  |    7359.0000  |  3984.0000  |  17.0000  |  30.0000  |  25.0000  |  19.0000  |  21.0000  |  25.0000  |  20.0000  |  16.0000  |  19.0000  |  27.0000  |  26.0000  |  16.0000  |  29.0000  |  19.0000  |  11.0000  |  14.0000  |  12.0000  |  13.0000  |  12.0000  |  27.0000  |  38.0000  |  39.0000  |  53.0000  |  63.0000  |  80.0000  |  110.0000  |  110.0000  |  100.0000  |  108.0000  |  125.0000  |  114.0000  |   95.0000  |   93.0000  |  105.0000  |   99.0000  |   84.0000  |   89.0000  |   71.0000  |   63.0000  |   75.0000  |   65.0000  |  52.0000  |  47.0000  |  76.0000  |  73.0000  |  54.0000  |  49.0000  |  45.0000  |  60.0000  |  49.0000  |  111.0000  |   73.0000  |   55.0000  |  81.0000  |  80.0000  |  61.0000  |  63.0000  |  56.0000  |  64.0000  |  48.0000  |  28.0000  |  34.0000  |  45.0000  |  36.0000  |  25.0000  |  28.0000  |  21.0000  |  28.0000  |  21.0000  |  27.0000  |  27.0000  |  21.0000  |  24.0000  |  29.0000  |  21.0000  |  19.0000  |  23.0000  |  20.0000  |  11.0000  |  19.0000  |  15.0000  |  14.0000  |   8.0000  |  15.0000  |  10.0000  |   6.0000  |  10.0000  |   5.0000  |   4.0000  |  3.0000  |     13  |    3375.0000  |  32.0000  |  30.0000  |  23.0000  |  21.0000  |  26.0000  |  16.0000  |  21.0000  |  15.0000  |  18.0000  |  24.0000  |  20.0000  |  20.0000  |  10.0000  |  26.0000  |  13.0000  |  13.0000  |  14.0000  |  39.0000  |  28.0000  |  35.0000  |  47.0000  |  39.0000  |  49.0000  |  54.0000  |  74.0000  |  93.0000  |   89.0000  |  80.0000  |   84.0000  |  107.0000  |   85.0000  |  83.0000  |  101.0000  |   66.0000  |  60.0000  |  54.0000  |  66.0000  |  60.0000  |  48.0000  |  45.0000  |  37.0000  |  28.0000  |  31.0000  |  63.0000  |  43.0000  |  53.0000  |  50.0000  |  47.0000  |  35.0000  |  42.0000  |  37.0000  |  65.0000  |  48.0000  |  57.0000  |  57.0000  |  38.0000  |  45.0000  |  38.0000  |  38.0000  |  33.0000  |  32.0000  |  32.0000  |  26.0000  |  25.0000  |  24.0000  |  29.0000  |  28.0000  |  27.0000  |  34.0000  |  27.0000  |  20.0000  |  26.0000  |  24.0000  |  31.0000  |  18.0000  |  24.0000  |  27.0000  |  13.0000  |  13.0000  |  15.0000  |  18.0000  |  22.0000  |  18.0000  |  11.0000  |  10.0000  |  10.0000  |  14.0000  |  26.0000  |  16.0000  |   7.0000  |     15  |\n (2002,City of London)  |  2002  |  City of London  |    7280.0000  |  3968.0000  |  29.0000  |  18.0000  |  28.0000  |  27.0000  |  20.0000  |  20.0000  |  24.0000  |  20.0000  |  17.0000  |  20.0000  |  25.0000  |  25.0000  |  14.0000  |  27.0000  |   9.0000  |   5.0000  |   8.0000  |  10.0000  |  12.0000  |  10.0000  |  20.0000  |  39.0000  |  50.0000  |  61.0000  |  68.0000  |   99.0000  |  113.0000  |  122.0000  |  102.0000  |  114.0000  |  114.0000  |  107.0000  |   99.0000  |   89.0000  |  101.0000  |   93.0000  |   84.0000  |   77.0000  |   73.0000  |   61.0000  |   78.0000  |  65.0000  |  57.0000  |  50.0000  |  75.0000  |  72.0000  |  56.0000  |  50.0000  |  45.0000  |  61.0000  |   46.0000  |  113.0000  |   70.0000  |  49.0000  |  85.0000  |  76.0000  |  59.0000  |  61.0000  |  55.0000  |  60.0000  |  45.0000  |  28.0000  |  31.0000  |  46.0000  |  40.0000  |  23.0000  |  26.0000  |  20.0000  |  25.0000  |  21.0000  |  24.0000  |  30.0000  |  21.0000  |  25.0000  |  29.0000  |  20.0000  |  18.0000  |  21.0000  |  17.0000  |  11.0000  |  17.0000  |  14.0000  |  13.0000  |   8.0000  |  15.0000  |   9.0000  |   6.0000  |   8.0000  |   4.0000  |  2.0000  |     14  |    3312.0000  |  28.0000  |  35.0000  |  30.0000  |  19.0000  |  21.0000  |  26.0000  |  15.0000  |  20.0000  |  13.0000  |  18.0000  |  17.0000  |  15.0000  |  17.0000  |   6.0000  |  19.0000  |  13.0000  |   5.0000  |  15.0000  |  35.0000  |  29.0000  |  35.0000  |  48.0000  |  52.0000  |  47.0000  |  67.0000  |  74.0000  |   84.0000  |  84.0000  |   83.0000  |   70.0000  |  115.0000  |  84.0000  |   94.0000  |  101.0000  |  63.0000  |  61.0000  |  57.0000  |  58.0000  |  62.0000  |  44.0000  |  47.0000  |  37.0000  |  28.0000  |  33.0000  |  60.0000  |  36.0000  |  47.0000  |  52.0000  |  45.0000  |  36.0000  |  38.0000  |  41.0000  |  63.0000  |  51.0000  |  54.0000  |  55.0000  |  37.0000  |  41.0000  |  40.0000  |  36.0000  |  30.0000  |  29.0000  |  30.0000  |  27.0000  |  23.0000  |  23.0000  |  28.0000  |  28.0000  |  25.0000  |  34.0000  |  25.0000  |  19.0000  |  24.0000  |  23.0000  |  31.0000  |  19.0000  |  25.0000  |  26.0000  |  10.0000  |  12.0000  |  14.0000  |  17.0000  |  22.0000  |  15.0000  |   9.0000  |  10.0000  |   7.0000  |  11.0000  |  26.0000  |  14.0000  |     20  |\n (2003,City of London)  |  2003  |  City of London  |    7115.0000  |  3892.0000  |  35.0000  |  24.0000  |  16.0000  |  22.0000  |  26.0000  |  17.0000  |  19.0000  |  24.0000  |  21.0000  |  21.0000  |  24.0000  |  25.0000  |  21.0000  |  10.0000  |  16.0000  |  10.0000  |   4.0000  |   5.0000  |  12.0000  |  12.0000  |  11.0000  |  19.0000  |  49.0000  |  60.0000  |  66.0000  |   80.0000  |  100.0000  |  118.0000  |  114.0000  |  105.0000  |  108.0000  |  105.0000  |   99.0000  |   94.0000  |   85.0000  |   97.0000  |   89.0000  |   82.0000  |   76.0000  |   69.0000  |   63.0000  |  74.0000  |  64.0000  |  57.0000  |  51.0000  |  75.0000  |  74.0000  |  56.0000  |  51.0000  |  39.0000  |   64.0000  |   44.0000  |  108.0000  |  65.0000  |  47.0000  |  80.0000  |  74.0000  |  56.0000  |  61.0000  |  55.0000  |  55.0000  |  44.0000  |  26.0000  |  30.0000  |  42.0000  |  36.0000  |  23.0000  |  24.0000  |  22.0000  |  22.0000  |  18.0000  |  23.0000  |  31.0000  |  18.0000  |  24.0000  |  27.0000  |  19.0000  |  17.0000  |  22.0000  |  15.0000  |  10.0000  |  16.0000  |  12.0000  |  10.0000  |   8.0000  |  14.0000  |   8.0000  |   6.0000  |   7.0000  |  2.0000  |     13  |    3223.0000  |  18.0000  |  25.0000  |  31.0000  |  29.0000  |  18.0000  |  21.0000  |  26.0000  |  14.0000  |  19.0000  |  12.0000  |  19.0000  |  13.0000  |  11.0000  |  12.0000  |   2.0000  |  16.0000  |   7.0000  |  12.0000  |  22.0000  |  31.0000  |  34.0000  |  37.0000  |  52.0000  |  61.0000  |  58.0000  |  85.0000  |   73.0000  |  86.0000  |   82.0000  |   85.0000  |   75.0000  |  99.0000  |   85.0000  |   89.0000  |  92.0000  |  60.0000  |  58.0000  |  45.0000  |  51.0000  |  53.0000  |  42.0000  |  47.0000  |  44.0000  |  29.0000  |  35.0000  |  59.0000  |  38.0000  |  41.0000  |  49.0000  |  39.0000  |  35.0000  |  40.0000  |  37.0000  |  61.0000  |  49.0000  |  53.0000  |  50.0000  |  35.0000  |  40.0000  |  34.0000  |  36.0000  |  30.0000  |  29.0000  |  30.0000  |  27.0000  |  24.0000  |  23.0000  |  28.0000  |  30.0000  |  25.0000  |  32.0000  |  24.0000  |  17.0000  |  23.0000  |  21.0000  |  30.0000  |  16.0000  |  23.0000  |  24.0000  |  10.0000  |  11.0000  |  14.0000  |  15.0000  |  16.0000  |  16.0000  |   9.0000  |   2.0000  |   5.0000  |  10.0000  |  22.0000  |     26  |\n (2004,City of London)  |  2004  |  City of London  |    7118.0000  |  3875.0000  |  28.0000  |  27.0000  |  25.0000  |  16.0000  |  20.0000  |  27.0000  |  17.0000  |  20.0000  |  31.0000  |  18.0000  |  22.0000  |  21.0000  |  22.0000  |  16.0000  |   4.0000  |  13.0000  |   8.0000  |   0.0000  |   2.0000  |  13.0000  |   9.0000  |  25.0000  |  26.0000  |  63.0000  |  70.0000  |   69.0000  |   89.0000  |  112.0000  |  115.0000  |  116.0000  |  100.0000  |   93.0000  |  102.0000  |   93.0000  |   86.0000  |   79.0000  |   99.0000  |   92.0000  |   88.0000  |   81.0000  |   73.0000  |  59.0000  |  65.0000  |  67.0000  |  58.0000  |  49.0000  |  76.0000  |  70.0000  |  55.0000  |  50.0000  |   44.0000  |   61.0000  |   45.0000  |  99.0000  |  65.0000  |  46.0000  |  77.0000  |  76.0000  |  55.0000  |  57.0000  |  53.0000  |  57.0000  |  43.0000  |  28.0000  |  32.0000  |  39.0000  |  34.0000  |  22.0000  |  25.0000  |  21.0000  |  22.0000  |  18.0000  |  20.0000  |  30.0000  |  16.0000  |  24.0000  |  28.0000  |  19.0000  |  14.0000  |  20.0000  |   9.0000  |  12.0000  |  14.0000  |  10.0000  |   8.0000  |   8.0000  |  15.0000  |   8.0000  |   5.0000  |  5.0000  |     12  |    3243.0000  |  31.0000  |  18.0000  |  20.0000  |  26.0000  |  25.0000  |  20.0000  |  25.0000  |  28.0000  |  14.0000  |  19.0000  |   7.0000  |  19.0000  |  10.0000  |   9.0000  |   9.0000  |   1.0000  |  13.0000  |  17.0000  |   9.0000  |  22.0000  |  38.0000  |  41.0000  |  47.0000  |  59.0000  |  79.0000  |  80.0000  |   91.0000  |  86.0000  |   79.0000  |   81.0000  |   79.0000  |  78.0000  |   91.0000  |   80.0000  |  87.0000  |  83.0000  |  56.0000  |  61.0000  |  47.0000  |  48.0000  |  55.0000  |  40.0000  |  45.0000  |  39.0000  |  30.0000  |  34.0000  |  59.0000  |  37.0000  |  39.0000  |  56.0000  |  38.0000  |  33.0000  |  38.0000  |  36.0000  |  60.0000  |  51.0000  |  52.0000  |  51.0000  |  34.0000  |  40.0000  |  35.0000  |  34.0000  |  28.0000  |  32.0000  |  28.0000  |  27.0000  |  25.0000  |  20.0000  |  29.0000  |  28.0000  |  24.0000  |  30.0000  |  24.0000  |  15.0000  |  21.0000  |  22.0000  |  29.0000  |  15.0000  |  23.0000  |  22.0000  |  12.0000  |  11.0000  |  12.0000  |  11.0000  |  12.0000  |  12.0000  |   7.0000  |   1.0000  |   3.0000  |   9.0000  |     42  |\n (2005,City of London)  |  2005  |  City of London  |    7131.0000  |  3869.0000  |  34.0000  |  25.0000  |  22.0000  |  23.0000  |  12.0000  |  19.0000  |  22.0000  |  19.0000  |  19.0000  |  31.0000  |  20.0000  |  21.0000  |  20.0000  |  19.0000  |   4.0000  |   3.0000  |  12.0000  |  10.0000  |   0.0000  |   7.0000  |   9.0000  |  16.0000  |  43.0000  |  40.0000  |  72.0000  |   93.0000  |   74.0000  |   89.0000  |  117.0000  |  113.0000  |  104.0000  |   88.0000  |   92.0000  |  109.0000  |   98.0000  |   77.0000  |   76.0000  |  103.0000  |   89.0000  |   78.0000  |   80.0000  |  68.0000  |  61.0000  |  69.0000  |  62.0000  |  58.0000  |  51.0000  |  82.0000  |  67.0000  |  58.0000  |   52.0000  |   42.0000  |   59.0000  |  50.0000  |  89.0000  |  67.0000  |  47.0000  |  73.0000  |  70.0000  |  55.0000  |  58.0000  |  50.0000  |  53.0000  |  42.0000  |  27.0000  |  30.0000  |  42.0000  |  31.0000  |  23.0000  |  24.0000  |  20.0000  |  17.0000  |  17.0000  |  19.0000  |  30.0000  |  15.0000  |  21.0000  |  27.0000  |  17.0000  |  14.0000  |  21.0000  |   8.0000  |  14.0000  |  12.0000  |   7.0000  |   7.0000  |   6.0000  |  14.0000  |   7.0000  |  2.0000  |     12  |    3262.0000  |  29.0000  |  29.0000  |  15.0000  |  15.0000  |  23.0000  |  25.0000  |  17.0000  |  23.0000  |  25.0000  |  14.0000  |  18.0000  |   7.0000  |  20.0000  |   7.0000  |   7.0000  |   6.0000  |   2.0000  |   7.0000  |  19.0000  |  15.0000  |  31.0000  |  43.0000  |  49.0000  |  50.0000  |  82.0000  |  76.0000  |   89.0000  |  89.0000  |   85.0000  |   76.0000  |   90.0000  |  76.0000  |   85.0000  |   91.0000  |  74.0000  |  80.0000  |  74.0000  |  60.0000  |  55.0000  |  52.0000  |  45.0000  |  52.0000  |  43.0000  |  47.0000  |  38.0000  |  34.0000  |  34.0000  |  55.0000  |  33.0000  |  38.0000  |  57.0000  |  38.0000  |  36.0000  |  39.0000  |  37.0000  |  58.0000  |  52.0000  |  55.0000  |  50.0000  |  34.0000  |  37.0000  |  35.0000  |  37.0000  |  26.0000  |  33.0000  |  30.0000  |  25.0000  |  24.0000  |  18.0000  |  27.0000  |  26.0000  |  20.0000  |  28.0000  |  22.0000  |  18.0000  |  21.0000  |  24.0000  |  26.0000  |  16.0000  |  21.0000  |  22.0000  |  10.0000  |  12.0000  |  11.0000  |  10.0000  |  12.0000  |   9.0000  |   2.0000  |   2.0000  |   3.0000  |     50  |\n (2006,City of London)  |  2006  |  City of London  |    7254.0000  |  3959.0000  |  26.0000  |  25.0000  |  26.0000  |  20.0000  |  18.0000  |  13.0000  |  16.0000  |  23.0000  |  22.0000  |  18.0000  |  30.0000  |  18.0000  |  20.0000  |  17.0000  |   7.0000  |   5.0000  |   5.0000  |  15.0000  |  12.0000  |   0.0000  |  12.0000  |  19.0000  |  32.0000  |  63.0000  |  63.0000  |   82.0000  |  109.0000  |   83.0000  |   99.0000  |  124.0000  |  114.0000  |  103.0000  |   83.0000  |   85.0000  |  116.0000  |   98.0000  |   77.0000  |   74.0000  |  107.0000  |   87.0000  |   75.0000  |  82.0000  |  68.0000  |  63.0000  |  69.0000  |  59.0000  |  62.0000  |  53.0000  |  83.0000  |  66.0000  |   60.0000  |   52.0000  |   48.0000  |  63.0000  |  48.0000  |  85.0000  |  67.0000  |  47.0000  |  68.0000  |  62.0000  |  56.0000  |  56.0000  |  48.0000  |  54.0000  |  40.0000  |  28.0000  |  26.0000  |  41.0000  |  31.0000  |  22.0000  |  23.0000  |  20.0000  |  14.0000  |  16.0000  |  19.0000  |  29.0000  |  17.0000  |  20.0000  |  28.0000  |  17.0000  |  14.0000  |  19.0000  |   6.0000  |  10.0000  |  12.0000  |   5.0000  |   4.0000  |   6.0000  |  12.0000  |  7.0000  |     13  |    3295.0000  |  28.0000  |  26.0000  |  31.0000  |  12.0000  |  17.0000  |  25.0000  |  23.0000  |  20.0000  |  22.0000  |  25.0000  |  14.0000  |  18.0000  |   5.0000  |  19.0000  |   4.0000  |   7.0000  |   4.0000  |  12.0000  |   7.0000  |  27.0000  |  24.0000  |  33.0000  |  51.0000  |  58.0000  |  71.0000  |  88.0000  |   80.0000  |  86.0000  |   86.0000  |   92.0000  |   84.0000  |  86.0000  |   78.0000  |   84.0000  |  80.0000  |  77.0000  |  74.0000  |  68.0000  |  62.0000  |  45.0000  |  53.0000  |  45.0000  |  51.0000  |  45.0000  |  46.0000  |  39.0000  |  38.0000  |  33.0000  |  53.0000  |  31.0000  |  37.0000  |  57.0000  |  39.0000  |  36.0000  |  38.0000  |  36.0000  |  56.0000  |  50.0000  |  53.0000  |  47.0000  |  30.0000  |  41.0000  |  31.0000  |  34.0000  |  24.0000  |  30.0000  |  29.0000  |  23.0000  |  26.0000  |  17.0000  |  27.0000  |  26.0000  |  20.0000  |  26.0000  |  24.0000  |  18.0000  |  20.0000  |  22.0000  |  27.0000  |  17.0000  |  20.0000  |  19.0000  |  11.0000  |  11.0000  |  10.0000  |   9.0000  |  11.0000  |   9.0000  |   2.0000  |   0.0000  |     45  |\n (2007,City of London)  |  2007  |  City of London  |    7607.0000  |  4197.0000  |  31.0000  |  25.0000  |  24.0000  |  22.0000  |  23.0000  |  16.0000  |  13.0000  |  14.0000  |  27.0000  |  23.0000  |  15.0000  |  33.0000  |  16.0000  |  19.0000  |   7.0000  |   7.0000  |   3.0000  |   9.0000  |  16.0000  |  27.0000  |   0.0000  |  30.0000  |  35.0000  |  64.0000  |  91.0000  |   90.0000  |  113.0000  |  123.0000  |   99.0000  |  107.0000  |  123.0000  |  102.0000  |  106.0000  |   84.0000  |   87.0000  |  114.0000  |  104.0000  |   86.0000  |   83.0000  |  117.0000  |   88.0000  |  79.0000  |  74.0000  |  70.0000  |  75.0000  |  74.0000  |  63.0000  |  64.0000  |  57.0000  |  80.0000  |   65.0000  |   62.0000  |   55.0000  |  51.0000  |  64.0000  |  53.0000  |  78.0000  |  64.0000  |  48.0000  |  63.0000  |  59.0000  |  54.0000  |  58.0000  |  46.0000  |  54.0000  |  36.0000  |  26.0000  |  30.0000  |  39.0000  |  30.0000  |  22.0000  |  23.0000  |  17.0000  |  16.0000  |  16.0000  |  19.0000  |  27.0000  |  15.0000  |  15.0000  |  29.0000  |  14.0000  |  14.0000  |  18.0000  |   6.0000  |   6.0000  |  10.0000  |   4.0000  |   4.0000  |   6.0000  |  9.0000  |     20  |    3410.0000  |  24.0000  |  24.0000  |  30.0000  |  25.0000  |  17.0000  |  17.0000  |  17.0000  |  21.0000  |  16.0000  |  21.0000  |  25.0000  |  15.0000  |  18.0000  |   4.0000  |  19.0000  |   3.0000  |  11.0000  |  24.0000  |  15.0000  |  24.0000  |  39.0000  |  34.0000  |  56.0000  |  70.0000  |  75.0000  |  91.0000  |  100.0000  |  85.0000  |  101.0000  |   83.0000  |   80.0000  |  76.0000  |   80.0000  |   72.0000  |  72.0000  |  72.0000  |  76.0000  |  73.0000  |  62.0000  |  65.0000  |  47.0000  |  56.0000  |  48.0000  |  55.0000  |  50.0000  |  49.0000  |  43.0000  |  36.0000  |  30.0000  |  48.0000  |  29.0000  |  38.0000  |  54.0000  |  41.0000  |  39.0000  |  39.0000  |  39.0000  |  56.0000  |  52.0000  |  53.0000  |  46.0000  |  27.0000  |  41.0000  |  35.0000  |  32.0000  |  27.0000  |  30.0000  |  32.0000  |  22.0000  |  24.0000  |  16.0000  |  27.0000  |  26.0000  |  18.0000  |  25.0000  |  23.0000  |  17.0000  |  18.0000  |  22.0000  |  23.0000  |  17.0000  |  17.0000  |  19.0000  |  11.0000  |  10.0000  |  10.0000  |   7.0000  |   9.0000  |   8.0000  |   1.0000  |     36  |\n (2008,City of London)  |  2008  |  City of London  |    7429.0000  |  4131.0000  |  29.0000  |  26.0000  |  22.0000  |  24.0000  |  18.0000  |  22.0000  |  13.0000  |  12.0000  |  17.0000  |  30.0000  |  20.0000  |  15.0000  |  30.0000  |  17.0000  |  10.0000  |   8.0000  |   6.0000  |   7.0000  |  11.0000  |  18.0000  |  28.0000  |  11.0000  |  43.0000  |  39.0000  |  75.0000  |  100.0000  |   85.0000  |  123.0000  |  122.0000  |  102.0000  |   93.0000  |  111.0000  |   96.0000  |  101.0000  |   75.0000  |   80.0000  |  103.0000  |   92.0000  |   83.0000  |   79.0000  |  121.0000  |  86.0000  |  79.0000  |  69.0000  |  70.0000  |  66.0000  |  77.0000  |  57.0000  |  67.0000  |  67.0000  |   81.0000  |   64.0000  |   61.0000  |  51.0000  |  52.0000  |  61.0000  |  55.0000  |  73.0000  |  61.0000  |  46.0000  |  60.0000  |  61.0000  |  53.0000  |  55.0000  |  42.0000  |  53.0000  |  34.0000  |  26.0000  |  29.0000  |  39.0000  |  29.0000  |  24.0000  |  22.0000  |  17.0000  |  15.0000  |  14.0000  |  19.0000  |  24.0000  |  14.0000  |  16.0000  |  27.0000  |  13.0000  |  12.0000  |  17.0000  |   6.0000  |   6.0000  |   7.0000  |   4.0000  |   3.0000  |  5.0000  |     25  |    3298.0000  |  12.0000  |  17.0000  |  23.0000  |  23.0000  |  24.0000  |  16.0000  |  17.0000  |  16.0000  |  19.0000  |  15.0000  |  21.0000  |  24.0000  |  15.0000  |  19.0000  |   1.0000  |  19.0000  |   0.0000  |  22.0000  |  26.0000  |  25.0000  |  35.0000  |  43.0000  |  35.0000  |  57.0000  |  73.0000  |  82.0000  |   89.0000  |  95.0000  |   88.0000  |   94.0000  |   76.0000  |  77.0000  |   68.0000  |   69.0000  |  57.0000  |  60.0000  |  61.0000  |  64.0000  |  68.0000  |  57.0000  |  63.0000  |  41.0000  |  52.0000  |  46.0000  |  52.0000  |  50.0000  |  50.0000  |  37.0000  |  35.0000  |  32.0000  |  42.0000  |  29.0000  |  38.0000  |  53.0000  |  40.0000  |  42.0000  |  42.0000  |  38.0000  |  52.0000  |  51.0000  |  51.0000  |  48.0000  |  27.0000  |  41.0000  |  34.0000  |  29.0000  |  30.0000  |  31.0000  |  29.0000  |  23.0000  |  22.0000  |  16.0000  |  28.0000  |  27.0000  |  20.0000  |  24.0000  |  22.0000  |  16.0000  |  15.0000  |  22.0000  |  24.0000  |  15.0000  |  17.0000  |  18.0000  |  10.0000  |   9.0000  |   9.0000  |   7.0000  |   9.0000  |   8.0000  |     30  |",
            "title": "ONS Population Dataset"
        },
        {
            "location": "/frame/access/#row-column-access",
            "text": "",
            "title": "Row / Column Access"
        },
        {
            "location": "/frame/access/#random-access",
            "text": "Random access to specific rows or columns can be achieved in O(1) time using either the ordinal or the key for\nthe row or column in question. Convenience functions to access the first and last row / column vectors is also provided\nvia a  first()  and  last()  method. Each call to  rowAt()  or  colAt()  creates a new row or column vector respectively, \nso iterating over the frame in this way is unlikely to be as fast as the   forEach()  technique described above.   //Load ONS dataset\nfinal DataFrame<Tuple,String> frame = loadPopulationDataset();\n\n//Random access to a row by ordinal or key\nDataFrameRow<Tuple,String> row1 = frame.rowAt(4);\nDataFrameRow<Tuple,String> row2 = frame.rowAt(Tuple.of(2003, \"City of London\"));\n\n//Random access to a column by ordinal or key\nDataFrameColumn<Tuple,String> column1 = frame.colAt(2);\nDataFrameColumn<Tuple,String> column2 = frame.colAt(\"All Persons\");\n\n//Access first and last rows\nOptional<DataFrameRow<Tuple,String>> firstRow = frame.rows().first();\nOptional<DataFrameRow<Tuple,String>> lastRow = frame.rows().last();\n\n//Access first and last columns\nOptional<DataFrameColumn<Tuple,String>> firstColumn = frame.cols().first();\nOptional<DataFrameColumn<Tuple,String>> lastColumn = frame.cols().last();  The example below is slightly more elaborate where we access the \"All Persons\" column in the ONS  DataFrame  in order to \nlocate the row which contains the largest population value by using the  max()  function. Before we do this however,\nwe access the largest value using the  stats().max()  function so we can assert the  max()  points us to a row with\na matching value - a nice unit test.   // Find row with max value for \"All Persons: column using column key\nfinal double expectedMax = frame.colAt(\"All Persons\").stats().max();\nframe.colAt(\"All Persons\").max().ifPresent(value -> {\n    final DataFrameRow<Tuple,String> row = frame.rowAt(value.rowKey());\n    final double actualMax = row.getDouble(\"All Persons\");\n    Asserts.assertEquals(actualMax, expectedMax, \"The max values match\");\n});",
            "title": "Random Access"
        },
        {
            "location": "/frame/access/#iteration",
            "text": "The  DataFrame  API is entirely symmetric in the row and column axis, so everything that you can do in the row dimension\nyou can also do in the column dimension. Some of the examples below operate on either the row or column axis simply for brevity, \nbut you can assume there is a direct analogue in the other dimension.  Iterating over the rows or columns of a  DataFrame  is a common operation, so Morpehus provides a simple API to do this \nefficiently, both via  sequential  and  parallel  execution. Rather than creating a new row or column object for each \nitem in the iteration, Morpheus presents the same proxy object which points at the appropriate row or column, thus reducing\nthe burden on the garbage collector. This means you cannot collect rows or columns, unless you call  copy()  on them to create \na detached view on the row or column vector. This is generally discouraged, especially for large frames as it would likely be \ninefficient both from a memory consumption and garbage collection perspective.  The code below uses the ONS dataset described in the introduction and shows one of several ways we could convert the male \nand female population counts into percentages by gender (for example, in the City of London in 1999, 24 out of 3519 males \nwere 0 years old (column M 0), making it 24 / 3519 = 0.6820%). The first code example illustrates how to iterate\nover each row sequentially, then for each row we iterate over all values and modify those that represent a male or\nfemale count with a percentage value. Note how we avoid any boxing / unboxing of primitives by using type specific\ngetters and setters when accessing what we know to be double precision values (since the CSV parsing was setup that way).   //Sequential: Convert male & female population counts into weights\nframe.rows().forEach(row -> row.forEach(value -> {\n    if (value.colKey().matches(\"M\\\\s+\\\\d+\")) {\n        double totalMales = value.row().getDouble(\"All Males\");\n        double count = value.getDouble();\n        value.setDouble(count / totalMales);\n    } else if (value.colKey().matches(\"F\\\\s+\\\\d+\")) {\n        double totalFemales = value.row().getDouble(\"All Females\");\n        double count = value.getDouble();\n        value.setDouble(count / totalFemales);\n    }\n}));\n\n//Print frame to std out with custom formatting\nframe.out().print(formats -> {\n    formats.withDecimalFormat(\"All Persons\", \"0;-0\", 1);\n    formats.withDecimalFormat(\"All Males\", \"0;-0\", 1);\n    formats.withDecimalFormat(\"All Females\", \"0;-0\", 1);\n    formats.withDecimalFormat(Double.class, \"0.00'%';-0.00'%'\", 100);\n});  \n         Index          |  Year  |     Borough      |  All Persons  |  All Males  |   M 0   |   M 1   |   M 2   |   M 3   |   M 4   |   M 5   |   M 6   |   M 7   |   M 8   |   M 9   |  M 10   |  M 11   |  M 12   |  M 13   |  M 14   |  M 15   |  M 16   |  M 17   |  M 18   |  M 19   |  M 20   |  M 21   |  M 22   |  M 23   |  M 24   |  M 25   |  M 26   |  M 27   |  M 28   |  M 29   |  M 30   |  M 31   |  M 32   |  M 33   |  M 34   |  M 35   |  M 36   |  M 37   |  M 38   |  M 39   |  M 40   |  M 41   |  M 42   |  M 43   |  M 44   |  M 45   |  M 46   |  M 47   |  M 48   |  M 49   |  M 50   |  M 51   |  M 52   |  M 53   |  M 54   |  M 55   |  M 56   |  M 57   |  M 58   |  M 59   |  M 60   |  M 61   |  M 62   |  M 63   |  M 64   |  M 65   |  M 66   |  M 67   |  M 68   |  M 69   |  M 70   |  M 71   |  M 72   |  M 73   |  M 74   |  M 75   |  M 76   |  M 77   |  M 78   |  M 79   |  M 80   |  M 81   |  M 82   |  M 83   |  M 84   |  M 85   |  M 86   |  M 87   |  M 88   |  M 89   |  M 90+  |  All Females  |   F 0   |   F 1   |   F 2   |   F 3   |   F 4   |   F 5   |   F 6   |   F 7   |   F 8   |   F 9   |  F 10   |  F 11   |  F 12   |  F 13   |  F 14   |  F 15   |  F 16   |  F 17   |  F 18   |  F 19   |  F 20   |  F 21   |  F 22   |  F 23   |  F 24   |  F 25   |  F 26   |  F 27   |  F 28   |  F 29   |  F 30   |  F 31   |  F 32   |  F 33   |  F 34   |  F 35   |  F 36   |  F 37   |  F 38   |  F 39   |  F 40   |  F 41   |  F 42   |  F 43   |  F 44   |  F 45   |  F 46   |  F 47   |  F 48   |  F 49   |  F 50   |  F 51   |  F 52   |  F 53   |  F 54   |  F 55   |  F 56   |  F 57   |  F 58   |  F 59   |  F 60   |  F 61   |  F 62   |  F 63   |  F 64   |  F 65   |  F 66   |  F 67   |  F 68   |  F 69   |  F 70   |  F 71   |  F 72   |  F 73   |  F 74   |  F 75   |  F 76   |  F 77   |  F 78   |  F 79   |  F 80   |  F 81   |  F 82   |  F 83   |  F 84   |  F 85   |  F 86   |  F 87   |  F 88   |  F 89   |  F 90+  |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n (1999,City of London)  |  1999  |  City of London  |         6581  |       3519  |  0.68%  |  0.65%  |  0.63%  |  0.60%  |  0.57%  |  0.60%  |  0.57%  |  0.60%  |  0.57%  |  0.63%  |  0.63%  |  0.60%  |  0.54%  |  0.51%  |  0.48%  |  0.48%  |  0.48%  |  0.60%  |  0.74%  |  0.91%  |  1.14%  |  1.34%  |  1.48%  |  1.71%  |  1.85%  |  1.99%  |  2.07%  |  2.07%  |  2.07%  |  2.22%  |  2.22%  |  2.19%  |  2.13%  |  2.05%  |  2.02%  |  2.02%  |  1.85%  |  1.71%  |  1.65%  |  1.65%  |  1.62%  |  1.56%  |  1.45%  |  1.51%  |  1.51%  |  1.68%  |  1.68%  |  1.71%  |  1.85%  |  1.93%  |  1.93%  |  1.99%  |  1.79%  |  1.76%  |  1.73%  |  1.53%  |  1.39%  |  1.34%  |  1.22%  |  1.08%  |  0.99%  |  0.97%  |  0.97%  |  0.94%  |  0.91%  |  0.88%  |  0.91%  |  0.88%  |  0.85%  |  0.85%  |  0.82%  |  0.77%  |  0.74%  |  0.68%  |  0.65%  |  0.57%  |  0.57%  |  0.51%  |  0.48%  |  0.43%  |  0.40%  |  0.34%  |  0.34%  |  0.31%  |  0.31%  |  1.22%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |         3062  |  0.78%  |  0.78%  |  0.72%  |  0.72%  |  0.69%  |  0.69%  |  0.62%  |  0.62%  |  0.59%  |  0.59%  |  0.59%  |  0.52%  |  0.56%  |  0.52%  |  0.52%  |  0.52%  |  0.65%  |  0.82%  |  0.85%  |  0.98%  |  1.27%  |  1.53%  |  1.63%  |  1.76%  |  1.99%  |  2.22%  |  2.32%  |  2.35%  |  2.32%  |  2.38%  |  2.32%  |  2.22%  |  2.16%  |  2.02%  |  1.80%  |  1.67%  |  1.53%  |  1.40%  |  1.37%  |  1.27%  |  1.27%  |  1.34%  |  1.37%  |  1.40%  |  1.44%  |  1.31%  |  1.44%  |  1.47%  |  1.53%  |  1.57%  |  1.60%  |  1.60%  |  1.63%  |  1.50%  |  1.44%  |  1.27%  |  1.18%  |  1.11%  |  1.08%  |  1.01%  |  0.98%  |  0.95%  |  0.95%  |  0.98%  |  1.01%  |  0.98%  |  0.98%  |  0.95%  |  0.95%  |  0.91%  |  0.85%  |  0.85%  |  0.82%  |  0.75%  |  0.69%  |  0.65%  |  0.65%  |  0.62%  |  0.56%  |  0.56%  |  0.56%  |  0.52%  |  0.49%  |  0.46%  |  0.42%  |  2.48%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |\n (2000,City of London)  |  2000  |  City of London  |         7014  |       3775  |  0.66%  |  0.66%  |  0.64%  |  0.61%  |  0.58%  |  0.56%  |  0.58%  |  0.56%  |  0.58%  |  0.56%  |  0.58%  |  0.53%  |  0.48%  |  0.42%  |  0.42%  |  0.40%  |  0.45%  |  0.50%  |  0.64%  |  0.79%  |  1.03%  |  1.27%  |  1.51%  |  1.70%  |  1.91%  |  2.15%  |  2.33%  |  2.38%  |  2.38%  |  2.41%  |  2.49%  |  2.49%  |  2.38%  |  2.28%  |  2.15%  |  2.12%  |  2.01%  |  1.85%  |  1.72%  |  1.64%  |  1.64%  |  1.64%  |  1.56%  |  1.48%  |  1.48%  |  1.48%  |  1.64%  |  1.64%  |  1.64%  |  1.77%  |  1.88%  |  1.88%  |  1.96%  |  1.75%  |  1.70%  |  1.67%  |  1.48%  |  1.32%  |  1.25%  |  1.14%  |  1.03%  |  0.90%  |  0.85%  |  0.82%  |  0.79%  |  0.77%  |  0.74%  |  0.74%  |  0.72%  |  0.74%  |  0.72%  |  0.69%  |  0.66%  |  0.61%  |  0.58%  |  0.58%  |  0.50%  |  0.50%  |  0.45%  |  0.42%  |  0.40%  |  0.37%  |  0.32%  |  0.32%  |  0.29%  |  1.03%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |         3239  |  0.80%  |  0.74%  |  0.74%  |  0.68%  |  0.65%  |  0.65%  |  0.62%  |  0.59%  |  0.59%  |  0.59%  |  0.59%  |  0.56%  |  0.52%  |  0.56%  |  0.52%  |  0.56%  |  0.62%  |  0.71%  |  0.86%  |  0.99%  |  1.17%  |  1.51%  |  1.79%  |  1.88%  |  2.13%  |  2.32%  |  2.53%  |  2.56%  |  2.59%  |  2.50%  |  2.47%  |  2.35%  |  2.25%  |  2.13%  |  1.98%  |  1.76%  |  1.64%  |  1.48%  |  1.36%  |  1.33%  |  1.27%  |  1.30%  |  1.27%  |  1.33%  |  1.33%  |  1.36%  |  1.27%  |  1.39%  |  1.39%  |  1.45%  |  1.51%  |  1.54%  |  1.57%  |  1.57%  |  1.45%  |  1.39%  |  1.27%  |  1.14%  |  1.08%  |  0.99%  |  0.93%  |  0.90%  |  0.86%  |  0.83%  |  0.86%  |  0.90%  |  0.86%  |  0.86%  |  0.86%  |  0.86%  |  0.83%  |  0.77%  |  0.77%  |  0.74%  |  0.68%  |  0.65%  |  0.59%  |  0.62%  |  0.59%  |  0.52%  |  0.49%  |  0.46%  |  0.46%  |  0.46%  |  0.40%  |  2.50%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |\n (2001,City of London)  |  2001  |  City of London  |         7359  |       3984  |  0.43%  |  0.75%  |  0.63%  |  0.48%  |  0.53%  |  0.63%  |  0.50%  |  0.40%  |  0.48%  |  0.68%  |  0.65%  |  0.40%  |  0.73%  |  0.48%  |  0.28%  |  0.35%  |  0.30%  |  0.33%  |  0.30%  |  0.68%  |  0.95%  |  0.98%  |  1.33%  |  1.58%  |  2.01%  |  2.76%  |  2.76%  |  2.51%  |  2.71%  |  3.14%  |  2.86%  |  2.38%  |  2.33%  |  2.64%  |  2.48%  |  2.11%  |  2.23%  |  1.78%  |  1.58%  |  1.88%  |  1.63%  |  1.31%  |  1.18%  |  1.91%  |  1.83%  |  1.36%  |  1.23%  |  1.13%  |  1.51%  |  1.23%  |  2.79%  |  1.83%  |  1.38%  |  2.03%  |  2.01%  |  1.53%  |  1.58%  |  1.41%  |  1.61%  |  1.20%  |  0.70%  |  0.85%  |  1.13%  |  0.90%  |  0.63%  |  0.70%  |  0.53%  |  0.70%  |  0.53%  |  0.68%  |  0.68%  |  0.53%  |  0.60%  |  0.73%  |  0.53%  |  0.48%  |  0.58%  |  0.50%  |  0.28%  |  0.48%  |  0.38%  |  0.35%  |  0.20%  |  0.38%  |  0.25%  |  0.15%  |  0.25%  |  0.13%  |  0.10%  |  0.08%  |     13  |         3375  |  0.95%  |  0.89%  |  0.68%  |  0.62%  |  0.77%  |  0.47%  |  0.62%  |  0.44%  |  0.53%  |  0.71%  |  0.59%  |  0.59%  |  0.30%  |  0.77%  |  0.39%  |  0.39%  |  0.41%  |  1.16%  |  0.83%  |  1.04%  |  1.39%  |  1.16%  |  1.45%  |  1.60%  |  2.19%  |  2.76%  |  2.64%  |  2.37%  |  2.49%  |  3.17%  |  2.52%  |  2.46%  |  2.99%  |  1.96%  |  1.78%  |  1.60%  |  1.96%  |  1.78%  |  1.42%  |  1.33%  |  1.10%  |  0.83%  |  0.92%  |  1.87%  |  1.27%  |  1.57%  |  1.48%  |  1.39%  |  1.04%  |  1.24%  |  1.10%  |  1.93%  |  1.42%  |  1.69%  |  1.69%  |  1.13%  |  1.33%  |  1.13%  |  1.13%  |  0.98%  |  0.95%  |  0.95%  |  0.77%  |  0.74%  |  0.71%  |  0.86%  |  0.83%  |  0.80%  |  1.01%  |  0.80%  |  0.59%  |  0.77%  |  0.71%  |  0.92%  |  0.53%  |  0.71%  |  0.80%  |  0.39%  |  0.39%  |  0.44%  |  0.53%  |  0.65%  |  0.53%  |  0.33%  |  0.30%  |  0.30%  |  0.41%  |  0.77%  |  0.47%  |  0.21%  |     15  |\n (2002,City of London)  |  2002  |  City of London  |         7280  |       3968  |  0.73%  |  0.45%  |  0.71%  |  0.68%  |  0.50%  |  0.50%  |  0.60%  |  0.50%  |  0.43%  |  0.50%  |  0.63%  |  0.63%  |  0.35%  |  0.68%  |  0.23%  |  0.13%  |  0.20%  |  0.25%  |  0.30%  |  0.25%  |  0.50%  |  0.98%  |  1.26%  |  1.54%  |  1.71%  |  2.49%  |  2.85%  |  3.07%  |  2.57%  |  2.87%  |  2.87%  |  2.70%  |  2.49%  |  2.24%  |  2.55%  |  2.34%  |  2.12%  |  1.94%  |  1.84%  |  1.54%  |  1.97%  |  1.64%  |  1.44%  |  1.26%  |  1.89%  |  1.81%  |  1.41%  |  1.26%  |  1.13%  |  1.54%  |  1.16%  |  2.85%  |  1.76%  |  1.23%  |  2.14%  |  1.92%  |  1.49%  |  1.54%  |  1.39%  |  1.51%  |  1.13%  |  0.71%  |  0.78%  |  1.16%  |  1.01%  |  0.58%  |  0.66%  |  0.50%  |  0.63%  |  0.53%  |  0.60%  |  0.76%  |  0.53%  |  0.63%  |  0.73%  |  0.50%  |  0.45%  |  0.53%  |  0.43%  |  0.28%  |  0.43%  |  0.35%  |  0.33%  |  0.20%  |  0.38%  |  0.23%  |  0.15%  |  0.20%  |  0.10%  |  0.05%  |     14  |         3312  |  0.85%  |  1.06%  |  0.91%  |  0.57%  |  0.63%  |  0.79%  |  0.45%  |  0.60%  |  0.39%  |  0.54%  |  0.51%  |  0.45%  |  0.51%  |  0.18%  |  0.57%  |  0.39%  |  0.15%  |  0.45%  |  1.06%  |  0.88%  |  1.06%  |  1.45%  |  1.57%  |  1.42%  |  2.02%  |  2.23%  |  2.54%  |  2.54%  |  2.51%  |  2.11%  |  3.47%  |  2.54%  |  2.84%  |  3.05%  |  1.90%  |  1.84%  |  1.72%  |  1.75%  |  1.87%  |  1.33%  |  1.42%  |  1.12%  |  0.85%  |  1.00%  |  1.81%  |  1.09%  |  1.42%  |  1.57%  |  1.36%  |  1.09%  |  1.15%  |  1.24%  |  1.90%  |  1.54%  |  1.63%  |  1.66%  |  1.12%  |  1.24%  |  1.21%  |  1.09%  |  0.91%  |  0.88%  |  0.91%  |  0.82%  |  0.69%  |  0.69%  |  0.85%  |  0.85%  |  0.75%  |  1.03%  |  0.75%  |  0.57%  |  0.72%  |  0.69%  |  0.94%  |  0.57%  |  0.75%  |  0.79%  |  0.30%  |  0.36%  |  0.42%  |  0.51%  |  0.66%  |  0.45%  |  0.27%  |  0.30%  |  0.21%  |  0.33%  |  0.79%  |  0.42%  |     20  |\n (2003,City of London)  |  2003  |  City of London  |         7115  |       3892  |  0.90%  |  0.62%  |  0.41%  |  0.57%  |  0.67%  |  0.44%  |  0.49%  |  0.62%  |  0.54%  |  0.54%  |  0.62%  |  0.64%  |  0.54%  |  0.26%  |  0.41%  |  0.26%  |  0.10%  |  0.13%  |  0.31%  |  0.31%  |  0.28%  |  0.49%  |  1.26%  |  1.54%  |  1.70%  |  2.06%  |  2.57%  |  3.03%  |  2.93%  |  2.70%  |  2.77%  |  2.70%  |  2.54%  |  2.42%  |  2.18%  |  2.49%  |  2.29%  |  2.11%  |  1.95%  |  1.77%  |  1.62%  |  1.90%  |  1.64%  |  1.46%  |  1.31%  |  1.93%  |  1.90%  |  1.44%  |  1.31%  |  1.00%  |  1.64%  |  1.13%  |  2.77%  |  1.67%  |  1.21%  |  2.06%  |  1.90%  |  1.44%  |  1.57%  |  1.41%  |  1.41%  |  1.13%  |  0.67%  |  0.77%  |  1.08%  |  0.92%  |  0.59%  |  0.62%  |  0.57%  |  0.57%  |  0.46%  |  0.59%  |  0.80%  |  0.46%  |  0.62%  |  0.69%  |  0.49%  |  0.44%  |  0.57%  |  0.39%  |  0.26%  |  0.41%  |  0.31%  |  0.26%  |  0.21%  |  0.36%  |  0.21%  |  0.15%  |  0.18%  |  0.05%  |     13  |         3223  |  0.56%  |  0.78%  |  0.96%  |  0.90%  |  0.56%  |  0.65%  |  0.81%  |  0.43%  |  0.59%  |  0.37%  |  0.59%  |  0.40%  |  0.34%  |  0.37%  |  0.06%  |  0.50%  |  0.22%  |  0.37%  |  0.68%  |  0.96%  |  1.05%  |  1.15%  |  1.61%  |  1.89%  |  1.80%  |  2.64%  |  2.26%  |  2.67%  |  2.54%  |  2.64%  |  2.33%  |  3.07%  |  2.64%  |  2.76%  |  2.85%  |  1.86%  |  1.80%  |  1.40%  |  1.58%  |  1.64%  |  1.30%  |  1.46%  |  1.37%  |  0.90%  |  1.09%  |  1.83%  |  1.18%  |  1.27%  |  1.52%  |  1.21%  |  1.09%  |  1.24%  |  1.15%  |  1.89%  |  1.52%  |  1.64%  |  1.55%  |  1.09%  |  1.24%  |  1.05%  |  1.12%  |  0.93%  |  0.90%  |  0.93%  |  0.84%  |  0.74%  |  0.71%  |  0.87%  |  0.93%  |  0.78%  |  0.99%  |  0.74%  |  0.53%  |  0.71%  |  0.65%  |  0.93%  |  0.50%  |  0.71%  |  0.74%  |  0.31%  |  0.34%  |  0.43%  |  0.47%  |  0.50%  |  0.50%  |  0.28%  |  0.06%  |  0.16%  |  0.31%  |  0.68%  |     26  |\n (2004,City of London)  |  2004  |  City of London  |         7118  |       3875  |  0.72%  |  0.70%  |  0.65%  |  0.41%  |  0.52%  |  0.70%  |  0.44%  |  0.52%  |  0.80%  |  0.46%  |  0.57%  |  0.54%  |  0.57%  |  0.41%  |  0.10%  |  0.34%  |  0.21%  |  0.00%  |  0.05%  |  0.34%  |  0.23%  |  0.65%  |  0.67%  |  1.63%  |  1.81%  |  1.78%  |  2.30%  |  2.89%  |  2.97%  |  2.99%  |  2.58%  |  2.40%  |  2.63%  |  2.40%  |  2.22%  |  2.04%  |  2.55%  |  2.37%  |  2.27%  |  2.09%  |  1.88%  |  1.52%  |  1.68%  |  1.73%  |  1.50%  |  1.26%  |  1.96%  |  1.81%  |  1.42%  |  1.29%  |  1.14%  |  1.57%  |  1.16%  |  2.55%  |  1.68%  |  1.19%  |  1.99%  |  1.96%  |  1.42%  |  1.47%  |  1.37%  |  1.47%  |  1.11%  |  0.72%  |  0.83%  |  1.01%  |  0.88%  |  0.57%  |  0.65%  |  0.54%  |  0.57%  |  0.46%  |  0.52%  |  0.77%  |  0.41%  |  0.62%  |  0.72%  |  0.49%  |  0.36%  |  0.52%  |  0.23%  |  0.31%  |  0.36%  |  0.26%  |  0.21%  |  0.21%  |  0.39%  |  0.21%  |  0.13%  |  0.13%  |     12  |         3243  |  0.96%  |  0.56%  |  0.62%  |  0.80%  |  0.77%  |  0.62%  |  0.77%  |  0.86%  |  0.43%  |  0.59%  |  0.22%  |  0.59%  |  0.31%  |  0.28%  |  0.28%  |  0.03%  |  0.40%  |  0.52%  |  0.28%  |  0.68%  |  1.17%  |  1.26%  |  1.45%  |  1.82%  |  2.44%  |  2.47%  |  2.81%  |  2.65%  |  2.44%  |  2.50%  |  2.44%  |  2.41%  |  2.81%  |  2.47%  |  2.68%  |  2.56%  |  1.73%  |  1.88%  |  1.45%  |  1.48%  |  1.70%  |  1.23%  |  1.39%  |  1.20%  |  0.93%  |  1.05%  |  1.82%  |  1.14%  |  1.20%  |  1.73%  |  1.17%  |  1.02%  |  1.17%  |  1.11%  |  1.85%  |  1.57%  |  1.60%  |  1.57%  |  1.05%  |  1.23%  |  1.08%  |  1.05%  |  0.86%  |  0.99%  |  0.86%  |  0.83%  |  0.77%  |  0.62%  |  0.89%  |  0.86%  |  0.74%  |  0.93%  |  0.74%  |  0.46%  |  0.65%  |  0.68%  |  0.89%  |  0.46%  |  0.71%  |  0.68%  |  0.37%  |  0.34%  |  0.37%  |  0.34%  |  0.37%  |  0.37%  |  0.22%  |  0.03%  |  0.09%  |  0.28%  |     42  |\n (2005,City of London)  |  2005  |  City of London  |         7131  |       3869  |  0.88%  |  0.65%  |  0.57%  |  0.59%  |  0.31%  |  0.49%  |  0.57%  |  0.49%  |  0.49%  |  0.80%  |  0.52%  |  0.54%  |  0.52%  |  0.49%  |  0.10%  |  0.08%  |  0.31%  |  0.26%  |  0.00%  |  0.18%  |  0.23%  |  0.41%  |  1.11%  |  1.03%  |  1.86%  |  2.40%  |  1.91%  |  2.30%  |  3.02%  |  2.92%  |  2.69%  |  2.27%  |  2.38%  |  2.82%  |  2.53%  |  1.99%  |  1.96%  |  2.66%  |  2.30%  |  2.02%  |  2.07%  |  1.76%  |  1.58%  |  1.78%  |  1.60%  |  1.50%  |  1.32%  |  2.12%  |  1.73%  |  1.50%  |  1.34%  |  1.09%  |  1.52%  |  1.29%  |  2.30%  |  1.73%  |  1.21%  |  1.89%  |  1.81%  |  1.42%  |  1.50%  |  1.29%  |  1.37%  |  1.09%  |  0.70%  |  0.78%  |  1.09%  |  0.80%  |  0.59%  |  0.62%  |  0.52%  |  0.44%  |  0.44%  |  0.49%  |  0.78%  |  0.39%  |  0.54%  |  0.70%  |  0.44%  |  0.36%  |  0.54%  |  0.21%  |  0.36%  |  0.31%  |  0.18%  |  0.18%  |  0.16%  |  0.36%  |  0.18%  |  0.05%  |     12  |         3262  |  0.89%  |  0.89%  |  0.46%  |  0.46%  |  0.71%  |  0.77%  |  0.52%  |  0.71%  |  0.77%  |  0.43%  |  0.55%  |  0.21%  |  0.61%  |  0.21%  |  0.21%  |  0.18%  |  0.06%  |  0.21%  |  0.58%  |  0.46%  |  0.95%  |  1.32%  |  1.50%  |  1.53%  |  2.51%  |  2.33%  |  2.73%  |  2.73%  |  2.61%  |  2.33%  |  2.76%  |  2.33%  |  2.61%  |  2.79%  |  2.27%  |  2.45%  |  2.27%  |  1.84%  |  1.69%  |  1.59%  |  1.38%  |  1.59%  |  1.32%  |  1.44%  |  1.16%  |  1.04%  |  1.04%  |  1.69%  |  1.01%  |  1.16%  |  1.75%  |  1.16%  |  1.10%  |  1.20%  |  1.13%  |  1.78%  |  1.59%  |  1.69%  |  1.53%  |  1.04%  |  1.13%  |  1.07%  |  1.13%  |  0.80%  |  1.01%  |  0.92%  |  0.77%  |  0.74%  |  0.55%  |  0.83%  |  0.80%  |  0.61%  |  0.86%  |  0.67%  |  0.55%  |  0.64%  |  0.74%  |  0.80%  |  0.49%  |  0.64%  |  0.67%  |  0.31%  |  0.37%  |  0.34%  |  0.31%  |  0.37%  |  0.28%  |  0.06%  |  0.06%  |  0.09%  |     50  |\n (2006,City of London)  |  2006  |  City of London  |         7254  |       3959  |  0.66%  |  0.63%  |  0.66%  |  0.51%  |  0.45%  |  0.33%  |  0.40%  |  0.58%  |  0.56%  |  0.45%  |  0.76%  |  0.45%  |  0.51%  |  0.43%  |  0.18%  |  0.13%  |  0.13%  |  0.38%  |  0.30%  |  0.00%  |  0.30%  |  0.48%  |  0.81%  |  1.59%  |  1.59%  |  2.07%  |  2.75%  |  2.10%  |  2.50%  |  3.13%  |  2.88%  |  2.60%  |  2.10%  |  2.15%  |  2.93%  |  2.48%  |  1.94%  |  1.87%  |  2.70%  |  2.20%  |  1.89%  |  2.07%  |  1.72%  |  1.59%  |  1.74%  |  1.49%  |  1.57%  |  1.34%  |  2.10%  |  1.67%  |  1.52%  |  1.31%  |  1.21%  |  1.59%  |  1.21%  |  2.15%  |  1.69%  |  1.19%  |  1.72%  |  1.57%  |  1.41%  |  1.41%  |  1.21%  |  1.36%  |  1.01%  |  0.71%  |  0.66%  |  1.04%  |  0.78%  |  0.56%  |  0.58%  |  0.51%  |  0.35%  |  0.40%  |  0.48%  |  0.73%  |  0.43%  |  0.51%  |  0.71%  |  0.43%  |  0.35%  |  0.48%  |  0.15%  |  0.25%  |  0.30%  |  0.13%  |  0.10%  |  0.15%  |  0.30%  |  0.18%  |     13  |         3295  |  0.85%  |  0.79%  |  0.94%  |  0.36%  |  0.52%  |  0.76%  |  0.70%  |  0.61%  |  0.67%  |  0.76%  |  0.42%  |  0.55%  |  0.15%  |  0.58%  |  0.12%  |  0.21%  |  0.12%  |  0.36%  |  0.21%  |  0.82%  |  0.73%  |  1.00%  |  1.55%  |  1.76%  |  2.15%  |  2.67%  |  2.43%  |  2.61%  |  2.61%  |  2.79%  |  2.55%  |  2.61%  |  2.37%  |  2.55%  |  2.43%  |  2.34%  |  2.25%  |  2.06%  |  1.88%  |  1.37%  |  1.61%  |  1.37%  |  1.55%  |  1.37%  |  1.40%  |  1.18%  |  1.15%  |  1.00%  |  1.61%  |  0.94%  |  1.12%  |  1.73%  |  1.18%  |  1.09%  |  1.15%  |  1.09%  |  1.70%  |  1.52%  |  1.61%  |  1.43%  |  0.91%  |  1.24%  |  0.94%  |  1.03%  |  0.73%  |  0.91%  |  0.88%  |  0.70%  |  0.79%  |  0.52%  |  0.82%  |  0.79%  |  0.61%  |  0.79%  |  0.73%  |  0.55%  |  0.61%  |  0.67%  |  0.82%  |  0.52%  |  0.61%  |  0.58%  |  0.33%  |  0.33%  |  0.30%  |  0.27%  |  0.33%  |  0.27%  |  0.06%  |  0.00%  |     45  |\n (2007,City of London)  |  2007  |  City of London  |         7607  |       4197  |  0.74%  |  0.60%  |  0.57%  |  0.52%  |  0.55%  |  0.38%  |  0.31%  |  0.33%  |  0.64%  |  0.55%  |  0.36%  |  0.79%  |  0.38%  |  0.45%  |  0.17%  |  0.17%  |  0.07%  |  0.21%  |  0.38%  |  0.64%  |  0.00%  |  0.71%  |  0.83%  |  1.52%  |  2.17%  |  2.14%  |  2.69%  |  2.93%  |  2.36%  |  2.55%  |  2.93%  |  2.43%  |  2.53%  |  2.00%  |  2.07%  |  2.72%  |  2.48%  |  2.05%  |  1.98%  |  2.79%  |  2.10%  |  1.88%  |  1.76%  |  1.67%  |  1.79%  |  1.76%  |  1.50%  |  1.52%  |  1.36%  |  1.91%  |  1.55%  |  1.48%  |  1.31%  |  1.22%  |  1.52%  |  1.26%  |  1.86%  |  1.52%  |  1.14%  |  1.50%  |  1.41%  |  1.29%  |  1.38%  |  1.10%  |  1.29%  |  0.86%  |  0.62%  |  0.71%  |  0.93%  |  0.71%  |  0.52%  |  0.55%  |  0.41%  |  0.38%  |  0.38%  |  0.45%  |  0.64%  |  0.36%  |  0.36%  |  0.69%  |  0.33%  |  0.33%  |  0.43%  |  0.14%  |  0.14%  |  0.24%  |  0.10%  |  0.10%  |  0.14%  |  0.21%  |     20  |         3410  |  0.70%  |  0.70%  |  0.88%  |  0.73%  |  0.50%  |  0.50%  |  0.50%  |  0.62%  |  0.47%  |  0.62%  |  0.73%  |  0.44%  |  0.53%  |  0.12%  |  0.56%  |  0.09%  |  0.32%  |  0.70%  |  0.44%  |  0.70%  |  1.14%  |  1.00%  |  1.64%  |  2.05%  |  2.20%  |  2.67%  |  2.93%  |  2.49%  |  2.96%  |  2.43%  |  2.35%  |  2.23%  |  2.35%  |  2.11%  |  2.11%  |  2.11%  |  2.23%  |  2.14%  |  1.82%  |  1.91%  |  1.38%  |  1.64%  |  1.41%  |  1.61%  |  1.47%  |  1.44%  |  1.26%  |  1.06%  |  0.88%  |  1.41%  |  0.85%  |  1.11%  |  1.58%  |  1.20%  |  1.14%  |  1.14%  |  1.14%  |  1.64%  |  1.52%  |  1.55%  |  1.35%  |  0.79%  |  1.20%  |  1.03%  |  0.94%  |  0.79%  |  0.88%  |  0.94%  |  0.65%  |  0.70%  |  0.47%  |  0.79%  |  0.76%  |  0.53%  |  0.73%  |  0.67%  |  0.50%  |  0.53%  |  0.65%  |  0.67%  |  0.50%  |  0.50%  |  0.56%  |  0.32%  |  0.29%  |  0.29%  |  0.21%  |  0.26%  |  0.23%  |  0.03%  |     36  |\n (2008,City of London)  |  2008  |  City of London  |         7429  |       4131  |  0.70%  |  0.63%  |  0.53%  |  0.58%  |  0.44%  |  0.53%  |  0.31%  |  0.29%  |  0.41%  |  0.73%  |  0.48%  |  0.36%  |  0.73%  |  0.41%  |  0.24%  |  0.19%  |  0.15%  |  0.17%  |  0.27%  |  0.44%  |  0.68%  |  0.27%  |  1.04%  |  0.94%  |  1.82%  |  2.42%  |  2.06%  |  2.98%  |  2.95%  |  2.47%  |  2.25%  |  2.69%  |  2.32%  |  2.44%  |  1.82%  |  1.94%  |  2.49%  |  2.23%  |  2.01%  |  1.91%  |  2.93%  |  2.08%  |  1.91%  |  1.67%  |  1.69%  |  1.60%  |  1.86%  |  1.38%  |  1.62%  |  1.62%  |  1.96%  |  1.55%  |  1.48%  |  1.23%  |  1.26%  |  1.48%  |  1.33%  |  1.77%  |  1.48%  |  1.11%  |  1.45%  |  1.48%  |  1.28%  |  1.33%  |  1.02%  |  1.28%  |  0.82%  |  0.63%  |  0.70%  |  0.94%  |  0.70%  |  0.58%  |  0.53%  |  0.41%  |  0.36%  |  0.34%  |  0.46%  |  0.58%  |  0.34%  |  0.39%  |  0.65%  |  0.31%  |  0.29%  |  0.41%  |  0.15%  |  0.15%  |  0.17%  |  0.10%  |  0.07%  |  0.12%  |     25  |         3298  |  0.36%  |  0.52%  |  0.70%  |  0.70%  |  0.73%  |  0.49%  |  0.52%  |  0.49%  |  0.58%  |  0.45%  |  0.64%  |  0.73%  |  0.45%  |  0.58%  |  0.03%  |  0.58%  |  0.00%  |  0.67%  |  0.79%  |  0.76%  |  1.06%  |  1.30%  |  1.06%  |  1.73%  |  2.21%  |  2.49%  |  2.70%  |  2.88%  |  2.67%  |  2.85%  |  2.30%  |  2.33%  |  2.06%  |  2.09%  |  1.73%  |  1.82%  |  1.85%  |  1.94%  |  2.06%  |  1.73%  |  1.91%  |  1.24%  |  1.58%  |  1.39%  |  1.58%  |  1.52%  |  1.52%  |  1.12%  |  1.06%  |  0.97%  |  1.27%  |  0.88%  |  1.15%  |  1.61%  |  1.21%  |  1.27%  |  1.27%  |  1.15%  |  1.58%  |  1.55%  |  1.55%  |  1.46%  |  0.82%  |  1.24%  |  1.03%  |  0.88%  |  0.91%  |  0.94%  |  0.88%  |  0.70%  |  0.67%  |  0.49%  |  0.85%  |  0.82%  |  0.61%  |  0.73%  |  0.67%  |  0.49%  |  0.45%  |  0.67%  |  0.73%  |  0.45%  |  0.52%  |  0.55%  |  0.30%  |  0.27%  |  0.27%  |  0.21%  |  0.27%  |  0.24%  |     30  |  Calling  parallel()  as per the example below enables concurrent iteration over the  DataFrame  rows which internally is \nimplemented using the  Fork and Join  framework introduced in \nJava 7. This is the exact same procedure as above, but would likely be significantly faster for a very large frame on a multi-core \nmachine. It has to be said that parallel execution will not necessarily always be faster due to the overhead of context switching, \nhowever if the consumer function being executed is expensive or the frame is very large, you will likely see a significant \nperformance boost.   //Parallel: Convert male & female population counts into weights\nframe.rows().parallel().forEach(row -> row.forEach(value -> {\n    if (value.colKey().matches(\"M\\\\s+\\\\d+\")) {\n        double totalMales = value.row().getDouble(\"All Males\");\n        double count = value.getDouble();\n        value.setDouble(count / totalMales);\n    } else if (value.colKey().matches(\"F\\\\s+\\\\d+\")) {\n        double totalFemales = value.row().getDouble(\"All Females\");\n        double count = value.getDouble();\n        value.setDouble(count / totalFemales);\n    }\n}));",
            "title": "Iteration"
        },
        {
            "location": "/frame/access/#value-access",
            "text": "",
            "title": "Value Access"
        },
        {
            "location": "/frame/access/#random-access_1",
            "text": "Forgetting about the ONS example for a moment, let's construct a new 5x5  DataFrame  programmatically with\nmixed types as per the code below. The first 4 columns are of a homogeneous type, while the fifth column contains\na mixture of types.   import java.time.LocalDate;\nimport java.time.Month;\nimport java.time.Year;\n\nimport com.zavtech.morpheus.util.Range;\nimport com.zavtech.morpheus.array.Array;\nimport com.zavtech.morpheus.frame.DataFrame;\n\n//Create 5x5 frame with columns of different types.\nfinal Range<Year> years = Range.of(2000 ,2005).map(Year::of);\nfinal DataFrame<Year,String> frame = DataFrame.of(years, String.class, columns -> {\n    columns.add(\"Column-0\", Array.of(true, false, false, true, true));\n    columns.add(\"Column-1\", Array.of(1, 2, 3, 4, 5));\n    columns.add(\"Column-2\", Array.of(10L, 11L, 12L, 13L, 14L));\n    columns.add(\"Column-3\", Array.of(20d, 21d, 22d, 23d, 24d));\n    columns.add(\"Column-4\", Array.of(\"Hello\", LocalDate.of(1998, 1, 1), Month.JANUARY, 56.45d, true));\n});  \n Index  |  Column-0  |  Column-1  |  Column-2  |  Column-3  |   Column-4   |\n----------------------------------------------------------------------------\n  2000  |      true  |         1  |        10  |   20.0000  |       Hello  |\n  2001  |     false  |         2  |        11  |   21.0000  |  1998-01-01  |\n  2002  |     false  |         3  |        12  |   22.0000  |     JANUARY  |\n  2003  |      true  |         4  |        13  |   23.0000  |      56.450  |\n  2004  |      true  |         5  |        14  |   24.0000  |        true  |  Random access to  DataFrame  values is possible via the function calls illustrated below. There are type specific\naccess methods for primitives, specifically with support for  boolean ,  int ,  long and  double . Type agnostic \nmethods are also available, namely  getValue()  and  setValue() , however when accessing primitive values in this \nmanner, boxing will take place with a small but inevitable performance hit. The random access API allows values to \nbe accessed by ordinals, keys or a combination of ordinals and keys for ultimate flexibility.  Type specific random access methods on this frame using a combination of ordinals and keys are shown below. Note \nthat there are corresponding setter methods analogous to these getters, but are omitted for brevity. If a type \nincompatible call is made, for example calling  getBoolean()  on a cell that contains a  double , a  DataFrameException \nwill be thrown.   //Random access to primitive boolean values via ordinal or keys or any combination thereof\nboolean b1 = frame.data().getBoolean(0, 0);\nboolean b2 = frame.data().getBoolean(Year.of(2003), 0);\nboolean b3 = frame.data().getBoolean(0, \"Column-0\");\nboolean b4 = frame.data().getBoolean(Year.of(2001), \"Column-0\");\n\n//Random access to primitive int values via ordinal or keys or any combination thereof\nint i1 = frame.data().getInt(4, 1);\nint i2 = frame.data().getInt(Year.of(2003), 1);\nint i3 = frame.data().getInt(0, \"Column-1\");\nint i4 = frame.data().getInt(Year.of(2001), \"Column-1\");\n\n//Random access to primitive long values via ordinal or keys or any combination thereof\nlong l1 = frame.data().getLong(4, 2);\nlong l2 = frame.data().getLong(Year.of(2003), 2);\nlong l3 = frame.data().getLong(0, \"Column-2\");\nlong l4 = frame.data().getLong(Year.of(2001), \"Column-2\");\n\n//Random access to primitive double values via ordinal or keys or any combination thereof\ndouble d1 = frame.data().getDouble(4, 3);\ndouble d2 = frame.data().getDouble(Year.of(2003), 3);\ndouble d3 = frame.data().getDouble(0, \"Column-3\");\ndouble d4 = frame.data().getDouble(Year.of(2001), \"Column-3\");\n\n//Random access to any values via ordinal or keys or any combination thereof\nString o1 = frame.data().getValue(0, 4);\nLocalDate o2 = frame.data().getValue(Year.of(2001), 4);\nMonth o3 = frame.data().getValue(2, \"Column-4\");\nDouble o4 = frame.data().getValue(Year.of(2004), \"Column-4\");",
            "title": "Random Access"
        },
        {
            "location": "/frame/access/#iteration_1",
            "text": "To iterate over all values in a  DataFrame , use the  forEachValue()  method or the  values()  method\nto access a Java 8 stream of  DataFrameValue  objects. The API makes it easy to iterate over all\nvalues in parallel much the same way as the  java.util.stream.Stream  interface, simply by calling  parallel()  on the frame. The example below first creates a 8x4 frame of doubles initialized with random \nvalues between 0 and 1, and then counts the number of values > 0.5, first sequentially, then in parallel.   //Create DataFrame of random doubles\nDataFrame<Integer,String> frame = DataFrame.ofDoubles(\n    Array.of(0, 1, 2, 3, 4, 5, 6, 7),\n    Array.of(\"A\", \"B\", \"C\", \"D\"), \n    value -> Math.random()\n);\n\n//Count number of values > 0.5d first sequentially, then in parallel\nlong count1 = frame.values().filter(v -> v.getDouble() > 0.5d).count();\nlong count2 = frame.values().parallel().filter(v -> v.getDouble() > 0.5d).count();\nAssert.assertEquals(count1, count2);  It should be noted that using the parallel API will not always speed up execution, and in some cases will \nactually result in worse performance. If one or more of the stream pipeline operations (such as  filter() )\nis slow, then parallel execution should significantly improve performance over sequential execution.",
            "title": "Iteration"
        },
        {
            "location": "/frame/access/#dataframevalue",
            "text": "The  forEachValue()  and the  values()  method operate on a class called  DataFrameValue , which provides a\nversatile API that wraps the actual datum value. For example, it provides an  isNull()  method to indicate if\na value is null, which avoids the consumer from having to code type specific checks, such as for  Double.isNaN .\nThe  DataFrameValue  also provides access to the row and column ordinals and keys associated with the datum,\nvia  rowOrdinal() ,  colOrdinal() ,  rowKey()  and  colKey()  methods, in addition to the frame itself via frame() . This makes it possible to write more reusable consumers and predicates for example.",
            "title": "DataFrameValue"
        },
        {
            "location": "/frame/access/#first-last",
            "text": "Selecting the first and last row or column returns an  Optional<DataFrameRow>  and  Optional<DataFrameColumn>  \nrespectively, as shown in the following examples:    //Load the Wimbledon dataset\nDataFrame<Integer,String> frame = getWimbledonData(2013);\n\n//Select the first row, and assert match date is 24-06-2013\nframe.rows().first().ifPresent(row -> {\n    LocalDate matchDate = row.getValue(\"Date\");\n    assert(matchDate.equals(LocalDate.of(2013, 6, 24)));\n});\n\n//Select last row, and assert the winner is A Murray.\nframe.rows().last().ifPresent(row -> {\n    String winner = row.getValue(\"Winner\");\n    assert(winner.equals(\"Murray A.\"));\n});\n\n//Select first column, namely the date column, and find max date\nframe.cols().first().ifPresent(column -> {\n    Optional<LocalDate> matchDate = column.max();\n    LocalDate finalDate = LocalDate.of(2013, 7, 7);\n    assert(matchDate.isPresent() && matchDate.get().equals(finalDate));\n});\n\n//Select last column and find the max AvgL betting odds\nframe.cols().last().ifPresent(column -> {\n    double matchDate = column.stats().max();\n    assert(matchDate == 23.26d);\n});",
            "title": "First &amp; Last"
        },
        {
            "location": "/frame/access/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/frame/access/#demean",
            "text": "Let's say we wish to demean the male and female population counts in the ONS  DataFrame  introduced earlier. That \nis to say, for each row, we want to compute the average male and female population value, and them subtract the \nmale average from all male counts, and subtract the female average from all female counts.   //Demean male and female population counts\nfinal DataFrame<Tuple,String> onsFrame = loadPopulationDataset();\n\n//Iterate over male then female column set\nArray.of(\"M\\\\s+\\\\d++\", \"F\\\\s+\\\\d++\").forEach(regex -> {\n    onsFrame.cols().select(c -> c.key().matches(regex)).rows().parallel().forEach(row -> {\n        final double mean = row.stats().mean();\n        row.applyDoubles(v -> v.getDouble() - mean);\n    });\n});\n\n//Print frame to std out with custom formatting\nframe.out().print(formats -> {\n    formats.withDecimalFormat(\"All Persons\", \"0;-0\", 1);\n    formats.withDecimalFormat(\"All Males\", \"0;-0\", 1);\n    formats.withDecimalFormat(\"All Females\", \"0;-0\", 1);\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});  \n         Index          |  Year  |     Borough      |  All Persons  |  All Males  |   M 0    |   M 1    |   M 2    |   M 3    |   M 4    |   M 5    |   M 6    |   M 7    |   M 8    |   M 9    |   M 10   |   M 11   |   M 12   |   M 13   |   M 14   |   M 15   |   M 16   |   M 17   |   M 18   |   M 19   |   M 20   |   M 21   |   M 22   |  M 23   |  M 24   |  M 25   |  M 26   |  M 27   |  M 28   |  M 29   |  M 30   |  M 31   |  M 32   |  M 33   |  M 34   |  M 35   |  M 36   |  M 37   |  M 38   |  M 39   |  M 40   |  M 41   |  M 42   |  M 43   |  M 44   |  M 45   |  M 46   |  M 47   |  M 48   |  M 49   |  M 50   |  M 51   |  M 52   |  M 53   |  M 54   |  M 55   |  M 56   |  M 57   |  M 58   |  M 59   |   M 60   |   M 61   |   M 62   |   M 63   |   M 64   |   M 65   |   M 66   |   M 67   |   M 68   |   M 69   |   M 70   |   M 71   |   M 72   |   M 73   |   M 74   |   M 75   |   M 76   |   M 77   |   M 78   |   M 79   |   M 80   |   M 81   |   M 82   |   M 83   |   M 84   |   M 85   |   M 86   |   M 87   |   M 88   |   M 89   |  M 90+  |  All Females  |   F 0    |   F 1    |   F 2    |   F 3    |   F 4    |   F 5    |   F 6    |   F 7    |   F 8    |   F 9    |   F 10   |   F 11   |   F 12   |   F 13   |   F 14   |   F 15   |   F 16   |   F 17   |   F 18   |   F 19   |   F 20   |  F 21   |  F 22   |  F 23   |  F 24   |  F 25   |  F 26   |  F 27   |  F 28   |  F 29   |  F 30   |  F 31   |  F 32   |  F 33   |  F 34   |  F 35   |  F 36   |  F 37   |  F 38   |  F 39   |  F 40   |  F 41   |  F 42   |  F 43   |  F 44   |  F 45   |  F 46   |  F 47   |  F 48   |  F 49   |  F 50   |  F 51   |  F 52   |  F 53   |  F 54   |  F 55   |  F 56   |  F 57   |  F 58   |  F 59   |  F 60   |   F 61   |   F 62   |   F 63   |   F 64   |   F 65   |   F 66   |   F 67   |   F 68   |   F 69   |   F 70   |   F 71   |   F 72   |   F 73   |   F 74   |   F 75   |   F 76   |   F 77   |   F 78   |   F 79   |   F 80   |   F 81   |   F 82   |   F 83   |   F 84   |   F 85   |   F 86   |   F 87   |   F 88   |   F 89   |  F 90+  |\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n (1999,City of London)  |  1999  |  City of London  |         6581  |       3519  |  -16.92  |  -17.92  |  -18.92  |  -19.92  |  -20.92  |  -19.92  |  -20.92  |  -19.92  |  -20.92  |  -18.92  |  -18.92  |  -19.92  |  -21.92  |  -22.92  |  -23.92  |  -23.92  |  -23.92  |  -19.92  |  -14.92  |   -8.92  |   -0.92  |    6.08  |   11.08  |  19.08  |  24.08  |  29.08  |  32.08  |  32.08  |  32.08  |  37.08  |  37.08  |  36.08  |  34.08  |  31.08  |  30.08  |  30.08  |  24.08  |  19.08  |  17.08  |  17.08  |  16.08  |  14.08  |  10.08  |  12.08  |  12.08  |  18.08  |  18.08  |  19.08  |  24.08  |  27.08  |  27.08  |  29.08  |  22.08  |  21.08  |  20.08  |  13.08  |   8.08  |   6.08  |   2.08  |  -2.92  |   -5.92  |   -6.92  |   -6.92  |   -7.92  |   -8.92  |   -9.92  |   -8.92  |   -9.92  |  -10.92  |  -10.92  |  -11.92  |  -13.92  |  -14.92  |  -16.92  |  -17.92  |  -20.92  |  -20.92  |  -22.92  |  -23.92  |  -25.92  |  -26.92  |  -28.92  |  -28.92  |  -29.92  |  -29.92  |    2.08  |     NaN  |     NaN  |     NaN  |     NaN  |      0  |         3062  |  -11.60  |  -11.60  |  -13.60  |  -13.60  |  -14.60  |  -14.60  |  -16.60  |  -16.60  |  -17.60  |  -17.60  |  -17.60  |  -19.60  |  -18.60  |  -19.60  |  -19.60  |  -19.60  |  -15.60  |  -10.60  |   -9.60  |   -5.60  |    3.40  |  11.40  |  14.40  |  18.40  |  25.40  |  32.40  |  35.40  |  36.40  |  35.40  |  37.40  |  35.40  |  32.40  |  30.40  |  26.40  |  19.40  |  15.40  |  11.40  |   7.40  |   6.40  |   3.40  |   3.40  |   5.40  |   6.40  |   7.40  |   8.40  |   4.40  |   8.40  |   9.40  |  11.40  |  12.40  |  13.40  |  13.40  |  14.40  |  10.40  |   8.40  |   3.40  |   0.40  |  -1.60  |  -2.60  |  -4.60  |  -5.60  |   -6.60  |   -6.60  |   -5.60  |   -4.60  |   -5.60  |   -5.60  |   -6.60  |   -6.60  |   -7.60  |   -9.60  |   -9.60  |  -10.60  |  -12.60  |  -14.60  |  -15.60  |  -15.60  |  -16.60  |  -18.60  |  -18.60  |  -18.60  |  -19.60  |  -20.60  |  -21.60  |  -22.60  |   40.40  |     NaN  |     NaN  |     NaN  |     NaN  |      0  |\n (2000,City of London)  |  2000  |  City of London  |         7014  |       3775  |  -18.90  |  -18.90  |  -19.90  |  -20.90  |  -21.90  |  -22.90  |  -21.90  |  -22.90  |  -21.90  |  -22.90  |  -21.90  |  -23.90  |  -25.90  |  -27.90  |  -27.90  |  -28.90  |  -26.90  |  -24.90  |  -19.90  |  -13.90  |   -4.90  |    4.10  |   13.10  |  20.10  |  28.10  |  37.10  |  44.10  |  46.10  |  46.10  |  47.10  |  50.10  |  50.10  |  46.10  |  42.10  |  37.10  |  36.10  |  32.10  |  26.10  |  21.10  |  18.10  |  18.10  |  18.10  |  15.10  |  12.10  |  12.10  |  12.10  |  18.10  |  18.10  |  18.10  |  23.10  |  27.10  |  27.10  |  30.10  |  22.10  |  20.10  |  19.10  |  12.10  |   6.10  |   3.10  |  -0.90  |   -4.90  |   -9.90  |  -11.90  |  -12.90  |  -13.90  |  -14.90  |  -15.90  |  -15.90  |  -16.90  |  -15.90  |  -16.90  |  -17.90  |  -18.90  |  -20.90  |  -21.90  |  -21.90  |  -24.90  |  -24.90  |  -26.90  |  -27.90  |  -28.90  |  -29.90  |  -31.90  |  -31.90  |  -32.90  |   -4.90  |     NaN  |     NaN  |     NaN  |     NaN  |      0  |         3239  |  -11.66  |  -13.66  |  -13.66  |  -15.66  |  -16.66  |  -16.66  |  -17.66  |  -18.66  |  -18.66  |  -18.66  |  -18.66  |  -19.66  |  -20.66  |  -19.66  |  -20.66  |  -19.66  |  -17.66  |  -14.66  |   -9.66  |   -5.66  |    0.34  |  11.34  |  20.34  |  23.34  |  31.34  |  37.34  |  44.34  |  45.34  |  46.34  |  43.34  |  42.34  |  38.34  |  35.34  |  31.34  |  26.34  |  19.34  |  15.34  |  10.34  |   6.34  |   5.34  |   3.34  |   4.34  |   3.34  |   5.34  |   5.34  |   6.34  |   3.34  |   7.34  |   7.34  |   9.34  |  11.34  |  12.34  |  13.34  |  13.34  |   9.34  |   7.34  |   3.34  |  -0.66  |  -2.66  |  -5.66  |  -7.66  |   -8.66  |   -9.66  |  -10.66  |   -9.66  |   -8.66  |   -9.66  |   -9.66  |   -9.66  |   -9.66  |  -10.66  |  -12.66  |  -12.66  |  -13.66  |  -15.66  |  -16.66  |  -18.66  |  -17.66  |  -18.66  |  -20.66  |  -21.66  |  -22.66  |  -22.66  |  -22.66  |  -24.66  |   43.34  |     NaN  |     NaN  |     NaN  |     NaN  |      0  |\n (2001,City of London)  |  2001  |  City of London  |         7359  |       3984  |  -27.12  |  -14.12  |  -19.12  |  -25.12  |  -23.12  |  -19.12  |  -24.12  |  -28.12  |  -25.12  |  -17.12  |  -18.12  |  -28.12  |  -15.12  |  -25.12  |  -33.12  |  -30.12  |  -32.12  |  -31.12  |  -32.12  |  -17.12  |   -6.12  |   -5.12  |    8.88  |  18.88  |  35.88  |  65.88  |  65.88  |  55.88  |  63.88  |  80.88  |  69.88  |  50.88  |  48.88  |  60.88  |  54.88  |  39.88  |  44.88  |  26.88  |  18.88  |  30.88  |  20.88  |   7.88  |   2.88  |  31.88  |  28.88  |   9.88  |   4.88  |   0.88  |  15.88  |   4.88  |  66.88  |  28.88  |  10.88  |  36.88  |  35.88  |  16.88  |  18.88  |  11.88  |  19.88  |   3.88  |  -16.12  |  -10.12  |    0.88  |   -8.12  |  -19.12  |  -16.12  |  -23.12  |  -16.12  |  -23.12  |  -17.12  |  -17.12  |  -23.12  |  -20.12  |  -15.12  |  -23.12  |  -25.12  |  -21.12  |  -24.12  |  -33.12  |  -25.12  |  -29.12  |  -30.12  |  -36.12  |  -29.12  |  -34.12  |  -38.12  |  -34.12  |  -39.12  |  -40.12  |  -41.12  |     13  |         3375  |   -5.33  |   -7.33  |  -14.33  |  -16.33  |  -11.33  |  -21.33  |  -16.33  |  -22.33  |  -19.33  |  -13.33  |  -17.33  |  -17.33  |  -27.33  |  -11.33  |  -24.33  |  -24.33  |  -23.33  |    1.67  |   -9.33  |   -2.33  |    9.67  |   1.67  |  11.67  |  16.67  |  36.67  |  55.67  |  51.67  |  42.67  |  46.67  |  69.67  |  47.67  |  45.67  |  63.67  |  28.67  |  22.67  |  16.67  |  28.67  |  22.67  |  10.67  |   7.67  |  -0.33  |  -9.33  |  -6.33  |  25.67  |   5.67  |  15.67  |  12.67  |   9.67  |  -2.33  |   4.67  |  -0.33  |  27.67  |  10.67  |  19.67  |  19.67  |   0.67  |   7.67  |   0.67  |   0.67  |  -4.33  |  -5.33  |   -5.33  |  -11.33  |  -12.33  |  -13.33  |   -8.33  |   -9.33  |  -10.33  |   -3.33  |  -10.33  |  -17.33  |  -11.33  |  -13.33  |   -6.33  |  -19.33  |  -13.33  |  -10.33  |  -24.33  |  -24.33  |  -22.33  |  -19.33  |  -15.33  |  -19.33  |  -26.33  |  -27.33  |  -27.33  |  -23.33  |  -11.33  |  -21.33  |  -30.33  |     15  |\n (2002,City of London)  |  2002  |  City of London  |         7280  |       3968  |  -14.93  |  -25.93  |  -15.93  |  -16.93  |  -23.93  |  -23.93  |  -19.93  |  -23.93  |  -26.93  |  -23.93  |  -18.93  |  -18.93  |  -29.93  |  -16.93  |  -34.93  |  -38.93  |  -35.93  |  -33.93  |  -31.93  |  -33.93  |  -23.93  |   -4.93  |    6.07  |  17.07  |  24.07  |  55.07  |  69.07  |  78.07  |  58.07  |  70.07  |  70.07  |  63.07  |  55.07  |  45.07  |  57.07  |  49.07  |  40.07  |  33.07  |  29.07  |  17.07  |  34.07  |  21.07  |  13.07  |   6.07  |  31.07  |  28.07  |  12.07  |   6.07  |   1.07  |  17.07  |   2.07  |  69.07  |  26.07  |   5.07  |  41.07  |  32.07  |  15.07  |  17.07  |  11.07  |  16.07  |    1.07  |  -15.93  |  -12.93  |    2.07  |   -3.93  |  -20.93  |  -17.93  |  -23.93  |  -18.93  |  -22.93  |  -19.93  |  -13.93  |  -22.93  |  -18.93  |  -14.93  |  -23.93  |  -25.93  |  -22.93  |  -26.93  |  -32.93  |  -26.93  |  -29.93  |  -30.93  |  -35.93  |  -28.93  |  -34.93  |  -37.93  |  -35.93  |  -39.93  |  -41.93  |     14  |         3312  |   -8.58  |   -1.58  |   -6.58  |  -17.58  |  -15.58  |  -10.58  |  -21.58  |  -16.58  |  -23.58  |  -18.58  |  -19.58  |  -21.58  |  -19.58  |  -30.58  |  -17.58  |  -23.58  |  -31.58  |  -21.58  |   -1.58  |   -7.58  |   -1.58  |  11.42  |  15.42  |  10.42  |  30.42  |  37.42  |  47.42  |  47.42  |  46.42  |  33.42  |  78.42  |  47.42  |  57.42  |  64.42  |  26.42  |  24.42  |  20.42  |  21.42  |  25.42  |   7.42  |  10.42  |   0.42  |  -8.58  |  -3.58  |  23.42  |  -0.58  |  10.42  |  15.42  |   8.42  |  -0.58  |   1.42  |   4.42  |  26.42  |  14.42  |  17.42  |  18.42  |   0.42  |   4.42  |   3.42  |  -0.58  |  -6.58  |   -7.58  |   -6.58  |   -9.58  |  -13.58  |  -13.58  |   -8.58  |   -8.58  |  -11.58  |   -2.58  |  -11.58  |  -17.58  |  -12.58  |  -13.58  |   -5.58  |  -17.58  |  -11.58  |  -10.58  |  -26.58  |  -24.58  |  -22.58  |  -19.58  |  -14.58  |  -21.58  |  -27.58  |  -26.58  |  -29.58  |  -25.58  |  -10.58  |  -22.58  |     20  |\n (2003,City of London)  |  2003  |  City of London  |         7115  |       3892  |   -8.10  |  -19.10  |  -27.10  |  -21.10  |  -17.10  |  -26.10  |  -24.10  |  -19.10  |  -22.10  |  -22.10  |  -19.10  |  -18.10  |  -22.10  |  -33.10  |  -27.10  |  -33.10  |  -39.10  |  -38.10  |  -31.10  |  -31.10  |  -32.10  |  -24.10  |    5.90  |  16.90  |  22.90  |  36.90  |  56.90  |  74.90  |  70.90  |  61.90  |  64.90  |  61.90  |  55.90  |  50.90  |  41.90  |  53.90  |  45.90  |  38.90  |  32.90  |  25.90  |  19.90  |  30.90  |  20.90  |  13.90  |   7.90  |  31.90  |  30.90  |  12.90  |   7.90  |  -4.10  |  20.90  |   0.90  |  64.90  |  21.90  |   3.90  |  36.90  |  30.90  |  12.90  |  17.90  |  11.90  |   11.90  |    0.90  |  -17.10  |  -13.10  |   -1.10  |   -7.10  |  -20.10  |  -19.10  |  -21.10  |  -21.10  |  -25.10  |  -20.10  |  -12.10  |  -25.10  |  -19.10  |  -16.10  |  -24.10  |  -26.10  |  -21.10  |  -28.10  |  -33.10  |  -27.10  |  -31.10  |  -33.10  |  -35.10  |  -29.10  |  -35.10  |  -37.10  |  -36.10  |  -41.10  |     13  |         3223  |  -17.52  |  -10.52  |   -4.52  |   -6.52  |  -17.52  |  -14.52  |   -9.52  |  -21.52  |  -16.52  |  -23.52  |  -16.52  |  -22.52  |  -24.52  |  -23.52  |  -33.52  |  -19.52  |  -28.52  |  -23.52  |  -13.52  |   -4.52  |   -1.52  |   1.48  |  16.48  |  25.48  |  22.48  |  49.48  |  37.48  |  50.48  |  46.48  |  49.48  |  39.48  |  63.48  |  49.48  |  53.48  |  56.48  |  24.48  |  22.48  |   9.48  |  15.48  |  17.48  |   6.48  |  11.48  |   8.48  |  -6.52  |  -0.52  |  23.48  |   2.48  |   5.48  |  13.48  |   3.48  |  -0.52  |   4.48  |   1.48  |  25.48  |  13.48  |  17.48  |  14.48  |  -0.52  |   4.48  |  -1.52  |   0.48  |   -5.52  |   -6.52  |   -5.52  |   -8.52  |  -11.52  |  -12.52  |   -7.52  |   -5.52  |  -10.52  |   -3.52  |  -11.52  |  -18.52  |  -12.52  |  -14.52  |   -5.52  |  -19.52  |  -12.52  |  -11.52  |  -25.52  |  -24.52  |  -21.52  |  -20.52  |  -19.52  |  -19.52  |  -26.52  |  -33.52  |  -30.52  |  -25.52  |  -13.52  |     26  |\n (2004,City of London)  |  2004  |  City of London  |         7118  |       3875  |  -14.92  |  -15.92  |  -17.92  |  -26.92  |  -22.92  |  -15.92  |  -25.92  |  -22.92  |  -11.92  |  -24.92  |  -20.92  |  -21.92  |  -20.92  |  -26.92  |  -38.92  |  -29.92  |  -34.92  |  -42.92  |  -40.92  |  -29.92  |  -33.92  |  -17.92  |  -16.92  |  20.08  |  27.08  |  26.08  |  46.08  |  69.08  |  72.08  |  73.08  |  57.08  |  50.08  |  59.08  |  50.08  |  43.08  |  36.08  |  56.08  |  49.08  |  45.08  |  38.08  |  30.08  |  16.08  |  22.08  |  24.08  |  15.08  |   6.08  |  33.08  |  27.08  |  12.08  |   7.08  |   1.08  |  18.08  |   2.08  |  56.08  |  22.08  |   3.08  |  34.08  |  33.08  |  12.08  |  14.08  |   10.08  |   14.08  |    0.08  |  -14.92  |  -10.92  |   -3.92  |   -8.92  |  -20.92  |  -17.92  |  -21.92  |  -20.92  |  -24.92  |  -22.92  |  -12.92  |  -26.92  |  -18.92  |  -14.92  |  -23.92  |  -28.92  |  -22.92  |  -33.92  |  -30.92  |  -28.92  |  -32.92  |  -34.92  |  -34.92  |  -27.92  |  -34.92  |  -37.92  |  -37.92  |     12  |         3243  |   -4.57  |  -17.57  |  -15.57  |   -9.57  |  -10.57  |  -15.57  |  -10.57  |   -7.57  |  -21.57  |  -16.57  |  -28.57  |  -16.57  |  -25.57  |  -26.57  |  -26.57  |  -34.57  |  -22.57  |  -18.57  |  -26.57  |  -13.57  |    2.43  |   5.43  |  11.43  |  23.43  |  43.43  |  44.43  |  55.43  |  50.43  |  43.43  |  45.43  |  43.43  |  42.43  |  55.43  |  44.43  |  51.43  |  47.43  |  20.43  |  25.43  |  11.43  |  12.43  |  19.43  |   4.43  |   9.43  |   3.43  |  -5.57  |  -1.57  |  23.43  |   1.43  |   3.43  |  20.43  |   2.43  |  -2.57  |   2.43  |   0.43  |  24.43  |  15.43  |  16.43  |  15.43  |  -1.57  |   4.43  |  -0.57  |   -1.57  |   -7.57  |   -3.57  |   -7.57  |   -8.57  |  -10.57  |  -15.57  |   -6.57  |   -7.57  |  -11.57  |   -5.57  |  -11.57  |  -20.57  |  -14.57  |  -13.57  |   -6.57  |  -20.57  |  -12.57  |  -13.57  |  -23.57  |  -24.57  |  -23.57  |  -24.57  |  -23.57  |  -23.57  |  -28.57  |  -34.57  |  -32.57  |  -26.57  |     42  |\n (2005,City of London)  |  2005  |  City of London  |         7131  |       3869  |   -8.86  |  -17.86  |  -20.86  |  -19.86  |  -30.86  |  -23.86  |  -20.86  |  -23.86  |  -23.86  |  -11.86  |  -22.86  |  -21.86  |  -22.86  |  -23.86  |  -38.86  |  -39.86  |  -30.86  |  -32.86  |  -42.86  |  -35.86  |  -33.86  |  -26.86  |    0.14  |  -2.86  |  29.14  |  50.14  |  31.14  |  46.14  |  74.14  |  70.14  |  61.14  |  45.14  |  49.14  |  66.14  |  55.14  |  34.14  |  33.14  |  60.14  |  46.14  |  35.14  |  37.14  |  25.14  |  18.14  |  26.14  |  19.14  |  15.14  |   8.14  |  39.14  |  24.14  |  15.14  |   9.14  |  -0.86  |  16.14  |   7.14  |  46.14  |  24.14  |   4.14  |  30.14  |  27.14  |  12.14  |   15.14  |    7.14  |   10.14  |   -0.86  |  -15.86  |  -12.86  |   -0.86  |  -11.86  |  -19.86  |  -18.86  |  -22.86  |  -25.86  |  -25.86  |  -23.86  |  -12.86  |  -27.86  |  -21.86  |  -15.86  |  -25.86  |  -28.86  |  -21.86  |  -34.86  |  -28.86  |  -30.86  |  -35.86  |  -35.86  |  -36.86  |  -28.86  |  -35.86  |  -40.86  |     12  |         3262  |   -6.69  |   -6.69  |  -20.69  |  -20.69  |  -12.69  |  -10.69  |  -18.69  |  -12.69  |  -10.69  |  -21.69  |  -17.69  |  -28.69  |  -15.69  |  -28.69  |  -28.69  |  -29.69  |  -33.69  |  -28.69  |  -16.69  |  -20.69  |   -4.69  |   7.31  |  13.31  |  14.31  |  46.31  |  40.31  |  53.31  |  53.31  |  49.31  |  40.31  |  54.31  |  40.31  |  49.31  |  55.31  |  38.31  |  44.31  |  38.31  |  24.31  |  19.31  |  16.31  |   9.31  |  16.31  |   7.31  |  11.31  |   2.31  |  -1.69  |  -1.69  |  19.31  |  -2.69  |   2.31  |  21.31  |   2.31  |   0.31  |   3.31  |   1.31  |  22.31  |  16.31  |  19.31  |  14.31  |  -1.69  |   1.31  |   -0.69  |    1.31  |   -9.69  |   -2.69  |   -5.69  |  -10.69  |  -11.69  |  -17.69  |   -8.69  |   -9.69  |  -15.69  |   -7.69  |  -13.69  |  -17.69  |  -14.69  |  -11.69  |   -9.69  |  -19.69  |  -14.69  |  -13.69  |  -25.69  |  -23.69  |  -24.69  |  -25.69  |  -23.69  |  -26.69  |  -33.69  |  -33.69  |  -32.69  |     50  |\n (2006,City of London)  |  2006  |  City of London  |         7254  |       3959  |  -17.84  |  -18.84  |  -17.84  |  -23.84  |  -25.84  |  -30.84  |  -27.84  |  -20.84  |  -21.84  |  -25.84  |  -13.84  |  -25.84  |  -23.84  |  -26.84  |  -36.84  |  -38.84  |  -38.84  |  -28.84  |  -31.84  |  -43.84  |  -31.84  |  -24.84  |  -11.84  |  19.16  |  19.16  |  38.16  |  65.16  |  39.16  |  55.16  |  80.16  |  70.16  |  59.16  |  39.16  |  41.16  |  72.16  |  54.16  |  33.16  |  30.16  |  63.16  |  43.16  |  31.16  |  38.16  |  24.16  |  19.16  |  25.16  |  15.16  |  18.16  |   9.16  |  39.16  |  22.16  |  16.16  |   8.16  |   4.16  |  19.16  |   4.16  |  41.16  |  23.16  |   3.16  |  24.16  |  18.16  |   12.16  |   12.16  |    4.16  |   10.16  |   -3.84  |  -15.84  |  -17.84  |   -2.84  |  -12.84  |  -21.84  |  -20.84  |  -23.84  |  -29.84  |  -27.84  |  -24.84  |  -14.84  |  -26.84  |  -23.84  |  -15.84  |  -26.84  |  -29.84  |  -24.84  |  -37.84  |  -33.84  |  -31.84  |  -38.84  |  -39.84  |  -37.84  |  -31.84  |  -36.84  |     13  |         3295  |   -8.11  |  -10.11  |   -5.11  |  -24.11  |  -19.11  |  -11.11  |  -13.11  |  -16.11  |  -14.11  |  -11.11  |  -22.11  |  -18.11  |  -31.11  |  -17.11  |  -32.11  |  -29.11  |  -32.11  |  -24.11  |  -29.11  |   -9.11  |  -12.11  |  -3.11  |  14.89  |  21.89  |  34.89  |  51.89  |  43.89  |  49.89  |  49.89  |  55.89  |  47.89  |  49.89  |  41.89  |  47.89  |  43.89  |  40.89  |  37.89  |  31.89  |  25.89  |   8.89  |  16.89  |   8.89  |  14.89  |   8.89  |   9.89  |   2.89  |   1.89  |  -3.11  |  16.89  |  -5.11  |   0.89  |  20.89  |   2.89  |  -0.11  |   1.89  |  -0.11  |  19.89  |  13.89  |  16.89  |  10.89  |  -6.11  |    4.89  |   -5.11  |   -2.11  |  -12.11  |   -6.11  |   -7.11  |  -13.11  |  -10.11  |  -19.11  |   -9.11  |  -10.11  |  -16.11  |  -10.11  |  -12.11  |  -18.11  |  -16.11  |  -14.11  |   -9.11  |  -19.11  |  -16.11  |  -17.11  |  -25.11  |  -25.11  |  -26.11  |  -27.11  |  -25.11  |  -27.11  |  -34.11  |  -36.11  |     45  |\n (2007,City of London)  |  2007  |  City of London  |         7607  |       4197  |  -15.41  |  -21.41  |  -22.41  |  -24.41  |  -23.41  |  -30.41  |  -33.41  |  -32.41  |  -19.41  |  -23.41  |  -31.41  |  -13.41  |  -30.41  |  -27.41  |  -39.41  |  -39.41  |  -43.41  |  -37.41  |  -30.41  |  -19.41  |  -46.41  |  -16.41  |  -11.41  |  17.59  |  44.59  |  43.59  |  66.59  |  76.59  |  52.59  |  60.59  |  76.59  |  55.59  |  59.59  |  37.59  |  40.59  |  67.59  |  57.59  |  39.59  |  36.59  |  70.59  |  41.59  |  32.59  |  27.59  |  23.59  |  28.59  |  27.59  |  16.59  |  17.59  |  10.59  |  33.59  |  18.59  |  15.59  |   8.59  |   4.59  |  17.59  |   6.59  |  31.59  |  17.59  |   1.59  |  16.59  |   12.59  |    7.59  |   11.59  |   -0.41  |    7.59  |  -10.41  |  -20.41  |  -16.41  |   -7.41  |  -16.41  |  -24.41  |  -23.41  |  -29.41  |  -30.41  |  -30.41  |  -27.41  |  -19.41  |  -31.41  |  -31.41  |  -17.41  |  -32.41  |  -32.41  |  -28.41  |  -40.41  |  -40.41  |  -36.41  |  -42.41  |  -42.41  |  -40.41  |  -37.41  |     20  |         3410  |  -13.49  |  -13.49  |   -7.49  |  -12.49  |  -20.49  |  -20.49  |  -20.49  |  -16.49  |  -21.49  |  -16.49  |  -12.49  |  -22.49  |  -19.49  |  -33.49  |  -18.49  |  -34.49  |  -26.49  |  -13.49  |  -22.49  |  -13.49  |    1.51  |  -3.49  |  18.51  |  32.51  |  37.51  |  53.51  |  62.51  |  47.51  |  63.51  |  45.51  |  42.51  |  38.51  |  42.51  |  34.51  |  34.51  |  34.51  |  38.51  |  35.51  |  24.51  |  27.51  |   9.51  |  18.51  |  10.51  |  17.51  |  12.51  |  11.51  |   5.51  |  -1.49  |  -7.49  |  10.51  |  -8.49  |   0.51  |  16.51  |   3.51  |   1.51  |   1.51  |   1.51  |  18.51  |  14.51  |  15.51  |   8.51  |  -10.49  |    3.51  |   -2.49  |   -5.49  |  -10.49  |   -7.49  |   -5.49  |  -15.49  |  -13.49  |  -21.49  |  -10.49  |  -11.49  |  -19.49  |  -12.49  |  -14.49  |  -20.49  |  -19.49  |  -15.49  |  -14.49  |  -20.49  |  -20.49  |  -18.49  |  -26.49  |  -27.49  |  -27.49  |  -30.49  |  -28.49  |  -29.49  |  -36.49  |     36  |\n (2008,City of London)  |  2008  |  City of London  |         7429  |       4131  |  -16.62  |  -19.62  |  -23.62  |  -21.62  |  -27.62  |  -23.62  |  -32.62  |  -33.62  |  -28.62  |  -15.62  |  -25.62  |  -30.62  |  -15.62  |  -28.62  |  -35.62  |  -37.62  |  -39.62  |  -38.62  |  -34.62  |  -27.62  |  -17.62  |  -34.62  |   -2.62  |  -6.62  |  29.38  |  54.38  |  39.38  |  77.38  |  76.38  |  56.38  |  47.38  |  65.38  |  50.38  |  55.38  |  29.38  |  34.38  |  57.38  |  46.38  |  37.38  |  33.38  |  75.38  |  40.38  |  33.38  |  23.38  |  24.38  |  20.38  |  31.38  |  11.38  |  21.38  |  21.38  |  35.38  |  18.38  |  15.38  |   5.38  |   6.38  |  15.38  |   9.38  |  27.38  |  15.38  |   0.38  |   14.38  |   15.38  |    7.38  |    9.38  |   -3.62  |    7.38  |  -11.62  |  -19.62  |  -16.62  |   -6.62  |  -16.62  |  -21.62  |  -23.62  |  -28.62  |  -30.62  |  -31.62  |  -26.62  |  -21.62  |  -31.62  |  -29.62  |  -18.62  |  -32.62  |  -33.62  |  -28.62  |  -39.62  |  -39.62  |  -38.62  |  -41.62  |  -42.62  |  -40.62  |     25  |         3298  |  -24.31  |  -19.31  |  -13.31  |  -13.31  |  -12.31  |  -20.31  |  -19.31  |  -20.31  |  -17.31  |  -21.31  |  -15.31  |  -12.31  |  -21.31  |  -17.31  |  -35.31  |  -17.31  |  -36.31  |  -14.31  |  -10.31  |  -11.31  |   -1.31  |   6.69  |  -1.31  |  20.69  |  36.69  |  45.69  |  52.69  |  58.69  |  51.69  |  57.69  |  39.69  |  40.69  |  31.69  |  32.69  |  20.69  |  23.69  |  24.69  |  27.69  |  31.69  |  20.69  |  26.69  |   4.69  |  15.69  |   9.69  |  15.69  |  13.69  |  13.69  |   0.69  |  -1.31  |  -4.31  |   5.69  |  -7.31  |   1.69  |  16.69  |   3.69  |   5.69  |   5.69  |   1.69  |  15.69  |  14.69  |  14.69  |   11.69  |   -9.31  |    4.69  |   -2.31  |   -7.31  |   -6.31  |   -5.31  |   -7.31  |  -13.31  |  -14.31  |  -20.31  |   -8.31  |   -9.31  |  -16.31  |  -12.31  |  -14.31  |  -20.31  |  -21.31  |  -14.31  |  -12.31  |  -21.31  |  -19.31  |  -18.31  |  -26.31  |  -27.31  |  -27.31  |  -29.31  |  -27.31  |  -28.31  |     30  |",
            "title": "Demean"
        },
        {
            "location": "/frame/access/#weights",
            "text": "This example demonstrates a scenario where we wish to convert all the male and female counts to be a percentage of\ntotal count per that Borough in 2007. For example, in the City of London in 2007, the total population was 7607 people.\nWe would like to divide all the age specific male and female counts for the City of London by this number so we can see values \nas a percentage of the baseline in 2007. The code below shows how a reusable  ToDoubleFunction  can be used to apply this\nlogic to different parts of the frame incrementally. Note this is done all in place, so if you would want to preserve the\noriginal frame, you ought to create a copy of the frame first using  DataFrame.copy() .   //Load ONS population dataset\nfinal DataFrame<Tuple,String> onsFrame = loadPopulationDataset();\n\n//Define function to compute population weight as a percentage of 2007 value per borough\nfinal ToDoubleFunction<DataFrameValue<Tuple,String>> compute = value -> {\n    final String borough = value.rowKey().item(1);\n    final Tuple rowKey2014 = Tuple.of(2007, borough);\n    final double boroughCountIn2014 = onsFrame.data().getDouble(rowKey2014, \"All Persons\");\n    return value.getDouble() / boroughCountIn2014;\n};\n\n//Apply function to various columns in order\nonsFrame.cols().select(c -> c.key().matches(\"[MF]\\\\s+\\\\d+\")).applyDoubles(compute);\nonsFrame.colAt(\"All Males\").applyDoubles(compute);\nonsFrame.colAt(\"All Females\").applyDoubles(compute);\n\n//Print frame to std out\nonsFrame.out().print(formats -> {\n    formats.withDecimalFormat(\"All Persons\", \"0.0;-0.0\", 1);\n    formats.withDecimalFormat(Double.class, \"0.00'%';-0.00'%'\", 100);\n});  \n         Index          |  Year  |     Borough      |  All Persons  |  All Males  |   M 0   |   M 1   |   M 2   |   M 3   |   M 4   |   M 5   |   M 6   |   M 7   |   M 8   |   M 9   |  M 10   |  M 11   |  M 12   |  M 13   |  M 14   |  M 15   |  M 16   |  M 17   |  M 18   |  M 19   |  M 20   |  M 21   |  M 22   |  M 23   |  M 24   |  M 25   |  M 26   |  M 27   |  M 28   |  M 29   |  M 30   |  M 31   |  M 32   |  M 33   |  M 34   |  M 35   |  M 36   |  M 37   |  M 38   |  M 39   |  M 40   |  M 41   |  M 42   |  M 43   |  M 44   |  M 45   |  M 46   |  M 47   |  M 48   |  M 49   |  M 50   |  M 51   |  M 52   |  M 53   |  M 54   |  M 55   |  M 56   |  M 57   |  M 58   |  M 59   |  M 60   |  M 61   |  M 62   |  M 63   |  M 64   |  M 65   |  M 66   |  M 67   |  M 68   |  M 69   |  M 70   |  M 71   |  M 72   |  M 73   |  M 74   |  M 75   |  M 76   |  M 77   |  M 78   |  M 79   |  M 80   |  M 81   |  M 82   |  M 83   |  M 84   |  M 85   |  M 86   |  M 87   |  M 88   |  M 89   |  M 90+  |  All Females  |   F 0   |   F 1   |   F 2   |   F 3   |   F 4   |   F 5   |   F 6   |   F 7   |   F 8   |   F 9   |  F 10   |  F 11   |  F 12   |  F 13   |  F 14   |  F 15   |  F 16   |  F 17   |  F 18   |  F 19   |  F 20   |  F 21   |  F 22   |  F 23   |  F 24   |  F 25   |  F 26   |  F 27   |  F 28   |  F 29   |  F 30   |  F 31   |  F 32   |  F 33   |  F 34   |  F 35   |  F 36   |  F 37   |  F 38   |  F 39   |  F 40   |  F 41   |  F 42   |  F 43   |  F 44   |  F 45   |  F 46   |  F 47   |  F 48   |  F 49   |  F 50   |  F 51   |  F 52   |  F 53   |  F 54   |  F 55   |  F 56   |  F 57   |  F 58   |  F 59   |  F 60   |  F 61   |  F 62   |  F 63   |  F 64   |  F 65   |  F 66   |  F 67   |  F 68   |  F 69   |  F 70   |  F 71   |  F 72   |  F 73   |  F 74   |  F 75   |  F 76   |  F 77   |  F 78   |  F 79   |  F 80   |  F 81   |  F 82   |  F 83   |  F 84   |  F 85   |  F 86   |  F 87   |  F 88   |  F 89   |  F 90+  |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n (1999,City of London)  |  1999  |  City of London  |       6581.0  |     46.26%  |  0.32%  |  0.30%  |  0.29%  |  0.28%  |  0.26%  |  0.28%  |  0.26%  |  0.28%  |  0.26%  |  0.29%  |  0.29%  |  0.28%  |  0.25%  |  0.24%  |  0.22%  |  0.22%  |  0.22%  |  0.28%  |  0.34%  |  0.42%  |  0.53%  |  0.62%  |  0.68%  |  0.79%  |  0.85%  |  0.92%  |  0.96%  |  0.96%  |  0.96%  |  1.03%  |  1.03%  |  1.01%  |  0.99%  |  0.95%  |  0.93%  |  0.93%  |  0.85%  |  0.79%  |  0.76%  |  0.76%  |  0.75%  |  0.72%  |  0.67%  |  0.70%  |  0.70%  |  0.78%  |  0.78%  |  0.79%  |  0.85%  |  0.89%  |  0.89%  |  0.92%  |  0.83%  |  0.82%  |  0.80%  |  0.71%  |  0.64%  |  0.62%  |  0.57%  |  0.50%  |  0.46%  |  0.45%  |  0.45%  |  0.43%  |  0.42%  |  0.41%  |  0.42%  |  0.41%  |  0.39%  |  0.39%  |  0.38%  |  0.35%  |  0.34%  |  0.32%  |  0.30%  |  0.26%  |  0.26%  |  0.24%  |  0.22%  |  0.20%  |  0.18%  |  0.16%  |  0.16%  |  0.14%  |  0.14%  |  0.57%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |       40.25%  |  0.32%  |  0.32%  |  0.29%  |  0.29%  |  0.28%  |  0.28%  |  0.25%  |  0.25%  |  0.24%  |  0.24%  |  0.24%  |  0.21%  |  0.22%  |  0.21%  |  0.21%  |  0.21%  |  0.26%  |  0.33%  |  0.34%  |  0.39%  |  0.51%  |  0.62%  |  0.66%  |  0.71%  |  0.80%  |  0.89%  |  0.93%  |  0.95%  |  0.93%  |  0.96%  |  0.93%  |  0.89%  |  0.87%  |  0.82%  |  0.72%  |  0.67%  |  0.62%  |  0.57%  |  0.55%  |  0.51%  |  0.51%  |  0.54%  |  0.55%  |  0.57%  |  0.58%  |  0.53%  |  0.58%  |  0.59%  |  0.62%  |  0.63%  |  0.64%  |  0.64%  |  0.66%  |  0.60%  |  0.58%  |  0.51%  |  0.47%  |  0.45%  |  0.43%  |  0.41%  |  0.39%  |  0.38%  |  0.38%  |  0.39%  |  0.41%  |  0.39%  |  0.39%  |  0.38%  |  0.38%  |  0.37%  |  0.34%  |  0.34%  |  0.33%  |  0.30%  |  0.28%  |  0.26%  |  0.26%  |  0.25%  |  0.22%  |  0.22%  |  0.22%  |  0.21%  |  0.20%  |  0.18%  |  0.17%  |  1.00%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |\n (2000,City of London)  |  2000  |  City of London  |       7014.0  |     49.63%  |  0.33%  |  0.33%  |  0.32%  |  0.30%  |  0.29%  |  0.28%  |  0.29%  |  0.28%  |  0.29%  |  0.28%  |  0.29%  |  0.26%  |  0.24%  |  0.21%  |  0.21%  |  0.20%  |  0.22%  |  0.25%  |  0.32%  |  0.39%  |  0.51%  |  0.63%  |  0.75%  |  0.84%  |  0.95%  |  1.06%  |  1.16%  |  1.18%  |  1.18%  |  1.20%  |  1.24%  |  1.24%  |  1.18%  |  1.13%  |  1.06%  |  1.05%  |  1.00%  |  0.92%  |  0.85%  |  0.82%  |  0.82%  |  0.82%  |  0.78%  |  0.74%  |  0.74%  |  0.74%  |  0.82%  |  0.82%  |  0.82%  |  0.88%  |  0.93%  |  0.93%  |  0.97%  |  0.87%  |  0.84%  |  0.83%  |  0.74%  |  0.66%  |  0.62%  |  0.57%  |  0.51%  |  0.45%  |  0.42%  |  0.41%  |  0.39%  |  0.38%  |  0.37%  |  0.37%  |  0.35%  |  0.37%  |  0.35%  |  0.34%  |  0.33%  |  0.30%  |  0.29%  |  0.29%  |  0.25%  |  0.25%  |  0.22%  |  0.21%  |  0.20%  |  0.18%  |  0.16%  |  0.16%  |  0.14%  |  0.51%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |       42.58%  |  0.34%  |  0.32%  |  0.32%  |  0.29%  |  0.28%  |  0.28%  |  0.26%  |  0.25%  |  0.25%  |  0.25%  |  0.25%  |  0.24%  |  0.22%  |  0.24%  |  0.22%  |  0.24%  |  0.26%  |  0.30%  |  0.37%  |  0.42%  |  0.50%  |  0.64%  |  0.76%  |  0.80%  |  0.91%  |  0.99%  |  1.08%  |  1.09%  |  1.10%  |  1.06%  |  1.05%  |  1.00%  |  0.96%  |  0.91%  |  0.84%  |  0.75%  |  0.70%  |  0.63%  |  0.58%  |  0.57%  |  0.54%  |  0.55%  |  0.54%  |  0.57%  |  0.57%  |  0.58%  |  0.54%  |  0.59%  |  0.59%  |  0.62%  |  0.64%  |  0.66%  |  0.67%  |  0.67%  |  0.62%  |  0.59%  |  0.54%  |  0.49%  |  0.46%  |  0.42%  |  0.39%  |  0.38%  |  0.37%  |  0.35%  |  0.37%  |  0.38%  |  0.37%  |  0.37%  |  0.37%  |  0.37%  |  0.35%  |  0.33%  |  0.33%  |  0.32%  |  0.29%  |  0.28%  |  0.25%  |  0.26%  |  0.25%  |  0.22%  |  0.21%  |  0.20%  |  0.20%  |  0.20%  |  0.17%  |  1.06%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |\n (2001,City of London)  |  2001  |  City of London  |       7359.0  |     52.37%  |  0.22%  |  0.39%  |  0.33%  |  0.25%  |  0.28%  |  0.33%  |  0.26%  |  0.21%  |  0.25%  |  0.35%  |  0.34%  |  0.21%  |  0.38%  |  0.25%  |  0.14%  |  0.18%  |  0.16%  |  0.17%  |  0.16%  |  0.35%  |  0.50%  |  0.51%  |  0.70%  |  0.83%  |  1.05%  |  1.45%  |  1.45%  |  1.31%  |  1.42%  |  1.64%  |  1.50%  |  1.25%  |  1.22%  |  1.38%  |  1.30%  |  1.10%  |  1.17%  |  0.93%  |  0.83%  |  0.99%  |  0.85%  |  0.68%  |  0.62%  |  1.00%  |  0.96%  |  0.71%  |  0.64%  |  0.59%  |  0.79%  |  0.64%  |  1.46%  |  0.96%  |  0.72%  |  1.06%  |  1.05%  |  0.80%  |  0.83%  |  0.74%  |  0.84%  |  0.63%  |  0.37%  |  0.45%  |  0.59%  |  0.47%  |  0.33%  |  0.37%  |  0.28%  |  0.37%  |  0.28%  |  0.35%  |  0.35%  |  0.28%  |  0.32%  |  0.38%  |  0.28%  |  0.25%  |  0.30%  |  0.26%  |  0.14%  |  0.25%  |  0.20%  |  0.18%  |  0.11%  |  0.20%  |  0.13%  |  0.08%  |  0.13%  |  0.07%  |  0.05%  |  0.04%  |     13  |       44.37%  |  0.42%  |  0.39%  |  0.30%  |  0.28%  |  0.34%  |  0.21%  |  0.28%  |  0.20%  |  0.24%  |  0.32%  |  0.26%  |  0.26%  |  0.13%  |  0.34%  |  0.17%  |  0.17%  |  0.18%  |  0.51%  |  0.37%  |  0.46%  |  0.62%  |  0.51%  |  0.64%  |  0.71%  |  0.97%  |  1.22%  |  1.17%  |  1.05%  |  1.10%  |  1.41%  |  1.12%  |  1.09%  |  1.33%  |  0.87%  |  0.79%  |  0.71%  |  0.87%  |  0.79%  |  0.63%  |  0.59%  |  0.49%  |  0.37%  |  0.41%  |  0.83%  |  0.57%  |  0.70%  |  0.66%  |  0.62%  |  0.46%  |  0.55%  |  0.49%  |  0.85%  |  0.63%  |  0.75%  |  0.75%  |  0.50%  |  0.59%  |  0.50%  |  0.50%  |  0.43%  |  0.42%  |  0.42%  |  0.34%  |  0.33%  |  0.32%  |  0.38%  |  0.37%  |  0.35%  |  0.45%  |  0.35%  |  0.26%  |  0.34%  |  0.32%  |  0.41%  |  0.24%  |  0.32%  |  0.35%  |  0.17%  |  0.17%  |  0.20%  |  0.24%  |  0.29%  |  0.24%  |  0.14%  |  0.13%  |  0.13%  |  0.18%  |  0.34%  |  0.21%  |  0.09%  |     15  |\n (2002,City of London)  |  2002  |  City of London  |       7280.0  |     52.16%  |  0.38%  |  0.24%  |  0.37%  |  0.35%  |  0.26%  |  0.26%  |  0.32%  |  0.26%  |  0.22%  |  0.26%  |  0.33%  |  0.33%  |  0.18%  |  0.35%  |  0.12%  |  0.07%  |  0.11%  |  0.13%  |  0.16%  |  0.13%  |  0.26%  |  0.51%  |  0.66%  |  0.80%  |  0.89%  |  1.30%  |  1.49%  |  1.60%  |  1.34%  |  1.50%  |  1.50%  |  1.41%  |  1.30%  |  1.17%  |  1.33%  |  1.22%  |  1.10%  |  1.01%  |  0.96%  |  0.80%  |  1.03%  |  0.85%  |  0.75%  |  0.66%  |  0.99%  |  0.95%  |  0.74%  |  0.66%  |  0.59%  |  0.80%  |  0.60%  |  1.49%  |  0.92%  |  0.64%  |  1.12%  |  1.00%  |  0.78%  |  0.80%  |  0.72%  |  0.79%  |  0.59%  |  0.37%  |  0.41%  |  0.60%  |  0.53%  |  0.30%  |  0.34%  |  0.26%  |  0.33%  |  0.28%  |  0.32%  |  0.39%  |  0.28%  |  0.33%  |  0.38%  |  0.26%  |  0.24%  |  0.28%  |  0.22%  |  0.14%  |  0.22%  |  0.18%  |  0.17%  |  0.11%  |  0.20%  |  0.12%  |  0.08%  |  0.11%  |  0.05%  |  0.03%  |     14  |       43.54%  |  0.37%  |  0.46%  |  0.39%  |  0.25%  |  0.28%  |  0.34%  |  0.20%  |  0.26%  |  0.17%  |  0.24%  |  0.22%  |  0.20%  |  0.22%  |  0.08%  |  0.25%  |  0.17%  |  0.07%  |  0.20%  |  0.46%  |  0.38%  |  0.46%  |  0.63%  |  0.68%  |  0.62%  |  0.88%  |  0.97%  |  1.10%  |  1.10%  |  1.09%  |  0.92%  |  1.51%  |  1.10%  |  1.24%  |  1.33%  |  0.83%  |  0.80%  |  0.75%  |  0.76%  |  0.82%  |  0.58%  |  0.62%  |  0.49%  |  0.37%  |  0.43%  |  0.79%  |  0.47%  |  0.62%  |  0.68%  |  0.59%  |  0.47%  |  0.50%  |  0.54%  |  0.83%  |  0.67%  |  0.71%  |  0.72%  |  0.49%  |  0.54%  |  0.53%  |  0.47%  |  0.39%  |  0.38%  |  0.39%  |  0.35%  |  0.30%  |  0.30%  |  0.37%  |  0.37%  |  0.33%  |  0.45%  |  0.33%  |  0.25%  |  0.32%  |  0.30%  |  0.41%  |  0.25%  |  0.33%  |  0.34%  |  0.13%  |  0.16%  |  0.18%  |  0.22%  |  0.29%  |  0.20%  |  0.12%  |  0.13%  |  0.09%  |  0.14%  |  0.34%  |  0.18%  |     20  |\n (2003,City of London)  |  2003  |  City of London  |       7115.0  |     51.16%  |  0.46%  |  0.32%  |  0.21%  |  0.29%  |  0.34%  |  0.22%  |  0.25%  |  0.32%  |  0.28%  |  0.28%  |  0.32%  |  0.33%  |  0.28%  |  0.13%  |  0.21%  |  0.13%  |  0.05%  |  0.07%  |  0.16%  |  0.16%  |  0.14%  |  0.25%  |  0.64%  |  0.79%  |  0.87%  |  1.05%  |  1.31%  |  1.55%  |  1.50%  |  1.38%  |  1.42%  |  1.38%  |  1.30%  |  1.24%  |  1.12%  |  1.28%  |  1.17%  |  1.08%  |  1.00%  |  0.91%  |  0.83%  |  0.97%  |  0.84%  |  0.75%  |  0.67%  |  0.99%  |  0.97%  |  0.74%  |  0.67%  |  0.51%  |  0.84%  |  0.58%  |  1.42%  |  0.85%  |  0.62%  |  1.05%  |  0.97%  |  0.74%  |  0.80%  |  0.72%  |  0.72%  |  0.58%  |  0.34%  |  0.39%  |  0.55%  |  0.47%  |  0.30%  |  0.32%  |  0.29%  |  0.29%  |  0.24%  |  0.30%  |  0.41%  |  0.24%  |  0.32%  |  0.35%  |  0.25%  |  0.22%  |  0.29%  |  0.20%  |  0.13%  |  0.21%  |  0.16%  |  0.13%  |  0.11%  |  0.18%  |  0.11%  |  0.08%  |  0.09%  |  0.03%  |     13  |       42.37%  |  0.24%  |  0.33%  |  0.41%  |  0.38%  |  0.24%  |  0.28%  |  0.34%  |  0.18%  |  0.25%  |  0.16%  |  0.25%  |  0.17%  |  0.14%  |  0.16%  |  0.03%  |  0.21%  |  0.09%  |  0.16%  |  0.29%  |  0.41%  |  0.45%  |  0.49%  |  0.68%  |  0.80%  |  0.76%  |  1.12%  |  0.96%  |  1.13%  |  1.08%  |  1.12%  |  0.99%  |  1.30%  |  1.12%  |  1.17%  |  1.21%  |  0.79%  |  0.76%  |  0.59%  |  0.67%  |  0.70%  |  0.55%  |  0.62%  |  0.58%  |  0.38%  |  0.46%  |  0.78%  |  0.50%  |  0.54%  |  0.64%  |  0.51%  |  0.46%  |  0.53%  |  0.49%  |  0.80%  |  0.64%  |  0.70%  |  0.66%  |  0.46%  |  0.53%  |  0.45%  |  0.47%  |  0.39%  |  0.38%  |  0.39%  |  0.35%  |  0.32%  |  0.30%  |  0.37%  |  0.39%  |  0.33%  |  0.42%  |  0.32%  |  0.22%  |  0.30%  |  0.28%  |  0.39%  |  0.21%  |  0.30%  |  0.32%  |  0.13%  |  0.14%  |  0.18%  |  0.20%  |  0.21%  |  0.21%  |  0.12%  |  0.03%  |  0.07%  |  0.13%  |  0.29%  |     26  |\n (2004,City of London)  |  2004  |  City of London  |       7118.0  |     50.94%  |  0.37%  |  0.35%  |  0.33%  |  0.21%  |  0.26%  |  0.35%  |  0.22%  |  0.26%  |  0.41%  |  0.24%  |  0.29%  |  0.28%  |  0.29%  |  0.21%  |  0.05%  |  0.17%  |  0.11%  |  0.00%  |  0.03%  |  0.17%  |  0.12%  |  0.33%  |  0.34%  |  0.83%  |  0.92%  |  0.91%  |  1.17%  |  1.47%  |  1.51%  |  1.52%  |  1.31%  |  1.22%  |  1.34%  |  1.22%  |  1.13%  |  1.04%  |  1.30%  |  1.21%  |  1.16%  |  1.06%  |  0.96%  |  0.78%  |  0.85%  |  0.88%  |  0.76%  |  0.64%  |  1.00%  |  0.92%  |  0.72%  |  0.66%  |  0.58%  |  0.80%  |  0.59%  |  1.30%  |  0.85%  |  0.60%  |  1.01%  |  1.00%  |  0.72%  |  0.75%  |  0.70%  |  0.75%  |  0.57%  |  0.37%  |  0.42%  |  0.51%  |  0.45%  |  0.29%  |  0.33%  |  0.28%  |  0.29%  |  0.24%  |  0.26%  |  0.39%  |  0.21%  |  0.32%  |  0.37%  |  0.25%  |  0.18%  |  0.26%  |  0.12%  |  0.16%  |  0.18%  |  0.13%  |  0.11%  |  0.11%  |  0.20%  |  0.11%  |  0.07%  |  0.07%  |     12  |       42.63%  |  0.41%  |  0.24%  |  0.26%  |  0.34%  |  0.33%  |  0.26%  |  0.33%  |  0.37%  |  0.18%  |  0.25%  |  0.09%  |  0.25%  |  0.13%  |  0.12%  |  0.12%  |  0.01%  |  0.17%  |  0.22%  |  0.12%  |  0.29%  |  0.50%  |  0.54%  |  0.62%  |  0.78%  |  1.04%  |  1.05%  |  1.20%  |  1.13%  |  1.04%  |  1.06%  |  1.04%  |  1.03%  |  1.20%  |  1.05%  |  1.14%  |  1.09%  |  0.74%  |  0.80%  |  0.62%  |  0.63%  |  0.72%  |  0.53%  |  0.59%  |  0.51%  |  0.39%  |  0.45%  |  0.78%  |  0.49%  |  0.51%  |  0.74%  |  0.50%  |  0.43%  |  0.50%  |  0.47%  |  0.79%  |  0.67%  |  0.68%  |  0.67%  |  0.45%  |  0.53%  |  0.46%  |  0.45%  |  0.37%  |  0.42%  |  0.37%  |  0.35%  |  0.33%  |  0.26%  |  0.38%  |  0.37%  |  0.32%  |  0.39%  |  0.32%  |  0.20%  |  0.28%  |  0.29%  |  0.38%  |  0.20%  |  0.30%  |  0.29%  |  0.16%  |  0.14%  |  0.16%  |  0.14%  |  0.16%  |  0.16%  |  0.09%  |  0.01%  |  0.04%  |  0.12%  |     42  |\n (2005,City of London)  |  2005  |  City of London  |       7131.0  |     50.86%  |  0.45%  |  0.33%  |  0.29%  |  0.30%  |  0.16%  |  0.25%  |  0.29%  |  0.25%  |  0.25%  |  0.41%  |  0.26%  |  0.28%  |  0.26%  |  0.25%  |  0.05%  |  0.04%  |  0.16%  |  0.13%  |  0.00%  |  0.09%  |  0.12%  |  0.21%  |  0.57%  |  0.53%  |  0.95%  |  1.22%  |  0.97%  |  1.17%  |  1.54%  |  1.49%  |  1.37%  |  1.16%  |  1.21%  |  1.43%  |  1.29%  |  1.01%  |  1.00%  |  1.35%  |  1.17%  |  1.03%  |  1.05%  |  0.89%  |  0.80%  |  0.91%  |  0.82%  |  0.76%  |  0.67%  |  1.08%  |  0.88%  |  0.76%  |  0.68%  |  0.55%  |  0.78%  |  0.66%  |  1.17%  |  0.88%  |  0.62%  |  0.96%  |  0.92%  |  0.72%  |  0.76%  |  0.66%  |  0.70%  |  0.55%  |  0.35%  |  0.39%  |  0.55%  |  0.41%  |  0.30%  |  0.32%  |  0.26%  |  0.22%  |  0.22%  |  0.25%  |  0.39%  |  0.20%  |  0.28%  |  0.35%  |  0.22%  |  0.18%  |  0.28%  |  0.11%  |  0.18%  |  0.16%  |  0.09%  |  0.09%  |  0.08%  |  0.18%  |  0.09%  |  0.03%  |     12  |       42.88%  |  0.38%  |  0.38%  |  0.20%  |  0.20%  |  0.30%  |  0.33%  |  0.22%  |  0.30%  |  0.33%  |  0.18%  |  0.24%  |  0.09%  |  0.26%  |  0.09%  |  0.09%  |  0.08%  |  0.03%  |  0.09%  |  0.25%  |  0.20%  |  0.41%  |  0.57%  |  0.64%  |  0.66%  |  1.08%  |  1.00%  |  1.17%  |  1.17%  |  1.12%  |  1.00%  |  1.18%  |  1.00%  |  1.12%  |  1.20%  |  0.97%  |  1.05%  |  0.97%  |  0.79%  |  0.72%  |  0.68%  |  0.59%  |  0.68%  |  0.57%  |  0.62%  |  0.50%  |  0.45%  |  0.45%  |  0.72%  |  0.43%  |  0.50%  |  0.75%  |  0.50%  |  0.47%  |  0.51%  |  0.49%  |  0.76%  |  0.68%  |  0.72%  |  0.66%  |  0.45%  |  0.49%  |  0.46%  |  0.49%  |  0.34%  |  0.43%  |  0.39%  |  0.33%  |  0.32%  |  0.24%  |  0.35%  |  0.34%  |  0.26%  |  0.37%  |  0.29%  |  0.24%  |  0.28%  |  0.32%  |  0.34%  |  0.21%  |  0.28%  |  0.29%  |  0.13%  |  0.16%  |  0.14%  |  0.13%  |  0.16%  |  0.12%  |  0.03%  |  0.03%  |  0.04%  |     50  |\n (2006,City of London)  |  2006  |  City of London  |       7254.0  |     52.04%  |  0.34%  |  0.33%  |  0.34%  |  0.26%  |  0.24%  |  0.17%  |  0.21%  |  0.30%  |  0.29%  |  0.24%  |  0.39%  |  0.24%  |  0.26%  |  0.22%  |  0.09%  |  0.07%  |  0.07%  |  0.20%  |  0.16%  |  0.00%  |  0.16%  |  0.25%  |  0.42%  |  0.83%  |  0.83%  |  1.08%  |  1.43%  |  1.09%  |  1.30%  |  1.63%  |  1.50%  |  1.35%  |  1.09%  |  1.12%  |  1.52%  |  1.29%  |  1.01%  |  0.97%  |  1.41%  |  1.14%  |  0.99%  |  1.08%  |  0.89%  |  0.83%  |  0.91%  |  0.78%  |  0.82%  |  0.70%  |  1.09%  |  0.87%  |  0.79%  |  0.68%  |  0.63%  |  0.83%  |  0.63%  |  1.12%  |  0.88%  |  0.62%  |  0.89%  |  0.82%  |  0.74%  |  0.74%  |  0.63%  |  0.71%  |  0.53%  |  0.37%  |  0.34%  |  0.54%  |  0.41%  |  0.29%  |  0.30%  |  0.26%  |  0.18%  |  0.21%  |  0.25%  |  0.38%  |  0.22%  |  0.26%  |  0.37%  |  0.22%  |  0.18%  |  0.25%  |  0.08%  |  0.13%  |  0.16%  |  0.07%  |  0.05%  |  0.08%  |  0.16%  |  0.09%  |     13  |       43.32%  |  0.37%  |  0.34%  |  0.41%  |  0.16%  |  0.22%  |  0.33%  |  0.30%  |  0.26%  |  0.29%  |  0.33%  |  0.18%  |  0.24%  |  0.07%  |  0.25%  |  0.05%  |  0.09%  |  0.05%  |  0.16%  |  0.09%  |  0.35%  |  0.32%  |  0.43%  |  0.67%  |  0.76%  |  0.93%  |  1.16%  |  1.05%  |  1.13%  |  1.13%  |  1.21%  |  1.10%  |  1.13%  |  1.03%  |  1.10%  |  1.05%  |  1.01%  |  0.97%  |  0.89%  |  0.82%  |  0.59%  |  0.70%  |  0.59%  |  0.67%  |  0.59%  |  0.60%  |  0.51%  |  0.50%  |  0.43%  |  0.70%  |  0.41%  |  0.49%  |  0.75%  |  0.51%  |  0.47%  |  0.50%  |  0.47%  |  0.74%  |  0.66%  |  0.70%  |  0.62%  |  0.39%  |  0.54%  |  0.41%  |  0.45%  |  0.32%  |  0.39%  |  0.38%  |  0.30%  |  0.34%  |  0.22%  |  0.35%  |  0.34%  |  0.26%  |  0.34%  |  0.32%  |  0.24%  |  0.26%  |  0.29%  |  0.35%  |  0.22%  |  0.26%  |  0.25%  |  0.14%  |  0.14%  |  0.13%  |  0.12%  |  0.14%  |  0.12%  |  0.03%  |  0.00%  |     45  |\n (2007,City of London)  |  2007  |  City of London  |       7607.0  |     55.17%  |  0.41%  |  0.33%  |  0.32%  |  0.29%  |  0.30%  |  0.21%  |  0.17%  |  0.18%  |  0.35%  |  0.30%  |  0.20%  |  0.43%  |  0.21%  |  0.25%  |  0.09%  |  0.09%  |  0.04%  |  0.12%  |  0.21%  |  0.35%  |  0.00%  |  0.39%  |  0.46%  |  0.84%  |  1.20%  |  1.18%  |  1.49%  |  1.62%  |  1.30%  |  1.41%  |  1.62%  |  1.34%  |  1.39%  |  1.10%  |  1.14%  |  1.50%  |  1.37%  |  1.13%  |  1.09%  |  1.54%  |  1.16%  |  1.04%  |  0.97%  |  0.92%  |  0.99%  |  0.97%  |  0.83%  |  0.84%  |  0.75%  |  1.05%  |  0.85%  |  0.82%  |  0.72%  |  0.67%  |  0.84%  |  0.70%  |  1.03%  |  0.84%  |  0.63%  |  0.83%  |  0.78%  |  0.71%  |  0.76%  |  0.60%  |  0.71%  |  0.47%  |  0.34%  |  0.39%  |  0.51%  |  0.39%  |  0.29%  |  0.30%  |  0.22%  |  0.21%  |  0.21%  |  0.25%  |  0.35%  |  0.20%  |  0.20%  |  0.38%  |  0.18%  |  0.18%  |  0.24%  |  0.08%  |  0.08%  |  0.13%  |  0.05%  |  0.05%  |  0.08%  |  0.12%  |     20  |       44.83%  |  0.32%  |  0.32%  |  0.39%  |  0.33%  |  0.22%  |  0.22%  |  0.22%  |  0.28%  |  0.21%  |  0.28%  |  0.33%  |  0.20%  |  0.24%  |  0.05%  |  0.25%  |  0.04%  |  0.14%  |  0.32%  |  0.20%  |  0.32%  |  0.51%  |  0.45%  |  0.74%  |  0.92%  |  0.99%  |  1.20%  |  1.31%  |  1.12%  |  1.33%  |  1.09%  |  1.05%  |  1.00%  |  1.05%  |  0.95%  |  0.95%  |  0.95%  |  1.00%  |  0.96%  |  0.82%  |  0.85%  |  0.62%  |  0.74%  |  0.63%  |  0.72%  |  0.66%  |  0.64%  |  0.57%  |  0.47%  |  0.39%  |  0.63%  |  0.38%  |  0.50%  |  0.71%  |  0.54%  |  0.51%  |  0.51%  |  0.51%  |  0.74%  |  0.68%  |  0.70%  |  0.60%  |  0.35%  |  0.54%  |  0.46%  |  0.42%  |  0.35%  |  0.39%  |  0.42%  |  0.29%  |  0.32%  |  0.21%  |  0.35%  |  0.34%  |  0.24%  |  0.33%  |  0.30%  |  0.22%  |  0.24%  |  0.29%  |  0.30%  |  0.22%  |  0.22%  |  0.25%  |  0.14%  |  0.13%  |  0.13%  |  0.09%  |  0.12%  |  0.11%  |  0.01%  |     36  |\n (2008,City of London)  |  2008  |  City of London  |       7429.0  |     54.31%  |  0.38%  |  0.34%  |  0.29%  |  0.32%  |  0.24%  |  0.29%  |  0.17%  |  0.16%  |  0.22%  |  0.39%  |  0.26%  |  0.20%  |  0.39%  |  0.22%  |  0.13%  |  0.11%  |  0.08%  |  0.09%  |  0.14%  |  0.24%  |  0.37%  |  0.14%  |  0.57%  |  0.51%  |  0.99%  |  1.31%  |  1.12%  |  1.62%  |  1.60%  |  1.34%  |  1.22%  |  1.46%  |  1.26%  |  1.33%  |  0.99%  |  1.05%  |  1.35%  |  1.21%  |  1.09%  |  1.04%  |  1.59%  |  1.13%  |  1.04%  |  0.91%  |  0.92%  |  0.87%  |  1.01%  |  0.75%  |  0.88%  |  0.88%  |  1.06%  |  0.84%  |  0.80%  |  0.67%  |  0.68%  |  0.80%  |  0.72%  |  0.96%  |  0.80%  |  0.60%  |  0.79%  |  0.80%  |  0.70%  |  0.72%  |  0.55%  |  0.70%  |  0.45%  |  0.34%  |  0.38%  |  0.51%  |  0.38%  |  0.32%  |  0.29%  |  0.22%  |  0.20%  |  0.18%  |  0.25%  |  0.32%  |  0.18%  |  0.21%  |  0.35%  |  0.17%  |  0.16%  |  0.22%  |  0.08%  |  0.08%  |  0.09%  |  0.05%  |  0.04%  |  0.07%  |     25  |       43.35%  |  0.16%  |  0.22%  |  0.30%  |  0.30%  |  0.32%  |  0.21%  |  0.22%  |  0.21%  |  0.25%  |  0.20%  |  0.28%  |  0.32%  |  0.20%  |  0.25%  |  0.01%  |  0.25%  |  0.00%  |  0.29%  |  0.34%  |  0.33%  |  0.46%  |  0.57%  |  0.46%  |  0.75%  |  0.96%  |  1.08%  |  1.17%  |  1.25%  |  1.16%  |  1.24%  |  1.00%  |  1.01%  |  0.89%  |  0.91%  |  0.75%  |  0.79%  |  0.80%  |  0.84%  |  0.89%  |  0.75%  |  0.83%  |  0.54%  |  0.68%  |  0.60%  |  0.68%  |  0.66%  |  0.66%  |  0.49%  |  0.46%  |  0.42%  |  0.55%  |  0.38%  |  0.50%  |  0.70%  |  0.53%  |  0.55%  |  0.55%  |  0.50%  |  0.68%  |  0.67%  |  0.67%  |  0.63%  |  0.35%  |  0.54%  |  0.45%  |  0.38%  |  0.39%  |  0.41%  |  0.38%  |  0.30%  |  0.29%  |  0.21%  |  0.37%  |  0.35%  |  0.26%  |  0.32%  |  0.29%  |  0.21%  |  0.20%  |  0.29%  |  0.32%  |  0.20%  |  0.22%  |  0.24%  |  0.13%  |  0.12%  |  0.12%  |  0.09%  |  0.12%  |  0.11%  |     30  |",
            "title": "Weights"
        },
        {
            "location": "/frame/reshaping/",
            "text": "Reshaping\n\n\nIt is often useful to be able to expand a \nDataFrame\n in either the row or column dimension, or both, in order to store\ncalculated data or other state as part of some analysis. For example, a frame could be used to capture real-time in-memory \nperformance metrics or other high frequency observations for expanding window statistical analysis, requiring new rows to be \nadded as data is collected. The following sections provide some demonstrations of how to use the API to add rows & columns, \nand discusses some of the caveats one needs to be aware of.\n\n\nAdding Rows\n\n\nSingular\n\n\nThe \nDataFrameRows\n interface provides a number of methods for \nefficiently\n adding rows to an existing frame in place \n(that is, modifying the existing frame as opposed to creating an entirely new instance). This is very much analogous to the \nway that Java collections like \nList\n, \nSet\n and \nMap\n can be expanded in place.  Consider as a starting point a 4x5 frame \nof doubles initialized with random values as follows.\n\n\n\n\n\n//Create frame of Random doubles keyed by LocalDate and String\nDataFrame<LocalDate,String> frame = DataFrame.ofDoubles(\n    Range.ofLocalDates(\"2014-01-01\", \"2014-01-05\"),\n    Range.of(0, 5).map(i -> \"Column-\" + i),\n    value -> Math.random() * 10d\n);\n\n\n\n\n\n   Index     |  Column-0  |  Column-1  |  Column-2  |  Column-3  |  Column-4  |\n-------------------------------------------------------------------------------\n 2014-01-01  |     7.378  |     5.395  |     8.915  |     7.491  |     7.747  |\n 2014-01-02  |     1.829  |     3.493  |     8.910  |     7.978  |     1.154  |\n 2014-01-03  |     1.416  |     8.203  |     1.520  |     8.569  |     8.612  |\n 2014-01-04  |     9.619  |     7.539  |     7.204  |     5.950  |     0.532  |\n\n\n\n\nBelow we add a single row with a \nLocalDate\n key that does not exist in the row axis. If the key did already exist, the operation \nwould be a no-op, however this behaviour can be controlled via \nDataFrameOptions\n (see section below on duplicate handling). Since\nall the columns of this frame are of type \ndouble\n, and we have not provided any initial values for the row, they all initialize\nas \nDouble.NaN\n.\n\n\n\n\n\n//Add one row, ignore if the key is a duplicate\nframe.rows().add(LocalDate.of(2014, 1, 6));\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});\n\n\n\n\n\n   Index     |  Column-0  |  Column-1  |  Column-2  |  Column-3  |  Column-4  |\n-------------------------------------------------------------------------------\n 2014-01-01  |     7.378  |     5.395  |     8.915  |     7.491  |     7.747  |\n 2014-01-02  |     1.829  |     3.493  |     8.910  |     7.978  |     1.154  |\n 2014-01-03  |     1.416  |     8.203  |     1.520  |     8.569  |     8.612  |\n 2014-01-04  |     9.619  |     7.539  |     7.204  |     5.950  |     0.532  |\n 2014-01-06  |       NaN  |       NaN  |       NaN  |       NaN  |       NaN  |\n\n\n\n\nIt is possible to initialize values for a newly added row by providing a lambda function that consumes a \nDataFrameValue\n\nas shown below. In this case, we add another row with values initialized to \n2.0\n. A more involved initialization function \nis easy to imagine of course.\n\n\n\n\n\n//Add another row, inital values set to 2\nframe.rows().add(LocalDate.of(2014, 1, 7), v -> 2d);\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});\n\n\n\n\n\n   Index     |  Column-0  |  Column-1  |  Column-2  |  Column-3  |  Column-4  |\n-------------------------------------------------------------------------------\n 2014-01-01  |     7.378  |     5.395  |     8.915  |     7.491  |     7.747  |\n 2014-01-02  |     1.829  |     3.493  |     8.910  |     7.978  |     1.154  |\n 2014-01-03  |     1.416  |     8.203  |     1.520  |     8.569  |     8.612  |\n 2014-01-04  |     9.619  |     7.539  |     7.204  |     5.950  |     0.532  |\n 2014-01-06  |       NaN  |       NaN  |       NaN  |       NaN  |       NaN  |\n 2014-01-07  |     2.000  |     2.000  |     2.000  |     2.000  |     2.000  |\n\n\n\n\nMultiple\n\n\nAdding multiple rows is also supported via overloaded \naddAll()\n methods, which expects an \nIterable\n of keys as an argument. \nOnce again, any duplicates will be ignored unless this behaviour is disabled via \nDataFrameOptions\n (see duplicate handling \nbelow). Continuing with the frame from the previous section, the code below adds additional rows based on a \nRange\n of \nLocalDate\n \nobjects, which works becase \nRange\n implements \nIterable\n (note that a range's start value is inclusive, and the end value is \nexclusive).\n\n\n\n\n\nfinal Range<LocalDate> range1 = Range.ofLocalDates(\"2014-02-08\", \"2014-02-10\");\nframe.rows().addAll(range1);\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});\n\n\n\n\n\n   Index     |  Column-0  |  Column-1  |  Column-2  |  Column-3  |  Column-4  |\n-------------------------------------------------------------------------------\n 2014-01-01  |     7.378  |     5.395  |     8.915  |     7.491  |     7.747  |\n 2014-01-02  |     1.829  |     3.493  |     8.910  |     7.978  |     1.154  |\n 2014-01-03  |     1.416  |     8.203  |     1.520  |     8.569  |     8.612  |\n 2014-01-04  |     9.619  |     7.539  |     7.204  |     5.950  |     0.532  |\n 2014-01-06  |       NaN  |       NaN  |       NaN  |       NaN  |       NaN  |\n 2014-01-07  |     2.000  |     2.000  |     2.000  |     2.000  |     2.000  |\n 2014-02-08  |       NaN  |       NaN  |       NaN  |       NaN  |       NaN  |\n 2014-02-09  |       NaN  |       NaN  |       NaN  |       NaN  |       NaN  |\n\n\n\n\nFinally, an initializing lambda function can be provided to set the values for newly added rows. In the example below,\nwe add rows based on another date \nRange\n and set initial values to 3.\n\n\n\n\n\n//Add multiple rows, with initial set to 3\nfinal Range<LocalDate> range2 = Range.ofLocalDates(\"2014-02-10\", \"2014-02-12\");\nframe.rows().addAll(range2, v -> 3d);\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});\n\n\n\n\n\n   Index     |  Column-0  |  Column-1  |  Column-2  |  Column-3  |  Column-4  |\n-------------------------------------------------------------------------------\n 2014-01-01  |     7.378  |     5.395  |     8.915  |     7.491  |     7.747  |\n 2014-01-02  |     1.829  |     3.493  |     8.910  |     7.978  |     1.154  |\n 2014-01-03  |     1.416  |     8.203  |     1.520  |     8.569  |     8.612  |\n 2014-01-04  |     9.619  |     7.539  |     7.204  |     5.950  |     0.532  |\n 2014-01-06  |       NaN  |       NaN  |       NaN  |       NaN  |       NaN  |\n 2014-01-07  |     2.000  |     2.000  |     2.000  |     2.000  |     2.000  |\n 2014-02-08  |       NaN  |       NaN  |       NaN  |       NaN  |       NaN  |\n 2014-02-09  |       NaN  |       NaN  |       NaN  |       NaN  |       NaN  |\n 2014-02-10  |     3.000  |     3.000  |     3.000  |     3.000  |     3.000  |\n 2014-02-11  |     3.000  |     3.000  |     3.000  |     3.000  |     3.000  |\n\n\n\n\nFrame\n\n\nThe scenarios discussed in the previous sections illustrate how rows can be added by specifying one or more keys, with an optional \nfunction to set the initial values for the newly added rows. It is often convenient to be able to add all the rows from one frame \nto another, in place (i.e. without creating a third frame). In these situations, data from the added frame is copied across based \non \nintersecting columns\n between the two frames. Consider the following example where we construct two frames that have both \nintersecting row and column keys.\n\n\n\n\n\n//Create a 5x5 DataFrame of doubles initialized with 1 for all values\nDataFrame<Integer,String> frame1 = DataFrame.ofDoubles(\n    Array.of(0, 1, 2, 3, 4),\n    Array.of(\"A\", \"B\", \"C\", \"D\", \"E\"),\n    value -> 1d\n);\n\n\n\n\n\n Index  |   A   |   B   |   C   |   D   |   E   |\n-------------------------------------------------\n     0  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     1  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     2  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     3  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     4  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n\n\n\n\n\n\n\n//Create a 7x5 DataFrame of doubles initialized with 2 for all values\nDataFrame<Integer,String> frame2 = DataFrame.ofDoubles(\n    Array.of(3, 4, 5, 6, 7, 8, 9),\n    Array.of(\"C\", \"D\", \"E\", \"F\", \"G\"),\n    value -> 2d\n);\n\n\n\n\n\n Index  |   C   |   D   |   E   |   F   |   G   |\n-------------------------------------------------\n     3  |  2.0  |  2.0  |  2.0  |  2.0  |  2.0  |\n     4  |  2.0  |  2.0  |  2.0  |  2.0  |  2.0  |\n     5  |  2.0  |  2.0  |  2.0  |  2.0  |  2.0  |\n     6  |  2.0  |  2.0  |  2.0  |  2.0  |  2.0  |\n     7  |  2.0  |  2.0  |  2.0  |  2.0  |  2.0  |\n     8  |  2.0  |  2.0  |  2.0  |  2.0  |  2.0  |\n     9  |  2.0  |  2.0  |  2.0  |  2.0  |  2.0  |\n\n\n\n\nThese two frames have intersecting row keys, namely \n3\n and \n4\n, and intersecting column keys, namely \nC\n, \nD\n and \nE\n. We\ncan add all rows from \nframe2\n to \nframe1\n as per the code below, and since by default duplicates are ignored, the rows \n3\n and \n\n4\n from \nframe2\n will not be added. In addition, only data for intersecting columns will be copied resulting in a 10x5 frame as \nshown below.\n\n\n\n\n\n//Add all rows from frame2 to frame1 and print to std out\nframe1.rows().addAll(frame2);\nframe1.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.0;-0.0\", 1);\n});\n\n\n\n\n\n Index  |   A   |   B   |   C   |   D   |   E   |\n-------------------------------------------------\n     0  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     1  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     2  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     3  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     4  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     5  |  NaN  |  NaN  |  2.0  |  2.0  |  2.0  |\n     6  |  NaN  |  NaN  |  2.0  |  2.0  |  2.0  |\n     7  |  NaN  |  NaN  |  2.0  |  2.0  |  2.0  |\n     8  |  NaN  |  NaN  |  2.0  |  2.0  |  2.0  |\n     9  |  NaN  |  NaN  |  2.0  |  2.0  |  2.0  |\n\n\n\n\nConcatenation\n\n\nWhen adding rows from one \nDataFrame\n to another as presented in the previous section, the data types for the intersecting \ncolumns must be compatible, otherwise a \nDataFrameException\n will be raised. In the prior example, both frames were made up \nof double precision columns, so this was not an issue. If the second frame, for argument sake, consisted of columns of type \n\nString\n the \naddAll()\n method would have failed when an attempt was made to copy the String data from the second frame to\nthe first.\n\n\nThere are situations when it is necessary to combine the rows of two frames that have \nincompatible\n data types, or when it is\npreferable to create a third frame with the combined data instead of mutating a frame in place. This is where row concatenation\ncomes in. If we consider a similar example to the previous section, but in this case initialize the second frame with \nString\n\nvalues, we can still combine the two frames using \nDataFrame.concatRows()\n.\n\n\nIn this scenario we initialize the second frame with \nString\n values as follows:\n\n\n\n\n\n//Create a 7x5 DataFrame of doubles initialized with coordinate string of the form (i,j)\nDataFrame<Integer,String> frame2 = DataFrame.ofObjects(\n    Array.of(3, 4, 5, 6, 7, 8, 9),\n    Array.of(\"C\", \"D\", \"E\", \"F\", \"G\"),\n    value -> String.format(\"(%s, %s)\", value.rowOrdinal(), value.colOrdinal())\n);\n\n\n\n\n\n Index  |    C     |    D     |    E     |    F     |    G     |\n----------------------------------------------------------------\n     3  |  (0, 0)  |  (0, 1)  |  (0, 2)  |  (0, 3)  |  (0, 4)  |\n     4  |  (1, 0)  |  (1, 1)  |  (1, 2)  |  (1, 3)  |  (1, 4)  |\n     5  |  (2, 0)  |  (2, 1)  |  (2, 2)  |  (2, 3)  |  (2, 4)  |\n     6  |  (3, 0)  |  (3, 1)  |  (3, 2)  |  (3, 3)  |  (3, 4)  |\n     7  |  (4, 0)  |  (4, 1)  |  (4, 2)  |  (4, 3)  |  (4, 4)  |\n     8  |  (5, 0)  |  (5, 1)  |  (5, 2)  |  (5, 3)  |  (5, 4)  |\n     9  |  (6, 0)  |  (6, 1)  |  (6, 2)  |  (6, 3)  |  (6, 4)  |\n\n\n\n\nTo combine the rows of this frame with the first frame of doubles created earlier, we can create a third frame with column data types \nthat support the combined data across all the frames in question. This technique can be used to concatenate the rows of any number of \nframes. The code below uses the \nDataFrame.concatRows()\n static method and prints the result to standard out.\n\n\n\n\n\n//Concatenate rows from frame1 and frame2\nDataFrame<Integer,String> frame3 = DataFrame.concatRows(frame1, frame2);\nframe3.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.0;-0.0\", 1);\n});\n\n\n\n\n\n Index  |   A   |   B   |    C     |    D     |    E     |\n----------------------------------------------------------\n     0  |  1.0  |  1.0  |     1.0  |     1.0  |     1.0  |\n     1  |  1.0  |  1.0  |     1.0  |     1.0  |     1.0  |\n     2  |  1.0  |  1.0  |     1.0  |     1.0  |     1.0  |\n     3  |  1.0  |  1.0  |     1.0  |     1.0  |     1.0  |\n     4  |  1.0  |  1.0  |     1.0  |     1.0  |     1.0  |\n     5  |  NaN  |  NaN  |  (2, 0)  |  (2, 1)  |  (2, 2)  |\n     6  |  NaN  |  NaN  |  (3, 0)  |  (3, 1)  |  (3, 2)  |\n     7  |  NaN  |  NaN  |  (4, 0)  |  (4, 1)  |  (4, 2)  |\n     8  |  NaN  |  NaN  |  (5, 0)  |  (5, 1)  |  (5, 2)  |\n     9  |  NaN  |  NaN  |  (6, 0)  |  (6, 1)  |  (6, 2)  |\n\n\n\n\nThe resulting \nDataFrame\n includes the 5 columns of the first frame passed to \nconcatRows()\n, however unlike the first\nframe, columns \nC\n, \nD\n and \nE\n are now of type \nObject\n rather than of type \ndouble\n because these columns now hold both\ndouble precision data (rows 0 through 4) and String data (rows 4 through 9).\n\n\nAdding Columns\n\n\nConsider the a 10x2 \nDataFrame\n of random double precision values as a starting point to illustrate the various\ncolumn expansion calls described in the following sections. The row axis is indexed by \nLocalDate\n values and the\ncolumn axis by \nString\n values.\n\n\n\n\n\nLocalDate start = LocalDate.of(2014, 1, 1);\nDataFrame<LocalDate,String> frame = DataFrame.ofDoubles(\n    Range.of(start, start.plusDays(10)),\n    Array.of(\"A\", \"B\"),\n    value -> Math.random() * 10d\n);\n\n\n\n\n\n   Index     |   A    |   B    |\n--------------------------------\n 2014-01-01  |  8.72  |  9.19  |\n 2014-01-02  |  6.42  |  3.42  |\n 2014-01-03  |  1.75  |  1.52  |\n 2014-01-04  |  1.15  |  0.94  |\n 2014-01-05  |  7.80  |  2.10  |\n 2014-01-06  |  9.23  |  0.41  |\n 2014-01-07  |  3.26  |  0.04  |\n 2014-01-08  |  1.84  |  5.10  |\n 2014-01-09  |  2.87  |  4.82  |\n 2014-01-10  |  6.61  |  7.15  |\n\n\n\n\nSingular\n\n\nThe \nDataFrameColumns\n interface exposes various \nadd()\n and \naddAll()\n methods, but they differ slightly to those on \nDataFrameRows\n.\nEach column in a frame is associated with a specific data type, so when adding additional columns the type needs to be either explicitly \nspecified, or implicitly defined through the data structure being added. Column data will ultimately end up being represented as a \nMorpehus \nArray\n within the frame, but it is possible to pass data as an \nIterable\n thereby supporting many input types.\n\n\nThe code below demonstrates 5 distinct ways of adding columns to the 10x2 \nDataFrame\n illustrated above, using the various \ntechniques that are supported for single column adds. In some cases we simply pass the desired data type for the column, while\nin other cases we pass an \nIterable\n in the form of a \nRange\n or \nArray\n object. The final state of the frame is shown below.\n\n\n\n\n\n//Add single column of Booleans, no initials\nframe.cols().add(\"C\", Boolean.class);\n//Add single column of type String with coordinate labels\nframe.cols().add(\"D\", String.class, v -> String.format(\"(%s,%s)\", v.rowOrdinal(), v.colOrdinal()));\n//Add single column given an Iterable of values\nframe.cols().add(\"E\", Range.of(0, frame.rowCount()));\n//Add single column of type LocalDate, with initializing function\nframe.cols().add(\"F\", LocalDate.class, v -> start.plusDays(v.rowOrdinal() * 2));\n//Add single column with explicit value array\nframe.cols().add(\"G\", Array.of(10d, 20d, 30d, 40d));\n//Print first 10 rows to std out\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});\n\n\n\n\n\n   Index     |   A    |   B    |    C    |    D    |  E  |      F       |    G    |\n-----------------------------------------------------------------------------------\n 2014-01-01  |  4.09  |  2.23  |  false  |  (0,3)  |  0  |  2014-01-01  |  10.00  |\n 2014-01-02  |  4.49  |  4.08  |  false  |  (1,3)  |  1  |  2014-01-03  |  20.00  |\n 2014-01-03  |  6.77  |  1.94  |  false  |  (2,3)  |  2  |  2014-01-05  |  30.00  |\n 2014-01-04  |  1.81  |  4.48  |  false  |  (3,3)  |  3  |  2014-01-07  |  40.00  |\n 2014-01-05  |  4.56  |  5.70  |  false  |  (4,3)  |  4  |  2014-01-09  |    NaN  |\n 2014-01-06  |  8.56  |  2.96  |  false  |  (5,3)  |  5  |  2014-01-11  |    NaN  |\n 2014-01-07  |  7.71  |  7.07  |  false  |  (6,3)  |  6  |  2014-01-13  |    NaN  |\n 2014-01-08  |  4.70  |  3.07  |  false  |  (7,3)  |  7  |  2014-01-15  |    NaN  |\n 2014-01-09  |  9.78  |  4.28  |  false  |  (8,3)  |  8  |  2014-01-17  |    NaN  |\n 2014-01-10  |  8.32  |  4.62  |  false  |  (9,3)  |  9  |  2014-01-19  |    NaN  |\n\n\n\n\nMultiple\n\n\nAdding multiple columns in one call is also supported as described in this section. The code below creates a similar 10x2 \nframe of random double precision values as a starting point, and then adds three columns labelled \nA\n, \nB\n and \nC\n of type \n\nString\n, and two additional columns presenting the column data directory via a \nRange\n and \nArray\n object. One thing to note\nabout this example is that the data structures for \nF\n and \nG\n are shorter than the row count of the original frame, so\nthey get extended as required with values initializing to the defaults for the type in question. If the data structures are\nlonger than the row count, they are effectively truncated to fit the frame.\n\n\n\n\n\nLocalDate start = LocalDate.of(2014, 1, 1);\nDataFrame<LocalDate,String> frame = DataFrame.ofDoubles(\n    Range.of(start, start.plusDays(10)),\n    Array.of(\"A\", \"B\"),\n    value -> Math.random() * 10d\n);\n//Add multiple columns to hold String data\nframe.cols().addAll(Array.of(\"C\", \"D\", \"E\"), String.class);\n//Add multiple columns via consumer\nframe.cols().addAll(columns -> {\n    columns.put(\"F\", Array.of(10d, 11d, 12d, 13d, 14d));\n    columns.put(\"G\", Range.of(start.plusDays(3), start.plusDays(10)));\n});\n//Print first 10 rows to std out\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});\n\n\n\n\n\n   Index     |   A    |   B    |   C    |   D    |   E    |    F    |      G       |\n------------------------------------------------------------------------------------\n 2014-01-01  |  2.65  |  3.49  |  null  |  null  |  null  |  10.00  |  2014-01-04  |\n 2014-01-02  |  4.80  |  2.07  |  null  |  null  |  null  |  11.00  |  2014-01-05  |\n 2014-01-03  |  5.81  |  2.70  |  null  |  null  |  null  |  12.00  |  2014-01-06  |\n 2014-01-04  |  8.46  |  7.00  |  null  |  null  |  null  |  13.00  |  2014-01-07  |\n 2014-01-05  |  4.65  |  4.72  |  null  |  null  |  null  |  14.00  |  2014-01-08  |\n 2014-01-06  |  3.39  |  3.95  |  null  |  null  |  null  |    NaN  |  2014-01-09  |\n 2014-01-07  |  3.18  |  5.29  |  null  |  null  |  null  |    NaN  |  2014-01-10  |\n 2014-01-08  |  1.22  |  7.64  |  null  |  null  |  null  |    NaN  |        null  |\n 2014-01-09  |  8.32  |  9.39  |  null  |  null  |  null  |    NaN  |        null  |\n 2014-01-10  |  0.66  |  3.38  |  null  |  null  |  null  |    NaN  |        null  |\n\n\n\n\nFrame\n\n\nSimilar to adding rows from one frame to another, it is also possible to add columns from one frame to another. Only columns \nthat are missing from the first frame are added by default, unless duplicate handling is changed via \nDataFrameOptions\n. In \naddition, only data from intersecting rows will be copied. Consider a 9x2 frame initialized with random double precision values\nas follows:\n\n\n\n\n\n//Create a 9x2 DataFrame of random double precision values\nDataFrame<Integer,String> frame1 = DataFrame.ofDoubles(\n    Range.of(0, 9, 1),\n    Array.of(\"A\", \"B\"),\n    value -> Math.random()\n);\n\n\n\n\n\n Index  |    A    |    B    |\n-----------------------------\n     0  |  0.015  |  0.278  |\n     1  |  0.577  |  0.229  |\n     2  |  0.837  |  0.273  |\n     3  |  0.400  |  0.059  |\n     4  |  0.407  |  0.696  |\n     5  |  0.318  |  0.657  |\n     6  |  0.047  |  0.682  |\n     7  |  0.344  |  0.365  |\n     8  |  0.633  |  0.476  |\n\n\n\n\nA second 6x5 frame is initialized with intersecting rows for keys \n0\n, \n2\n, \n4\n, \n6\n and \n8\n, and one intersecting column, \nnamely for label \nB\n. The columns for this frame include various data types, namely of \nInteger\n. \nDouble\n, \nString\n and\n\nBoolean\n, and all values are initialized by directly presenting a data structure representing the column values.\n\n\n\n\n\n//Create 6x5 frame with intersecting rows and columns to the first frame\nDataFrame<Integer,String> frame2 = DataFrame.of(Range.of(0, 12, 2), String.class, columns -> {\n    columns.add(\"B\", Array.of(10, 20, 30, 40, 50, 60));\n    columns.add(\"C\", Array.of(1d, 3d, 5d, 7d, 9d, 11d));\n    columns.add(\"D\", Range.of(1, 7));\n    columns.add(\"E\", String.class, v -> String.format(\"(%s,%s)\", v.rowOrdinal(), v.colOrdinal()));\n    columns.add(\"F\", Boolean.class, v -> Math.random() > 0.5d);\n});\n\n\n\n\n\n Index  |  B   |    C     |  D  |    E    |    F    |\n-----------------------------------------------------\n     0  |  10  |   1.000  |  1  |  (0,3)  |   true  |\n     2  |  20  |   3.000  |  2  |  (1,3)  |   true  |\n     4  |  30  |   5.000  |  3  |  (2,3)  |   true  |\n     6  |  40  |   7.000  |  4  |  (3,3)  |  false  |\n     8  |  50  |   9.000  |  5  |  (4,3)  |  false  |\n    10  |  60  |  11.000  |  6  |  (5,3)  |  false  |\n\n\n\n\nUsing the \naddAll()\n method on the \nDataFrameColumns\n interface we can add columns from the second 6x5 frame to the original \n9x2 frame. Since duplicates are ignored by default, we expect that column \nB\n from the first frame will be retained and not \nget over written by the values from the second frame. In addition, only data for intersecting rows will get transferred. The\ncode to combine these frames and the 9x6 result is shown below. \n\n\n\n\n\n//Add all columns from second frame to first frame, copy data from intersecting rows\nframe1.cols().addAll(frame2);\n//Print frame to standard out with custom formatting\nframe1.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});\n\n\n\n\n\n Index  |    A    |    B    |    C    |  D  |    E    |    F    |\n-----------------------------------------------------------------\n     0  |  0.015  |  0.278  |  1.000  |  1  |  (0,3)  |   true  |\n     1  |  0.577  |  0.229  |    NaN  |  0  |   null  |  false  |\n     2  |  0.837  |  0.273  |  3.000  |  2  |  (1,3)  |   true  |\n     3  |  0.400  |  0.059  |    NaN  |  0  |   null  |  false  |\n     4  |  0.407  |  0.696  |  5.000  |  3  |  (2,3)  |   true  |\n     5  |  0.318  |  0.657  |    NaN  |  0  |   null  |  false  |\n     6  |  0.047  |  0.682  |  7.000  |  4  |  (3,3)  |  false  |\n     7  |  0.344  |  0.365  |    NaN  |  0  |   null  |  false  |\n     8  |  0.633  |  0.476  |  9.000  |  5  |  (4,3)  |  false  |\n\n\n\n\nConcatenation\n\n\nThe above examples demonstrate adding columns to an existing frame which mutates the frame in place. At times it is preferable\nhowever to create a new frame which combines the data of two or more input frames. To do this, a static \nconcatColumns()\n method \non the \nDataFrame\n interface exists which cam combine any number of frames. Again, duplicate columns across the input frames are \nignored with the first frame being the winner, and only data from intersecting rows to the first frame are captured.\n\n\nConsider 3 frames initialized as follows:\n\n\n\n\n\n//Create a 9x2 DataFrame of random double precision values\nDataFrame<Integer,String> frame1 = DataFrame.ofDoubles(\n    Range.of(0, 9),\n    Array.of(\"A\", \"B\"),\n    value -> Math.random()\n);\n\n\n\n\n\n Index  |    A    |    B    |\n-----------------------------\n     0  |  0.027  |  0.526  |\n     1  |  0.643  |  0.154  |\n     2  |  0.149  |  0.110  |\n     3  |  0.670  |  0.085  |\n     4  |  0.939  |  0.779  |\n     5  |  0.106  |  0.524  |\n     6  |  0.665  |  0.842  |\n     7  |  0.221  |  0.662  |\n     8  |  0.506  |  0.484  |\n\n\n\n\n\n\n\n//Create 6x5 frame with intersecting rows and columns to the first frame\nDataFrame<Integer,String> frame2 = DataFrame.of(Range.of(0, 12, 2), String.class, columns -> {\n    columns.add(\"B\", Array.of(10, 20, 30, 40, 50, 60));\n    columns.add(\"C\", Array.of(1d, 3d, 5d, 7d, 9d, 11d));\n    columns.add(\"D\", Range.of(1, 7));\n    columns.add(\"E\", String.class, v -> String.format(\"(%s,%s)\", v.rowOrdinal(), v.colOrdinal()));\n    columns.add(\"F\", Boolean.class, v -> Math.random() > 0.5d);\n});\n\n\n\n\n\n Index  |  B   |    C     |  D  |    E    |    F    |\n-----------------------------------------------------\n     0  |  10  |   1.000  |  1  |  (0,3)  |  false  |\n     2  |  20  |   3.000  |  2  |  (1,3)  |   true  |\n     4  |  30  |   5.000  |  3  |  (2,3)  |  false  |\n     6  |  40  |   7.000  |  4  |  (3,3)  |   true  |\n     8  |  50  |   9.000  |  5  |  (4,3)  |   true  |\n    10  |  60  |  11.000  |  6  |  (5,3)  |   true  |\n\n\n\n\n\n\n\n//Create a 9x4 DataFrame of random double precision values\nDataFrame<Integer,String> frame3 = DataFrame.ofDoubles(\n    Range.of(0, 5),\n    Array.of(\"B\", \"F\", \"G\", \"H\"),\n    value -> Math.random()\n);\n\n\n\n\n\n Index  |    B    |    F    |    G    |    H    |\n-------------------------------------------------\n     0  |  0.899  |  0.959  |  0.182  |  0.960  |\n     1  |  0.138  |  0.941  |  0.111  |  0.124  |\n     2  |  0.582  |  0.739  |  0.446  |  0.868  |\n     3  |  0.049  |  0.594  |  0.133  |  0.602  |\n     4  |  0.592  |  0.738  |  0.832  |  0.380  |\n\n\n\n\nNow we use \nconcatColumns()\n to create a new frame combining the columns of all 3, ignoring duplicates. The second frame's\ncolumn \nB\n will be ignored in favour of the first frame, and the third frame's column \nB\n and \nF\n will be ignored in favour\nof the first and second frame respectively. Only data from rows intersecting with the first frame's row axis will be included\nin the combined frame.\n\n\n\n\n\n//Concatenate columns from all 3 frames to create a new result\nDataFrame<Integer,String> frame4 = DataFrame.concatColumns(frame1, frame2, frame3);\n//Print frame to standard out with custom formatting\nframe4.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});\n\n\n\n\n\n Index  |    A    |    B    |    C    |  D  |    E    |    F    |    G    |    H    |\n-------------------------------------------------------------------------------------\n     0  |  0.027  |  0.526  |  1.000  |  1  |  (0,3)  |  false  |  0.182  |  0.960  |\n     1  |  0.643  |  0.154  |    NaN  |  0  |   null  |  false  |  0.111  |  0.124  |\n     2  |  0.149  |  0.110  |  3.000  |  2  |  (1,3)  |   true  |  0.446  |  0.868  |\n     3  |  0.670  |  0.085  |    NaN  |  0  |   null  |  false  |  0.133  |  0.602  |\n     4  |  0.939  |  0.779  |  5.000  |  3  |  (2,3)  |  false  |  0.832  |  0.380  |\n     5  |  0.106  |  0.524  |    NaN  |  0  |   null  |  false  |    NaN  |    NaN  |\n     6  |  0.665  |  0.842  |  7.000  |  4  |  (3,3)  |   true  |    NaN  |    NaN  |\n     7  |  0.221  |  0.662  |    NaN  |  0  |   null  |  false  |    NaN  |    NaN  |\n     8  |  0.506  |  0.484  |  9.000  |  5  |  (4,3)  |   true  |    NaN  |    NaN  |\n\n\n\n\nDataFrame Union\n\n\nRow and column concatenation only copies data based on columns and rows respectively that intersect with the entries in the \nfirst frame. At times a full union of N frames is desirable, also taking into account the fact that the combined columns may \nnot share the same data type. In order to support this, a static method called \nunion()\n on the \nDataFrame\n interface is \nprovided for this purpose.\n\n\nConsider a 5x2, 6x5 and 6x4 frame with intersecting rows and columns initialized as follows. \n\n\n\n\n\n//Create a 5x2 DataFrame of random double precision values\nDataFrame<Integer,String> frame1 = DataFrame.ofDoubles(\n    Range.of(0, 5),\n    Array.of(\"A\", \"B\"),\n    value -> Math.random()\n);\n\n\n\n\n\n Index  |    A    |    B    |\n-----------------------------\n     0  |  0.968  |  0.013  |\n     1  |  0.105  |  0.593  |\n     2  |  0.012  |  0.312  |\n     3  |  0.489  |  0.015  |\n     4  |  0.304  |  0.269  |\n\n\n\n\n\n\n\n//Create 6x5 frame with intersecting rows and columns to the first frame\nDataFrame<Integer,String> frame2 = DataFrame.of(Range.of(0, 12, 2), String.class, columns -> {\n    columns.add(\"B\", Array.of(10, 20, 30, 40, 50, 60));\n    columns.add(\"C\", Array.of(1d, 3d, 5d, 7d, 9d, 11d));\n    columns.add(\"D\", Range.of(1, 7));\n    columns.add(\"E\", String.class, v -> String.format(\"(%s,%s)\", v.rowOrdinal(), v.colOrdinal()));\n    columns.add(\"F\", Boolean.class, v -> Math.random() > 0.5d);\n});\n\n\n\n\n\n Index  |  B   |    C     |  D  |    E    |    F    |\n-----------------------------------------------------\n     0  |  10  |   1.000  |  1  |  (0,3)  |  false  |\n     2  |  20  |   3.000  |  2  |  (1,3)  |  false  |\n     4  |  30  |   5.000  |  3  |  (2,3)  |   true  |\n     6  |  40  |   7.000  |  4  |  (3,3)  |   true  |\n     8  |  50  |   9.000  |  5  |  (4,3)  |  false  |\n    10  |  60  |  11.000  |  6  |  (5,3)  |   true  |\n\n\n\n\n\n\n\n//Create a 6x4 DataFrame of random double precision values\nDataFrame<Integer,String> frame3 = DataFrame.ofDoubles(\n    Range.of(0, 6),\n    Array.of(\"B\", \"F\", \"G\", \"H\"),\n    value -> Math.random()\n);\n\n\n\n\n\n Index  |    B    |    F    |    G    |    H    |\n-------------------------------------------------\n     0  |  0.924  |  0.975  |  0.751  |  0.764  |\n     1  |  0.441  |  0.160  |  0.626  |  0.477  |\n     2  |  0.777  |  0.549  |  0.568  |  0.209  |\n     3  |  0.148  |  0.178  |  0.320  |  0.500  |\n     4  |  0.880  |  0.351  |  0.025  |  0.998  |\n     5  |  0.780  |  0.839  |  0.479  |  0.607  |\n\n\n\n\nThe union of these 3 frames would be a 9x8 structure, as duplicates would be handled on a combine first basis. Note\nthat column \nB\n in the first and third frame is of type \ndouble\n and in the second frame it's of type \nint\n. The union\nframe will have to represent the combined column \nB\n as type \nObject\n for this to work.\n\n\nThe code to create the union frame and the resulting structure are shown below:\n\n\n\n\n\n//Create the union of all 3 frames which should yield an 9x8 frame\nDataFrame<Integer,String> frame4 =  DataFrame.union(frame1, frame2, frame3);\n//Print frame to standard out with custom formatting\nframe4.rows().sort(true);\nframe4.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});\n\n\n\n\n\n Index  |    A    |    B    |    C     |  D  |    E    |    F    |    G    |    H    |\n--------------------------------------------------------------------------------------\n     0  |  0.968  |  0.013  |   1.000  |  1  |  (0,3)  |  false  |  0.751  |  0.764  |\n     1  |  0.105  |  0.593  |     NaN  |  0  |   null  |  false  |  0.626  |  0.477  |\n     2  |  0.012  |  0.312  |   3.000  |  2  |  (1,3)  |  false  |  0.568  |  0.209  |\n     3  |  0.489  |  0.015  |     NaN  |  0  |   null  |  false  |  0.320  |  0.500  |\n     4  |  0.304  |  0.269  |   5.000  |  3  |  (2,3)  |   true  |  0.025  |  0.998  |\n     5  |    NaN  |  0.780  |     NaN  |  0  |   null  |  false  |  0.479  |  0.607  |\n     6  |    NaN  |     40  |   7.000  |  4  |  (3,3)  |   true  |    NaN  |    NaN  |\n     8  |    NaN  |     50  |   9.000  |  5  |  (4,3)  |  false  |    NaN  |    NaN  |\n    10  |    NaN  |     60  |  11.000  |  6  |  (5,3)  |   true  |    NaN  |    NaN  |\n\n\n\n\nDuplicate Handling\n\n\nBy design, a \nDataFrame\n row and column axis does not support duplicate keys. When expanding a frame, an attempt \nto add row or column keys which are already present in the respective axis results in a no-op. In addition, \nfunctions to concatenate or create a union of two or more frames operate on a combine first basis. While this \nmay suffice for most use cases, there are scenarios where you would not expect duplicates to exist across frames, \nand would rather such an event raise an exception as opposed to operating on the combine first approach. This \nbehaviour, among others, can be achieved via the \nDataFrameOptions\n class which provides various methods to change \nthe behaviour of the currently executing thread via the use of internal \nThreadLocal\n variables.\n\n\nIf we consider the union example from the previous section, we can cause it to fail (because there are duplicates \nkeys across the input frames) by wrapping the code in a \nCallable\n to \nwhileNotIgnoringDuplicates()\n as follows.\n\n\n\n\n\nDataFrame<Integer,String> frame4 = DataFrameOptions.whileNotIgnoringDuplicates(() -> {\n    return DataFrame.union(frame1, frame2, frame3).rows().sort(true);\n});\n\n\n\n\nRunning this code results in an exception as follows:\n\n\n\n    Caused by: com.zavtech.morpheus.frame.DataFrameException: A column for key already exists in this frame: B\n        at com.zavtech.morpheus.reference.XDataFrameColumns.lambda$addAll$121(XDataFrameColumns.java:139)\n        at com.zavtech.morpheus.reference.XDataFrameAxisBase.forEach(XDataFrameAxisBase.java:281)\n        at com.zavtech.morpheus.reference.XDataFrameColumns.addAll(XDataFrameColumns.java:135)\n        at com.zavtech.morpheus.reference.XDataFrameFactory.lambda$union$8(XDataFrameFactory.java:161)\n        at java.util.Arrays$ArrayList.forEach(Arrays.java:3880)\n        at com.zavtech.morpheus.reference.XDataFrameFactory.union(XDataFrameFactory.java:159)\n        at com.zavtech.morpheus.frame.DataFrame.union(DataFrame.java:407)\n        at com.zavtech.morpheus.examples.ReshapingDocs.lambda$union$49(ReshapingDocs.java:304)\n        at com.zavtech.morpheus.frame.DataFrameOptions.whileNotIgnoringDuplicates(DataFrameOptions.java:219)\n\n\n\n\nRe-Labelling\n\n\nA \nDataFrame\n row and column axis contains keys of a certain data type, and it is often useful to either replace\none or more keys or to switch out an axis entirely. While this does not in effect change the shape of the frame in\nterms of the row and column count, it does change its constitution. This section demonstrates how the API can be\nused to replace one or more keys in place, or to generate a shallow copy of a frame with an entirely new axis.\n\n\nConsider a 5x5 frame keyed by \nInteger\n rows and \nString\n columns initialized with random doubles as follows:\n\n\n\n\n\n//Create a 5x5 DataFrame of random doubles\nDataFrame<Integer,String> frame = DataFrame.ofDoubles(\n    Array.of(0, 1, 2, 3, 4),\n    Array.of(\"A\", \"B\", \"C\", \"D\", \"E\"),\n    value -> Math.random() * 10d\n);\n\n\n\n\n\n Index  |    A    |    B    |    C    |    D    |    E    |\n-----------------------------------------------------------\n     0  |  2.995  |  9.439  |  2.817  |  0.973  |  5.424  |\n     1  |  9.350  |  5.138  |  8.626  |  0.368  |  7.935  |\n     2  |  5.407  |  2.157  |  4.207  |  8.540  |  4.385  |\n     3  |  8.410  |  6.536  |  8.796  |  8.220  |  7.743  |\n     4  |  0.248  |  6.475  |  2.560  |  8.391  |  5.810  |\n\n\n\n\nThe \nDataFraneAxis\n interface exposes a \nreplaceKey()\n method that can be used to change any number of keys in place. \nThe only two caveats are that the replacement key must not already exist in the axis, as duplicates are not supported,\nand the replacment key must be of the same data type. If the replacement key does already exist, expect a \nDataFrameException\n \nto be raised. The code below demonstrates how to replace a single key in both the row oe column axis, and prints the result. \n\n\n\n\n\nframe.rows().replaceKey(4, 40);\nframe.cols().replaceKey(\"C\", \"X\");\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});\n\n\n\n\n\n Index  |    A    |    B    |    X    |    D    |    E    |\n-----------------------------------------------------------\n     0  |  2.995  |  9.439  |  2.817  |  0.973  |  5.424  |\n     1  |  9.350  |  5.138  |  8.626  |  0.368  |  7.935  |\n     2  |  5.407  |  2.157  |  4.207  |  8.540  |  4.385  |\n     3  |  8.410  |  6.536  |  8.796  |  8.220  |  7.743  |\n    40  |  0.248  |  6.475  |  2.560  |  8.391  |  5.810  |\n\n\n\n\nWhile the above technique is useful to replace specific keys, it is often necessary to replace an axis entirely, and \npossibly with a different key type. Consider the 10x5 frame of random double precision values below which is keyed by \n\nLocalDate\n on the row axis, and \nString\n on the column axis.\n\n\n\n\n\n//Create a 10x4 DataFrame of random doubles\nDataFrame<LocalDate,String> frame = DataFrame.ofDoubles(\n    Range.ofLocalDates(\"2014-01-01\", \"2014-01-11\"),\n    Array.of(\"A\", \"B\", \"C\", \"D\"),\n    value -> Math.random() * 10d\n);\n\n\n\n\n\n   Index     |    A    |    B    |    C    |    D    |\n------------------------------------------------------\n 2014-01-01  |  1.725  |  7.107  |  1.979  |  5.899  |\n 2014-01-02  |  1.232  |  5.779  |  6.550  |  7.534  |\n 2014-01-03  |  9.706  |  7.316  |  1.277  |  1.122  |\n 2014-01-04  |  2.382  |  9.778  |  6.011  |  1.200  |\n 2014-01-05  |  3.800  |  7.071  |  2.743  |  8.742  |\n 2014-01-06  |  5.157  |  1.242  |  3.834  |  0.047  |\n 2014-01-07  |  4.474  |  0.511  |  5.953  |  5.698  |\n 2014-01-08  |  4.784  |  6.298  |  5.771  |  3.975  |\n 2014-01-09  |  9.525  |  7.301  |  8.257  |  3.144  |\n 2014-01-10  |  6.245  |  9.614  |  8.318  |  0.056  |\n\n\n\n\nLet us assume that the data for this hypothetical experient needs to be date shifted forward by 5 calendar days,\nand the row axis \nLocalDate\n objects need to be converted to \nLocalDateTime\n objects with a time set to 13:30. One \nway to achieve this would be to create an entirely new frame, and copy the data across, although this would not be \nparticularly efficient, especially if the frame was extremely large. A better way is to use the \nmapKeys()\n method\non \nDataFeameAxis\n and provide a mapping function that does the conversion as shown below.\n\n\n\n\n\nDataFrame<LocalDateTime,String> shifted = frame.rows().mapKeys(row -> {\n    final LocalDate rowKey = row.key().plusDays(5);\n    return LocalDateTime.of(rowKey, LocalTime.of(13, 30));\n});\n\n\n\n\n\n      Index        |    A    |    B    |    C    |    D    |\n------------------------------------------------------------\n 2014-01-06T13:30  |  1.725  |  7.107  |  1.979  |  5.899  |\n 2014-01-07T13:30  |  1.232  |  5.779  |  6.550  |  7.534  |\n 2014-01-08T13:30  |  9.706  |  7.316  |  1.277  |  1.122  |\n 2014-01-09T13:30  |  2.382  |  9.778  |  6.011  |  1.200  |\n 2014-01-10T13:30  |  3.800  |  7.071  |  2.743  |  8.742  |\n 2014-01-11T13:30  |  5.157  |  1.242  |  3.834  |  0.047  |\n 2014-01-12T13:30  |  4.474  |  0.511  |  5.953  |  5.698  |\n 2014-01-13T13:30  |  4.784  |  6.298  |  5.771  |  3.975  |\n 2014-01-14T13:30  |  9.525  |  7.301  |  8.257  |  3.144  |\n 2014-01-15T13:30  |  6.245  |  9.614  |  8.318  |  0.056  |\n\n\n\n\nUnlike the \nreplaceKey()\n method, \nmapKeys()\n takes a lambda function to generate an entirely new axis, and returns\na new \nDataFrame\n initialized with the replacement axis. The data in the shifted frame is referentially the same as the \noriginal frame, so the cost of this operation is limited to the cost of creating the new axis. We can confirm this by \nsetting all the values in the shifted frame to zero, and then confirming the original frame has also been zero'd out.\n\n\n\n\n\n//Zero out shifted frame values, and print original frame\nshifted.applyDoubles(v -> 0d);\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});\n\n\n\n\n\n   Index     |    A    |    B    |    C    |    D    |\n------------------------------------------------------\n 2014-01-01  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-02  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-03  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-04  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-05  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-06  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-07  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-08  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-09  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-10  |  0.000  |  0.000  |  0.000  |  0.000  |\n\n\n\n\nTranspose\n\n\nWhen a \nDataFrame\n is transposed, it changes from being a column store data structure to row store, making the tranpose\noperation itself, even for very large frames, very cheap. One drawback of this design choice however is that a transposed \nframe cannot be reshaped by adding rows and/or columns.  Attempting to re-shape a transposed \nDataFrame\n will result in a \n\nDataFrameException\n as the following example demonstrates.\n\n\n\n\n\n//Create a 5x5 DataFrame of random doubles\nDataFrame<Integer,String> frame = DataFrame.ofDoubles(\n    Array.of(0, 1, 2, 3, 4),\n    Array.of(\"A\", \"B\", \"C\", \"D\", \"E\"),\n    value -> Math.random() * 10d\n);\n\n//Transpose and try add a row...\nframe.transpose().rows().add(\"F\");\n\n\n\n\n\ncom.zavtech.morpheus.frame.DataFrameException: Cannot add rows to a transposed DataFrame, call transpose() and then add columns\n    at com.zavtech.morpheus.reference.XDataFrameContent.addRow(XDataFrameContent.java:280)\n    at com.zavtech.morpheus.reference.XDataFrameRows.add(XDataFrameRows.java:77)\n    at com.zavtech.morpheus.reference.XDataFrameRows.add(XDataFrameRows.java:64)\n    at com.zavtech.morpheus.examples.ReshapingDocs.transpose(ReshapingDocs.java:403)\n\n\n\n\nThe workaround to this limitation is to call \ntranspose()\n on the row major version of the frame, which yields the un-tranposed \noriginal that can be re-shaped by adding rows and/or columns. Consider the same 5x5 frame created above, which we transpose,\nand then add a new column to the transposed frame by calling \naddRow()\n as shown below.\n\n\n\n\n\n//Tranpose the transposed frame to get back to a colum store which can be re-shaped\nDataFrame<String,Integer> transposed = frame.transpose();\ntransposed.transpose().rows().add(5);\ntransposed.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});\n\n\n\n\n\n Index  |   0    |   1    |   2    |   3    |   4    |   5   |\n--------------------------------------------------------------\n     A  |  4.23  |  6.74  |  3.33  |  7.19  |  6.87  |  NaN  |\n     B  |  4.61  |  7.58  |  5.55  |  9.78  |  7.32  |  NaN  |\n     C  |  0.54  |  3.68  |  6.18  |  4.06  |  2.88  |  NaN  |\n     D  |  9.48  |  9.36  |  4.10  |  8.89  |  3.68  |  NaN  |\n     E  |  1.44  |  1.64  |  3.81  |  7.75  |  6.25  |  NaN  |",
            "title": "Reshaping"
        },
        {
            "location": "/frame/reshaping/#reshaping",
            "text": "It is often useful to be able to expand a  DataFrame  in either the row or column dimension, or both, in order to store\ncalculated data or other state as part of some analysis. For example, a frame could be used to capture real-time in-memory \nperformance metrics or other high frequency observations for expanding window statistical analysis, requiring new rows to be \nadded as data is collected. The following sections provide some demonstrations of how to use the API to add rows & columns, \nand discusses some of the caveats one needs to be aware of.",
            "title": "Reshaping"
        },
        {
            "location": "/frame/reshaping/#adding-rows",
            "text": "",
            "title": "Adding Rows"
        },
        {
            "location": "/frame/reshaping/#singular",
            "text": "The  DataFrameRows  interface provides a number of methods for  efficiently  adding rows to an existing frame in place \n(that is, modifying the existing frame as opposed to creating an entirely new instance). This is very much analogous to the \nway that Java collections like  List ,  Set  and  Map  can be expanded in place.  Consider as a starting point a 4x5 frame \nof doubles initialized with random values as follows.   //Create frame of Random doubles keyed by LocalDate and String\nDataFrame<LocalDate,String> frame = DataFrame.ofDoubles(\n    Range.ofLocalDates(\"2014-01-01\", \"2014-01-05\"),\n    Range.of(0, 5).map(i -> \"Column-\" + i),\n    value -> Math.random() * 10d\n);  \n   Index     |  Column-0  |  Column-1  |  Column-2  |  Column-3  |  Column-4  |\n-------------------------------------------------------------------------------\n 2014-01-01  |     7.378  |     5.395  |     8.915  |     7.491  |     7.747  |\n 2014-01-02  |     1.829  |     3.493  |     8.910  |     7.978  |     1.154  |\n 2014-01-03  |     1.416  |     8.203  |     1.520  |     8.569  |     8.612  |\n 2014-01-04  |     9.619  |     7.539  |     7.204  |     5.950  |     0.532  |  Below we add a single row with a  LocalDate  key that does not exist in the row axis. If the key did already exist, the operation \nwould be a no-op, however this behaviour can be controlled via  DataFrameOptions  (see section below on duplicate handling). Since\nall the columns of this frame are of type  double , and we have not provided any initial values for the row, they all initialize\nas  Double.NaN .   //Add one row, ignore if the key is a duplicate\nframe.rows().add(LocalDate.of(2014, 1, 6));\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});  \n   Index     |  Column-0  |  Column-1  |  Column-2  |  Column-3  |  Column-4  |\n-------------------------------------------------------------------------------\n 2014-01-01  |     7.378  |     5.395  |     8.915  |     7.491  |     7.747  |\n 2014-01-02  |     1.829  |     3.493  |     8.910  |     7.978  |     1.154  |\n 2014-01-03  |     1.416  |     8.203  |     1.520  |     8.569  |     8.612  |\n 2014-01-04  |     9.619  |     7.539  |     7.204  |     5.950  |     0.532  |\n 2014-01-06  |       NaN  |       NaN  |       NaN  |       NaN  |       NaN  |  It is possible to initialize values for a newly added row by providing a lambda function that consumes a  DataFrameValue \nas shown below. In this case, we add another row with values initialized to  2.0 . A more involved initialization function \nis easy to imagine of course.   //Add another row, inital values set to 2\nframe.rows().add(LocalDate.of(2014, 1, 7), v -> 2d);\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});  \n   Index     |  Column-0  |  Column-1  |  Column-2  |  Column-3  |  Column-4  |\n-------------------------------------------------------------------------------\n 2014-01-01  |     7.378  |     5.395  |     8.915  |     7.491  |     7.747  |\n 2014-01-02  |     1.829  |     3.493  |     8.910  |     7.978  |     1.154  |\n 2014-01-03  |     1.416  |     8.203  |     1.520  |     8.569  |     8.612  |\n 2014-01-04  |     9.619  |     7.539  |     7.204  |     5.950  |     0.532  |\n 2014-01-06  |       NaN  |       NaN  |       NaN  |       NaN  |       NaN  |\n 2014-01-07  |     2.000  |     2.000  |     2.000  |     2.000  |     2.000  |",
            "title": "Singular"
        },
        {
            "location": "/frame/reshaping/#multiple",
            "text": "Adding multiple rows is also supported via overloaded  addAll()  methods, which expects an  Iterable  of keys as an argument. \nOnce again, any duplicates will be ignored unless this behaviour is disabled via  DataFrameOptions  (see duplicate handling \nbelow). Continuing with the frame from the previous section, the code below adds additional rows based on a  Range  of  LocalDate  \nobjects, which works becase  Range  implements  Iterable  (note that a range's start value is inclusive, and the end value is \nexclusive).   final Range<LocalDate> range1 = Range.ofLocalDates(\"2014-02-08\", \"2014-02-10\");\nframe.rows().addAll(range1);\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});  \n   Index     |  Column-0  |  Column-1  |  Column-2  |  Column-3  |  Column-4  |\n-------------------------------------------------------------------------------\n 2014-01-01  |     7.378  |     5.395  |     8.915  |     7.491  |     7.747  |\n 2014-01-02  |     1.829  |     3.493  |     8.910  |     7.978  |     1.154  |\n 2014-01-03  |     1.416  |     8.203  |     1.520  |     8.569  |     8.612  |\n 2014-01-04  |     9.619  |     7.539  |     7.204  |     5.950  |     0.532  |\n 2014-01-06  |       NaN  |       NaN  |       NaN  |       NaN  |       NaN  |\n 2014-01-07  |     2.000  |     2.000  |     2.000  |     2.000  |     2.000  |\n 2014-02-08  |       NaN  |       NaN  |       NaN  |       NaN  |       NaN  |\n 2014-02-09  |       NaN  |       NaN  |       NaN  |       NaN  |       NaN  |  Finally, an initializing lambda function can be provided to set the values for newly added rows. In the example below,\nwe add rows based on another date  Range  and set initial values to 3.   //Add multiple rows, with initial set to 3\nfinal Range<LocalDate> range2 = Range.ofLocalDates(\"2014-02-10\", \"2014-02-12\");\nframe.rows().addAll(range2, v -> 3d);\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});  \n   Index     |  Column-0  |  Column-1  |  Column-2  |  Column-3  |  Column-4  |\n-------------------------------------------------------------------------------\n 2014-01-01  |     7.378  |     5.395  |     8.915  |     7.491  |     7.747  |\n 2014-01-02  |     1.829  |     3.493  |     8.910  |     7.978  |     1.154  |\n 2014-01-03  |     1.416  |     8.203  |     1.520  |     8.569  |     8.612  |\n 2014-01-04  |     9.619  |     7.539  |     7.204  |     5.950  |     0.532  |\n 2014-01-06  |       NaN  |       NaN  |       NaN  |       NaN  |       NaN  |\n 2014-01-07  |     2.000  |     2.000  |     2.000  |     2.000  |     2.000  |\n 2014-02-08  |       NaN  |       NaN  |       NaN  |       NaN  |       NaN  |\n 2014-02-09  |       NaN  |       NaN  |       NaN  |       NaN  |       NaN  |\n 2014-02-10  |     3.000  |     3.000  |     3.000  |     3.000  |     3.000  |\n 2014-02-11  |     3.000  |     3.000  |     3.000  |     3.000  |     3.000  |",
            "title": "Multiple"
        },
        {
            "location": "/frame/reshaping/#frame",
            "text": "The scenarios discussed in the previous sections illustrate how rows can be added by specifying one or more keys, with an optional \nfunction to set the initial values for the newly added rows. It is often convenient to be able to add all the rows from one frame \nto another, in place (i.e. without creating a third frame). In these situations, data from the added frame is copied across based \non  intersecting columns  between the two frames. Consider the following example where we construct two frames that have both \nintersecting row and column keys.   //Create a 5x5 DataFrame of doubles initialized with 1 for all values\nDataFrame<Integer,String> frame1 = DataFrame.ofDoubles(\n    Array.of(0, 1, 2, 3, 4),\n    Array.of(\"A\", \"B\", \"C\", \"D\", \"E\"),\n    value -> 1d\n);  \n Index  |   A   |   B   |   C   |   D   |   E   |\n-------------------------------------------------\n     0  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     1  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     2  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     3  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     4  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |   //Create a 7x5 DataFrame of doubles initialized with 2 for all values\nDataFrame<Integer,String> frame2 = DataFrame.ofDoubles(\n    Array.of(3, 4, 5, 6, 7, 8, 9),\n    Array.of(\"C\", \"D\", \"E\", \"F\", \"G\"),\n    value -> 2d\n);  \n Index  |   C   |   D   |   E   |   F   |   G   |\n-------------------------------------------------\n     3  |  2.0  |  2.0  |  2.0  |  2.0  |  2.0  |\n     4  |  2.0  |  2.0  |  2.0  |  2.0  |  2.0  |\n     5  |  2.0  |  2.0  |  2.0  |  2.0  |  2.0  |\n     6  |  2.0  |  2.0  |  2.0  |  2.0  |  2.0  |\n     7  |  2.0  |  2.0  |  2.0  |  2.0  |  2.0  |\n     8  |  2.0  |  2.0  |  2.0  |  2.0  |  2.0  |\n     9  |  2.0  |  2.0  |  2.0  |  2.0  |  2.0  |  These two frames have intersecting row keys, namely  3  and  4 , and intersecting column keys, namely  C ,  D  and  E . We\ncan add all rows from  frame2  to  frame1  as per the code below, and since by default duplicates are ignored, the rows  3  and  4  from  frame2  will not be added. In addition, only data for intersecting columns will be copied resulting in a 10x5 frame as \nshown below.   //Add all rows from frame2 to frame1 and print to std out\nframe1.rows().addAll(frame2);\nframe1.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.0;-0.0\", 1);\n});  \n Index  |   A   |   B   |   C   |   D   |   E   |\n-------------------------------------------------\n     0  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     1  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     2  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     3  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     4  |  1.0  |  1.0  |  1.0  |  1.0  |  1.0  |\n     5  |  NaN  |  NaN  |  2.0  |  2.0  |  2.0  |\n     6  |  NaN  |  NaN  |  2.0  |  2.0  |  2.0  |\n     7  |  NaN  |  NaN  |  2.0  |  2.0  |  2.0  |\n     8  |  NaN  |  NaN  |  2.0  |  2.0  |  2.0  |\n     9  |  NaN  |  NaN  |  2.0  |  2.0  |  2.0  |",
            "title": "Frame"
        },
        {
            "location": "/frame/reshaping/#concatenation",
            "text": "When adding rows from one  DataFrame  to another as presented in the previous section, the data types for the intersecting \ncolumns must be compatible, otherwise a  DataFrameException  will be raised. In the prior example, both frames were made up \nof double precision columns, so this was not an issue. If the second frame, for argument sake, consisted of columns of type  String  the  addAll()  method would have failed when an attempt was made to copy the String data from the second frame to\nthe first.  There are situations when it is necessary to combine the rows of two frames that have  incompatible  data types, or when it is\npreferable to create a third frame with the combined data instead of mutating a frame in place. This is where row concatenation\ncomes in. If we consider a similar example to the previous section, but in this case initialize the second frame with  String \nvalues, we can still combine the two frames using  DataFrame.concatRows() .  In this scenario we initialize the second frame with  String  values as follows:   //Create a 7x5 DataFrame of doubles initialized with coordinate string of the form (i,j)\nDataFrame<Integer,String> frame2 = DataFrame.ofObjects(\n    Array.of(3, 4, 5, 6, 7, 8, 9),\n    Array.of(\"C\", \"D\", \"E\", \"F\", \"G\"),\n    value -> String.format(\"(%s, %s)\", value.rowOrdinal(), value.colOrdinal())\n);  \n Index  |    C     |    D     |    E     |    F     |    G     |\n----------------------------------------------------------------\n     3  |  (0, 0)  |  (0, 1)  |  (0, 2)  |  (0, 3)  |  (0, 4)  |\n     4  |  (1, 0)  |  (1, 1)  |  (1, 2)  |  (1, 3)  |  (1, 4)  |\n     5  |  (2, 0)  |  (2, 1)  |  (2, 2)  |  (2, 3)  |  (2, 4)  |\n     6  |  (3, 0)  |  (3, 1)  |  (3, 2)  |  (3, 3)  |  (3, 4)  |\n     7  |  (4, 0)  |  (4, 1)  |  (4, 2)  |  (4, 3)  |  (4, 4)  |\n     8  |  (5, 0)  |  (5, 1)  |  (5, 2)  |  (5, 3)  |  (5, 4)  |\n     9  |  (6, 0)  |  (6, 1)  |  (6, 2)  |  (6, 3)  |  (6, 4)  |  To combine the rows of this frame with the first frame of doubles created earlier, we can create a third frame with column data types \nthat support the combined data across all the frames in question. This technique can be used to concatenate the rows of any number of \nframes. The code below uses the  DataFrame.concatRows()  static method and prints the result to standard out.   //Concatenate rows from frame1 and frame2\nDataFrame<Integer,String> frame3 = DataFrame.concatRows(frame1, frame2);\nframe3.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.0;-0.0\", 1);\n});  \n Index  |   A   |   B   |    C     |    D     |    E     |\n----------------------------------------------------------\n     0  |  1.0  |  1.0  |     1.0  |     1.0  |     1.0  |\n     1  |  1.0  |  1.0  |     1.0  |     1.0  |     1.0  |\n     2  |  1.0  |  1.0  |     1.0  |     1.0  |     1.0  |\n     3  |  1.0  |  1.0  |     1.0  |     1.0  |     1.0  |\n     4  |  1.0  |  1.0  |     1.0  |     1.0  |     1.0  |\n     5  |  NaN  |  NaN  |  (2, 0)  |  (2, 1)  |  (2, 2)  |\n     6  |  NaN  |  NaN  |  (3, 0)  |  (3, 1)  |  (3, 2)  |\n     7  |  NaN  |  NaN  |  (4, 0)  |  (4, 1)  |  (4, 2)  |\n     8  |  NaN  |  NaN  |  (5, 0)  |  (5, 1)  |  (5, 2)  |\n     9  |  NaN  |  NaN  |  (6, 0)  |  (6, 1)  |  (6, 2)  |  The resulting  DataFrame  includes the 5 columns of the first frame passed to  concatRows() , however unlike the first\nframe, columns  C ,  D  and  E  are now of type  Object  rather than of type  double  because these columns now hold both\ndouble precision data (rows 0 through 4) and String data (rows 4 through 9).",
            "title": "Concatenation"
        },
        {
            "location": "/frame/reshaping/#adding-columns",
            "text": "Consider the a 10x2  DataFrame  of random double precision values as a starting point to illustrate the various\ncolumn expansion calls described in the following sections. The row axis is indexed by  LocalDate  values and the\ncolumn axis by  String  values.   LocalDate start = LocalDate.of(2014, 1, 1);\nDataFrame<LocalDate,String> frame = DataFrame.ofDoubles(\n    Range.of(start, start.plusDays(10)),\n    Array.of(\"A\", \"B\"),\n    value -> Math.random() * 10d\n);  \n   Index     |   A    |   B    |\n--------------------------------\n 2014-01-01  |  8.72  |  9.19  |\n 2014-01-02  |  6.42  |  3.42  |\n 2014-01-03  |  1.75  |  1.52  |\n 2014-01-04  |  1.15  |  0.94  |\n 2014-01-05  |  7.80  |  2.10  |\n 2014-01-06  |  9.23  |  0.41  |\n 2014-01-07  |  3.26  |  0.04  |\n 2014-01-08  |  1.84  |  5.10  |\n 2014-01-09  |  2.87  |  4.82  |\n 2014-01-10  |  6.61  |  7.15  |",
            "title": "Adding Columns"
        },
        {
            "location": "/frame/reshaping/#singular_1",
            "text": "The  DataFrameColumns  interface exposes various  add()  and  addAll()  methods, but they differ slightly to those on  DataFrameRows .\nEach column in a frame is associated with a specific data type, so when adding additional columns the type needs to be either explicitly \nspecified, or implicitly defined through the data structure being added. Column data will ultimately end up being represented as a \nMorpehus  Array  within the frame, but it is possible to pass data as an  Iterable  thereby supporting many input types.  The code below demonstrates 5 distinct ways of adding columns to the 10x2  DataFrame  illustrated above, using the various \ntechniques that are supported for single column adds. In some cases we simply pass the desired data type for the column, while\nin other cases we pass an  Iterable  in the form of a  Range  or  Array  object. The final state of the frame is shown below.   //Add single column of Booleans, no initials\nframe.cols().add(\"C\", Boolean.class);\n//Add single column of type String with coordinate labels\nframe.cols().add(\"D\", String.class, v -> String.format(\"(%s,%s)\", v.rowOrdinal(), v.colOrdinal()));\n//Add single column given an Iterable of values\nframe.cols().add(\"E\", Range.of(0, frame.rowCount()));\n//Add single column of type LocalDate, with initializing function\nframe.cols().add(\"F\", LocalDate.class, v -> start.plusDays(v.rowOrdinal() * 2));\n//Add single column with explicit value array\nframe.cols().add(\"G\", Array.of(10d, 20d, 30d, 40d));\n//Print first 10 rows to std out\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});  \n   Index     |   A    |   B    |    C    |    D    |  E  |      F       |    G    |\n-----------------------------------------------------------------------------------\n 2014-01-01  |  4.09  |  2.23  |  false  |  (0,3)  |  0  |  2014-01-01  |  10.00  |\n 2014-01-02  |  4.49  |  4.08  |  false  |  (1,3)  |  1  |  2014-01-03  |  20.00  |\n 2014-01-03  |  6.77  |  1.94  |  false  |  (2,3)  |  2  |  2014-01-05  |  30.00  |\n 2014-01-04  |  1.81  |  4.48  |  false  |  (3,3)  |  3  |  2014-01-07  |  40.00  |\n 2014-01-05  |  4.56  |  5.70  |  false  |  (4,3)  |  4  |  2014-01-09  |    NaN  |\n 2014-01-06  |  8.56  |  2.96  |  false  |  (5,3)  |  5  |  2014-01-11  |    NaN  |\n 2014-01-07  |  7.71  |  7.07  |  false  |  (6,3)  |  6  |  2014-01-13  |    NaN  |\n 2014-01-08  |  4.70  |  3.07  |  false  |  (7,3)  |  7  |  2014-01-15  |    NaN  |\n 2014-01-09  |  9.78  |  4.28  |  false  |  (8,3)  |  8  |  2014-01-17  |    NaN  |\n 2014-01-10  |  8.32  |  4.62  |  false  |  (9,3)  |  9  |  2014-01-19  |    NaN  |",
            "title": "Singular"
        },
        {
            "location": "/frame/reshaping/#multiple_1",
            "text": "Adding multiple columns in one call is also supported as described in this section. The code below creates a similar 10x2 \nframe of random double precision values as a starting point, and then adds three columns labelled  A ,  B  and  C  of type  String , and two additional columns presenting the column data directory via a  Range  and  Array  object. One thing to note\nabout this example is that the data structures for  F  and  G  are shorter than the row count of the original frame, so\nthey get extended as required with values initializing to the defaults for the type in question. If the data structures are\nlonger than the row count, they are effectively truncated to fit the frame.   LocalDate start = LocalDate.of(2014, 1, 1);\nDataFrame<LocalDate,String> frame = DataFrame.ofDoubles(\n    Range.of(start, start.plusDays(10)),\n    Array.of(\"A\", \"B\"),\n    value -> Math.random() * 10d\n);\n//Add multiple columns to hold String data\nframe.cols().addAll(Array.of(\"C\", \"D\", \"E\"), String.class);\n//Add multiple columns via consumer\nframe.cols().addAll(columns -> {\n    columns.put(\"F\", Array.of(10d, 11d, 12d, 13d, 14d));\n    columns.put(\"G\", Range.of(start.plusDays(3), start.plusDays(10)));\n});\n//Print first 10 rows to std out\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});  \n   Index     |   A    |   B    |   C    |   D    |   E    |    F    |      G       |\n------------------------------------------------------------------------------------\n 2014-01-01  |  2.65  |  3.49  |  null  |  null  |  null  |  10.00  |  2014-01-04  |\n 2014-01-02  |  4.80  |  2.07  |  null  |  null  |  null  |  11.00  |  2014-01-05  |\n 2014-01-03  |  5.81  |  2.70  |  null  |  null  |  null  |  12.00  |  2014-01-06  |\n 2014-01-04  |  8.46  |  7.00  |  null  |  null  |  null  |  13.00  |  2014-01-07  |\n 2014-01-05  |  4.65  |  4.72  |  null  |  null  |  null  |  14.00  |  2014-01-08  |\n 2014-01-06  |  3.39  |  3.95  |  null  |  null  |  null  |    NaN  |  2014-01-09  |\n 2014-01-07  |  3.18  |  5.29  |  null  |  null  |  null  |    NaN  |  2014-01-10  |\n 2014-01-08  |  1.22  |  7.64  |  null  |  null  |  null  |    NaN  |        null  |\n 2014-01-09  |  8.32  |  9.39  |  null  |  null  |  null  |    NaN  |        null  |\n 2014-01-10  |  0.66  |  3.38  |  null  |  null  |  null  |    NaN  |        null  |",
            "title": "Multiple"
        },
        {
            "location": "/frame/reshaping/#frame_1",
            "text": "Similar to adding rows from one frame to another, it is also possible to add columns from one frame to another. Only columns \nthat are missing from the first frame are added by default, unless duplicate handling is changed via  DataFrameOptions . In \naddition, only data from intersecting rows will be copied. Consider a 9x2 frame initialized with random double precision values\nas follows:   //Create a 9x2 DataFrame of random double precision values\nDataFrame<Integer,String> frame1 = DataFrame.ofDoubles(\n    Range.of(0, 9, 1),\n    Array.of(\"A\", \"B\"),\n    value -> Math.random()\n);  \n Index  |    A    |    B    |\n-----------------------------\n     0  |  0.015  |  0.278  |\n     1  |  0.577  |  0.229  |\n     2  |  0.837  |  0.273  |\n     3  |  0.400  |  0.059  |\n     4  |  0.407  |  0.696  |\n     5  |  0.318  |  0.657  |\n     6  |  0.047  |  0.682  |\n     7  |  0.344  |  0.365  |\n     8  |  0.633  |  0.476  |  A second 6x5 frame is initialized with intersecting rows for keys  0 ,  2 ,  4 ,  6  and  8 , and one intersecting column, \nnamely for label  B . The columns for this frame include various data types, namely of  Integer .  Double ,  String  and Boolean , and all values are initialized by directly presenting a data structure representing the column values.   //Create 6x5 frame with intersecting rows and columns to the first frame\nDataFrame<Integer,String> frame2 = DataFrame.of(Range.of(0, 12, 2), String.class, columns -> {\n    columns.add(\"B\", Array.of(10, 20, 30, 40, 50, 60));\n    columns.add(\"C\", Array.of(1d, 3d, 5d, 7d, 9d, 11d));\n    columns.add(\"D\", Range.of(1, 7));\n    columns.add(\"E\", String.class, v -> String.format(\"(%s,%s)\", v.rowOrdinal(), v.colOrdinal()));\n    columns.add(\"F\", Boolean.class, v -> Math.random() > 0.5d);\n});  \n Index  |  B   |    C     |  D  |    E    |    F    |\n-----------------------------------------------------\n     0  |  10  |   1.000  |  1  |  (0,3)  |   true  |\n     2  |  20  |   3.000  |  2  |  (1,3)  |   true  |\n     4  |  30  |   5.000  |  3  |  (2,3)  |   true  |\n     6  |  40  |   7.000  |  4  |  (3,3)  |  false  |\n     8  |  50  |   9.000  |  5  |  (4,3)  |  false  |\n    10  |  60  |  11.000  |  6  |  (5,3)  |  false  |  Using the  addAll()  method on the  DataFrameColumns  interface we can add columns from the second 6x5 frame to the original \n9x2 frame. Since duplicates are ignored by default, we expect that column  B  from the first frame will be retained and not \nget over written by the values from the second frame. In addition, only data for intersecting rows will get transferred. The\ncode to combine these frames and the 9x6 result is shown below.    //Add all columns from second frame to first frame, copy data from intersecting rows\nframe1.cols().addAll(frame2);\n//Print frame to standard out with custom formatting\nframe1.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});  \n Index  |    A    |    B    |    C    |  D  |    E    |    F    |\n-----------------------------------------------------------------\n     0  |  0.015  |  0.278  |  1.000  |  1  |  (0,3)  |   true  |\n     1  |  0.577  |  0.229  |    NaN  |  0  |   null  |  false  |\n     2  |  0.837  |  0.273  |  3.000  |  2  |  (1,3)  |   true  |\n     3  |  0.400  |  0.059  |    NaN  |  0  |   null  |  false  |\n     4  |  0.407  |  0.696  |  5.000  |  3  |  (2,3)  |   true  |\n     5  |  0.318  |  0.657  |    NaN  |  0  |   null  |  false  |\n     6  |  0.047  |  0.682  |  7.000  |  4  |  (3,3)  |  false  |\n     7  |  0.344  |  0.365  |    NaN  |  0  |   null  |  false  |\n     8  |  0.633  |  0.476  |  9.000  |  5  |  (4,3)  |  false  |",
            "title": "Frame"
        },
        {
            "location": "/frame/reshaping/#concatenation_1",
            "text": "The above examples demonstrate adding columns to an existing frame which mutates the frame in place. At times it is preferable\nhowever to create a new frame which combines the data of two or more input frames. To do this, a static  concatColumns()  method \non the  DataFrame  interface exists which cam combine any number of frames. Again, duplicate columns across the input frames are \nignored with the first frame being the winner, and only data from intersecting rows to the first frame are captured.  Consider 3 frames initialized as follows:   //Create a 9x2 DataFrame of random double precision values\nDataFrame<Integer,String> frame1 = DataFrame.ofDoubles(\n    Range.of(0, 9),\n    Array.of(\"A\", \"B\"),\n    value -> Math.random()\n);  \n Index  |    A    |    B    |\n-----------------------------\n     0  |  0.027  |  0.526  |\n     1  |  0.643  |  0.154  |\n     2  |  0.149  |  0.110  |\n     3  |  0.670  |  0.085  |\n     4  |  0.939  |  0.779  |\n     5  |  0.106  |  0.524  |\n     6  |  0.665  |  0.842  |\n     7  |  0.221  |  0.662  |\n     8  |  0.506  |  0.484  |   //Create 6x5 frame with intersecting rows and columns to the first frame\nDataFrame<Integer,String> frame2 = DataFrame.of(Range.of(0, 12, 2), String.class, columns -> {\n    columns.add(\"B\", Array.of(10, 20, 30, 40, 50, 60));\n    columns.add(\"C\", Array.of(1d, 3d, 5d, 7d, 9d, 11d));\n    columns.add(\"D\", Range.of(1, 7));\n    columns.add(\"E\", String.class, v -> String.format(\"(%s,%s)\", v.rowOrdinal(), v.colOrdinal()));\n    columns.add(\"F\", Boolean.class, v -> Math.random() > 0.5d);\n});  \n Index  |  B   |    C     |  D  |    E    |    F    |\n-----------------------------------------------------\n     0  |  10  |   1.000  |  1  |  (0,3)  |  false  |\n     2  |  20  |   3.000  |  2  |  (1,3)  |   true  |\n     4  |  30  |   5.000  |  3  |  (2,3)  |  false  |\n     6  |  40  |   7.000  |  4  |  (3,3)  |   true  |\n     8  |  50  |   9.000  |  5  |  (4,3)  |   true  |\n    10  |  60  |  11.000  |  6  |  (5,3)  |   true  |   //Create a 9x4 DataFrame of random double precision values\nDataFrame<Integer,String> frame3 = DataFrame.ofDoubles(\n    Range.of(0, 5),\n    Array.of(\"B\", \"F\", \"G\", \"H\"),\n    value -> Math.random()\n);  \n Index  |    B    |    F    |    G    |    H    |\n-------------------------------------------------\n     0  |  0.899  |  0.959  |  0.182  |  0.960  |\n     1  |  0.138  |  0.941  |  0.111  |  0.124  |\n     2  |  0.582  |  0.739  |  0.446  |  0.868  |\n     3  |  0.049  |  0.594  |  0.133  |  0.602  |\n     4  |  0.592  |  0.738  |  0.832  |  0.380  |  Now we use  concatColumns()  to create a new frame combining the columns of all 3, ignoring duplicates. The second frame's\ncolumn  B  will be ignored in favour of the first frame, and the third frame's column  B  and  F  will be ignored in favour\nof the first and second frame respectively. Only data from rows intersecting with the first frame's row axis will be included\nin the combined frame.   //Concatenate columns from all 3 frames to create a new result\nDataFrame<Integer,String> frame4 = DataFrame.concatColumns(frame1, frame2, frame3);\n//Print frame to standard out with custom formatting\nframe4.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});  \n Index  |    A    |    B    |    C    |  D  |    E    |    F    |    G    |    H    |\n-------------------------------------------------------------------------------------\n     0  |  0.027  |  0.526  |  1.000  |  1  |  (0,3)  |  false  |  0.182  |  0.960  |\n     1  |  0.643  |  0.154  |    NaN  |  0  |   null  |  false  |  0.111  |  0.124  |\n     2  |  0.149  |  0.110  |  3.000  |  2  |  (1,3)  |   true  |  0.446  |  0.868  |\n     3  |  0.670  |  0.085  |    NaN  |  0  |   null  |  false  |  0.133  |  0.602  |\n     4  |  0.939  |  0.779  |  5.000  |  3  |  (2,3)  |  false  |  0.832  |  0.380  |\n     5  |  0.106  |  0.524  |    NaN  |  0  |   null  |  false  |    NaN  |    NaN  |\n     6  |  0.665  |  0.842  |  7.000  |  4  |  (3,3)  |   true  |    NaN  |    NaN  |\n     7  |  0.221  |  0.662  |    NaN  |  0  |   null  |  false  |    NaN  |    NaN  |\n     8  |  0.506  |  0.484  |  9.000  |  5  |  (4,3)  |   true  |    NaN  |    NaN  |",
            "title": "Concatenation"
        },
        {
            "location": "/frame/reshaping/#dataframe-union",
            "text": "Row and column concatenation only copies data based on columns and rows respectively that intersect with the entries in the \nfirst frame. At times a full union of N frames is desirable, also taking into account the fact that the combined columns may \nnot share the same data type. In order to support this, a static method called  union()  on the  DataFrame  interface is \nprovided for this purpose.  Consider a 5x2, 6x5 and 6x4 frame with intersecting rows and columns initialized as follows.    //Create a 5x2 DataFrame of random double precision values\nDataFrame<Integer,String> frame1 = DataFrame.ofDoubles(\n    Range.of(0, 5),\n    Array.of(\"A\", \"B\"),\n    value -> Math.random()\n);  \n Index  |    A    |    B    |\n-----------------------------\n     0  |  0.968  |  0.013  |\n     1  |  0.105  |  0.593  |\n     2  |  0.012  |  0.312  |\n     3  |  0.489  |  0.015  |\n     4  |  0.304  |  0.269  |   //Create 6x5 frame with intersecting rows and columns to the first frame\nDataFrame<Integer,String> frame2 = DataFrame.of(Range.of(0, 12, 2), String.class, columns -> {\n    columns.add(\"B\", Array.of(10, 20, 30, 40, 50, 60));\n    columns.add(\"C\", Array.of(1d, 3d, 5d, 7d, 9d, 11d));\n    columns.add(\"D\", Range.of(1, 7));\n    columns.add(\"E\", String.class, v -> String.format(\"(%s,%s)\", v.rowOrdinal(), v.colOrdinal()));\n    columns.add(\"F\", Boolean.class, v -> Math.random() > 0.5d);\n});  \n Index  |  B   |    C     |  D  |    E    |    F    |\n-----------------------------------------------------\n     0  |  10  |   1.000  |  1  |  (0,3)  |  false  |\n     2  |  20  |   3.000  |  2  |  (1,3)  |  false  |\n     4  |  30  |   5.000  |  3  |  (2,3)  |   true  |\n     6  |  40  |   7.000  |  4  |  (3,3)  |   true  |\n     8  |  50  |   9.000  |  5  |  (4,3)  |  false  |\n    10  |  60  |  11.000  |  6  |  (5,3)  |   true  |   //Create a 6x4 DataFrame of random double precision values\nDataFrame<Integer,String> frame3 = DataFrame.ofDoubles(\n    Range.of(0, 6),\n    Array.of(\"B\", \"F\", \"G\", \"H\"),\n    value -> Math.random()\n);  \n Index  |    B    |    F    |    G    |    H    |\n-------------------------------------------------\n     0  |  0.924  |  0.975  |  0.751  |  0.764  |\n     1  |  0.441  |  0.160  |  0.626  |  0.477  |\n     2  |  0.777  |  0.549  |  0.568  |  0.209  |\n     3  |  0.148  |  0.178  |  0.320  |  0.500  |\n     4  |  0.880  |  0.351  |  0.025  |  0.998  |\n     5  |  0.780  |  0.839  |  0.479  |  0.607  |  The union of these 3 frames would be a 9x8 structure, as duplicates would be handled on a combine first basis. Note\nthat column  B  in the first and third frame is of type  double  and in the second frame it's of type  int . The union\nframe will have to represent the combined column  B  as type  Object  for this to work.  The code to create the union frame and the resulting structure are shown below:   //Create the union of all 3 frames which should yield an 9x8 frame\nDataFrame<Integer,String> frame4 =  DataFrame.union(frame1, frame2, frame3);\n//Print frame to standard out with custom formatting\nframe4.rows().sort(true);\nframe4.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});  \n Index  |    A    |    B    |    C     |  D  |    E    |    F    |    G    |    H    |\n--------------------------------------------------------------------------------------\n     0  |  0.968  |  0.013  |   1.000  |  1  |  (0,3)  |  false  |  0.751  |  0.764  |\n     1  |  0.105  |  0.593  |     NaN  |  0  |   null  |  false  |  0.626  |  0.477  |\n     2  |  0.012  |  0.312  |   3.000  |  2  |  (1,3)  |  false  |  0.568  |  0.209  |\n     3  |  0.489  |  0.015  |     NaN  |  0  |   null  |  false  |  0.320  |  0.500  |\n     4  |  0.304  |  0.269  |   5.000  |  3  |  (2,3)  |   true  |  0.025  |  0.998  |\n     5  |    NaN  |  0.780  |     NaN  |  0  |   null  |  false  |  0.479  |  0.607  |\n     6  |    NaN  |     40  |   7.000  |  4  |  (3,3)  |   true  |    NaN  |    NaN  |\n     8  |    NaN  |     50  |   9.000  |  5  |  (4,3)  |  false  |    NaN  |    NaN  |\n    10  |    NaN  |     60  |  11.000  |  6  |  (5,3)  |   true  |    NaN  |    NaN  |",
            "title": "DataFrame Union"
        },
        {
            "location": "/frame/reshaping/#duplicate-handling",
            "text": "By design, a  DataFrame  row and column axis does not support duplicate keys. When expanding a frame, an attempt \nto add row or column keys which are already present in the respective axis results in a no-op. In addition, \nfunctions to concatenate or create a union of two or more frames operate on a combine first basis. While this \nmay suffice for most use cases, there are scenarios where you would not expect duplicates to exist across frames, \nand would rather such an event raise an exception as opposed to operating on the combine first approach. This \nbehaviour, among others, can be achieved via the  DataFrameOptions  class which provides various methods to change \nthe behaviour of the currently executing thread via the use of internal  ThreadLocal  variables.  If we consider the union example from the previous section, we can cause it to fail (because there are duplicates \nkeys across the input frames) by wrapping the code in a  Callable  to  whileNotIgnoringDuplicates()  as follows.   DataFrame<Integer,String> frame4 = DataFrameOptions.whileNotIgnoringDuplicates(() -> {\n    return DataFrame.union(frame1, frame2, frame3).rows().sort(true);\n});  Running this code results in an exception as follows:  \n    Caused by: com.zavtech.morpheus.frame.DataFrameException: A column for key already exists in this frame: B\n        at com.zavtech.morpheus.reference.XDataFrameColumns.lambda$addAll$121(XDataFrameColumns.java:139)\n        at com.zavtech.morpheus.reference.XDataFrameAxisBase.forEach(XDataFrameAxisBase.java:281)\n        at com.zavtech.morpheus.reference.XDataFrameColumns.addAll(XDataFrameColumns.java:135)\n        at com.zavtech.morpheus.reference.XDataFrameFactory.lambda$union$8(XDataFrameFactory.java:161)\n        at java.util.Arrays$ArrayList.forEach(Arrays.java:3880)\n        at com.zavtech.morpheus.reference.XDataFrameFactory.union(XDataFrameFactory.java:159)\n        at com.zavtech.morpheus.frame.DataFrame.union(DataFrame.java:407)\n        at com.zavtech.morpheus.examples.ReshapingDocs.lambda$union$49(ReshapingDocs.java:304)\n        at com.zavtech.morpheus.frame.DataFrameOptions.whileNotIgnoringDuplicates(DataFrameOptions.java:219)",
            "title": "Duplicate Handling"
        },
        {
            "location": "/frame/reshaping/#re-labelling",
            "text": "A  DataFrame  row and column axis contains keys of a certain data type, and it is often useful to either replace\none or more keys or to switch out an axis entirely. While this does not in effect change the shape of the frame in\nterms of the row and column count, it does change its constitution. This section demonstrates how the API can be\nused to replace one or more keys in place, or to generate a shallow copy of a frame with an entirely new axis.  Consider a 5x5 frame keyed by  Integer  rows and  String  columns initialized with random doubles as follows:   //Create a 5x5 DataFrame of random doubles\nDataFrame<Integer,String> frame = DataFrame.ofDoubles(\n    Array.of(0, 1, 2, 3, 4),\n    Array.of(\"A\", \"B\", \"C\", \"D\", \"E\"),\n    value -> Math.random() * 10d\n);  \n Index  |    A    |    B    |    C    |    D    |    E    |\n-----------------------------------------------------------\n     0  |  2.995  |  9.439  |  2.817  |  0.973  |  5.424  |\n     1  |  9.350  |  5.138  |  8.626  |  0.368  |  7.935  |\n     2  |  5.407  |  2.157  |  4.207  |  8.540  |  4.385  |\n     3  |  8.410  |  6.536  |  8.796  |  8.220  |  7.743  |\n     4  |  0.248  |  6.475  |  2.560  |  8.391  |  5.810  |  The  DataFraneAxis  interface exposes a  replaceKey()  method that can be used to change any number of keys in place. \nThe only two caveats are that the replacement key must not already exist in the axis, as duplicates are not supported,\nand the replacment key must be of the same data type. If the replacement key does already exist, expect a  DataFrameException  \nto be raised. The code below demonstrates how to replace a single key in both the row oe column axis, and prints the result.    frame.rows().replaceKey(4, 40);\nframe.cols().replaceKey(\"C\", \"X\");\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});  \n Index  |    A    |    B    |    X    |    D    |    E    |\n-----------------------------------------------------------\n     0  |  2.995  |  9.439  |  2.817  |  0.973  |  5.424  |\n     1  |  9.350  |  5.138  |  8.626  |  0.368  |  7.935  |\n     2  |  5.407  |  2.157  |  4.207  |  8.540  |  4.385  |\n     3  |  8.410  |  6.536  |  8.796  |  8.220  |  7.743  |\n    40  |  0.248  |  6.475  |  2.560  |  8.391  |  5.810  |  While the above technique is useful to replace specific keys, it is often necessary to replace an axis entirely, and \npossibly with a different key type. Consider the 10x5 frame of random double precision values below which is keyed by  LocalDate  on the row axis, and  String  on the column axis.   //Create a 10x4 DataFrame of random doubles\nDataFrame<LocalDate,String> frame = DataFrame.ofDoubles(\n    Range.ofLocalDates(\"2014-01-01\", \"2014-01-11\"),\n    Array.of(\"A\", \"B\", \"C\", \"D\"),\n    value -> Math.random() * 10d\n);  \n   Index     |    A    |    B    |    C    |    D    |\n------------------------------------------------------\n 2014-01-01  |  1.725  |  7.107  |  1.979  |  5.899  |\n 2014-01-02  |  1.232  |  5.779  |  6.550  |  7.534  |\n 2014-01-03  |  9.706  |  7.316  |  1.277  |  1.122  |\n 2014-01-04  |  2.382  |  9.778  |  6.011  |  1.200  |\n 2014-01-05  |  3.800  |  7.071  |  2.743  |  8.742  |\n 2014-01-06  |  5.157  |  1.242  |  3.834  |  0.047  |\n 2014-01-07  |  4.474  |  0.511  |  5.953  |  5.698  |\n 2014-01-08  |  4.784  |  6.298  |  5.771  |  3.975  |\n 2014-01-09  |  9.525  |  7.301  |  8.257  |  3.144  |\n 2014-01-10  |  6.245  |  9.614  |  8.318  |  0.056  |  Let us assume that the data for this hypothetical experient needs to be date shifted forward by 5 calendar days,\nand the row axis  LocalDate  objects need to be converted to  LocalDateTime  objects with a time set to 13:30. One \nway to achieve this would be to create an entirely new frame, and copy the data across, although this would not be \nparticularly efficient, especially if the frame was extremely large. A better way is to use the  mapKeys()  method\non  DataFeameAxis  and provide a mapping function that does the conversion as shown below.   DataFrame<LocalDateTime,String> shifted = frame.rows().mapKeys(row -> {\n    final LocalDate rowKey = row.key().plusDays(5);\n    return LocalDateTime.of(rowKey, LocalTime.of(13, 30));\n});  \n      Index        |    A    |    B    |    C    |    D    |\n------------------------------------------------------------\n 2014-01-06T13:30  |  1.725  |  7.107  |  1.979  |  5.899  |\n 2014-01-07T13:30  |  1.232  |  5.779  |  6.550  |  7.534  |\n 2014-01-08T13:30  |  9.706  |  7.316  |  1.277  |  1.122  |\n 2014-01-09T13:30  |  2.382  |  9.778  |  6.011  |  1.200  |\n 2014-01-10T13:30  |  3.800  |  7.071  |  2.743  |  8.742  |\n 2014-01-11T13:30  |  5.157  |  1.242  |  3.834  |  0.047  |\n 2014-01-12T13:30  |  4.474  |  0.511  |  5.953  |  5.698  |\n 2014-01-13T13:30  |  4.784  |  6.298  |  5.771  |  3.975  |\n 2014-01-14T13:30  |  9.525  |  7.301  |  8.257  |  3.144  |\n 2014-01-15T13:30  |  6.245  |  9.614  |  8.318  |  0.056  |  Unlike the  replaceKey()  method,  mapKeys()  takes a lambda function to generate an entirely new axis, and returns\na new  DataFrame  initialized with the replacement axis. The data in the shifted frame is referentially the same as the \noriginal frame, so the cost of this operation is limited to the cost of creating the new axis. We can confirm this by \nsetting all the values in the shifted frame to zero, and then confirming the original frame has also been zero'd out.   //Zero out shifted frame values, and print original frame\nshifted.applyDoubles(v -> 0d);\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});  \n   Index     |    A    |    B    |    C    |    D    |\n------------------------------------------------------\n 2014-01-01  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-02  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-03  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-04  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-05  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-06  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-07  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-08  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-09  |  0.000  |  0.000  |  0.000  |  0.000  |\n 2014-01-10  |  0.000  |  0.000  |  0.000  |  0.000  |",
            "title": "Re-Labelling"
        },
        {
            "location": "/frame/reshaping/#transpose",
            "text": "When a  DataFrame  is transposed, it changes from being a column store data structure to row store, making the tranpose\noperation itself, even for very large frames, very cheap. One drawback of this design choice however is that a transposed \nframe cannot be reshaped by adding rows and/or columns.  Attempting to re-shape a transposed  DataFrame  will result in a  DataFrameException  as the following example demonstrates.   //Create a 5x5 DataFrame of random doubles\nDataFrame<Integer,String> frame = DataFrame.ofDoubles(\n    Array.of(0, 1, 2, 3, 4),\n    Array.of(\"A\", \"B\", \"C\", \"D\", \"E\"),\n    value -> Math.random() * 10d\n);\n\n//Transpose and try add a row...\nframe.transpose().rows().add(\"F\");  \ncom.zavtech.morpheus.frame.DataFrameException: Cannot add rows to a transposed DataFrame, call transpose() and then add columns\n    at com.zavtech.morpheus.reference.XDataFrameContent.addRow(XDataFrameContent.java:280)\n    at com.zavtech.morpheus.reference.XDataFrameRows.add(XDataFrameRows.java:77)\n    at com.zavtech.morpheus.reference.XDataFrameRows.add(XDataFrameRows.java:64)\n    at com.zavtech.morpheus.examples.ReshapingDocs.transpose(ReshapingDocs.java:403)  The workaround to this limitation is to call  transpose()  on the row major version of the frame, which yields the un-tranposed \noriginal that can be re-shaped by adding rows and/or columns. Consider the same 5x5 frame created above, which we transpose,\nand then add a new column to the transposed frame by calling  addRow()  as shown below.   //Tranpose the transposed frame to get back to a colum store which can be re-shaped\nDataFrame<String,Integer> transposed = frame.transpose();\ntransposed.transpose().rows().add(5);\ntransposed.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});  \n Index  |   0    |   1    |   2    |   3    |   4    |   5   |\n--------------------------------------------------------------\n     A  |  4.23  |  6.74  |  3.33  |  7.19  |  6.87  |  NaN  |\n     B  |  4.61  |  7.58  |  5.55  |  9.78  |  7.32  |  NaN  |\n     C  |  0.54  |  3.68  |  6.18  |  4.06  |  2.88  |  NaN  |\n     D  |  9.48  |  9.36  |  4.10  |  8.89  |  3.68  |  NaN  |\n     E  |  1.44  |  1.64  |  3.81  |  7.75  |  6.25  |  NaN  |",
            "title": "Transpose"
        },
        {
            "location": "/frame/filtering/",
            "text": "Filtering\n\n\nIntroduction\n\n\nFiltering a \nDataFrame\n along the row and/or column dimensions is a common requirement in order to easily \nanalyze or operate on a subset of the data.  Applying a filter is a fairly cheap operation because the resulting \nframe is merely a projection onto its parent, as opposed to being a deep copy. This implies that very little \nadditional memory is required to create filters or slices, and it also means that any data modifications to the \nfilter are in effect applied to the parent. Of course, if a detached copy of the data subset is required, simply \ncalling \nDataFrame.copy()\n on the filter frame will yield the desired result. \n\n\nIt should also be noted that while the contents of a filter frame can be modified, the structure of the filter is \nimmutable in the row dimension, but can be expanded in the column dimension (see section below titled Immutability \nthat explains these restrictions in more detail). It is of course permissible to further filter an already filtered \nframe.\n\n\nExample Data\n\n\nThere is a large amount of professional tennis match data available \nhere\n for both \nthe men's (ATP) and women's (WTA) tours. The code examples in this section operate on a dataset containing all match \nresults for the 2013 ATP tour, which can be loaded using the code below. The dataset contains 2631 rows and 41 \ncolumns, including player statistics, match play statistics, as well as betting odds from various bookies in the UK. \nMore information on the exact content of this file can be located \nhere\n\n\n\n\n\n/**\n * Returns the ATP match results for the year specified\n * @param year      the year for ATP results\n * @return          the ATP match results\n */\nstatic DataFrame<Integer,String> loadTennisMatchData(int year) {\n    final DateTimeFormatter dateFormat = DateTimeFormatter.ofPattern(\"dd/MM/yy\");\n    return DataFrame.read().csv(options -> {\n        options.setHeader(true);\n        options.setResource(\"http://www.zavtech.com/data/tennis/atp/atp-\" + year + \".csv\");\n        options.setParser(\"Date\", Parser.ofLocalDate(dateFormat));\n        options.setExcludeColumns(\"ATP\");\n    });\n}\n\n\n\n\n\nIndex  |  Location  |        Tournament        |     Date     |  Series  |   Court   |  Surface  |    Round    |  Best of  |     Winner     |     Loser      |  WRank  |  LRank  |  WPts  |  LPts  |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W  |  B365L  |  EXW   |  EXL   |  LBW   |  LBL   |  PSW   |  PSL   |  SJW   |  SJL   |  MaxW  |  MaxL  |  AvgW  |  AvgL  |\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    0  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |      Mayer F.  |    Giraldo S.  |     28  |     57  |  1215  |   778  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |   1.36  |    3.0  |  1.45  |  2.65  |  1.44  |  2.62  |  1.47  |  2.85  |  1.44  |  2.63  |  1.47  |   3.2  |  1.42  |  2.78  |\n    1  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |   Nieminen J.  |  Benneteau J.  |     41  |     35  |   927  |  1075  |   6  |   3  |   2  |   6  |   6  |   1  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |   1.61  |    2.2  |  1.75  |   2.0  |   1.8  |  1.91  |   1.8  |   2.1  |  1.73  |   2.0  |   1.8  |  2.26  |  1.73  |  2.05  |\n    2  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |  Nishikori K.  |  Matosevic M.  |     19  |     49  |  1830  |   845  |   7  |   5  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |   1.25  |   3.75  |  1.25  |  3.75  |  1.29  |   3.5  |   1.3  |  3.85  |   1.3  |   3.2  |   1.3  |   4.2  |  1.28  |  3.58  |\n    3  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |  Baghdatis M.  |   Mitchell B.  |     36  |    326  |  1070  |   137  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |   1.07  |    9.0  |  1.06  |   8.0  |  1.08  |   7.0  |  1.08  |  9.43  |  1.07  |   7.0  |   1.1  |   9.5  |  1.08  |  7.76  |\n    4  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Istomin D.  |     Klizan M.  |     43  |     30  |   897  |  1175  |   6  |   1  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |    1.9  |    1.8  |  1.87  |  1.87  |  1.91  |   1.8  |  1.88  |   2.0  |  1.91  |   1.8  |  2.05  |   2.0  |  1.88  |  1.85  |\n    5  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Millman J.  |        Ito T.  |    199  |     79  |   239  |   655  |   6  |   4  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |   1.61  |    2.2  |  1.75  |   2.0  |  1.73  |   2.0  |   1.7  |  2.27  |   1.8  |  1.91  |  1.85  |  2.28  |  1.71  |  2.08  |\n    6  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |      Falla A.  |     Levine J.  |     54  |    104  |   809  |   530  |   6  |   1  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |    2.2  |   1.61  |  2.08  |  1.67  |  1.91  |   1.8  |  2.26  |   1.7  |   2.0  |  1.73  |  2.32  |  1.83  |  2.08  |   1.7  |\n    7  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |     Melzer J.  |      Kudla D.  |     29  |    137  |  1177  |   402  |   2  |   6  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |   1.44  |   2.62  |  1.55  |  2.35  |  1.44  |  2.62  |   1.6  |  2.47  |   1.5  |   2.5  |  1.63  |  2.82  |  1.52  |  2.46  |\n    8  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Robredo T.  |   Harrison R.  |    114  |     69  |   495  |   710  |   6  |   4  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |    3.0  |   1.36  |   2.5  |   1.5  |  2.38  |  1.53  |  2.93  |  1.45  |   2.5  |   1.5  |  3.25  |  1.53  |  2.66  |  1.47  |\n    9  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |   Dimitrov G.  |      Baker B.  |     48  |     61  |   866  |   756  |   6  |   3  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |   1.36  |    3.0  |   1.4  |   2.8  |  1.44  |  2.62  |  1.38  |   3.3  |   1.4  |  2.75  |  1.45  |  3.55  |  1.39  |  2.87  |\n\n\n\n\nInitializing this frame from CSV is fairly straightforward as most of the data can be coerced into the appropriate \ntype without any intervention, except for the Date column which is expressed in dd/MM/yy format and therefore requires\na custom parser. We also exclude a number of columns that are of little interest.\n\n\nSelect Rows\n\n\nFiltering along the row dimension of the ATP match results can be achieved by calling one of the over-loaded \nselect()\n \nmethods available on the row dimension operator, which is accessed with a call to \nDataFrame.rows()\n. The \nselect()\n \nmethods either take the set of row keys to include, or more commonly a lambda expression that applies some logic on \nthe row entity. For very large frames, parallel processing is also supported with a call to \nDataFrame.rows().parallel()\n. \n\n\nThe code below shows a simple way of filtering the ATP match results to only include the 5 set matches in 2013.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\nDataFrame<Integer,String> filter = frame.rows().select(row -> {\n    final int wonSets = row.getInt(\"Wsets\");\n    final int lostSets = row.getInt(\"Lsets\");\n    return  wonSets + lostSets == 5;\n});\n\n\n\n\n\n Index  |  Location   |    Tournament     |     Date     |    Series    |   Court   |  Surface  |    Round    |  Best of  |       Winner        |     Loser      |  WRank  |  LRank  |  WPts  |  LPts  |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n   139  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |         Almagro N.  |    Johnson S.  |     11  |    175  |  2515  |   302  |   7  |   5  |   6  |   7  |   6  |   2  |   6  |   7  |   6  |   2  |      3  |      2  |  Completed  |  1.1400  |  5.5000  |  1.1300  |  5.7500  |  1.1100  |  6.0000  |  1.1900  |  5.5400  |  1.1300  |  6.0000  |  1.1900  |  7.9000  |  1.1400  |  5.4900  |\n   144  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |   Gimeno-Traver D.  |      Kubot L.  |     71  |     76  |   704  |   675  |   6  |   7  |   6  |   4  |   6  |   0  |   4  |   6  |   6  |   4  |      3  |      2  |  Completed  |  3.2500  |  1.3300  |  3.1500  |  1.3500  |  3.4000  |  1.3000  |  3.9700  |  1.2900  |  2.7500  |  1.4400  |  3.9700  |  1.4400  |  3.2500  |  1.3300  |\n   145  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |  Roger-Vasselin E.  |  Bemelmans R.  |    101  |    114  |   539  |   486  |   6  |   3  |   6  |   7  |   2  |   6  |   7  |   5  |  11  |   9  |      3  |      2  |  Completed  |  2.2000  |  1.6100  |  2.0500  |  1.7500  |  2.0000  |  1.7300  |  2.1100  |  1.8100  |  2.0000  |  1.8000  |  2.3800  |  1.8100  |  2.0700  |  1.7200  |\n   147  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |        Verdasco F.  |     Goffin D.  |     24  |     49  |  1445  |   843  |   6  |   3  |   3  |   6  |   4  |   6  |   6  |   3  |   6  |   4  |      3  |      2  |  Completed  |  1.6100  |  2.2000  |  1.6500  |  2.2000  |  1.6200  |  2.2500  |  1.7200  |  2.2500  |  1.5700  |  2.3800  |  1.8000  |  2.3800  |  1.6600  |  2.1900  |\n   153  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |         Youzhny M.  |      Ebden M.  |     25  |    108  |  1335  |   509  |   4  |   6  |   6  |   7  |   6  |   2  |   7  |   6  |   6  |   3  |      3  |      2  |  Completed  |  1.1600  |  5.0000  |  1.2000  |  4.4000  |  1.1700  |  5.0000  |  1.1900  |  5.3900  |  1.1700  |  5.0000  |  1.2000  |  6.7000  |  1.1700  |  4.9300  |\n   156  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |           Baker B.  |  Bogomolov A.  |     57  |    133  |   776  |   422  |   7  |   6  |   6  |   3  |   6  |   7  |   3  |   6  |   6  |   2  |      3  |      2  |  Completed  |  1.7200  |  2.0000  |  1.6500  |  2.2000  |  1.6700  |  2.1000  |  1.8500  |  2.0700  |  1.6200  |  2.2500  |  1.8500  |  2.2500  |  1.7100  |  2.0800  |\n   161  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |             Ito T.  |    Millman J.  |     84  |    184  |   633  |   279  |   6  |   4  |   6  |   4  |   3  |   6  |   0  |   6  |   7  |   5  |      3  |      2  |  Completed  |  3.7500  |  1.2500  |  3.4000  |  1.3000  |  3.0000  |  1.3600  |  3.8900  |  1.3000  |  3.0000  |  1.3600  |  4.0500  |  1.3600  |  3.4800  |  1.3000  |\n   165  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |        Stepanek R.  |    Troicki V.  |     34  |     37  |  1090  |  1030  |   5  |   7  |   4  |   6  |   6  |   3  |   6  |   3  |   7  |   5  |      3  |      2  |  Completed  |  2.5000  |  1.5000  |  2.5500  |  1.5000  |  2.6200  |  1.4400  |  2.6200  |  1.5600  |  2.2000  |  1.6700  |  2.8800  |  1.5600  |  2.5900  |  1.4900  |\n   166  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |        Bautista R.  |    Fognini F.  |     56  |     47  |   786  |   880  |   6  |   0  |   2  |   6  |   6  |   4  |   3  |   6  |   6  |   1  |      3  |      2  |  Completed  |  2.0000  |  1.7200  |  1.9800  |  1.8000  |  2.0000  |  1.7300  |  2.0300  |  1.8700  |  2.0000  |  1.8000  |  2.1800  |  1.8900  |  2.0000  |  1.7700  |\n   170  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |       Baghdatis M.  |      Ramos A.  |     35  |     51  |  1070  |   830  |   6  |   7  |   7  |   6  |   6  |   4  |   3  |   6  |   6  |   3  |      3  |      2  |  Completed  |  1.0800  |  8.0000  |  1.0700  |  8.0000  |  1.0700  |  8.0000  |  1.0900  |  8.9100  |  1.0800  |  7.5000  |  1.1000  |  9.4000  |  1.0800  |  7.5000  |\n\n\n\n\nSelect Columns\n\n\nThe API is completely symmetrical in the row and column dimension, so filtering a frame along the latter axis \nsimply involves a call to \nselect()\n on the column dimension operator (\nDataFrame.cols()\n). The code below \ndemonstrates column filtering where an explicit set of column names are provided rather than using a lambda expression \nas in the previous row selection example. In this scenario we wish to select all the betting odds that were \navailable on the eventual winner of the games in 2013.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\nDataFrame<Integer,String> filter = frame.cols().select(\"B365W\", \"EXW\", \"LBW\", \"PSW\" ,\"SJW\");\nfilter.out().print(10);\n\n\n\n\n\n Index  |  B365W   |   EXW    |   LBW    |   PSW    |   SJW    |\n----------------------------------------------------------------\n     0  |  1.3600  |  1.4500  |  1.4400  |  1.4700  |  1.4400  |\n     1  |  1.6100  |  1.7500  |  1.8000  |  1.8000  |  1.7300  |\n     2  |  1.2500  |  1.2500  |  1.2900  |  1.3000  |  1.3000  |\n     3  |  1.0700  |  1.0600  |  1.0800  |  1.0800  |  1.0700  |\n     4  |  1.9000  |  1.8700  |  1.9100  |  1.8800  |  1.9100  |\n     5  |  1.6100  |  1.7500  |  1.7300  |  1.7000  |  1.8000  |\n     6  |  2.2000  |  2.0800  |  1.9100  |  2.2600  |  2.0000  |\n     7  |  1.4400  |  1.5500  |  1.4400  |  1.6000  |  1.5000  |\n     8  |  3.0000  |  2.5000  |  2.3800  |  2.9300  |  2.5000  |\n     9  |  1.3600  |  1.4000  |  1.4400  |  1.3800  |  1.4000  |\n\n\n\n\nSelect Rows & Columns\n\n\nFiltering in the row and column dimension simultaneously is also possible via one of the overloaded \nselect()\n methods \navailable on the \nDataFrame\n class itself, which either takes the row and column keys to include, or lambda expressions\nto operate on the row and column entities. Parallel select is also supported by calling the \nparallel()\nmethod on the \nframe prior to \nselect()\n.\n\n\nLet's investigate which if any of the bookmakers provided more attractive odds in the various finals in 2013. \nBefore we do this, we need to standardize the betting odds so that we can not only make cross-sectional comparisons \nfor a given match, but also across different matches. To do this, we transform all the odds to a \nz-score\n \nby first subtracting the mean across the bookmakers, and then dividing by the standard deviation. The code below performs \nthis transformation.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\n\n//Array of the various bookmaker keys\nArray<String> bookies = Array.of(\"B365\", \"EX\", \"LB\", \"PS\" ,\"SJ\");\n\n//Compute z-scores for winner & loser odds independently\nStream.of(\"W\", \"L\").forEach(x -> {\n    Array<String> colNames = bookies.mapValues(v -> v.getValue() + x);\n    frame.cols().select(colNames).rows().forEach(row -> {\n        double mean = row.stats().mean();\n        double stdDev = row.stats().stdDev();\n        row.values().forEach(v -> {\n            double rawValue = v.getDouble();\n            double zScore = (rawValue - mean) / stdDev;\n            v.setDouble(zScore);\n        });\n    });\n});\n\n//Print last 14 columns and first 10 rows to std out\nframe.right(14).out().print(10, options -> {\n    options.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});\n\n\n\n\n\n Index  |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |  MaxW   |  MaxL   |  AvgW   |  AvgL   |\n---------------------------------------------------------------------------------------------------------------------------------------------------------------\n     0  |  -1.711  |   1.482  |   0.428  |  -0.593  |   0.190  |  -0.771  |   0.903  |   0.593  |   0.190  |  -0.711  |  1.470  |  3.200  |  1.420  |  2.780  |\n     1  |  -1.643  |   1.423  |   0.154  |  -0.378  |   0.796  |  -1.189  |   0.796  |   0.523  |  -0.103  |  -0.378  |  1.800  |  2.260  |  1.730  |  2.050  |\n     2  |  -1.082  |   0.532  |  -1.082  |   0.532  |   0.464  |  -0.418  |   0.850  |   0.912  |   0.850  |  -1.558  |  1.300  |  4.200  |  1.280  |  3.580  |\n     3  |  -0.239  |   0.817  |  -1.434  |  -0.077  |   0.956  |  -0.971  |   0.956  |   1.201  |  -0.239  |  -0.971  |  1.100  |  9.500  |  1.080  |  7.760  |\n     4  |   0.330  |  -0.620  |  -1.321  |   0.184  |   0.881  |  -0.620  |  -0.771  |   1.677  |   0.881  |  -0.620  |  2.050  |  2.000  |  1.880  |  1.850  |\n     5  |  -1.532  |   0.817  |   0.454  |  -0.501  |   0.170  |  -0.501  |  -0.255  |   1.278  |   1.163  |  -1.094  |  1.850  |  2.280  |  1.710  |  2.080  |\n     6  |   0.770  |  -1.305  |  -0.070  |  -0.454  |  -1.260  |   1.390  |   1.190  |  -0.028  |  -0.630  |   0.397  |  2.320  |  1.830  |  2.080  |  1.700  |\n     7  |  -0.945  |   0.952  |   0.630  |  -1.428  |  -0.945  |   0.952  |   1.346  |  -0.370  |  -0.086  |  -0.106  |  1.630  |  2.820  |  1.520  |  2.460  |\n     8  |   1.199  |  -1.615  |  -0.574  |   0.479  |  -1.000  |   0.927  |   0.950  |  -0.269  |  -0.574  |   0.479  |  3.250  |  1.530  |  2.660  |  1.470  |\n     9  |  -1.214  |   0.400  |   0.135  |  -0.355  |   1.483  |  -1.034  |  -0.539  |   1.533  |   0.135  |  -0.544  |  1.450  |  3.550  |  1.390  |  2.870  |\n\n\n\n\nNow that we have standardized all the odds, we can try and make an assessment of which bookmaker on average provided\nthe most attractive odds. It should be noted that this example is purely to demonstrate API usage, it is not intended to \nrepresent rigorous statistical analysis of this data, so take the results with a huge grain of salt.  This data could\nbe fairly noisy, so let's only consider the finals since there is likely to be a higher degree of consensus for those\nmatches. We therefore need to filter on a subset of rows (finals only) and also only include odds on the eventual winner \nfor brevity.\n\n\n\n\n\n//Select z-score odds on eventual winner across all bookmakers\nSet<String> colNames = Collect.asSet(\"B365W\", \"EXW\", \"LBW\", \"PSW\" ,\"SJW\");\nDataFrame<Integer,String> finalWinnerOdds = frame.select(\n    row -> row.getValue(\"Round\").equals(\"The Final\"),\n    col -> colNames.contains(col.key())\n);\n\n\n\n\nWe can now compute the mean of these columns, which gives us the average z-score for each bookmaker across all \nthe finals in 2013. The code below demonstrates this, and prints the results to std-out with formatting applied.\n\n\n\n\n\n//Compute the mean of these z-scores and print\nfinalWinnerOdds.cols().stats().mean().out().print(options -> {\n    options.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});\n\n\n\n\n\n Index  |  B365W   |   EXW    |   LBW    |   PSW   |   SJW    |\n---------------------------------------------------------------\n  MEAN  |  -0.259  |  -0.583  |  -0.033  |  0.953  |  -0.078  |\n\n\n\n\nThis naive analysis suggests that using a sample that includes all the ATP finals in 2013, on average, Pinnacles \nSports (PSW) provided the most attractive odds and Expekt (EX) provided the least attractive odds. These standardized\nscores represent the number of standard deviations above / below the mean, and if there was a very high degree of \nconsensus across the bookmakers, in that the absolute odds were very similar, these figures may not really tell\nus much. Again, this example is to illustrate API usage, it is not intended to draw any far reaching conclusions \non the skills of bookmakers.\n\n\nMonadic\n\n\nThe Morpheus API enables data analysis procedures to be chained together in order to facilitate powerful transformations \nto be applied with fairly little code. To demonstrate, consider an example where we wish to compute the summary statistics \n(say min, max, mean and std. deviation) on the number of games played in 5 set matches in 2013 across all the tournaments. \nThe first code example below demonstrates a longer form solution where the intermediate frames are explicitly captured as \nvariables, which for a beginner would likely be easier to follow:\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Select all 5 set matches, and the number of games won/lost in each set\nDataFrame<Integer,String> filter = frame.select(\n    row -> row.getDouble(\"Wsets\") + row.getDouble(\"Lsets\") == 5,\n    col -> col.key().matches(\"(W|L)\\\\d+\")\n);\n//Sum the rows which yields total games played per 5 set match\nDataFrame<Integer,StatType> gameCounts = filter.rows().stats().sum();\n//The game count frame is Nx1, so compute summary stats on game counts\nDataFrame<StatType,StatType> gameStats = gameCounts.cols().describe(MIN, MAX, MEAN, STD_DEV);\n//Print results to std-out\ngameStats.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});\n\n\n\n\n\n Index  |   MIN   |   MAX   |  MEAN   |  STD_DEV  |\n---------------------------------------------------\n   SUM  |  40.00  |  66.00  |  51.15  |     5.29  |\n\n\n\n\nBy leveraging the monadic nature of the Morpheus API, it's possible to perform this analysis in fewer lines of code\nas shown below. Whether this is preferable or not is another question, at least in the context of this example,\nbut suffice to say that the monadic nature of the API can facilitate brevity for trivial steps in more complicated\nanalyzes.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\nframe.select(\n    row -> row.getDouble(\"Wsets\") + row.getDouble(\"Lsets\") == 5,\n    col -> col.key().matches(\"(W|L)\\\\d+\")\n).rows().stats().sum().cols().describe(\n    MIN, MAX, MEAN, STD_DEV\n).out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});\n\n\n\n\n\n Index  |   MIN   |   MAX   |  MEAN   |  STD_DEV  |\n---------------------------------------------------\n   SUM  |  40.00  |  66.00  |  51.15  |     5.29  |\n\n\n\n\nHead & Tail\n\n\nThe \nhead()\n method on the \nDataFrame\n class provides an easy mechanism to select the first N rows as follows:\n\n\n\n\n\n//Filter on the first 5 rows\nframe.head(5).out().print();\n\n\n\n\n\n Index  |  Location  |        Tournament        |     Date     |  Series  |   Court   |  Surface  |    Round    |  Best of  |     Winner     |     Loser      |  WRank  |  LRank  |  WPts  |  LPts  |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     0  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |      Mayer F.  |    Giraldo S.  |     28  |     57  |  1215  |   778  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.3600  |  3.0000  |  1.4500  |  2.6500  |  1.4400  |  2.6200  |  1.4700  |  2.8500  |  1.4400  |  2.6300  |  1.4700  |  3.2000  |  1.4200  |  2.7800  |\n     1  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |   Nieminen J.  |  Benneteau J.  |     41  |     35  |   927  |  1075  |   6  |   3  |   2  |   6  |   6  |   1  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.8000  |  1.9100  |  1.8000  |  2.1000  |  1.7300  |  2.0000  |  1.8000  |  2.2600  |  1.7300  |  2.0500  |\n     2  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |  Nishikori K.  |  Matosevic M.  |     19  |     49  |  1830  |   845  |   7  |   5  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.2500  |  3.7500  |  1.2500  |  3.7500  |  1.2900  |  3.5000  |  1.3000  |  3.8500  |  1.3000  |  3.2000  |  1.3000  |  4.2000  |  1.2800  |  3.5800  |\n     3  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |  Baghdatis M.  |   Mitchell B.  |     36  |    326  |  1070  |   137  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.0700  |  9.0000  |  1.0600  |  8.0000  |  1.0800  |  7.0000  |  1.0800  |  9.4300  |  1.0700  |  7.0000  |  1.1000  |  9.5000  |  1.0800  |  7.7600  |\n     4  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Istomin D.  |     Klizan M.  |     43  |     30  |   897  |  1175  |   6  |   1  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.9000  |  1.8000  |  1.8700  |  1.8700  |  1.9100  |  1.8000  |  1.8800  |  2.0000  |  1.9100  |  1.8000  |  2.0500  |  2.0000  |  1.8800  |  1.8500  |\n\n\n\n\nThe \ntail()\n method on the \nDataFrame\n class provides an easy mechanism to select the last N rows as follows:\n\n\n\n\n\n//Filter on the last 5 rows\nframe.tail(5).out().print();\n\n\n\n\n\n Index  |  Location  |  Tournament   |     Date     |    Series     |  Court   |  Surface  |     Round     |  Best of  |    Winner     |      Loser       |  WRank  |  LRank  |  WPts   |  LPts   |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |   B365L   |   EXW    |   EXL    |   LBW    |    LBL    |   PSW    |    PSL    |   SJW    |   SJL    |   MaxW   |   MaxL    |   AvgW   |   AvgL   |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  2626  |    London  |  Masters Cup  |  2013-11-09  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |   Federer R.  |  Del Potro J.M.  |      7  |      5  |   3805  |   5055  |   4  |   6  |   7  |   6  |   7  |   5  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  2.0000  |   1.8000  |  2.0000  |  1.8000  |  2.0000  |   1.8000  |  2.0700  |   1.8500  |  2.0000  |  1.8000  |  2.1000  |   1.9500  |  1.9800  |  1.8100  |\n  2627  |    London  |  Masters Cup  |  2013-11-09  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |  Djokovic N.  |      Gasquet R.  |      2  |      9  |  10610  |   3300  |   7  |   6  |   4  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.0600  |  10.0000  |  1.0500  |  9.0000  |  1.0500  |  10.0000  |  1.0700  |  10.7500  |  1.0500  |  9.0000  |  1.1000  |  10.7500  |  1.0600  |  9.2200  |\n  2628  |    London  |  Masters Cup  |  2013-11-10  |  Masters Cup  |  Indoor  |     Hard  |   Semifinals  |        3  |     Nadal R.  |      Federer R.  |      1  |      7  |  12030  |   3805  |   7  |   5  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.3000  |   3.5000  |  1.3300  |  3.2000  |  1.3000  |   3.5000  |  1.3400  |   3.6700  |  1.3300  |  3.4000  |  1.3700  |   4.0500  |  1.3300  |  3.3400  |\n  2629  |    London  |  Masters Cup  |  2013-11-10  |  Masters Cup  |  Indoor  |     Hard  |   Semifinals  |        3  |  Djokovic N.  |     Wawrinka S.  |      2  |      8  |  10610  |   3330  |   6  |   3  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.1400  |   5.5000  |  1.1300  |  5.5000  |  1.1400  |   5.5000  |  1.1900  |   5.6100  |  1.1500  |  5.5000  |  1.1900  |   6.3000  |  1.1600  |  5.2500  |\n  2630  |    London  |  Masters Cup  |  2013-11-11  |  Masters Cup  |  Indoor  |     Hard  |    The Final  |        3  |  Djokovic N.  |        Nadal R.  |      2  |      1  |  10610  |  12030  |   6  |   3  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.5700  |   2.3700  |  1.5500  |  2.4000  |  1.5700  |   2.3800  |  1.5800  |   2.5800  |  1.5700  |  2.3800  |  1.6500  |   2.5800  |  1.5700  |  2.4300  |\n\n\n\n\nRight & Left\n\n\nThe \nleft()\n method is an analog to the \nhead()\n method but selects the first N columns as follows:\n\n\n\n\n\n//Filter on the first 5 columns\nframe.left(5).out().print();\n\n\n\n\n\n Index  |  Location  |        Tournament        |     Date     |  Series  |   Court   |\n---------------------------------------------------------------------------------------\n     0  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |\n     1  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |\n     2  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |\n     3  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |\n     4  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |\n     5  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |\n     6  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |\n     7  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |\n     8  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |\n     9  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |\n\n\n\n\nThe \nright()\n method is an analog to the \ntail()\n method but selects the last N columns as follows:\n\n\n\n\n\n//Filter on the last 14 columns\nframe.right(14).out().print();\n\n\n\n\n\n Index  |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     0  |  1.3600  |  3.0000  |  1.4500  |  2.6500  |  1.4400  |  2.6200  |  1.4700  |  2.8500  |  1.4400  |  2.6300  |  1.4700  |  3.2000  |  1.4200  |  2.7800  |\n     1  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.8000  |  1.9100  |  1.8000  |  2.1000  |  1.7300  |  2.0000  |  1.8000  |  2.2600  |  1.7300  |  2.0500  |\n     2  |  1.2500  |  3.7500  |  1.2500  |  3.7500  |  1.2900  |  3.5000  |  1.3000  |  3.8500  |  1.3000  |  3.2000  |  1.3000  |  4.2000  |  1.2800  |  3.5800  |\n     3  |  1.0700  |  9.0000  |  1.0600  |  8.0000  |  1.0800  |  7.0000  |  1.0800  |  9.4300  |  1.0700  |  7.0000  |  1.1000  |  9.5000  |  1.0800  |  7.7600  |\n     4  |  1.9000  |  1.8000  |  1.8700  |  1.8700  |  1.9100  |  1.8000  |  1.8800  |  2.0000  |  1.9100  |  1.8000  |  2.0500  |  2.0000  |  1.8800  |  1.8500  |\n     5  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.7300  |  2.0000  |  1.7000  |  2.2700  |  1.8000  |  1.9100  |  1.8500  |  2.2800  |  1.7100  |  2.0800  |\n     6  |  2.2000  |  1.6100  |  2.0800  |  1.6700  |  1.9100  |  1.8000  |  2.2600  |  1.7000  |  2.0000  |  1.7300  |  2.3200  |  1.8300  |  2.0800  |  1.7000  |\n     7  |  1.4400  |  2.6200  |  1.5500  |  2.3500  |  1.4400  |  2.6200  |  1.6000  |  2.4700  |  1.5000  |  2.5000  |  1.6300  |  2.8200  |  1.5200  |  2.4600  |\n     8  |  3.0000  |  1.3600  |  2.5000  |  1.5000  |  2.3800  |  1.5300  |  2.9300  |  1.4500  |  2.5000  |  1.5000  |  3.2500  |  1.5300  |  2.6600  |  1.4700  |\n     9  |  1.3600  |  3.0000  |  1.4000  |  2.8000  |  1.4400  |  2.6200  |  1.3800  |  3.3000  |  1.4000  |  2.7500  |  1.4500  |  3.5500  |  1.3900  |  2.8700  |\n\n\n\n\nIt is sometimes useful to combine these functions to select parts of the frame. For example, consider a scenario\nwhere we wish to select all the betting odds, but excluding the maximum and averages that are represented by the\nlast 4 columns of the dataset. This can be achieved with a combined \nright()\n and \nleft()\n operation as follows:\n\n\n\n\n\n//Filter on all betting ods, excluding max & averages\nframe.right(14).left(10).out().print();\n\n\n\n\n\n Index  |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |\n-----------------------------------------------------------------------------------------------------------------------\n     0  |  1.3600  |  3.0000  |  1.4500  |  2.6500  |  1.4400  |  2.6200  |  1.4700  |  2.8500  |  1.4400  |  2.6300  |\n     1  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.8000  |  1.9100  |  1.8000  |  2.1000  |  1.7300  |  2.0000  |\n     2  |  1.2500  |  3.7500  |  1.2500  |  3.7500  |  1.2900  |  3.5000  |  1.3000  |  3.8500  |  1.3000  |  3.2000  |\n     3  |  1.0700  |  9.0000  |  1.0600  |  8.0000  |  1.0800  |  7.0000  |  1.0800  |  9.4300  |  1.0700  |  7.0000  |\n     4  |  1.9000  |  1.8000  |  1.8700  |  1.8700  |  1.9100  |  1.8000  |  1.8800  |  2.0000  |  1.9100  |  1.8000  |\n     5  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.7300  |  2.0000  |  1.7000  |  2.2700  |  1.8000  |  1.9100  |\n     6  |  2.2000  |  1.6100  |  2.0800  |  1.6700  |  1.9100  |  1.8000  |  2.2600  |  1.7000  |  2.0000  |  1.7300  |\n     7  |  1.4400  |  2.6200  |  1.5500  |  2.3500  |  1.4400  |  2.6200  |  1.6000  |  2.4700  |  1.5000  |  2.5000  |\n     8  |  3.0000  |  1.3600  |  2.5000  |  1.5000  |  2.3800  |  1.5300  |  2.9300  |  1.4500  |  2.5000  |  1.5000  |\n     9  |  1.3600  |  3.0000  |  1.4000  |  2.8000  |  1.4400  |  2.6200  |  1.3800  |  3.3000  |  1.4000  |  2.7500  |\n\n\n\n\nFilter Method\n\n\nThe \nDataFrame.rows()\n and \nDataFrame.cols()\n methods return a versatile interface to operate on the row\nand column dimension respectively. The examples above demonstrate how to use the \nselect()\n method to\ncreate a \nDataFrame\n filter to operate on a subset of the data. A \nfilter()\n method also exists which\nprovides a shortcut to more directly operate on a subset of rows or columns without creating the \nintermediate filter frame.\n\n\nThe example below demonstrates how to iterate over a subset of rows that represent an upset in a finals match \nin 2013 according to the average bookmaker odds. Higher odds represent a less likely outcome, so if the\naverage odds on the eventual winner of a game were greater than those for the eventual loser, that \nclassifies\n\nas an upset because the favourite did not prevail. Concurrent iteration is also supported with a call to \n\nparallel()\n, however then order is not guaranteed which may or may not be important.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\nframe.rows().filter(row -> {\n    String round = row.getValue(\"Round\");\n    double avgW = row.getDouble(\"AvgW\");\n    double avgL = row.getDouble(\"AvgL\");\n    return round.equals(\"The Final\") && avgW > avgL;\n}).forEach(row -> {\n    String loser = row.getValue(\"Loser\");\n    String winner = row.getValue(\"Winner\");\n    String tournament = row.getValue(\"Tournament\");\n    LocalDate date = row.getValue(\"Date\");\n    System.out.printf(\"Finals upset when %s beat %s at %s on %s\\n\", winner, loser, tournament, date);\n});\n\n\n\n\n\nFinals upset when Gasquet R. beat Davydenko N. at Qatar Exxon Mobil Open on 2013-01-05\nFinals upset when Zeballos H. beat Nadal R. at VTR Open on 2013-02-10\nFinals upset when Tsonga J.W. beat Berdych T. at Open 13 on 2013-02-24\nFinals upset when Robredo T. beat Anderson K. at Grand Prix Hassan II on 2013-04-14\nFinals upset when Isner J. beat Almagro N. at U.S. Men's Clay Court Championships on 2013-04-14\nFinals upset when Djokovic N. beat Nadal R. at Monte Carlo Masters on 2013-04-21\nFinals upset when Wawrinka S. beat Ferrer D. at Portugal Open on 2013-05-05\nFinals upset when Montanes A. beat Monfils G. at Open de Nice C\ufffdte d\ufffdAzur on 2013-05-25\nFinals upset when Mahut N. beat Wawrinka S. at Topshelf Open on 2013-06-22\nFinals upset when Murray A. beat Djokovic N. at Wimbledon on 2013-07-07\nFinals upset when Berlocq C. beat Verdasco F. at SkiStar Swedish Open on 2013-07-14\nFinals upset when Fognini F. beat Kohlschreiber P. at Mercedes Cup on 2013-07-14\nFinals upset when Karlovic I. beat Falla A. at Claro Open Colombia on 2013-07-21\nFinals upset when Robredo T. beat Fognini F. at ATP Vegeta Croatia Open on 2013-07-28\nFinals upset when Granollers M. beat Monaco J. at Bet-At-Home Cup on 2013-08-03\nFinals upset when Melzer J. beat Monfils G. at Winston-Salem Open at Wake Forest University on 2013-08-24\nFinals upset when Simon G. beat Tsonga J.W. at Open de Moselle on 2013-09-22\nFinals upset when Raonic M. beat Berdych T. at Thailand Open on 2013-09-29\nFinals upset when Sousa J. beat Benneteau J. at Malaysian Open on 2013-09-29\nFinals upset when Dimitrov G. beat Ferrer D. at Stockholm Open on 2013-10-20\nFinals upset when Youzhny M. beat Ferrer D. at Valencia Open 500 on 2013-10-27\n\n\n\n\nAgain, to re-iterate the symmetrical nature of the API, consider the following where we iterate over a\nsubset of the columns to discover what the minimum and maximum betting odds that were available on\nthe eventual winner of each match by the 5 different book makers.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\nframe.cols().filter(\"B365W\", \"EXW\", \"LBW\", \"PSW\" ,\"SJW\").forEach(column -> {\n    Optional<Bounds<Double>> bounds = column.bounds();\n    bounds.ifPresent(b -> {\n        System.out.println(column.key() + \" max odds=\" + b.upper() + \", min odds=\" + b.lower());\n    });\n});\n\n\n\n\n\nB365W max odds=29.0, min odds=1.0\nEXW max odds=20.0, min odds=1.01\nLBW max odds=26.0, min odds=1.0\nPSW max odds=46.0, min odds=1.01\nSJW max odds=17.0, min odds=1.01\n\n\n\n\nImmutability\n\n\nIn the introduction, it was suggested that a \nDataFrame\n that represents a filter on another frame is structurally \nimmutable in some cases, in that additional columns cannot always be added (in all cases, additional rows cannot be\nadded to a filter). Data mutability on the other hand is supported on a filter, as illustrated by the example where \nwe transformed absolute betting odds to a Z-score.  The structural immutability caveats are in explained in this section.\n\n\nColumn Store\n\n\nA \nDataFrame\n is by default a column store, where each column is represented by a Morpheus array. When filtering such \na frame, it is not possible to add rows to the filter frame, but adding columns is supported. The reason for this \nrestriction is because the filter frame shares its content with the parent it was created from, so adding rows would \nresult in also modifying the column arrays of the parent frame, which would lead to all sorts of issues. The filter \nframe does have a completely independent column axis however, so it is possible to add additional columns without this \nhaving any impact on the parent.\n\n\nThe following example demonstrates how an additional column can be added to the filter frame after we have selected all \nmatches in which Novak Djokovic was the victor. Here we add an additional column to store the the total number of games\nthat Novak won in each match.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Select rows where Djokovic was the victor\nDataFrame<Integer,String> filter = frame.rows().select(row -> row.getValue(\"Winner\").equals(\"Djokovic N.\"));\n//Add a column to the filter and seed with number of games won by Djokovic\nArray<String> colNames = Range.of(1, 6).map(i -> \"W\" + i).toArray();\n//Add new column to capture total number of games won by novak\nfilter.cols().add(\"WonCount\", Double.class, value -> {\n    final DataFrameRow<Integer,String> row = value.row(); //Access row associated with this value\n    return colNames.mapToDoubles(v -> row.getDouble(v.getValue())).stats().sum().doubleValue();\n});\n\n//Print 10x10 section of right most columns\nfilter.right(10).out().print(10);\n\n\n\n\nThe column on the far right is the newly added column initialized with the number of games won by Novak.\n\n\n\n Index  |    LBL    |   PSW    |    PSL    |   SJW    |    SJL    |   MaxW   |   MaxL    |   AvgW   |   AvgL    |  WonCount  |\n------------------------------------------------------------------------------------------------------------------------------\n   152  |  13.0000  |  1.0100  |  41.0000  |  1.0100  |  19.0000  |  1.0200  |  46.0000  |  1.0100  |  21.7000  |   19.0000  |\n   218  |  21.0000  |  1.0100  |  31.0000  |  1.0100  |  19.0000  |  1.0200  |  51.0000  |  1.0100  |  23.3800  |   18.0000  |\n   236  |  17.0000  |  1.0100  |  41.0000  |  1.0100  |  13.0000  |  1.0200  |  63.2500  |  1.0100  |  24.9400  |   19.0000  |\n   254  |  13.0000  |  1.0200  |  21.5000  |  1.0200  |  13.0000  |  1.0300  |  29.0000  |  1.0200  |  15.9400  |   32.0000  |\n   260  |   5.0000  |  1.2200  |   4.9200  |  1.1800  |   5.2500  |  1.2300  |   5.3000  |  1.1800  |   4.7300  |   22.0000  |\n   263  |   6.5000  |  1.1200  |   7.9400  |  1.1000  |   8.0000  |  1.1400  |   9.4000  |  1.1000  |   6.9100  |   18.0000  |\n   265  |   2.5000  |  1.5400  |   2.7400  |  1.5300  |   2.6300  |  1.5700  |   2.9000  |  1.5000  |   2.6100  |   25.0000  |\n   597  |  17.0000  |  1.0200  |  21.9700  |  1.0300  |  12.0000  |  1.0300  |  31.0000  |  1.0200  |  16.5400  |   12.0000  |\n   606  |  17.0000  |  1.0100  |  29.1600  |  1.0100  |  15.0000  |  1.0400  |  46.0000  |  1.0100  |  20.0900  |   13.0000  |\n   609  |   9.0000  |  1.0300  |  19.0000  |  1.0400  |  10.0000  |  1.0500  |  24.0000  |  1.0300  |  13.4300  |   12.0000  |\n\n\n\n\nIf we print the right most columns of the original frame, the `WonCount\" column is not present.\n\n\n\n\n\n//Print 10x10 section of right most columns of original frame\nframe.right(10).out().print(10);\n\n\n\n\n\n Index  |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n-----------------------------------------------------------------------------------------------------------------------\n     0  |  1.4400  |  2.6200  |  1.4700  |  2.8500  |  1.4400  |  2.6300  |  1.4700  |  3.2000  |  1.4200  |  2.7800  |\n     1  |  1.8000  |  1.9100  |  1.8000  |  2.1000  |  1.7300  |  2.0000  |  1.8000  |  2.2600  |  1.7300  |  2.0500  |\n     2  |  1.2900  |  3.5000  |  1.3000  |  3.8500  |  1.3000  |  3.2000  |  1.3000  |  4.2000  |  1.2800  |  3.5800  |\n     3  |  1.0800  |  7.0000  |  1.0800  |  9.4300  |  1.0700  |  7.0000  |  1.1000  |  9.5000  |  1.0800  |  7.7600  |\n     4  |  1.9100  |  1.8000  |  1.8800  |  2.0000  |  1.9100  |  1.8000  |  2.0500  |  2.0000  |  1.8800  |  1.8500  |\n     5  |  1.7300  |  2.0000  |  1.7000  |  2.2700  |  1.8000  |  1.9100  |  1.8500  |  2.2800  |  1.7100  |  2.0800  |\n     6  |  1.9100  |  1.8000  |  2.2600  |  1.7000  |  2.0000  |  1.7300  |  2.3200  |  1.8300  |  2.0800  |  1.7000  |\n     7  |  1.4400  |  2.6200  |  1.6000  |  2.4700  |  1.5000  |  2.5000  |  1.6300  |  2.8200  |  1.5200  |  2.4600  |\n     8  |  2.3800  |  1.5300  |  2.9300  |  1.4500  |  2.5000  |  1.5000  |  3.2500  |  1.5300  |  2.6600  |  1.4700  |\n     9  |  1.4400  |  2.6200  |  1.3800  |  3.3000  |  1.4000  |  2.7500  |  1.4500  |  3.5500  |  1.3900  |  2.8700  |\n\n\n\n\nIn contrast, if we attempt to add rows to the filter frame, we will get a \nDataFrameException\n because modifying\nthe shared column arrays would impact the parent from which the filter was created. Let's attempt to add 5 more \nrows as follows:\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Select rows where Djokovic was the victor\nDataFrame<Integer,String> filter = frame.rows().select(row -> row.getValue(\"Winner\").equals(\"Djokovic N.\"));\n//Define a range of 10 additional keys\nRange<Integer> rowKeys = Range.of(frame.rowCount(), frame.rowCount()+5);\n//Try add rows for new row keys\nfilter.rows().addAll(rowKeys);\n\n\n\n\n\ncom.zavtech.morpheus.frame.DataFrameException: Cannot add keys to a sliced axis of a DataFrame\n    at com.zavtech.morpheus.reference.XDataFrameContent.addRows(XDataFrameContent.java:300)\n    at com.zavtech.morpheus.reference.XDataFrameRows.addAll(XDataFrameRows.java:96)\n    at com.zavtech.morpheus.reference.XDataFrameRows.addAll(XDataFrameRows.java:89)\n\n\n\n\nIf we create a deep copy of the filter frame, then we end up with a completely new data structure\ncontaining just the rows of the filter, so we can now add rows as usual.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Select rows where Djokovic was the victor\nDataFrame<Integer,String> filter = frame.rows().select(row -> row.getValue(\"Winner\").equals(\"Djokovic N.\"));\n//Create a deep copy of the filter frame\nDataFrame<Integer,String> copy = filter.copy();\n//Define a range of 5 additional keys\nRange<Integer> rowKeys = Range.of(frame.rowCount(), frame.rowCount()+5);\n//Add 5 new rows to the copy\ncopy.rows().addAll(rowKeys);\n//Print last 10 rows\ncopy.tail(10).out().print();\n\n\n\n\n\n Index  |  Location  |  Tournament   |     Date     |    Series     |  Court   |  Surface  |     Round     |  Best of  |    Winner     |      Loser       |  WRank  |  LRank  |  WPts   |  LPts   |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |   B365L   |   EXW    |   EXL    |   LBW    |    LBL    |   PSW    |    PSL    |   SJW    |   SJL    |   MaxW   |   MaxL    |   AvgW   |   AvgL   |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  2619  |    London  |  Masters Cup  |  2013-11-05  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |  Djokovic N.  |      Federer R.  |      2  |      7  |  10610  |   3805  |   6  |   4  |   6  |   7  |   6  |   2  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.2800  |   3.7500  |  1.2800  |  3.5000  |  1.2900  |   3.7500  |  1.3000  |   3.9300  |  1.2900  |  3.7500  |  1.3300  |   4.0000  |  1.2900  |  3.6000  |\n  2623  |    London  |  Masters Cup  |  2013-11-07  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |  Djokovic N.  |  Del Potro J.M.  |      2  |      5  |  10610  |   5055  |   6  |   3  |   3  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.2800  |   3.7500  |  1.2800  |  3.5000  |  1.3000  |   3.5000  |  1.3000  |   3.8900  |  1.2900  |  3.7500  |  1.3300  |   4.0500  |  1.2800  |  3.6100  |\n  2627  |    London  |  Masters Cup  |  2013-11-09  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |  Djokovic N.  |      Gasquet R.  |      2  |      9  |  10610  |   3300  |   7  |   6  |   4  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.0600  |  10.0000  |  1.0500  |  9.0000  |  1.0500  |  10.0000  |  1.0700  |  10.7500  |  1.0500  |  9.0000  |  1.1000  |  10.7500  |  1.0600  |  9.2200  |\n  2629  |    London  |  Masters Cup  |  2013-11-10  |  Masters Cup  |  Indoor  |     Hard  |   Semifinals  |        3  |  Djokovic N.  |     Wawrinka S.  |      2  |      8  |  10610  |   3330  |   6  |   3  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.1400  |   5.5000  |  1.1300  |  5.5000  |  1.1400  |   5.5000  |  1.1900  |   5.6100  |  1.1500  |  5.5000  |  1.1900  |   6.3000  |  1.1600  |  5.2500  |\n  2630  |    London  |  Masters Cup  |  2013-11-11  |  Masters Cup  |  Indoor  |     Hard  |    The Final  |        3  |  Djokovic N.  |        Nadal R.  |      2  |      1  |  10610  |  12030  |   6  |   3  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.5700  |   2.3700  |  1.5500  |  2.4000  |  1.5700  |   2.3800  |  1.5800  |   2.5800  |  1.5700  |  2.3800  |  1.6500  |   2.5800  |  1.5700  |  2.4300  |\n  2631  |      null  |         null  |        null  |         null  |    null  |     null  |         null  |        0  |         null  |            null  |      0  |      0  |      0  |      0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |      0  |      0  |       null  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |     NaN  |\n  2632  |      null  |         null  |        null  |         null  |    null  |     null  |         null  |        0  |         null  |            null  |      0  |      0  |      0  |      0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |      0  |      0  |       null  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |     NaN  |\n  2633  |      null  |         null  |        null  |         null  |    null  |     null  |         null  |        0  |         null  |            null  |      0  |      0  |      0  |      0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |      0  |      0  |       null  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |     NaN  |\n  2634  |      null  |         null  |        null  |         null  |    null  |     null  |         null  |        0  |         null  |            null  |      0  |      0  |      0  |      0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |      0  |      0  |       null  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |     NaN  |\n  2635  |      null  |         null  |        null  |         null  |    null  |     null  |         null  |        0  |         null  |            null  |      0  |      0  |      0  |      0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |      0  |      0  |       null  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |     NaN  |\n\n\n\n\nTranspose\n\n\nThere is one additional caveat with respect to structural immutability which involves the transpose\nof a \nDataFrame\n. While a frame is by design a column store, when you call \ntranspose()\n to switch\nthe row and column axis, it effectively becomes row major. This implementation choice was made\nso that creating a transpose frame is extremely cheap, and requires almost no additional memory.\n\n\nA transposed \nDataFrame\n is structurally immutable in both the row and column dimensions, which \nis a design choice that may be relaxed at some point in the future. If structural changes to a \ntranspose is a requirement, a copy will need to be created, which will then yield a column store\ndata structure with the data still in its transposed state.\n\n\nIn the case of a frame with mixed column types, like the ATP match results, one should be aware that \ntransposing it and then creating a copy will result in a frame that is much less memory efficient \nthan the non-transposed original. The reason for this is that the copied transpose frame will have\ncolumn arrays of type \nObject\n in order to cater to the non-homogeneous types for each column, and\nthereby introduce object header overhead as well as significantly higher garbage collection times.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Transpose the original DataFrame\nDataFrame<String,Integer> transpose = frame.transpose();\n//Print 10x5 section of the frame\ntranspose.left(5).out().print();\n//Attempt to add a column, which we know will fail\ntranspose.cols().add(transpose.colCount()+1, Double.class);\n\n\n\n\n\n   Index     |            0             |            1             |            2             |            3             |            4             |\n-----------------------------------------------------------------------------------------------------------------------------------------------------\n   Location  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |\n Tournament  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |\n       Date  |              2012-12-31  |              2012-12-31  |              2012-12-31  |              2012-12-31  |              2013-01-01  |\n     Series  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |\n      Court  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |\n    Surface  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |\n      Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |\n    Best of  |                       3  |                       3  |                       3  |                       3  |                       3  |\n     Winner  |                Mayer F.  |             Nieminen J.  |            Nishikori K.  |            Baghdatis M.  |              Istomin D.  |\n      Loser  |              Giraldo S.  |            Benneteau J.  |            Matosevic M.  |             Mitchell B.  |               Klizan M.  |\n\n\n\n\n\ncom.zavtech.morpheus.frame.DataFrameException: This DataFrame is configured as a row store, transpose() first\n    at com.zavtech.morpheus.reference.XDataFrameContent.addColumn(XDataFrameContent.java:326)\n    at com.zavtech.morpheus.reference.XDataFrameColumns.lambda$null$14(XDataFrameColumns.java:216)\n    at java.util.LinkedHashMap$LinkedKeySet.forEach(LinkedHashMap.java:555)\n    at com.zavtech.morpheus.reference.XDataFrameColumns.lambda$addColumns$15(XDataFrameColumns.java:213)\n    at java.util.function.Function.lambda$andThen$6(Function.java:88)\n    at java.util.function.Function.lambda$andThen$6(Function.java:88)\n    at com.zavtech.morpheus.reference.XDataFrameColumns.add(XDataFrameColumns.java:88)",
            "title": "Filtering"
        },
        {
            "location": "/frame/filtering/#filtering",
            "text": "",
            "title": "Filtering"
        },
        {
            "location": "/frame/filtering/#introduction",
            "text": "Filtering a  DataFrame  along the row and/or column dimensions is a common requirement in order to easily \nanalyze or operate on a subset of the data.  Applying a filter is a fairly cheap operation because the resulting \nframe is merely a projection onto its parent, as opposed to being a deep copy. This implies that very little \nadditional memory is required to create filters or slices, and it also means that any data modifications to the \nfilter are in effect applied to the parent. Of course, if a detached copy of the data subset is required, simply \ncalling  DataFrame.copy()  on the filter frame will yield the desired result.   It should also be noted that while the contents of a filter frame can be modified, the structure of the filter is \nimmutable in the row dimension, but can be expanded in the column dimension (see section below titled Immutability \nthat explains these restrictions in more detail). It is of course permissible to further filter an already filtered \nframe.",
            "title": "Introduction"
        },
        {
            "location": "/frame/filtering/#example-data",
            "text": "There is a large amount of professional tennis match data available  here  for both \nthe men's (ATP) and women's (WTA) tours. The code examples in this section operate on a dataset containing all match \nresults for the 2013 ATP tour, which can be loaded using the code below. The dataset contains 2631 rows and 41 \ncolumns, including player statistics, match play statistics, as well as betting odds from various bookies in the UK. \nMore information on the exact content of this file can be located  here   /**\n * Returns the ATP match results for the year specified\n * @param year      the year for ATP results\n * @return          the ATP match results\n */\nstatic DataFrame<Integer,String> loadTennisMatchData(int year) {\n    final DateTimeFormatter dateFormat = DateTimeFormatter.ofPattern(\"dd/MM/yy\");\n    return DataFrame.read().csv(options -> {\n        options.setHeader(true);\n        options.setResource(\"http://www.zavtech.com/data/tennis/atp/atp-\" + year + \".csv\");\n        options.setParser(\"Date\", Parser.ofLocalDate(dateFormat));\n        options.setExcludeColumns(\"ATP\");\n    });\n}  \nIndex  |  Location  |        Tournament        |     Date     |  Series  |   Court   |  Surface  |    Round    |  Best of  |     Winner     |     Loser      |  WRank  |  LRank  |  WPts  |  LPts  |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W  |  B365L  |  EXW   |  EXL   |  LBW   |  LBL   |  PSW   |  PSL   |  SJW   |  SJL   |  MaxW  |  MaxL  |  AvgW  |  AvgL  |\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    0  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |      Mayer F.  |    Giraldo S.  |     28  |     57  |  1215  |   778  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |   1.36  |    3.0  |  1.45  |  2.65  |  1.44  |  2.62  |  1.47  |  2.85  |  1.44  |  2.63  |  1.47  |   3.2  |  1.42  |  2.78  |\n    1  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |   Nieminen J.  |  Benneteau J.  |     41  |     35  |   927  |  1075  |   6  |   3  |   2  |   6  |   6  |   1  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |   1.61  |    2.2  |  1.75  |   2.0  |   1.8  |  1.91  |   1.8  |   2.1  |  1.73  |   2.0  |   1.8  |  2.26  |  1.73  |  2.05  |\n    2  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |  Nishikori K.  |  Matosevic M.  |     19  |     49  |  1830  |   845  |   7  |   5  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |   1.25  |   3.75  |  1.25  |  3.75  |  1.29  |   3.5  |   1.3  |  3.85  |   1.3  |   3.2  |   1.3  |   4.2  |  1.28  |  3.58  |\n    3  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |  Baghdatis M.  |   Mitchell B.  |     36  |    326  |  1070  |   137  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |   1.07  |    9.0  |  1.06  |   8.0  |  1.08  |   7.0  |  1.08  |  9.43  |  1.07  |   7.0  |   1.1  |   9.5  |  1.08  |  7.76  |\n    4  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Istomin D.  |     Klizan M.  |     43  |     30  |   897  |  1175  |   6  |   1  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |    1.9  |    1.8  |  1.87  |  1.87  |  1.91  |   1.8  |  1.88  |   2.0  |  1.91  |   1.8  |  2.05  |   2.0  |  1.88  |  1.85  |\n    5  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Millman J.  |        Ito T.  |    199  |     79  |   239  |   655  |   6  |   4  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |   1.61  |    2.2  |  1.75  |   2.0  |  1.73  |   2.0  |   1.7  |  2.27  |   1.8  |  1.91  |  1.85  |  2.28  |  1.71  |  2.08  |\n    6  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |      Falla A.  |     Levine J.  |     54  |    104  |   809  |   530  |   6  |   1  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |    2.2  |   1.61  |  2.08  |  1.67  |  1.91  |   1.8  |  2.26  |   1.7  |   2.0  |  1.73  |  2.32  |  1.83  |  2.08  |   1.7  |\n    7  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |     Melzer J.  |      Kudla D.  |     29  |    137  |  1177  |   402  |   2  |   6  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |   1.44  |   2.62  |  1.55  |  2.35  |  1.44  |  2.62  |   1.6  |  2.47  |   1.5  |   2.5  |  1.63  |  2.82  |  1.52  |  2.46  |\n    8  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Robredo T.  |   Harrison R.  |    114  |     69  |   495  |   710  |   6  |   4  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |    3.0  |   1.36  |   2.5  |   1.5  |  2.38  |  1.53  |  2.93  |  1.45  |   2.5  |   1.5  |  3.25  |  1.53  |  2.66  |  1.47  |\n    9  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |   Dimitrov G.  |      Baker B.  |     48  |     61  |   866  |   756  |   6  |   3  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |   1.36  |    3.0  |   1.4  |   2.8  |  1.44  |  2.62  |  1.38  |   3.3  |   1.4  |  2.75  |  1.45  |  3.55  |  1.39  |  2.87  |  Initializing this frame from CSV is fairly straightforward as most of the data can be coerced into the appropriate \ntype without any intervention, except for the Date column which is expressed in dd/MM/yy format and therefore requires\na custom parser. We also exclude a number of columns that are of little interest.",
            "title": "Example Data"
        },
        {
            "location": "/frame/filtering/#select-rows",
            "text": "Filtering along the row dimension of the ATP match results can be achieved by calling one of the over-loaded  select()  \nmethods available on the row dimension operator, which is accessed with a call to  DataFrame.rows() . The  select()  \nmethods either take the set of row keys to include, or more commonly a lambda expression that applies some logic on \nthe row entity. For very large frames, parallel processing is also supported with a call to  DataFrame.rows().parallel() .   The code below shows a simple way of filtering the ATP match results to only include the 5 set matches in 2013.   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\nDataFrame<Integer,String> filter = frame.rows().select(row -> {\n    final int wonSets = row.getInt(\"Wsets\");\n    final int lostSets = row.getInt(\"Lsets\");\n    return  wonSets + lostSets == 5;\n});  \n Index  |  Location   |    Tournament     |     Date     |    Series    |   Court   |  Surface  |    Round    |  Best of  |       Winner        |     Loser      |  WRank  |  LRank  |  WPts  |  LPts  |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n   139  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |         Almagro N.  |    Johnson S.  |     11  |    175  |  2515  |   302  |   7  |   5  |   6  |   7  |   6  |   2  |   6  |   7  |   6  |   2  |      3  |      2  |  Completed  |  1.1400  |  5.5000  |  1.1300  |  5.7500  |  1.1100  |  6.0000  |  1.1900  |  5.5400  |  1.1300  |  6.0000  |  1.1900  |  7.9000  |  1.1400  |  5.4900  |\n   144  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |   Gimeno-Traver D.  |      Kubot L.  |     71  |     76  |   704  |   675  |   6  |   7  |   6  |   4  |   6  |   0  |   4  |   6  |   6  |   4  |      3  |      2  |  Completed  |  3.2500  |  1.3300  |  3.1500  |  1.3500  |  3.4000  |  1.3000  |  3.9700  |  1.2900  |  2.7500  |  1.4400  |  3.9700  |  1.4400  |  3.2500  |  1.3300  |\n   145  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |  Roger-Vasselin E.  |  Bemelmans R.  |    101  |    114  |   539  |   486  |   6  |   3  |   6  |   7  |   2  |   6  |   7  |   5  |  11  |   9  |      3  |      2  |  Completed  |  2.2000  |  1.6100  |  2.0500  |  1.7500  |  2.0000  |  1.7300  |  2.1100  |  1.8100  |  2.0000  |  1.8000  |  2.3800  |  1.8100  |  2.0700  |  1.7200  |\n   147  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |        Verdasco F.  |     Goffin D.  |     24  |     49  |  1445  |   843  |   6  |   3  |   3  |   6  |   4  |   6  |   6  |   3  |   6  |   4  |      3  |      2  |  Completed  |  1.6100  |  2.2000  |  1.6500  |  2.2000  |  1.6200  |  2.2500  |  1.7200  |  2.2500  |  1.5700  |  2.3800  |  1.8000  |  2.3800  |  1.6600  |  2.1900  |\n   153  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |         Youzhny M.  |      Ebden M.  |     25  |    108  |  1335  |   509  |   4  |   6  |   6  |   7  |   6  |   2  |   7  |   6  |   6  |   3  |      3  |      2  |  Completed  |  1.1600  |  5.0000  |  1.2000  |  4.4000  |  1.1700  |  5.0000  |  1.1900  |  5.3900  |  1.1700  |  5.0000  |  1.2000  |  6.7000  |  1.1700  |  4.9300  |\n   156  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |           Baker B.  |  Bogomolov A.  |     57  |    133  |   776  |   422  |   7  |   6  |   6  |   3  |   6  |   7  |   3  |   6  |   6  |   2  |      3  |      2  |  Completed  |  1.7200  |  2.0000  |  1.6500  |  2.2000  |  1.6700  |  2.1000  |  1.8500  |  2.0700  |  1.6200  |  2.2500  |  1.8500  |  2.2500  |  1.7100  |  2.0800  |\n   161  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |             Ito T.  |    Millman J.  |     84  |    184  |   633  |   279  |   6  |   4  |   6  |   4  |   3  |   6  |   0  |   6  |   7  |   5  |      3  |      2  |  Completed  |  3.7500  |  1.2500  |  3.4000  |  1.3000  |  3.0000  |  1.3600  |  3.8900  |  1.3000  |  3.0000  |  1.3600  |  4.0500  |  1.3600  |  3.4800  |  1.3000  |\n   165  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |        Stepanek R.  |    Troicki V.  |     34  |     37  |  1090  |  1030  |   5  |   7  |   4  |   6  |   6  |   3  |   6  |   3  |   7  |   5  |      3  |      2  |  Completed  |  2.5000  |  1.5000  |  2.5500  |  1.5000  |  2.6200  |  1.4400  |  2.6200  |  1.5600  |  2.2000  |  1.6700  |  2.8800  |  1.5600  |  2.5900  |  1.4900  |\n   166  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |        Bautista R.  |    Fognini F.  |     56  |     47  |   786  |   880  |   6  |   0  |   2  |   6  |   6  |   4  |   3  |   6  |   6  |   1  |      3  |      2  |  Completed  |  2.0000  |  1.7200  |  1.9800  |  1.8000  |  2.0000  |  1.7300  |  2.0300  |  1.8700  |  2.0000  |  1.8000  |  2.1800  |  1.8900  |  2.0000  |  1.7700  |\n   170  |  Melbourne  |  Australian Open  |  2013-01-14  |  Grand Slam  |  Outdoor  |     Hard  |  1st Round  |        5  |       Baghdatis M.  |      Ramos A.  |     35  |     51  |  1070  |   830  |   6  |   7  |   7  |   6  |   6  |   4  |   3  |   6  |   6  |   3  |      3  |      2  |  Completed  |  1.0800  |  8.0000  |  1.0700  |  8.0000  |  1.0700  |  8.0000  |  1.0900  |  8.9100  |  1.0800  |  7.5000  |  1.1000  |  9.4000  |  1.0800  |  7.5000  |",
            "title": "Select Rows"
        },
        {
            "location": "/frame/filtering/#select-columns",
            "text": "The API is completely symmetrical in the row and column dimension, so filtering a frame along the latter axis \nsimply involves a call to  select()  on the column dimension operator ( DataFrame.cols() ). The code below \ndemonstrates column filtering where an explicit set of column names are provided rather than using a lambda expression \nas in the previous row selection example. In this scenario we wish to select all the betting odds that were \navailable on the eventual winner of the games in 2013.   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\nDataFrame<Integer,String> filter = frame.cols().select(\"B365W\", \"EXW\", \"LBW\", \"PSW\" ,\"SJW\");\nfilter.out().print(10);  \n Index  |  B365W   |   EXW    |   LBW    |   PSW    |   SJW    |\n----------------------------------------------------------------\n     0  |  1.3600  |  1.4500  |  1.4400  |  1.4700  |  1.4400  |\n     1  |  1.6100  |  1.7500  |  1.8000  |  1.8000  |  1.7300  |\n     2  |  1.2500  |  1.2500  |  1.2900  |  1.3000  |  1.3000  |\n     3  |  1.0700  |  1.0600  |  1.0800  |  1.0800  |  1.0700  |\n     4  |  1.9000  |  1.8700  |  1.9100  |  1.8800  |  1.9100  |\n     5  |  1.6100  |  1.7500  |  1.7300  |  1.7000  |  1.8000  |\n     6  |  2.2000  |  2.0800  |  1.9100  |  2.2600  |  2.0000  |\n     7  |  1.4400  |  1.5500  |  1.4400  |  1.6000  |  1.5000  |\n     8  |  3.0000  |  2.5000  |  2.3800  |  2.9300  |  2.5000  |\n     9  |  1.3600  |  1.4000  |  1.4400  |  1.3800  |  1.4000  |",
            "title": "Select Columns"
        },
        {
            "location": "/frame/filtering/#select-rows-columns",
            "text": "Filtering in the row and column dimension simultaneously is also possible via one of the overloaded  select()  methods \navailable on the  DataFrame  class itself, which either takes the row and column keys to include, or lambda expressions\nto operate on the row and column entities. Parallel select is also supported by calling the  parallel() method on the \nframe prior to  select() .  Let's investigate which if any of the bookmakers provided more attractive odds in the various finals in 2013. \nBefore we do this, we need to standardize the betting odds so that we can not only make cross-sectional comparisons \nfor a given match, but also across different matches. To do this, we transform all the odds to a  z-score  \nby first subtracting the mean across the bookmakers, and then dividing by the standard deviation. The code below performs \nthis transformation.   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\n\n//Array of the various bookmaker keys\nArray<String> bookies = Array.of(\"B365\", \"EX\", \"LB\", \"PS\" ,\"SJ\");\n\n//Compute z-scores for winner & loser odds independently\nStream.of(\"W\", \"L\").forEach(x -> {\n    Array<String> colNames = bookies.mapValues(v -> v.getValue() + x);\n    frame.cols().select(colNames).rows().forEach(row -> {\n        double mean = row.stats().mean();\n        double stdDev = row.stats().stdDev();\n        row.values().forEach(v -> {\n            double rawValue = v.getDouble();\n            double zScore = (rawValue - mean) / stdDev;\n            v.setDouble(zScore);\n        });\n    });\n});\n\n//Print last 14 columns and first 10 rows to std out\nframe.right(14).out().print(10, options -> {\n    options.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});  \n Index  |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |  MaxW   |  MaxL   |  AvgW   |  AvgL   |\n---------------------------------------------------------------------------------------------------------------------------------------------------------------\n     0  |  -1.711  |   1.482  |   0.428  |  -0.593  |   0.190  |  -0.771  |   0.903  |   0.593  |   0.190  |  -0.711  |  1.470  |  3.200  |  1.420  |  2.780  |\n     1  |  -1.643  |   1.423  |   0.154  |  -0.378  |   0.796  |  -1.189  |   0.796  |   0.523  |  -0.103  |  -0.378  |  1.800  |  2.260  |  1.730  |  2.050  |\n     2  |  -1.082  |   0.532  |  -1.082  |   0.532  |   0.464  |  -0.418  |   0.850  |   0.912  |   0.850  |  -1.558  |  1.300  |  4.200  |  1.280  |  3.580  |\n     3  |  -0.239  |   0.817  |  -1.434  |  -0.077  |   0.956  |  -0.971  |   0.956  |   1.201  |  -0.239  |  -0.971  |  1.100  |  9.500  |  1.080  |  7.760  |\n     4  |   0.330  |  -0.620  |  -1.321  |   0.184  |   0.881  |  -0.620  |  -0.771  |   1.677  |   0.881  |  -0.620  |  2.050  |  2.000  |  1.880  |  1.850  |\n     5  |  -1.532  |   0.817  |   0.454  |  -0.501  |   0.170  |  -0.501  |  -0.255  |   1.278  |   1.163  |  -1.094  |  1.850  |  2.280  |  1.710  |  2.080  |\n     6  |   0.770  |  -1.305  |  -0.070  |  -0.454  |  -1.260  |   1.390  |   1.190  |  -0.028  |  -0.630  |   0.397  |  2.320  |  1.830  |  2.080  |  1.700  |\n     7  |  -0.945  |   0.952  |   0.630  |  -1.428  |  -0.945  |   0.952  |   1.346  |  -0.370  |  -0.086  |  -0.106  |  1.630  |  2.820  |  1.520  |  2.460  |\n     8  |   1.199  |  -1.615  |  -0.574  |   0.479  |  -1.000  |   0.927  |   0.950  |  -0.269  |  -0.574  |   0.479  |  3.250  |  1.530  |  2.660  |  1.470  |\n     9  |  -1.214  |   0.400  |   0.135  |  -0.355  |   1.483  |  -1.034  |  -0.539  |   1.533  |   0.135  |  -0.544  |  1.450  |  3.550  |  1.390  |  2.870  |  Now that we have standardized all the odds, we can try and make an assessment of which bookmaker on average provided\nthe most attractive odds. It should be noted that this example is purely to demonstrate API usage, it is not intended to \nrepresent rigorous statistical analysis of this data, so take the results with a huge grain of salt.  This data could\nbe fairly noisy, so let's only consider the finals since there is likely to be a higher degree of consensus for those\nmatches. We therefore need to filter on a subset of rows (finals only) and also only include odds on the eventual winner \nfor brevity.   //Select z-score odds on eventual winner across all bookmakers\nSet<String> colNames = Collect.asSet(\"B365W\", \"EXW\", \"LBW\", \"PSW\" ,\"SJW\");\nDataFrame<Integer,String> finalWinnerOdds = frame.select(\n    row -> row.getValue(\"Round\").equals(\"The Final\"),\n    col -> colNames.contains(col.key())\n);  We can now compute the mean of these columns, which gives us the average z-score for each bookmaker across all \nthe finals in 2013. The code below demonstrates this, and prints the results to std-out with formatting applied.   //Compute the mean of these z-scores and print\nfinalWinnerOdds.cols().stats().mean().out().print(options -> {\n    options.withDecimalFormat(Double.class, \"0.000;-0.000\", 1);\n});  \n Index  |  B365W   |   EXW    |   LBW    |   PSW   |   SJW    |\n---------------------------------------------------------------\n  MEAN  |  -0.259  |  -0.583  |  -0.033  |  0.953  |  -0.078  |  This naive analysis suggests that using a sample that includes all the ATP finals in 2013, on average, Pinnacles \nSports (PSW) provided the most attractive odds and Expekt (EX) provided the least attractive odds. These standardized\nscores represent the number of standard deviations above / below the mean, and if there was a very high degree of \nconsensus across the bookmakers, in that the absolute odds were very similar, these figures may not really tell\nus much. Again, this example is to illustrate API usage, it is not intended to draw any far reaching conclusions \non the skills of bookmakers.",
            "title": "Select Rows &amp; Columns"
        },
        {
            "location": "/frame/filtering/#monadic",
            "text": "The Morpheus API enables data analysis procedures to be chained together in order to facilitate powerful transformations \nto be applied with fairly little code. To demonstrate, consider an example where we wish to compute the summary statistics \n(say min, max, mean and std. deviation) on the number of games played in 5 set matches in 2013 across all the tournaments. \nThe first code example below demonstrates a longer form solution where the intermediate frames are explicitly captured as \nvariables, which for a beginner would likely be easier to follow:   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Select all 5 set matches, and the number of games won/lost in each set\nDataFrame<Integer,String> filter = frame.select(\n    row -> row.getDouble(\"Wsets\") + row.getDouble(\"Lsets\") == 5,\n    col -> col.key().matches(\"(W|L)\\\\d+\")\n);\n//Sum the rows which yields total games played per 5 set match\nDataFrame<Integer,StatType> gameCounts = filter.rows().stats().sum();\n//The game count frame is Nx1, so compute summary stats on game counts\nDataFrame<StatType,StatType> gameStats = gameCounts.cols().describe(MIN, MAX, MEAN, STD_DEV);\n//Print results to std-out\ngameStats.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});  \n Index  |   MIN   |   MAX   |  MEAN   |  STD_DEV  |\n---------------------------------------------------\n   SUM  |  40.00  |  66.00  |  51.15  |     5.29  |  By leveraging the monadic nature of the Morpheus API, it's possible to perform this analysis in fewer lines of code\nas shown below. Whether this is preferable or not is another question, at least in the context of this example,\nbut suffice to say that the monadic nature of the API can facilitate brevity for trivial steps in more complicated\nanalyzes.   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\nframe.select(\n    row -> row.getDouble(\"Wsets\") + row.getDouble(\"Lsets\") == 5,\n    col -> col.key().matches(\"(W|L)\\\\d+\")\n).rows().stats().sum().cols().describe(\n    MIN, MAX, MEAN, STD_DEV\n).out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});  \n Index  |   MIN   |   MAX   |  MEAN   |  STD_DEV  |\n---------------------------------------------------\n   SUM  |  40.00  |  66.00  |  51.15  |     5.29  |",
            "title": "Monadic"
        },
        {
            "location": "/frame/filtering/#head-tail",
            "text": "The  head()  method on the  DataFrame  class provides an easy mechanism to select the first N rows as follows:   //Filter on the first 5 rows\nframe.head(5).out().print();  \n Index  |  Location  |        Tournament        |     Date     |  Series  |   Court   |  Surface  |    Round    |  Best of  |     Winner     |     Loser      |  WRank  |  LRank  |  WPts  |  LPts  |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     0  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |      Mayer F.  |    Giraldo S.  |     28  |     57  |  1215  |   778  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.3600  |  3.0000  |  1.4500  |  2.6500  |  1.4400  |  2.6200  |  1.4700  |  2.8500  |  1.4400  |  2.6300  |  1.4700  |  3.2000  |  1.4200  |  2.7800  |\n     1  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |   Nieminen J.  |  Benneteau J.  |     41  |     35  |   927  |  1075  |   6  |   3  |   2  |   6  |   6  |   1  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.8000  |  1.9100  |  1.8000  |  2.1000  |  1.7300  |  2.0000  |  1.8000  |  2.2600  |  1.7300  |  2.0500  |\n     2  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |  Nishikori K.  |  Matosevic M.  |     19  |     49  |  1830  |   845  |   7  |   5  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.2500  |  3.7500  |  1.2500  |  3.7500  |  1.2900  |  3.5000  |  1.3000  |  3.8500  |  1.3000  |  3.2000  |  1.3000  |  4.2000  |  1.2800  |  3.5800  |\n     3  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |  Baghdatis M.  |   Mitchell B.  |     36  |    326  |  1070  |   137  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.0700  |  9.0000  |  1.0600  |  8.0000  |  1.0800  |  7.0000  |  1.0800  |  9.4300  |  1.0700  |  7.0000  |  1.1000  |  9.5000  |  1.0800  |  7.7600  |\n     4  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Istomin D.  |     Klizan M.  |     43  |     30  |   897  |  1175  |   6  |   1  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.9000  |  1.8000  |  1.8700  |  1.8700  |  1.9100  |  1.8000  |  1.8800  |  2.0000  |  1.9100  |  1.8000  |  2.0500  |  2.0000  |  1.8800  |  1.8500  |  The  tail()  method on the  DataFrame  class provides an easy mechanism to select the last N rows as follows:   //Filter on the last 5 rows\nframe.tail(5).out().print();  \n Index  |  Location  |  Tournament   |     Date     |    Series     |  Court   |  Surface  |     Round     |  Best of  |    Winner     |      Loser       |  WRank  |  LRank  |  WPts   |  LPts   |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |   B365L   |   EXW    |   EXL    |   LBW    |    LBL    |   PSW    |    PSL    |   SJW    |   SJL    |   MaxW   |   MaxL    |   AvgW   |   AvgL   |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  2626  |    London  |  Masters Cup  |  2013-11-09  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |   Federer R.  |  Del Potro J.M.  |      7  |      5  |   3805  |   5055  |   4  |   6  |   7  |   6  |   7  |   5  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  2.0000  |   1.8000  |  2.0000  |  1.8000  |  2.0000  |   1.8000  |  2.0700  |   1.8500  |  2.0000  |  1.8000  |  2.1000  |   1.9500  |  1.9800  |  1.8100  |\n  2627  |    London  |  Masters Cup  |  2013-11-09  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |  Djokovic N.  |      Gasquet R.  |      2  |      9  |  10610  |   3300  |   7  |   6  |   4  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.0600  |  10.0000  |  1.0500  |  9.0000  |  1.0500  |  10.0000  |  1.0700  |  10.7500  |  1.0500  |  9.0000  |  1.1000  |  10.7500  |  1.0600  |  9.2200  |\n  2628  |    London  |  Masters Cup  |  2013-11-10  |  Masters Cup  |  Indoor  |     Hard  |   Semifinals  |        3  |     Nadal R.  |      Federer R.  |      1  |      7  |  12030  |   3805  |   7  |   5  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.3000  |   3.5000  |  1.3300  |  3.2000  |  1.3000  |   3.5000  |  1.3400  |   3.6700  |  1.3300  |  3.4000  |  1.3700  |   4.0500  |  1.3300  |  3.3400  |\n  2629  |    London  |  Masters Cup  |  2013-11-10  |  Masters Cup  |  Indoor  |     Hard  |   Semifinals  |        3  |  Djokovic N.  |     Wawrinka S.  |      2  |      8  |  10610  |   3330  |   6  |   3  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.1400  |   5.5000  |  1.1300  |  5.5000  |  1.1400  |   5.5000  |  1.1900  |   5.6100  |  1.1500  |  5.5000  |  1.1900  |   6.3000  |  1.1600  |  5.2500  |\n  2630  |    London  |  Masters Cup  |  2013-11-11  |  Masters Cup  |  Indoor  |     Hard  |    The Final  |        3  |  Djokovic N.  |        Nadal R.  |      2  |      1  |  10610  |  12030  |   6  |   3  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.5700  |   2.3700  |  1.5500  |  2.4000  |  1.5700  |   2.3800  |  1.5800  |   2.5800  |  1.5700  |  2.3800  |  1.6500  |   2.5800  |  1.5700  |  2.4300  |",
            "title": "Head &amp; Tail"
        },
        {
            "location": "/frame/filtering/#right-left",
            "text": "The  left()  method is an analog to the  head()  method but selects the first N columns as follows:   //Filter on the first 5 columns\nframe.left(5).out().print();  \n Index  |  Location  |        Tournament        |     Date     |  Series  |   Court   |\n---------------------------------------------------------------------------------------\n     0  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |\n     1  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |\n     2  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |\n     3  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |\n     4  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |\n     5  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |\n     6  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |\n     7  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |\n     8  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |\n     9  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |  The  right()  method is an analog to the  tail()  method but selects the last N columns as follows:   //Filter on the last 14 columns\nframe.right(14).out().print();  \n Index  |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     0  |  1.3600  |  3.0000  |  1.4500  |  2.6500  |  1.4400  |  2.6200  |  1.4700  |  2.8500  |  1.4400  |  2.6300  |  1.4700  |  3.2000  |  1.4200  |  2.7800  |\n     1  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.8000  |  1.9100  |  1.8000  |  2.1000  |  1.7300  |  2.0000  |  1.8000  |  2.2600  |  1.7300  |  2.0500  |\n     2  |  1.2500  |  3.7500  |  1.2500  |  3.7500  |  1.2900  |  3.5000  |  1.3000  |  3.8500  |  1.3000  |  3.2000  |  1.3000  |  4.2000  |  1.2800  |  3.5800  |\n     3  |  1.0700  |  9.0000  |  1.0600  |  8.0000  |  1.0800  |  7.0000  |  1.0800  |  9.4300  |  1.0700  |  7.0000  |  1.1000  |  9.5000  |  1.0800  |  7.7600  |\n     4  |  1.9000  |  1.8000  |  1.8700  |  1.8700  |  1.9100  |  1.8000  |  1.8800  |  2.0000  |  1.9100  |  1.8000  |  2.0500  |  2.0000  |  1.8800  |  1.8500  |\n     5  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.7300  |  2.0000  |  1.7000  |  2.2700  |  1.8000  |  1.9100  |  1.8500  |  2.2800  |  1.7100  |  2.0800  |\n     6  |  2.2000  |  1.6100  |  2.0800  |  1.6700  |  1.9100  |  1.8000  |  2.2600  |  1.7000  |  2.0000  |  1.7300  |  2.3200  |  1.8300  |  2.0800  |  1.7000  |\n     7  |  1.4400  |  2.6200  |  1.5500  |  2.3500  |  1.4400  |  2.6200  |  1.6000  |  2.4700  |  1.5000  |  2.5000  |  1.6300  |  2.8200  |  1.5200  |  2.4600  |\n     8  |  3.0000  |  1.3600  |  2.5000  |  1.5000  |  2.3800  |  1.5300  |  2.9300  |  1.4500  |  2.5000  |  1.5000  |  3.2500  |  1.5300  |  2.6600  |  1.4700  |\n     9  |  1.3600  |  3.0000  |  1.4000  |  2.8000  |  1.4400  |  2.6200  |  1.3800  |  3.3000  |  1.4000  |  2.7500  |  1.4500  |  3.5500  |  1.3900  |  2.8700  |  It is sometimes useful to combine these functions to select parts of the frame. For example, consider a scenario\nwhere we wish to select all the betting odds, but excluding the maximum and averages that are represented by the\nlast 4 columns of the dataset. This can be achieved with a combined  right()  and  left()  operation as follows:   //Filter on all betting ods, excluding max & averages\nframe.right(14).left(10).out().print();  \n Index  |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |\n-----------------------------------------------------------------------------------------------------------------------\n     0  |  1.3600  |  3.0000  |  1.4500  |  2.6500  |  1.4400  |  2.6200  |  1.4700  |  2.8500  |  1.4400  |  2.6300  |\n     1  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.8000  |  1.9100  |  1.8000  |  2.1000  |  1.7300  |  2.0000  |\n     2  |  1.2500  |  3.7500  |  1.2500  |  3.7500  |  1.2900  |  3.5000  |  1.3000  |  3.8500  |  1.3000  |  3.2000  |\n     3  |  1.0700  |  9.0000  |  1.0600  |  8.0000  |  1.0800  |  7.0000  |  1.0800  |  9.4300  |  1.0700  |  7.0000  |\n     4  |  1.9000  |  1.8000  |  1.8700  |  1.8700  |  1.9100  |  1.8000  |  1.8800  |  2.0000  |  1.9100  |  1.8000  |\n     5  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.7300  |  2.0000  |  1.7000  |  2.2700  |  1.8000  |  1.9100  |\n     6  |  2.2000  |  1.6100  |  2.0800  |  1.6700  |  1.9100  |  1.8000  |  2.2600  |  1.7000  |  2.0000  |  1.7300  |\n     7  |  1.4400  |  2.6200  |  1.5500  |  2.3500  |  1.4400  |  2.6200  |  1.6000  |  2.4700  |  1.5000  |  2.5000  |\n     8  |  3.0000  |  1.3600  |  2.5000  |  1.5000  |  2.3800  |  1.5300  |  2.9300  |  1.4500  |  2.5000  |  1.5000  |\n     9  |  1.3600  |  3.0000  |  1.4000  |  2.8000  |  1.4400  |  2.6200  |  1.3800  |  3.3000  |  1.4000  |  2.7500  |",
            "title": "Right &amp; Left"
        },
        {
            "location": "/frame/filtering/#filter-method",
            "text": "The  DataFrame.rows()  and  DataFrame.cols()  methods return a versatile interface to operate on the row\nand column dimension respectively. The examples above demonstrate how to use the  select()  method to\ncreate a  DataFrame  filter to operate on a subset of the data. A  filter()  method also exists which\nprovides a shortcut to more directly operate on a subset of rows or columns without creating the \nintermediate filter frame.  The example below demonstrates how to iterate over a subset of rows that represent an upset in a finals match \nin 2013 according to the average bookmaker odds. Higher odds represent a less likely outcome, so if the\naverage odds on the eventual winner of a game were greater than those for the eventual loser, that  classifies \nas an upset because the favourite did not prevail. Concurrent iteration is also supported with a call to  parallel() , however then order is not guaranteed which may or may not be important.   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\nframe.rows().filter(row -> {\n    String round = row.getValue(\"Round\");\n    double avgW = row.getDouble(\"AvgW\");\n    double avgL = row.getDouble(\"AvgL\");\n    return round.equals(\"The Final\") && avgW > avgL;\n}).forEach(row -> {\n    String loser = row.getValue(\"Loser\");\n    String winner = row.getValue(\"Winner\");\n    String tournament = row.getValue(\"Tournament\");\n    LocalDate date = row.getValue(\"Date\");\n    System.out.printf(\"Finals upset when %s beat %s at %s on %s\\n\", winner, loser, tournament, date);\n});  \nFinals upset when Gasquet R. beat Davydenko N. at Qatar Exxon Mobil Open on 2013-01-05\nFinals upset when Zeballos H. beat Nadal R. at VTR Open on 2013-02-10\nFinals upset when Tsonga J.W. beat Berdych T. at Open 13 on 2013-02-24\nFinals upset when Robredo T. beat Anderson K. at Grand Prix Hassan II on 2013-04-14\nFinals upset when Isner J. beat Almagro N. at U.S. Men's Clay Court Championships on 2013-04-14\nFinals upset when Djokovic N. beat Nadal R. at Monte Carlo Masters on 2013-04-21\nFinals upset when Wawrinka S. beat Ferrer D. at Portugal Open on 2013-05-05\nFinals upset when Montanes A. beat Monfils G. at Open de Nice C\ufffdte d\ufffdAzur on 2013-05-25\nFinals upset when Mahut N. beat Wawrinka S. at Topshelf Open on 2013-06-22\nFinals upset when Murray A. beat Djokovic N. at Wimbledon on 2013-07-07\nFinals upset when Berlocq C. beat Verdasco F. at SkiStar Swedish Open on 2013-07-14\nFinals upset when Fognini F. beat Kohlschreiber P. at Mercedes Cup on 2013-07-14\nFinals upset when Karlovic I. beat Falla A. at Claro Open Colombia on 2013-07-21\nFinals upset when Robredo T. beat Fognini F. at ATP Vegeta Croatia Open on 2013-07-28\nFinals upset when Granollers M. beat Monaco J. at Bet-At-Home Cup on 2013-08-03\nFinals upset when Melzer J. beat Monfils G. at Winston-Salem Open at Wake Forest University on 2013-08-24\nFinals upset when Simon G. beat Tsonga J.W. at Open de Moselle on 2013-09-22\nFinals upset when Raonic M. beat Berdych T. at Thailand Open on 2013-09-29\nFinals upset when Sousa J. beat Benneteau J. at Malaysian Open on 2013-09-29\nFinals upset when Dimitrov G. beat Ferrer D. at Stockholm Open on 2013-10-20\nFinals upset when Youzhny M. beat Ferrer D. at Valencia Open 500 on 2013-10-27  Again, to re-iterate the symmetrical nature of the API, consider the following where we iterate over a\nsubset of the columns to discover what the minimum and maximum betting odds that were available on\nthe eventual winner of each match by the 5 different book makers.   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\nframe.cols().filter(\"B365W\", \"EXW\", \"LBW\", \"PSW\" ,\"SJW\").forEach(column -> {\n    Optional<Bounds<Double>> bounds = column.bounds();\n    bounds.ifPresent(b -> {\n        System.out.println(column.key() + \" max odds=\" + b.upper() + \", min odds=\" + b.lower());\n    });\n});  \nB365W max odds=29.0, min odds=1.0\nEXW max odds=20.0, min odds=1.01\nLBW max odds=26.0, min odds=1.0\nPSW max odds=46.0, min odds=1.01\nSJW max odds=17.0, min odds=1.01",
            "title": "Filter Method"
        },
        {
            "location": "/frame/filtering/#immutability",
            "text": "In the introduction, it was suggested that a  DataFrame  that represents a filter on another frame is structurally \nimmutable in some cases, in that additional columns cannot always be added (in all cases, additional rows cannot be\nadded to a filter). Data mutability on the other hand is supported on a filter, as illustrated by the example where \nwe transformed absolute betting odds to a Z-score.  The structural immutability caveats are in explained in this section.",
            "title": "Immutability"
        },
        {
            "location": "/frame/filtering/#column-store",
            "text": "A  DataFrame  is by default a column store, where each column is represented by a Morpheus array. When filtering such \na frame, it is not possible to add rows to the filter frame, but adding columns is supported. The reason for this \nrestriction is because the filter frame shares its content with the parent it was created from, so adding rows would \nresult in also modifying the column arrays of the parent frame, which would lead to all sorts of issues. The filter \nframe does have a completely independent column axis however, so it is possible to add additional columns without this \nhaving any impact on the parent.  The following example demonstrates how an additional column can be added to the filter frame after we have selected all \nmatches in which Novak Djokovic was the victor. Here we add an additional column to store the the total number of games\nthat Novak won in each match.   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Select rows where Djokovic was the victor\nDataFrame<Integer,String> filter = frame.rows().select(row -> row.getValue(\"Winner\").equals(\"Djokovic N.\"));\n//Add a column to the filter and seed with number of games won by Djokovic\nArray<String> colNames = Range.of(1, 6).map(i -> \"W\" + i).toArray();\n//Add new column to capture total number of games won by novak\nfilter.cols().add(\"WonCount\", Double.class, value -> {\n    final DataFrameRow<Integer,String> row = value.row(); //Access row associated with this value\n    return colNames.mapToDoubles(v -> row.getDouble(v.getValue())).stats().sum().doubleValue();\n});\n\n//Print 10x10 section of right most columns\nfilter.right(10).out().print(10);  The column on the far right is the newly added column initialized with the number of games won by Novak.  \n Index  |    LBL    |   PSW    |    PSL    |   SJW    |    SJL    |   MaxW   |   MaxL    |   AvgW   |   AvgL    |  WonCount  |\n------------------------------------------------------------------------------------------------------------------------------\n   152  |  13.0000  |  1.0100  |  41.0000  |  1.0100  |  19.0000  |  1.0200  |  46.0000  |  1.0100  |  21.7000  |   19.0000  |\n   218  |  21.0000  |  1.0100  |  31.0000  |  1.0100  |  19.0000  |  1.0200  |  51.0000  |  1.0100  |  23.3800  |   18.0000  |\n   236  |  17.0000  |  1.0100  |  41.0000  |  1.0100  |  13.0000  |  1.0200  |  63.2500  |  1.0100  |  24.9400  |   19.0000  |\n   254  |  13.0000  |  1.0200  |  21.5000  |  1.0200  |  13.0000  |  1.0300  |  29.0000  |  1.0200  |  15.9400  |   32.0000  |\n   260  |   5.0000  |  1.2200  |   4.9200  |  1.1800  |   5.2500  |  1.2300  |   5.3000  |  1.1800  |   4.7300  |   22.0000  |\n   263  |   6.5000  |  1.1200  |   7.9400  |  1.1000  |   8.0000  |  1.1400  |   9.4000  |  1.1000  |   6.9100  |   18.0000  |\n   265  |   2.5000  |  1.5400  |   2.7400  |  1.5300  |   2.6300  |  1.5700  |   2.9000  |  1.5000  |   2.6100  |   25.0000  |\n   597  |  17.0000  |  1.0200  |  21.9700  |  1.0300  |  12.0000  |  1.0300  |  31.0000  |  1.0200  |  16.5400  |   12.0000  |\n   606  |  17.0000  |  1.0100  |  29.1600  |  1.0100  |  15.0000  |  1.0400  |  46.0000  |  1.0100  |  20.0900  |   13.0000  |\n   609  |   9.0000  |  1.0300  |  19.0000  |  1.0400  |  10.0000  |  1.0500  |  24.0000  |  1.0300  |  13.4300  |   12.0000  |  If we print the right most columns of the original frame, the `WonCount\" column is not present.   //Print 10x10 section of right most columns of original frame\nframe.right(10).out().print(10);  \n Index  |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n-----------------------------------------------------------------------------------------------------------------------\n     0  |  1.4400  |  2.6200  |  1.4700  |  2.8500  |  1.4400  |  2.6300  |  1.4700  |  3.2000  |  1.4200  |  2.7800  |\n     1  |  1.8000  |  1.9100  |  1.8000  |  2.1000  |  1.7300  |  2.0000  |  1.8000  |  2.2600  |  1.7300  |  2.0500  |\n     2  |  1.2900  |  3.5000  |  1.3000  |  3.8500  |  1.3000  |  3.2000  |  1.3000  |  4.2000  |  1.2800  |  3.5800  |\n     3  |  1.0800  |  7.0000  |  1.0800  |  9.4300  |  1.0700  |  7.0000  |  1.1000  |  9.5000  |  1.0800  |  7.7600  |\n     4  |  1.9100  |  1.8000  |  1.8800  |  2.0000  |  1.9100  |  1.8000  |  2.0500  |  2.0000  |  1.8800  |  1.8500  |\n     5  |  1.7300  |  2.0000  |  1.7000  |  2.2700  |  1.8000  |  1.9100  |  1.8500  |  2.2800  |  1.7100  |  2.0800  |\n     6  |  1.9100  |  1.8000  |  2.2600  |  1.7000  |  2.0000  |  1.7300  |  2.3200  |  1.8300  |  2.0800  |  1.7000  |\n     7  |  1.4400  |  2.6200  |  1.6000  |  2.4700  |  1.5000  |  2.5000  |  1.6300  |  2.8200  |  1.5200  |  2.4600  |\n     8  |  2.3800  |  1.5300  |  2.9300  |  1.4500  |  2.5000  |  1.5000  |  3.2500  |  1.5300  |  2.6600  |  1.4700  |\n     9  |  1.4400  |  2.6200  |  1.3800  |  3.3000  |  1.4000  |  2.7500  |  1.4500  |  3.5500  |  1.3900  |  2.8700  |  In contrast, if we attempt to add rows to the filter frame, we will get a  DataFrameException  because modifying\nthe shared column arrays would impact the parent from which the filter was created. Let's attempt to add 5 more \nrows as follows:   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Select rows where Djokovic was the victor\nDataFrame<Integer,String> filter = frame.rows().select(row -> row.getValue(\"Winner\").equals(\"Djokovic N.\"));\n//Define a range of 10 additional keys\nRange<Integer> rowKeys = Range.of(frame.rowCount(), frame.rowCount()+5);\n//Try add rows for new row keys\nfilter.rows().addAll(rowKeys);  \ncom.zavtech.morpheus.frame.DataFrameException: Cannot add keys to a sliced axis of a DataFrame\n    at com.zavtech.morpheus.reference.XDataFrameContent.addRows(XDataFrameContent.java:300)\n    at com.zavtech.morpheus.reference.XDataFrameRows.addAll(XDataFrameRows.java:96)\n    at com.zavtech.morpheus.reference.XDataFrameRows.addAll(XDataFrameRows.java:89)  If we create a deep copy of the filter frame, then we end up with a completely new data structure\ncontaining just the rows of the filter, so we can now add rows as usual.   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Select rows where Djokovic was the victor\nDataFrame<Integer,String> filter = frame.rows().select(row -> row.getValue(\"Winner\").equals(\"Djokovic N.\"));\n//Create a deep copy of the filter frame\nDataFrame<Integer,String> copy = filter.copy();\n//Define a range of 5 additional keys\nRange<Integer> rowKeys = Range.of(frame.rowCount(), frame.rowCount()+5);\n//Add 5 new rows to the copy\ncopy.rows().addAll(rowKeys);\n//Print last 10 rows\ncopy.tail(10).out().print();  \n Index  |  Location  |  Tournament   |     Date     |    Series     |  Court   |  Surface  |     Round     |  Best of  |    Winner     |      Loser       |  WRank  |  LRank  |  WPts   |  LPts   |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |   B365L   |   EXW    |   EXL    |   LBW    |    LBL    |   PSW    |    PSL    |   SJW    |   SJL    |   MaxW   |   MaxL    |   AvgW   |   AvgL   |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  2619  |    London  |  Masters Cup  |  2013-11-05  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |  Djokovic N.  |      Federer R.  |      2  |      7  |  10610  |   3805  |   6  |   4  |   6  |   7  |   6  |   2  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.2800  |   3.7500  |  1.2800  |  3.5000  |  1.2900  |   3.7500  |  1.3000  |   3.9300  |  1.2900  |  3.7500  |  1.3300  |   4.0000  |  1.2900  |  3.6000  |\n  2623  |    London  |  Masters Cup  |  2013-11-07  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |  Djokovic N.  |  Del Potro J.M.  |      2  |      5  |  10610  |   5055  |   6  |   3  |   3  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.2800  |   3.7500  |  1.2800  |  3.5000  |  1.3000  |   3.5000  |  1.3000  |   3.8900  |  1.2900  |  3.7500  |  1.3300  |   4.0500  |  1.2800  |  3.6100  |\n  2627  |    London  |  Masters Cup  |  2013-11-09  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |  Djokovic N.  |      Gasquet R.  |      2  |      9  |  10610  |   3300  |   7  |   6  |   4  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.0600  |  10.0000  |  1.0500  |  9.0000  |  1.0500  |  10.0000  |  1.0700  |  10.7500  |  1.0500  |  9.0000  |  1.1000  |  10.7500  |  1.0600  |  9.2200  |\n  2629  |    London  |  Masters Cup  |  2013-11-10  |  Masters Cup  |  Indoor  |     Hard  |   Semifinals  |        3  |  Djokovic N.  |     Wawrinka S.  |      2  |      8  |  10610  |   3330  |   6  |   3  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.1400  |   5.5000  |  1.1300  |  5.5000  |  1.1400  |   5.5000  |  1.1900  |   5.6100  |  1.1500  |  5.5000  |  1.1900  |   6.3000  |  1.1600  |  5.2500  |\n  2630  |    London  |  Masters Cup  |  2013-11-11  |  Masters Cup  |  Indoor  |     Hard  |    The Final  |        3  |  Djokovic N.  |        Nadal R.  |      2  |      1  |  10610  |  12030  |   6  |   3  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.5700  |   2.3700  |  1.5500  |  2.4000  |  1.5700  |   2.3800  |  1.5800  |   2.5800  |  1.5700  |  2.3800  |  1.6500  |   2.5800  |  1.5700  |  2.4300  |\n  2631  |      null  |         null  |        null  |         null  |    null  |     null  |         null  |        0  |         null  |            null  |      0  |      0  |      0  |      0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |      0  |      0  |       null  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |     NaN  |\n  2632  |      null  |         null  |        null  |         null  |    null  |     null  |         null  |        0  |         null  |            null  |      0  |      0  |      0  |      0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |      0  |      0  |       null  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |     NaN  |\n  2633  |      null  |         null  |        null  |         null  |    null  |     null  |         null  |        0  |         null  |            null  |      0  |      0  |      0  |      0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |      0  |      0  |       null  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |     NaN  |\n  2634  |      null  |         null  |        null  |         null  |    null  |     null  |         null  |        0  |         null  |            null  |      0  |      0  |      0  |      0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |      0  |      0  |       null  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |     NaN  |\n  2635  |      null  |         null  |        null  |         null  |    null  |     null  |         null  |        0  |         null  |            null  |      0  |      0  |      0  |      0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |   0  |      0  |      0  |       null  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |      NaN  |     NaN  |     NaN  |     NaN  |      NaN  |     NaN  |     NaN  |",
            "title": "Column Store"
        },
        {
            "location": "/frame/filtering/#transpose",
            "text": "There is one additional caveat with respect to structural immutability which involves the transpose\nof a  DataFrame . While a frame is by design a column store, when you call  transpose()  to switch\nthe row and column axis, it effectively becomes row major. This implementation choice was made\nso that creating a transpose frame is extremely cheap, and requires almost no additional memory.  A transposed  DataFrame  is structurally immutable in both the row and column dimensions, which \nis a design choice that may be relaxed at some point in the future. If structural changes to a \ntranspose is a requirement, a copy will need to be created, which will then yield a column store\ndata structure with the data still in its transposed state.  In the case of a frame with mixed column types, like the ATP match results, one should be aware that \ntransposing it and then creating a copy will result in a frame that is much less memory efficient \nthan the non-transposed original. The reason for this is that the copied transpose frame will have\ncolumn arrays of type  Object  in order to cater to the non-homogeneous types for each column, and\nthereby introduce object header overhead as well as significantly higher garbage collection times.   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Transpose the original DataFrame\nDataFrame<String,Integer> transpose = frame.transpose();\n//Print 10x5 section of the frame\ntranspose.left(5).out().print();\n//Attempt to add a column, which we know will fail\ntranspose.cols().add(transpose.colCount()+1, Double.class);  \n   Index     |            0             |            1             |            2             |            3             |            4             |\n-----------------------------------------------------------------------------------------------------------------------------------------------------\n   Location  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |\n Tournament  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |\n       Date  |              2012-12-31  |              2012-12-31  |              2012-12-31  |              2012-12-31  |              2013-01-01  |\n     Series  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |\n      Court  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |\n    Surface  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |\n      Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |\n    Best of  |                       3  |                       3  |                       3  |                       3  |                       3  |\n     Winner  |                Mayer F.  |             Nieminen J.  |            Nishikori K.  |            Baghdatis M.  |              Istomin D.  |\n      Loser  |              Giraldo S.  |            Benneteau J.  |            Matosevic M.  |             Mitchell B.  |               Klizan M.  |  \ncom.zavtech.morpheus.frame.DataFrameException: This DataFrame is configured as a row store, transpose() first\n    at com.zavtech.morpheus.reference.XDataFrameContent.addColumn(XDataFrameContent.java:326)\n    at com.zavtech.morpheus.reference.XDataFrameColumns.lambda$null$14(XDataFrameColumns.java:216)\n    at java.util.LinkedHashMap$LinkedKeySet.forEach(LinkedHashMap.java:555)\n    at com.zavtech.morpheus.reference.XDataFrameColumns.lambda$addColumns$15(XDataFrameColumns.java:213)\n    at java.util.function.Function.lambda$andThen$6(Function.java:88)\n    at java.util.function.Function.lambda$andThen$6(Function.java:88)\n    at com.zavtech.morpheus.reference.XDataFrameColumns.add(XDataFrameColumns.java:88)",
            "title": "Transpose"
        },
        {
            "location": "/frame/sorting/",
            "text": "Sorting\n\n\nIntroduction\n\n\nThe Morpheus \nDataFrame\n can be sorted in both the row and column dimension, either by the axis keys, or by \nvalues in one or more columns or rows respectively. In addition, custom sort logic is supported via a user provided\n\nComparator\n that consumes either \nDataFrameRow\n or \nDataFrameColumn\n objects. Finally, all sort functions are \ncapable of executing in \nparallel\n which can lead to dramatic performance improvements on large frames, and some\nresults in this regard are presented in the section on performance below. In all cases, an optimized single pivot \nquick sort is used, with an implementation made available in the excellent \nFastUtil\n \nlibrary, so a thank you goes out to \nSebastiano Vigna\n.\n\n\nExample Data\n\n\nIn this section, we will continue to use the ATP 2013 dataset that was introduced earlier in the filtering discussion \n\nhere\n. In some examples a custom frame is created of random double precision values in order to\ndemonstrate how to sort columns by row values. The ATP match \nDataFrame\n does not have homogeneous types along the rows, and \ntherefore a column sort based on these values is nonsensical. In addition, the section on performance creates much larger frames\nwith millions of rows in order to get more measurable timing statistics.\n\n\nThe unsorted 2103 ATP match \nDataFrame\n looks as follows (at least the first 10 rows).\n\n\n\n Index  |  Location  |        Tournament        |     Date     |  Series  |   Court   |  Surface  |    Round    |  Best of  |     Winner     |     Loser      |  WRank  |  LRank  |  WPts  |  LPts  |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     0  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |      Mayer F.  |    Giraldo S.  |     28  |     57  |  1215  |   778  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.3600  |  3.0000  |  1.4500  |  2.6500  |  1.4400  |  2.6200  |  1.4700  |  2.8500  |  1.4400  |  2.6300  |  1.4700  |  3.2000  |  1.4200  |  2.7800  |\n     1  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |   Nieminen J.  |  Benneteau J.  |     41  |     35  |   927  |  1075  |   6  |   3  |   2  |   6  |   6  |   1  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.8000  |  1.9100  |  1.8000  |  2.1000  |  1.7300  |  2.0000  |  1.8000  |  2.2600  |  1.7300  |  2.0500  |\n     2  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |  Nishikori K.  |  Matosevic M.  |     19  |     49  |  1830  |   845  |   7  |   5  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.2500  |  3.7500  |  1.2500  |  3.7500  |  1.2900  |  3.5000  |  1.3000  |  3.8500  |  1.3000  |  3.2000  |  1.3000  |  4.2000  |  1.2800  |  3.5800  |\n     3  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |  Baghdatis M.  |   Mitchell B.  |     36  |    326  |  1070  |   137  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.0700  |  9.0000  |  1.0600  |  8.0000  |  1.0800  |  7.0000  |  1.0800  |  9.4300  |  1.0700  |  7.0000  |  1.1000  |  9.5000  |  1.0800  |  7.7600  |\n     4  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Istomin D.  |     Klizan M.  |     43  |     30  |   897  |  1175  |   6  |   1  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.9000  |  1.8000  |  1.8700  |  1.8700  |  1.9100  |  1.8000  |  1.8800  |  2.0000  |  1.9100  |  1.8000  |  2.0500  |  2.0000  |  1.8800  |  1.8500  |\n     5  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Millman J.  |        Ito T.  |    199  |     79  |   239  |   655  |   6  |   4  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.7300  |  2.0000  |  1.7000  |  2.2700  |  1.8000  |  1.9100  |  1.8500  |  2.2800  |  1.7100  |  2.0800  |\n     6  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |      Falla A.  |     Levine J.  |     54  |    104  |   809  |   530  |   6  |   1  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  2.2000  |  1.6100  |  2.0800  |  1.6700  |  1.9100  |  1.8000  |  2.2600  |  1.7000  |  2.0000  |  1.7300  |  2.3200  |  1.8300  |  2.0800  |  1.7000  |\n     7  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |     Melzer J.  |      Kudla D.  |     29  |    137  |  1177  |   402  |   2  |   6  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.4400  |  2.6200  |  1.5500  |  2.3500  |  1.4400  |  2.6200  |  1.6000  |  2.4700  |  1.5000  |  2.5000  |  1.6300  |  2.8200  |  1.5200  |  2.4600  |\n     8  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Robredo T.  |   Harrison R.  |    114  |     69  |   495  |   710  |   6  |   4  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  3.0000  |  1.3600  |  2.5000  |  1.5000  |  2.3800  |  1.5300  |  2.9300  |  1.4500  |  2.5000  |  1.5000  |  3.2500  |  1.5300  |  2.6600  |  1.4700  |\n     9  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |   Dimitrov G.  |      Baker B.  |     48  |     61  |   866  |   756  |   6  |   3  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.3600  |  3.0000  |  1.4000  |  2.8000  |  1.4400  |  2.6200  |  1.3800  |  3.3000  |  1.4000  |  2.7500  |  1.4500  |  3.5500  |  1.3900  |  2.8700  |\n\n\n\n\nRow Sorting\n\n\nKey Sort\n\n\nThere are several overloaded \nsort()\n methods on the \nDataFrameAxis\n interface, the most basic of which takes a \n\nboolean\n to signal ascending (\ntrue\n) or descending (\nfalse\n) order. The example below illustrates how to sort the ATP\n2013 match frame by descending row key order. In the unsorted frame, the row keys are monotonically increasing \nintegers so the sort effectively reverses the order of the frame revealing the last matches at the top of the \nframe.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Sort rows by row keys in descending order\nframe.rows().sort(false);\n//Print first ten rows\nframe.out().print(10);\n\n\n\n\n\n Index  |  Location  |  Tournament   |     Date     |    Series     |  Court   |  Surface  |     Round     |  Best of  |    Winner     |      Loser       |  WRank  |  LRank  |  WPts   |  LPts   |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |   B365L   |   EXW    |   EXL    |   LBW    |    LBL    |   PSW    |    PSL    |   SJW    |   SJL    |   MaxW   |   MaxL    |   AvgW   |   AvgL   |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  2630  |    London  |  Masters Cup  |  2013-11-11  |  Masters Cup  |  Indoor  |     Hard  |    The Final  |        3  |  Djokovic N.  |        Nadal R.  |      2  |      1  |  10610  |  12030  |   6  |   3  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.5700  |   2.3700  |  1.5500  |  2.4000  |  1.5700  |   2.3800  |  1.5800  |   2.5800  |  1.5700  |  2.3800  |  1.6500  |   2.5800  |  1.5700  |  2.4300  |\n  2629  |    London  |  Masters Cup  |  2013-11-10  |  Masters Cup  |  Indoor  |     Hard  |   Semifinals  |        3  |  Djokovic N.  |     Wawrinka S.  |      2  |      8  |  10610  |   3330  |   6  |   3  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.1400  |   5.5000  |  1.1300  |  5.5000  |  1.1400  |   5.5000  |  1.1900  |   5.6100  |  1.1500  |  5.5000  |  1.1900  |   6.3000  |  1.1600  |  5.2500  |\n  2628  |    London  |  Masters Cup  |  2013-11-10  |  Masters Cup  |  Indoor  |     Hard  |   Semifinals  |        3  |     Nadal R.  |      Federer R.  |      1  |      7  |  12030  |   3805  |   7  |   5  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.3000  |   3.5000  |  1.3300  |  3.2000  |  1.3000  |   3.5000  |  1.3400  |   3.6700  |  1.3300  |  3.4000  |  1.3700  |   4.0500  |  1.3300  |  3.3400  |\n  2627  |    London  |  Masters Cup  |  2013-11-09  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |  Djokovic N.  |      Gasquet R.  |      2  |      9  |  10610  |   3300  |   7  |   6  |   4  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.0600  |  10.0000  |  1.0500  |  9.0000  |  1.0500  |  10.0000  |  1.0700  |  10.7500  |  1.0500  |  9.0000  |  1.1000  |  10.7500  |  1.0600  |  9.2200  |\n  2626  |    London  |  Masters Cup  |  2013-11-09  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |   Federer R.  |  Del Potro J.M.  |      7  |      5  |   3805  |   5055  |   4  |   6  |   7  |   6  |   7  |   5  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  2.0000  |   1.8000  |  2.0000  |  1.8000  |  2.0000  |   1.8000  |  2.0700  |   1.8500  |  2.0000  |  1.8000  |  2.1000  |   1.9500  |  1.9800  |  1.8100  |\n  2625  |    London  |  Masters Cup  |  2013-11-08  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |     Nadal R.  |      Berdych T.  |      1  |      6  |  12030  |   3980  |   6  |   4  |   1  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.2000  |   4.5000  |  1.2200  |  4.0000  |  1.2000  |   4.5000  |  1.2700  |   4.2600  |  1.2200  |  4.0000  |  1.3000  |   4.6000  |  1.2300  |  4.1200  |\n  2624  |    London  |  Masters Cup  |  2013-11-08  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |  Wawrinka S.  |       Ferrer D.  |      8  |      3  |   3330  |   5800  |   6  |   7  |   6  |   4  |   6  |   1  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.4400  |   2.7500  |  1.4500  |  2.7000  |  1.4400  |   2.7500  |  1.5600  |   2.6200  |  1.4400  |  2.7500  |  1.5700  |   2.8000  |  1.4800  |  2.6300  |\n  2623  |    London  |  Masters Cup  |  2013-11-07  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |  Djokovic N.  |  Del Potro J.M.  |      2  |      5  |  10610  |   5055  |   6  |   3  |   3  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.2800  |   3.7500  |  1.2800  |  3.5000  |  1.3000  |   3.5000  |  1.3000  |   3.8900  |  1.2900  |  3.7500  |  1.3300  |   4.0500  |  1.2800  |  3.6100  |\n  2622  |    London  |  Masters Cup  |  2013-11-07  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |   Federer R.  |      Gasquet R.  |      7  |      9  |   3805  |   3300  |   6  |   4  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.2800  |   3.7500  |  1.3000  |  3.4000  |  1.2900  |   3.7500  |  1.3400  |   3.6000  |  1.3000  |  3.5000  |  1.3500  |   4.2000  |  1.2900  |  3.5900  |\n  2621  |    London  |  Masters Cup  |  2013-11-06  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |   Berdych T.  |       Ferrer D.  |      6  |      3  |   3980  |   5800  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  2.1000  |   1.7200  |  2.0000  |  1.8000  |  2.0000  |   1.8000  |  2.0600  |   1.8500  |  2.0000  |  1.8000  |  2.2000  |   1.8500  |  2.0300  |  1.7800  |\n\n\n\n\nValue Sort\n\n\nA more common scenario may be to sort rows according to values in a specific column. Any data type that implements the\n\nComparable\n interface can be sorted in this way (the same applies to sorting by keys as described in the previous section). \nIf the \nDataFrame\n happens to have a column of data whose type does not implement \nComparable\n and sorting according to\nthis data is required, a user provided \nComparator\n will have to be applied, which is discussed later.\n\n\nThe example below demonstrates how to sort the 2013 ATP match results by the rank of the eventual winner of the match (\nWRank\n). \nOddly, there appears to be a player ranked zero (namely Khachanov K.), and thus he appears first. Beyond this anomaly, you find \nthe expected names in the Winner column, namely Djokovic and Nadal.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Sort rows by the WRank (winner rank) column values\nframe.rows().sort(true, \"WRank\");\n//Print first ten rows\nframe.out().print(10);\n\n\n\n\n\n Index  |     Location     |          Tournament           |     Date     |     Series     |   Court   |  Surface  |      Round      |  Best of  |     Winner     |      Loser       |  WRank  |  LRank  |  WPts   |  LPts  |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |   B365L   |   EXW    |    EXL    |   LBW    |    LBL    |   PSW    |    PSL    |   SJW    |    SJL    |   MaxW   |   MaxL    |   AvgW   |   AvgL    |\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  2224  |  St. Petersburg  |          St. Petersburg Open  |  2013-09-16  |        ATP250  |   Indoor  |     Hard  |      1st Round  |        3  |  Khachanov K.  |      Hanescu V.  |      0  |     63  |      0  |   771  |   7  |   6  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  7.0000  |   1.1000  |  5.2500  |   1.1200  |  5.5000  |   1.1200  |  7.7400  |   1.1100  |  5.5000  |   1.1400  |  8.2000  |   1.1500  |  6.1400  |   1.1200  |\n   152  |       Melbourne  |              Australian Open  |  2013-01-14  |    Grand Slam  |  Outdoor  |     Hard  |      1st Round  |        5  |   Djokovic N.  |    Mathieu P.H.  |      1  |     60  |  12920  |   763  |   6  |   2  |   6  |   4  |   7  |   5  |   0  |   0  |   0  |   0  |      3  |      0  |  Completed  |  1.0020  |  29.0000  |  1.0200  |  15.0000  |  1.0200  |  13.0000  |  1.0100  |  41.0000  |  1.0100  |  19.0000  |  1.0200  |  46.0000  |  1.0100  |  21.7000  |\n  1121  |            Rome  |  Internazionali BNL d'Italia  |  2013-05-14  |  Masters 1000  |  Outdoor  |     Clay  |      2nd Round  |        3  |   Djokovic N.  |     Montanes A.  |      1  |     89  |  12730  |   573  |   6  |   2  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.0100  |  21.0000  |  1.0200  |  12.0000  |  1.0200  |  13.0000  |  1.0500  |  13.0000  |  1.0200  |  13.0000  |  1.0500  |  24.5000  |  1.0200  |  14.5500  |\n  1137  |            Rome  |  Internazionali BNL d'Italia  |  2013-05-16  |  Masters 1000  |  Outdoor  |     Clay  |      3rd Round  |        3  |   Djokovic N.  |   Dolgopolov O.  |      1  |     23  |  12730  |  1420  |   6  |   1  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.0600  |  10.0000  |  1.0800  |   7.0000  |  1.0600  |   9.0000  |  1.0800  |  10.7500  |  1.0800  |   7.5000  |  1.1000  |  11.5000  |  1.0700  |   8.5500  |\n  1592  |          London  |                    Wimbledon  |  2013-07-03  |    Grand Slam  |  Outdoor  |    Grass  |  Quarterfinals  |        5  |   Djokovic N.  |      Berdych T.  |      1  |      6  |  11830  |  4515  |   7  |   6  |   6  |   4  |   6  |   3  |   0  |   0  |   0  |   0  |      3  |      0  |  Completed  |  1.1000  |   7.0000  |  1.1200  |   6.0000  |  1.1200  |   6.0000  |  1.1300  |   7.9400  |     NaN  |      NaN  |  1.1500  |   7.9400  |  1.1200  |   6.4100  |\n   704  |    Indian Wells  |             BNP Paribas Open  |  2013-03-15  |  Masters 1000  |  Outdoor  |     Hard  |  Quarterfinals  |        3  |   Djokovic N.  |     Tsonga J.W.  |      1  |      8  |  13280  |  3660  |   6  |   3  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.0800  |   8.0000  |  8.0000  |   6.7100  |  1.1100  |   6.5000  |  1.1100  |   8.5000  |  1.1300  |   6.5000  |  1.1400  |   8.7000  |  1.1000  |   6.8800  |\n  1596  |          London  |                    Wimbledon  |  2013-07-05  |    Grand Slam  |  Outdoor  |    Grass  |     Semifinals  |        5  |   Djokovic N.  |  Del Potro J.M.  |      1  |      8  |  11830  |  3960  |   7  |   5  |   4  |   6  |   7  |   6  |   6  |   7  |   6  |   3  |      3  |      2  |  Completed  |  1.1600  |   5.0000  |  1.1200  |   6.0000  |  1.1400  |   5.5000  |  1.1600  |   6.6000  |  1.1400  |   6.0000  |  1.2000  |   6.6000  |  1.1400  |   5.6200  |\n   674  |    Indian Wells  |             BNP Paribas Open  |  2013-03-11  |  Masters 1000  |  Outdoor  |     Hard  |      2nd Round  |        3  |   Djokovic N.  |      Fognini F.  |      1  |     36  |  13280  |  1065  |   6  |   0  |   5  |   7  |   6  |   2  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.0020  |  29.0000  |  1.0100  |  15.0000  |  1.0100  |  17.0000  |  1.0100  |  33.8900  |  1.0100  |  15.0000  |  1.0400  |  41.0000  |  1.0100  |  20.1700  |\n   690  |    Indian Wells  |             BNP Paribas Open  |  2013-03-12  |  Masters 1000  |  Outdoor  |     Hard  |      3rd Round  |        3  |   Djokovic N.  |     Dimitrov G.  |      1  |     31  |  13280  |  1137  |   7  |   6  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.0200  |  17.0000  |  1.0300  |  10.0000  |  1.0400  |   9.0000  |  1.0200  |  23.0000  |  1.0500  |   9.0000  |  1.0500  |  23.0000  |  1.0300  |  12.7200  |\n  2611  |           Paris  |          BNP Paribas Masters  |  2013-11-01  |  Masters 1000  |   Indoor  |     Hard  |  Quarterfinals  |        3  |      Nadal R.  |      Gasquet R.  |      1  |     10  |  11670  |  3130  |   6  |   4  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.1400  |   5.5000  |  1.1300  |   5.5000  |  1.1400  |   5.5000  |  1.2000  |   5.3700  |  1.1400  |   5.5000  |  1.2000  |   6.3000  |  1.1500  |   5.2400  |\n\n\n\n\nMultidimensional Sort\n\n\nThe \nDataFrameAxis\n interface also supports a \nmultidimensional\n sort where the row order is first sorted by the values in one column \nand then by a second, third and so on. The example below again uses the 2013 ATP match results to sort first by the match \nDate\n column \n(because we expect to find many matches occurring on the same day), and then subsequently by the rank of the eventual winner (\nWRank\n). \nThe output below shows the first 10 rows of the sorted frame, with the earliest matches occurring on 2012-10-01 (it appears the ATP tour \ndoes not follow a calendar year), followed by a WRank sort which is also ascending.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Multidimensional row sort (ascending) first by Date, then WRank\nframe.rows().sort(true, Collect.asList(\"Date\", \"WRank\"));\n//Print first ten rows\nframe.out().print(10);\n\n\n\n\n\n Index  |  Location  |                Tournament                 |     Date     |  Series  |   Court   |  Surface  |    Round    |  Best of  |       Winner       |     Loser     |  WRank  |  LRank  |  WPts  |  LPts  |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  2342  |     Tokyo  |  Rakuten Japan Open Tennis Championships  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |       Tsonga J.W.  |   Monfils G.  |      8  |     42  |  3325  |  1030  |   6  |   3  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.4000  |  2.7500  |  1.4200  |  2.6500  |  1.4400  |  2.6200  |  1.5300  |  2.6600  |  1.4400  |  2.7500  |  1.5500  |  2.8500  |  1.4700  |  2.6300  |\n  2315  |   Beijing  |                               China Open  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |        Gasquet R.  |     Mayer F.  |     10  |     41  |  3005  |  1030  |   6  |   3  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.2500  |  3.7500  |  1.2800  |  3.3000  |  1.3000  |  3.4000  |  1.3600  |  3.4300  |  1.2900  |  3.5000  |  1.3600  |  3.8000  |  1.2900  |  3.4600  |\n  2340  |     Tokyo  |  Rakuten Japan Open Tennis Championships  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |        Almagro N.  |    Becker B.  |     17  |     73  |  1940  |   672  |   7  |   6  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.6100  |  2.2000  |  1.6200  |  2.1500  |  1.6700  |  2.1000  |  1.5700  |  2.5400  |  1.6200  |  2.2500  |  1.7100  |  2.5500  |  1.6000  |  2.2800  |\n  2310  |   Beijing  |                               China Open  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |        Fognini F.  |   Robredo T.  |     19  |     18  |  1840  |  1855  |   7  |   5  |   4  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  3.0000  |  1.3600  |  2.6500  |  1.4200  |  2.6200  |  1.4400  |  2.9300  |  1.4500  |  2.7500  |  1.4400  |  3.6500  |  1.4600  |  2.7900  |  1.4200  |\n  2312  |   Beijing  |                               China Open  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |  Kohlschreiber P.  |  Montanes A.  |     25  |     58  |  1445  |   836  |   7  |   5  |   1  |   6  |   7  |   6  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.1600  |  5.0000  |  1.1500  |  4.7500  |  1.1700  |  4.5000  |  1.1700  |  5.6500  |  1.1800  |  4.5000  |  1.2200  |  5.6500  |  1.1700  |  4.7500  |\n  2314  |   Beijing  |                               China Open  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |        Querrey S.  |   Youzhny M.  |     30  |     20  |  1265  |  1780  |   7  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  2.3700  |  1.5300  |  2.3000  |  1.5500  |  2.2500  |  1.5700  |  2.7000  |  1.5200  |  2.3800  |  1.5700  |  2.7500  |  1.6200  |  2.4900  |  1.5100  |\n  2341  |     Tokyo  |  Rakuten Japan Open Tennis Championships  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |     Dolgopolov O.  |    Brands D.  |     39  |     61  |  1080  |   813  |   6  |   3  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.5000  |  2.5000  |  1.5000  |  2.4000  |  1.5000  |  2.5000  |  1.5300  |  2.6600  |  1.5700  |  2.3800  |  1.6000  |  2.6600  |  1.5200  |  2.4700  |\n  2343  |     Tokyo  |  Rakuten Japan Open Tennis Championships  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |       Nieminen J.  |    Monaco J.  |     44  |     36  |   980  |  1115  |   7  |   6  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.6100  |  2.2000  |  1.6000  |  2.2000  |  1.6200  |  2.2000  |  1.7000  |  2.2700  |  1.6200  |  2.2500  |  1.7300  |  2.3800  |  1.6400  |  2.2000  |\n  2309  |   Beijing  |                               China Open  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |          Tomic B.  |     Zhang Z.  |     55  |    193  |   855  |   251  |   7  |   6  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.1400  |  5.0000  |  1.1500  |  4.7500  |  1.1700  |  4.5000  |  1.1800  |  5.4700  |  1.1400  |  5.5000  |  1.2200  |  5.5000  |  1.1600  |  4.9500  |\n  2313  |   Beijing  |                               China Open  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |         Hewitt L.  |      Haas T.  |     59  |     12  |   825  |  2265  |   7  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  2.3700  |  1.5300  |  2.3000  |  1.5500  |  2.2500  |  1.5700  |  2.4100  |  1.6200  |  2.3800  |  1.5700  |  2.5000  |  1.6200  |  2.3500  |  1.5700  |\n\n\n\n\nComparator Sort\n\n\nIf the two row sort mechanisms discussed above are functionally insufficient for your needs, a final \nsort()\n method \non the \nDataFrameAxis\n interface is provided which accepts a user implemented \nComparator\n, allowing for sorting logic \nof arbitrary complexity. \n\n\nDepending on the implementation of the \nComparator\n, performance may not be quite as good as the two prior use cases as \nthose mechanisms can leverage private access to the internal data structures of the frame. If that proves to be the case, \nusing parallel sort on a multi-core machine is likely to boost performance significantly (see section below which provides\nsome hard numbers on this topic).\n\n\nThe example below shows how to sort the 2013 ATP match results by a derived value (i.e. one where a column representing\nthe sort quantity does not exist on the frame). In this setup we apply a row sort such that rows that have the smallest \nabsolute difference between the betting odds on the eventual winner (\nAvgW\n) and eventual loser (\nAvgL\n) appear first.\nThis helps identity matches that as far as the bookmakers are concerned, are simply too close to call.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Sort rows so that matches smallest difference in betting odds between winner and looser.\nframe.rows().sort((row1, row2) -> {\n    double diff1 = Math.abs(row1.getDouble(\"AvgW\") - row1.getDouble(\"AvgL\"));\n    double diff2 = Math.abs(row2.getDouble(\"AvgW\") - row2.getDouble(\"AvgL\"));\n    return Double.compare(diff1, diff2);\n});\n//Print first ten rows\nframe.out().print(10);\n\n\n\n\n\n Index  |    Location     |                   Tournament                   |     Date     |     Series     |   Court   |  Surface  |      Round      |  Best of  |       Winner       |      Loser      |  WRank  |  LRank  |  WPts  |  LPts  |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n   640  |   Indian Wells  |                              BNP Paribas Open  |  2013-03-09  |  Masters 1000  |  Outdoor  |     Hard  |      1st Round  |        3  |          Blake J.  |       Haase R.  |     99  |     47  |   538  |   845  |   6  |   3  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.8300  |  1.8300  |  1.9000  |  1.8500  |  1.8300  |  1.8300  |  1.9300  |  1.9700  |  1.8300  |  1.9100  |  2.0800  |  1.9700  |  1.8800  |  1.8700  |\n  1981  |     Cincinnati  |    Western & Southern Financial Group Masters  |  2013-08-13  |  Masters 1000  |  Outdoor  |     Hard  |      1st Round  |        3  |           Haas T.  |    Anderson K.  |     13  |     19  |  2140  |  1740  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.9000  |  1.8000  |  1.8500  |  1.8500  |  1.9100  |  1.8000  |  1.9200  |  1.9900  |  1.9100  |  1.9100  |  1.9700  |  2.0400  |  1.8700  |  1.8800  |\n  2050  |  Winston-Salem  |  Winston-Salem Open at Wake Forest University  |  2013-08-20  |        ATP250  |  Outdoor  |     Hard  |      2nd Round  |        3  |         Melzer J.  |   De Bakker T.  |     32  |    101  |  1220  |   547  |   7  |   5  |   6  |   7  |   4  |   2  |   0  |   0  |   0  |   0  |      1  |      1  |    Retired  |  1.8300  |  1.8300  |  1.8500  |  1.8500  |  1.8300  |  1.8300  |  1.9300  |  1.9800  |  1.8300  |  1.9100  |  1.9300  |  1.9800  |  1.8700  |  1.8800  |\n   341  |         Zagreb  |                            PBZ Zagreb Indoors  |  2013-02-08  |        ATP250  |   Indoor  |     Hard  |  Quarterfinals  |        3  |          Haase R.  |  Petzschner P.  |     58  |    122  |   755  |   436  |   6  |   4  |   3  |   6  |   6  |   0  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.8300  |  1.8300  |  1.8500  |  1.9000  |  1.8300  |  1.8300  |  1.9500  |  1.9700  |  1.8000  |  2.0000  |  2.0000  |  2.1000  |  1.8700  |  1.8800  |\n   937  |      Barcelona  |                           Open Banco Sabadell  |  2013-04-24  |        ATP500  |  Outdoor  |     Clay  |      2nd Round  |        3  |         Klizan M.  |    Montanes A.  |     31  |     80  |  1205  |   630  |   6  |   3  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.8300  |  1.8300  |  1.8500  |  1.9000  |  1.7300  |  2.0000  |  2.0400  |  1.8700  |  1.8000  |  2.0000  |  2.0400  |  2.0000  |  1.8800  |  1.8700  |\n  2545  |       Valencia  |                             Valencia Open 500  |  2013-10-22  |        ATP500  |   Indoor  |     Hard  |      1st Round  |        3  |      Benneteau J.  |       Lopez F.  |     33  |     30  |  1195  |  1275  |   6  |   3  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.9000  |  1.8000  |  1.9000  |  1.8500  |  1.9100  |  1.8000  |  1.9900  |  1.9000  |  1.9000  |  1.9000  |  2.0400  |  1.9500  |  1.8800  |  1.8700  |\n   729  |          Miami  |                            Sony Ericsson Open  |  2013-03-21  |  Masters 1000  |  Outdoor  |     Hard  |      1st Round  |        3  |          Falla A.  |       Soeda G.  |     60  |     83  |   776  |   600  |   7  |   5  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.8000  |  1.9000  |  1.8500  |  1.9000  |  1.8300  |  1.8300  |  1.9100  |  2.0000  |  2.0000  |  1.8000  |  2.0000  |  2.0500  |  1.8800  |  1.8700  |\n   858  |    Monte Carlo  |                           Monte Carlo Masters  |  2013-04-14  |  Masters 1000  |  Outdoor  |     Clay  |      1st Round  |        3  |  Kohlschreiber P.  |    Bellucci T.  |     21  |     39  |  1670  |   987  |   6  |   4  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.8300  |  1.8300  |  1.9000  |  1.8500  |  1.9100  |  1.8000  |  1.9300  |  1.9700  |  1.9000  |  1.9000  |  2.0000  |  2.0000  |  1.8700  |  1.8800  |\n  2000  |     Cincinnati  |    Western & Southern Financial Group Masters  |  2013-08-14  |  Masters 1000  |  Outdoor  |     Hard  |      2nd Round  |        3  |          Isner J.  |     Gasquet R.  |     22  |     11  |  1585  |  2625  |   7  |   6  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.8000  |  1.9000  |  1.8500  |  1.8500  |  1.7300  |  2.0000  |  2.0200  |  1.8700  |  1.9100  |  1.8300  |  2.0200  |  2.0800  |  1.8700  |  1.8800  |\n   632  |   Indian Wells  |                              BNP Paribas Open  |  2013-03-08  |  Masters 1000  |  Outdoor  |     Hard  |      1st Round  |        3  |      Matosevic M.  |     Robredo T.  |     53  |     69  |   813  |   680  |   7  |   5  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.8300  |  1.8300  |  1.8500  |  1.9000  |  1.8300  |  1.8300  |  1.9300  |  1.9800  |  1.8300  |  1.9100  |  1.9500  |  2.1000  |  1.8800  |  1.8700  |\n\n\n\n\nColumn Sorting\n\n\nKey Sort\n\n\nThe Morpheus API is almost entirely symmetrical in the row and column dimension, so the sorting mechanisms presented \nin the previous sections can also be used to sort columns. The first code example below sorts the columns of the 2013 \nATP match results in ascending order by calling the \nsort(ascending=true)\n method on the \nDataFrameAxis\n returned \nfrom \nDataFrame.cols()\n.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Sort columns by column keys in ascending order\nframe.cols().sort(true);\n//Print first ten rows\nframe.out().print(10);\n\n\n\n\n\n Index  |   AvgL   |   AvgW   |  B365L   |  B365W   |  Best of  |   Comment   |   Court   |     Date     |   EXL    |   EXW    |  L1  |  L2  |  L3  |  L4  |  L5  |   LBL    |   LBW    |  LPts  |  LRank  |  Location  |     Loser      |  Lsets  |   MaxL   |   MaxW   |   PSL    |   PSW    |    Round    |   SJL    |   SJW    |  Series  |  Surface  |        Tournament        |  W1  |  W2  |  W3  |  W4  |  W5  |  WPts  |  WRank  |     Winner     |  Wsets  |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     0  |  2.7800  |  1.4200  |  3.0000  |  1.3600  |        3  |  Completed  |  Outdoor  |  2012-12-31  |  2.6500  |  1.4500  |   4  |   4  |   0  |   0  |   0  |  2.6200  |  1.4400  |   778  |     57  |  Brisbane  |    Giraldo S.  |      0  |  3.2000  |  1.4700  |  2.8500  |  1.4700  |  1st Round  |  2.6300  |  1.4400  |  ATP250  |     Hard  |  Brisbane International  |   6  |   6  |   0  |   0  |   0  |  1215  |     28  |      Mayer F.  |      2  |\n     1  |  2.0500  |  1.7300  |  2.2000  |  1.6100  |        3  |  Completed  |  Outdoor  |  2012-12-31  |  2.0000  |  1.7500  |   3  |   6  |   1  |   0  |   0  |  1.9100  |  1.8000  |  1075  |     35  |  Brisbane  |  Benneteau J.  |      1  |  2.2600  |  1.8000  |  2.1000  |  1.8000  |  1st Round  |  2.0000  |  1.7300  |  ATP250  |     Hard  |  Brisbane International  |   6  |   2  |   6  |   0  |   0  |   927  |     41  |   Nieminen J.  |      2  |\n     2  |  3.5800  |  1.2800  |  3.7500  |  1.2500  |        3  |  Completed  |  Outdoor  |  2012-12-31  |  3.7500  |  1.2500  |   5  |   2  |   0  |   0  |   0  |  3.5000  |  1.2900  |   845  |     49  |  Brisbane  |  Matosevic M.  |      0  |  4.2000  |  1.3000  |  3.8500  |  1.3000  |  1st Round  |  3.2000  |  1.3000  |  ATP250  |     Hard  |  Brisbane International  |   7  |   6  |   0  |   0  |   0  |  1830  |     19  |  Nishikori K.  |      2  |\n     3  |  7.7600  |  1.0800  |  9.0000  |  1.0700  |        3  |  Completed  |  Outdoor  |  2012-12-31  |  8.0000  |  1.0600  |   4  |   4  |   0  |   0  |   0  |  7.0000  |  1.0800  |   137  |    326  |  Brisbane  |   Mitchell B.  |      0  |  9.5000  |  1.1000  |  9.4300  |  1.0800  |  1st Round  |  7.0000  |  1.0700  |  ATP250  |     Hard  |  Brisbane International  |   6  |   6  |   0  |   0  |   0  |  1070  |     36  |  Baghdatis M.  |      2  |\n     4  |  1.8500  |  1.8800  |  1.8000  |  1.9000  |        3  |  Completed  |  Outdoor  |  2013-01-01  |  1.8700  |  1.8700  |   1  |   2  |   0  |   0  |   0  |  1.8000  |  1.9100  |  1175  |     30  |  Brisbane  |     Klizan M.  |      0  |  2.0000  |  2.0500  |  2.0000  |  1.8800  |  1st Round  |  1.8000  |  1.9100  |  ATP250  |     Hard  |  Brisbane International  |   6  |   6  |   0  |   0  |   0  |   897  |     43  |    Istomin D.  |      2  |\n     5  |  2.0800  |  1.7100  |  2.2000  |  1.6100  |        3  |  Completed  |  Outdoor  |  2013-01-01  |  2.0000  |  1.7500  |   4  |   1  |   0  |   0  |   0  |  2.0000  |  1.7300  |   655  |     79  |  Brisbane  |        Ito T.  |      0  |  2.2800  |  1.8500  |  2.2700  |  1.7000  |  1st Round  |  1.9100  |  1.8000  |  ATP250  |     Hard  |  Brisbane International  |   6  |   6  |   0  |   0  |   0  |   239  |    199  |    Millman J.  |      2  |\n     6  |  1.7000  |  2.0800  |  1.6100  |  2.2000  |        3  |  Completed  |  Outdoor  |  2013-01-01  |  1.6700  |  2.0800  |   1  |   6  |   0  |   0  |   0  |  1.8000  |  1.9100  |   530  |    104  |  Brisbane  |     Levine J.  |      0  |  1.8300  |  2.3200  |  1.7000  |  2.2600  |  1st Round  |  1.7300  |  2.0000  |  ATP250  |     Hard  |  Brisbane International  |   6  |   7  |   0  |   0  |   0  |   809  |     54  |      Falla A.  |      2  |\n     7  |  2.4600  |  1.5200  |  2.6200  |  1.4400  |        3  |  Completed  |  Outdoor  |  2013-01-01  |  2.3500  |  1.5500  |   6  |   4  |   4  |   0  |   0  |  2.6200  |  1.4400  |   402  |    137  |  Brisbane  |      Kudla D.  |      1  |  2.8200  |  1.6300  |  2.4700  |  1.6000  |  1st Round  |  2.5000  |  1.5000  |  ATP250  |     Hard  |  Brisbane International  |   2  |   6  |   6  |   0  |   0  |  1177  |     29  |     Melzer J.  |      2  |\n     8  |  1.4700  |  2.6600  |  1.3600  |  3.0000  |        3  |  Completed  |  Outdoor  |  2013-01-01  |  1.5000  |  2.5000  |   4  |   6  |   0  |   0  |   0  |  1.5300  |  2.3800  |   710  |     69  |  Brisbane  |   Harrison R.  |      0  |  1.5300  |  3.2500  |  1.4500  |  2.9300  |  1st Round  |  1.5000  |  2.5000  |  ATP250  |     Hard  |  Brisbane International  |   6  |   7  |   0  |   0  |   0  |   495  |    114  |    Robredo T.  |      2  |\n     9  |  2.8700  |  1.3900  |  3.0000  |  1.3600  |        3  |  Completed  |  Outdoor  |  2013-01-01  |  2.8000  |  1.4000  |   3  |   6  |   0  |   0  |   0  |  2.6200  |  1.4400  |   756  |     61  |  Brisbane  |      Baker B.  |      0  |  3.5500  |  1.4500  |  3.3000  |  1.3800  |  1st Round  |  2.7500  |  1.4000  |  ATP250  |     Hard  |  Brisbane International  |   6  |   7  |   0  |   0  |   0  |   866  |     48  |   Dimitrov G.  |      2  |\n\n\n\n\nValue Sort\n\n\nTo sort columns according to values in one or more rows, the values need to be of a homogeneous type. It doesn't make\nmuch sense to sort the 2013 ATP match results in this way, and in fact you will get a \nClassCastException\n when a \nString\n\nis compared to a \nDouble\n value by calling its \ncompareTo()\n method. You could of course implement a custom \nComparator\n\nand sort the columns based on the \ntoString()\n of all the values, but it's hard to see what the value is in that.\n\n\nTo demonstrate a column sort based on row data, the following example creates a \n10x10\n frame initialized with random double \nprecision values. Given the homogeneity of the types along the rows in this frame, a column sort is a reasonable proposition. \nThe column keys in the unsorted frame are \nC0\n, \nC1\n, \nC2\n to \nC9\n, and the new order post sort is shown below. \n\n\n\n\n\n//Create a 10x10 frame initialized with random doubles\nDataFrame<String,String> frame = DataFrame.ofDoubles(\n    Range.of(0, 10).map(i -> \"R\" + i),\n    Range.of(0, 10).map(i -> \"C\" + i), \n    value -> Math.random() * 100d\n);\n//Sort columns by the data in first row in ascending order\nframe.cols().sort(true, \"R0\");\n//Print first ten rows\nframe.out().print(10);\n\n\n\n\n\n Index  |      C3       |      C1       |      C8       |      C7       |      C4       |      C6       |      C9       |      C2       |      C0       |      C5       |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    R0  |  11.70306561  |  29.51025702  |  44.56451853  |  49.14276083  |  49.30009688  |  49.66602735  |  59.72386052  |  64.69244099  |  83.13030538  |  98.60728081  |\n    R1  |  96.31980655  |  85.87996651  |  79.06930692  |  48.08819333  |  84.33796355  |   8.83992696  |  73.19731538  |  65.38285662  |  71.82216915  |   80.3000506  |\n    R2  |  49.53677506  |   9.45207021  |   1.54748873  |  57.99794713  |  20.02291114  |   8.96502162  |   46.1696664  |  77.33926623  |  34.21577299  |  94.85120102  |\n    R3  |   46.3516546  |  74.97630129  |  73.49083701  |  27.68089003  |  33.72130957  |  52.23839606  |  65.37188146  |  48.36073597  |  51.23686989  |  95.92785717  |\n    R4  |  72.01951644  |  39.75814373  |  69.23633177  |  46.96902638  |  70.82327915  |  91.53783289  |  57.33616312  |  38.94393019  |   1.68280501  |   87.2170405  |\n    R5  |  62.49904384  |   7.52041005  |  34.26879199  |  47.23321809  |   99.3061582  |  98.79472512  |    4.5477916  |  65.23264192  |   3.96089302  |  24.99717992  |\n    R6  |  42.63425518  |  26.61492898  |  16.50642183  |  15.04284983  |   67.1590594  |  44.72721238  |  77.61992004  |  53.96632198  |  60.65017717  |  31.01324133  |\n    R7  |   76.7178214  |   11.1839268  |  93.51531804  |  66.05863202  |  46.75806412  |   7.82498305  |  17.40931894  |  94.65796849  |  55.14862369  |  28.87315085  |\n    R8  |   48.7670682  |   1.44288709  |  51.16369446  |   32.3675847  |  93.95447717  |  10.09004133  |    6.6242657  |  69.63114684  |  37.92679795  |    2.1543318  |\n    R9  |  42.25961018  |  55.62679387  |   1.01423147  |  22.75711435  |  85.05398313  |  92.11731992  |  79.68760819  |  29.04198229  |  97.24438511  |  17.33341345  |\n\n\n\n\nWe can of course sort by data in both the row and column dimension as follows:\n\n\n\n\n\n//Create a 10x10 frame initialized with random doubles\nDataFrame<String,String> frame = DataFrame.ofDoubles(\n    Range.of(0, 10).map(i -> \"R\" + i),\n    Range.of(0, 10).map(i -> \"C\" + i),\n    value -> Math.random() * 100d\n);\n//Sort columns by the data in first row\nframe.cols().sort(true, \"R0\");\n//Sort rows by the data that is now in the first column\nframe.rows().sort(true, frame.cols().key(0));\n//Print first ten rows\nframe.out().print(10);\n\n\n\n\n\n Index  |      C8       |      C2       |      C1       |      C6       |      C7       |      C9       |      C4       |      C3       |      C0       |      C5       |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    R6  |  10.79109008  |   98.6524288  |  84.86273268  |  94.70606475  |  53.38375256  |  61.10043302  |  15.12273981  |  69.89835361  |  57.66658584  |  57.27005464  |\n    R0  |  11.43931147  |  20.54256763  |  23.79181572  |  27.47288193  |  29.95263252  |  75.42157524  |  76.42720119  |  77.22820182  |  77.67104703  |  95.18532196  |\n    R5  |  36.90300534  |  37.55942619  |  73.84924663  |  96.27509348  |  89.51587605  |  56.83564479  |   8.43705948  |  65.56679789  |  26.18719945  |  19.35039888  |\n    R8  |  48.92084538  |  74.04830396  |  15.13429492  |  79.55867974  |  69.78011977  |  92.60505236  |   0.90437748  |  66.04955254  |   4.24454634  |  16.08194424  |\n    R1  |  49.72933364  |  23.54721145  |  13.76578354  |  36.82903111  |  89.45754038  |  88.36190839  |  88.35626403  |   57.3339813  |   5.07104083  |   64.8812482  |\n    R3  |  50.05896919  |  20.99271329  |   19.3462031  |  84.85646161  |  18.70819105  |  36.02397058  |  39.60181063  |  97.98311929  |  23.15751737  |  11.19755199  |\n    R2  |  70.91560241  |  48.22161882  |  36.04857549  |  17.43486886  |  53.59280303  |  24.17841927  |   7.32760917  |    93.077888  |  95.69590224  |   70.1948771  |\n    R7  |  71.66128613  |  25.04195311  |  25.24586372  |  45.55143352  |   1.95783573  |  56.47302595  |  84.33080973  |  47.08666346  |  21.68500396  |  63.38464116  |\n    R9  |   72.5569825  |   3.69441849  |  40.53699445  |  58.65763148  |   3.58546753  |  73.53537208  |   7.97752157  |  15.53034669  |    0.8829291  |  44.36481597  |\n    R4  |  75.16120178  |  42.44021922  |  72.00614468  |  79.09004161  |   4.43077647  |  43.89845937  |  44.84293408  |  62.85029955  |  77.98911497  |  48.24598168  |\n\n\n\n\nFilters\n\n\nSlicing or filtering a \nDataFrame\n is a common operation and is discussed in some detail \nhere\n. There are \nstructural immutability constraints on filtered frames, however sorting in both the row and column dimensions is \nsupported.\n\n\nThe example below first filters the 2013 ATP match results so as to capture all rows in which Novak Djokovic was the victor, \nand then sorts these rows in ascending order according to the rank of the match loser (\nLRank\n). In that way, the highest\nrank players he beat appear first, so it is no surprise to see names like Nadal, Murray, Federer and so on.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//First filter frame to include only rows where Novak Djokovic was the victor\nDataFrame<Integer,String> filter = frame.rows().select(row -> row.getValue(\"Winner\").equals(\"Djokovic N.\"));\n//Sort rows so that the highest rank players he beat come first\nfilter.rows().sort(true, \"LRank\");\n//Print first ten rows\nfilter.out().print(10);\n\n\n\n\n\n Index  |   Location    |          Tournament          |     Date     |     Series     |   Court   |  Surface  |     Round     |  Best of  |    Winner     |      Loser       |  WRank  |  LRank  |  WPts   |  LPts   |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  2630  |       London  |                 Masters Cup  |  2013-11-11  |   Masters Cup  |   Indoor  |     Hard  |    The Final  |        3  |  Djokovic N.  |        Nadal R.  |      2  |      1  |  10610  |  12030  |   6  |   3  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.5700  |  2.3700  |  1.5500  |  2.4000  |  1.5700  |  2.3800  |  1.5800  |  2.5800  |  1.5700  |  2.3800  |  1.6500  |  2.5800  |  1.5700  |  2.4300  |\n  2339  |      Beijing  |                  China Open  |  2012-10-07  |        ATP500  |  Outdoor  |     Hard  |    The Final  |        3  |  Djokovic N.  |        Nadal R.  |      1  |      2  |  11120  |  10860  |   6  |   3  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.7200  |  2.1000  |  1.7000  |  2.0500  |  1.7300  |  2.1000  |  1.8000  |  2.1500  |  1.6700  |  2.2000  |  1.8300  |  2.2000  |  1.7400  |  2.0600  |\n   265  |    Melbourne  |             Australian Open  |  2013-01-27  |    Grand Slam  |  Outdoor  |     Hard  |    The Final  |        5  |  Djokovic N.  |       Murray A.  |      1  |      3  |  12920  |   8000  |   6  |   7  |   7  |   6  |   6  |   3  |   6  |   2  |   0  |   0  |      3  |      1  |  Completed  |  1.5300  |  2.6200  |  1.5000  |  2.5500  |  1.5300  |  2.5000  |  1.5400  |  2.7400  |  1.5300  |  2.6300  |  1.5700  |  2.9000  |  1.5000  |  2.6100  |\n  2615  |        Paris  |         BNP Paribas Masters  |  2013-11-03  |  Masters 1000  |   Indoor  |     Hard  |    The Final  |        3  |  Djokovic N.  |       Ferrer D.  |      2  |      3  |  11120  |   6600  |   7  |   5  |   7  |   5  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.1200  |  6.0000  |  1.1500  |  5.2500  |  1.1400  |  5.5000  |  1.1700  |  6.0000  |  1.1500  |  5.5000  |  1.1500  |  5.4200  |  1.1800  |  6.5000  |\n  2623  |       London  |                 Masters Cup  |  2013-11-07  |   Masters Cup  |   Indoor  |     Hard  |  Round Robin  |        3  |  Djokovic N.  |  Del Potro J.M.  |      2  |      5  |  10610  |   5055  |   6  |   3  |   3  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.2800  |  3.7500  |  1.2800  |  3.5000  |  1.3000  |  3.5000  |  1.3000  |  3.8900  |  1.2900  |  3.7500  |  1.3300  |  4.0500  |  1.2800  |  3.6100  |\n   263  |    Melbourne  |             Australian Open  |  2013-01-24  |    Grand Slam  |  Outdoor  |     Hard  |   Semifinals  |        5  |  Djokovic N.  |       Ferrer D.  |      1  |      5  |  12920  |   6505  |   6  |   2  |   6  |   2  |   6  |   1  |   0  |   0  |   0  |   0  |      3  |      0  |  Completed  |  1.0800  |  8.0000  |  1.1000  |  6.5000  |  1.1100  |  6.5000  |  1.1200  |  7.9400  |  1.1000  |  8.0000  |  1.1400  |  9.4000  |  1.1000  |  6.9100  |\n  2425  |     Shanghai  |            Shanghai Masters  |  2013-10-13  |  Masters 1000  |  Outdoor  |     Hard  |    The Final  |        3  |  Djokovic N.  |  Del Potro J.M.  |      2  |      5  |  11120  |   4925  |   6  |   1  |   3  |   6  |   7  |   6  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.3300  |  3.4000  |  1.3500  |  2.9000  |  1.3600  |  3.2500  |  1.3800  |  3.4100  |  1.3600  |  3.2500  |  1.4000  |  3.5000  |  1.3600  |  3.1500  |\n   912  |  Monte Carlo  |         Monte Carlo Masters  |  2013-04-21  |  Masters 1000  |  Outdoor  |     Clay  |    The Final  |        3  |  Djokovic N.  |        Nadal R.  |      1  |      5  |  12500  |   6385  |   6  |   2  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  2.8700  |  1.4000  |  2.6500  |  1.4500  |  2.7500  |  1.4400  |  2.9700  |  1.4600  |  2.7500  |  1.4400  |  3.0000  |  1.5000  |  2.7600  |  1.4400  |\n  2613  |        Paris  |         BNP Paribas Masters  |  2013-11-02  |  Masters 1000  |   Indoor  |     Hard  |   Semifinals  |        3  |  Djokovic N.  |      Federer R.  |      2  |      6  |  11120  |   4245  |   4  |   6  |   6  |   3  |   6  |   2  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.2800  |  3.7500  |  1.2500  |  3.7500  |  1.2900  |  3.7500  |  1.3100  |  3.9400  |  1.2900  |  3.7500  |  1.3200  |  4.4000  |  1.2800  |  3.6200  |\n   613  |        Dubai  |  Dubai Tennis Championships  |  2013-03-02  |        ATP500  |  Outdoor  |     Hard  |    The Final  |        3  |  Djokovic N.  |      Berdych T.  |      1  |      6  |  12960  |   4545  |   7  |   5  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.1100  |  6.5000  |  1.1400  |  5.2000  |  1.1100  |  6.0000  |  1.1500  |  6.5400  |  1.1300  |  6.5000  |  1.1500  |  8.7000  |  1.1200  |  6.1700  |\n\n\n\n\nPerformance\n\n\nMorpheus has been designed specifically to deal with very large data, and the section on general performance\nwhich is discussed \nhere\n and \nhere\n describes some of the implementation choices and their rationale. \nMorpheus is not a traditional Big Data solution in that it is focused on addressing a class of problem that \ncan fit in the memory space of a single operating system process. These days it is not uncommon to find machines \nwith 250GB of physical memory, which can certainly be used to analyze substantial data. A distributed solution\nusing Morpheus while possible, is not addressed as part of the framework, at least not at the time of writing.\n\n\nGreat care has been spent designing efficient data structures that scale well on the Java Virtual Machine, and\nmany of the common \nDataFrame\n operations can execute in \nparallel\n based on internal algorithms that leverage\nthe fork and join framework introduced in Java 7. Sorting is one such operation.\n\n\nSequential Sorting\n\n\nBy default, \nDataFrame\n functions implement sequential routines unless a concurrent implementation is requested\nby calling the \nparallel()\n method on the interface in question. This design has been influenced by the \nJava 8 Stream API, which is not only fluid to work with, but should also serve to reduce the learning curve\nsomewhat (assuming you are familiar with some of these new APIs introduced in JDK 8).\n\n\nThe test results presented below were performed on a late 2013 Macbook Pro with a 2.6Ghz Core i7 Quad Core \nprocessor. Like all benchmarks, they should be taken with a pinch of salt, but the key point to convey here is \nthe substantial gain in performance of the \nparallel\n sort over the \nsequential\n while adding no implementation \ncomplexity to the end user.\n\n\nIn all cases, these benchmarks execute 10 runs of each scenario, and compute summary statistics on the timimg results,\nnamely a \nmin\n, \nmax\n, \nmean\n, \nmedian\n and \nstandard deviation\n. This should give adequate time for the Hotspot \nto perform its magic, and should also give us a sense on the level of dispersion we see across the runs. Obviously \nthe lower the dispersion, the more confidence we can have in the results.\n\n\nThe code below is the full benchmark. Before each test run, we call \nDataFrame.rows().sort(null)\n which is effectively\npassing a null comparator, and thereby resetting the frame to its original unsorted state. If we didn't do this\nthe first test would be much slower as subsequent tests would be asked to sort an already sorted frame. We would\nsee a very big difference between the \nmin\n and \nmax\n sort times if that was the case.\n\n\n\n\n\n//Define range of row counts we want to test, from 1M to 5M inclusive\nRange<Integer> rowCounts = Range.of(1, 6).map(i -> i * 1000000);\n\n//Time DataFrame sort operations on frame of random doubles with row counts ranging from 1M to 6M\nDataFrame<String,String> results = DataFrame.combineFirst(rowCounts.map(rowCount -> {\n    Range<Integer> rowKeys = Range.of(0, rowCount.intValue());\n    Range<String> colKeys = Range.of(0, 5).map(i -> \"C\" + i);\n    //Create frame initialized with random double values\n    DataFrame<Integer,String> frame = DataFrame.ofDoubles(rowKeys, colKeys, v -> Math.random() * 100d);\n    String label = \"Rows(\" + (rowCount / 1000000) + \"M)\";\n    //Run each test 10 times, clear the sort before running the test with sort(null)\n    return PerfStat.run(10, TimeUnit.MILLISECONDS, false, tasks -> {\n        tasks.beforeEach(() -> frame.rows().sort(null));\n        tasks.put(label, () -> frame.rows().sort(true, \"C1\"));\n    });\n}));\n\n//Plot the results of the combined DataFrame with timings\nChart.create().withBarPlot(results, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time In Milliseconds\");\n    chart.title().withText(\"DataFrame Sorting Performance (Sequential)\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.subtitle().withText(\"Row Sort with counts from 1M to 5M rows\");\n    chart.legend().on().bottom();\n    chart.show();\n});\n\n\n\n\nThe results in the chart below show a fairly linear increase in sort times as we progress from a frame with 1 million \nto 5 million rows. The dispersion in the 10 iterations as measured by the standard deviation is fairly low in all cases, \nat least as a percentage of the time taken.\n\n\n\n    \n\n\n\n\n\nParallel Sorting\n\n\nThe sequential benchmark code presented in the previous section can be ever so slightly modified to leverage a parallel \nsort by ensuring that we call \nDataFrame.rows().parallel().sort()\n. The modified code is as follows:\n\n\n\n\n\n//Define range of row counts we want to test, from 1M to 5M inclusive\nRange<Integer> rowCounts = Range.of(1, 6).map(i -> i * 1000000);\n\n//Time DataFrame sort operations on frame of random doubles with row counts ranging from 1M to 6M\nDataFrame<String,String> results = DataFrame.combineFirst(rowCounts.map(rowCount -> {\n    Range<Integer> rowKeys = Range.of(0, rowCount.intValue());\n    Range<String> colKeys = Range.of(0, 5).map(i -> \"C\" + i);\n    //Create frame initialized with random double values\n    DataFrame<Integer,String> frame = DataFrame.ofDoubles(rowKeys, colKeys, v -> Math.random() * 100d);\n    String label = \"Rows(\" + (rowCount / 1000000) + \"M)\";\n    //Run each test 10 times, clear the sort before running the test with sort(null)\n    return PerfStat.run(10, TimeUnit.MILLISECONDS, false, tasks -> {\n        tasks.beforeEach(() -> frame.rows().sort(null));\n        tasks.put(label, () -> frame.rows().parallel().sort(true, \"C1\"));\n    });\n}));\n\n//Plot the results of the combined DataFrame with timings\nChart.create().withBarPlot(results, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time In Milliseconds\");\n    chart.title().withText(\"DataFrame Sorting Performance (Parallel)\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.subtitle().withText(\"Row Sort with counts from 1M to 5M rows\");\n    chart.legend().on().bottom();\n    chart.show();\n});\n\n\n\n\nThe plot with the parallel execution results below shows a very significant improvement, and also has lower \ndispersion in the results. For the largest frame with 5 million rows the \nmedian\n sort time went from around \n1400 milliseconds in the sequential case to just over 500 milliseconds with the parallel execution. As processor \ncache sizes increase and ever more cores become available, we can only expect this spread to widen as Moore's \nLaw begins to plateau.\n\n\n\n    \n\n\n\n\n\nComparator Performance\n\n\nA final performance comparison that may be of interest is to consider the relative cost of sorting\nwith a user provided \nComparator\n as opposed to using the \nsort()\n functions that just take a row\nor column key. The example below addresses this question by sorting a frame with 1 million rows\nand using identical logic where in one case we simply sort the rows based on a column key, and\nin the second we provide a \nComparator\n implementation that effectively does the same thing. We \nalso run a sequential and parallel setup for these two cases, so 4 test scenarios in all. For each\nscenario, we run 10 iterations so we can collect some summary statistics. The benchmark code is as\nfollows:\n\n\n\n\n\n//Create frame initialized with random double values\nRange<Integer> rowKeys = Range.of(0, 1000000);\nRange<String> colKeys = Range.of(0, 5).map(i -> \"C\" + i);\nDataFrame<Integer,String> frame = DataFrame.ofDoubles(rowKeys, colKeys, v -> Math.random() * 100d);\n//Define comparator to sort rows by column C1, which is ordinal 1\nComparator<DataFrameRow<Integer,String>> comparator = (row1, row2) -> {\n    double v1 = row1.getDouble(1);\n    double v2 = row2.getDouble(1);\n    return Double.compare(v1, v2);\n};\n\n//Time sorting in various modes (with & without comparator in both sequential & parallel mode)\nDataFrame<String,String> results = PerfStat.run(10, TimeUnit.MILLISECONDS, false, tasks -> {\n    tasks.beforeEach(() -> frame.rows().sort(null));\n    tasks.put(\"W/O Comparator (seq)\", () -> frame.rows().sort(true, \"C1\"));\n    tasks.put(\"W/O Comparator (par)\", () -> frame.rows().parallel().sort(true, \"C1\"));\n    tasks.put(\"W/ Comparator (seq)\", () -> frame.rows().sort(comparator));\n    tasks.put(\"W/ Comparator (par)\", () -> frame.rows().parallel().sort(comparator));\n});\n\n//Plot the results of the combined DataFrame with timings\nChart.create().withBarPlot(results, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time In Milliseconds\");\n    chart.title().withText(\"DataFrame Sorting Performance With & Without Comparator\");\n    chart.subtitle().withText(\"1 Million rows of random double precision values\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.legend().on().bottom();\n    chart.show();\n});\n\n\n\n\nThe test results presented below are not entirely surprising, and seem to suggest that the user \nComparator\n\nbased sort is more than twice as slow as the built in sort. As might be expected, the built in sort\nhas private access to the internal structures of the \nDataFrame\n, and therefore has to make fewer calls\nto get to the data in question. An interesting outcome in the sequential versus parallel battle\nis that the parallel sort with the \nComparator\n is still faster than the sequential sort without\nthe \nComparator\n. \n\n\n\n    \n\n\n\n\n\nThese performance benchmarks are obviously very controlled test cases, and real world performance is likely\nto differ for all kinds of reasons. In addition, at the time of writing the Morpheus Library is at version 1.0, \nso performance improvements are likely to come in follow up releases.",
            "title": "Sorting"
        },
        {
            "location": "/frame/sorting/#sorting",
            "text": "",
            "title": "Sorting"
        },
        {
            "location": "/frame/sorting/#introduction",
            "text": "The Morpheus  DataFrame  can be sorted in both the row and column dimension, either by the axis keys, or by \nvalues in one or more columns or rows respectively. In addition, custom sort logic is supported via a user provided Comparator  that consumes either  DataFrameRow  or  DataFrameColumn  objects. Finally, all sort functions are \ncapable of executing in  parallel  which can lead to dramatic performance improvements on large frames, and some\nresults in this regard are presented in the section on performance below. In all cases, an optimized single pivot \nquick sort is used, with an implementation made available in the excellent  FastUtil  \nlibrary, so a thank you goes out to  Sebastiano Vigna .",
            "title": "Introduction"
        },
        {
            "location": "/frame/sorting/#example-data",
            "text": "In this section, we will continue to use the ATP 2013 dataset that was introduced earlier in the filtering discussion  here . In some examples a custom frame is created of random double precision values in order to\ndemonstrate how to sort columns by row values. The ATP match  DataFrame  does not have homogeneous types along the rows, and \ntherefore a column sort based on these values is nonsensical. In addition, the section on performance creates much larger frames\nwith millions of rows in order to get more measurable timing statistics.  The unsorted 2103 ATP match  DataFrame  looks as follows (at least the first 10 rows).  \n Index  |  Location  |        Tournament        |     Date     |  Series  |   Court   |  Surface  |    Round    |  Best of  |     Winner     |     Loser      |  WRank  |  LRank  |  WPts  |  LPts  |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     0  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |      Mayer F.  |    Giraldo S.  |     28  |     57  |  1215  |   778  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.3600  |  3.0000  |  1.4500  |  2.6500  |  1.4400  |  2.6200  |  1.4700  |  2.8500  |  1.4400  |  2.6300  |  1.4700  |  3.2000  |  1.4200  |  2.7800  |\n     1  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |   Nieminen J.  |  Benneteau J.  |     41  |     35  |   927  |  1075  |   6  |   3  |   2  |   6  |   6  |   1  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.8000  |  1.9100  |  1.8000  |  2.1000  |  1.7300  |  2.0000  |  1.8000  |  2.2600  |  1.7300  |  2.0500  |\n     2  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |  Nishikori K.  |  Matosevic M.  |     19  |     49  |  1830  |   845  |   7  |   5  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.2500  |  3.7500  |  1.2500  |  3.7500  |  1.2900  |  3.5000  |  1.3000  |  3.8500  |  1.3000  |  3.2000  |  1.3000  |  4.2000  |  1.2800  |  3.5800  |\n     3  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |  Baghdatis M.  |   Mitchell B.  |     36  |    326  |  1070  |   137  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.0700  |  9.0000  |  1.0600  |  8.0000  |  1.0800  |  7.0000  |  1.0800  |  9.4300  |  1.0700  |  7.0000  |  1.1000  |  9.5000  |  1.0800  |  7.7600  |\n     4  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Istomin D.  |     Klizan M.  |     43  |     30  |   897  |  1175  |   6  |   1  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.9000  |  1.8000  |  1.8700  |  1.8700  |  1.9100  |  1.8000  |  1.8800  |  2.0000  |  1.9100  |  1.8000  |  2.0500  |  2.0000  |  1.8800  |  1.8500  |\n     5  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Millman J.  |        Ito T.  |    199  |     79  |   239  |   655  |   6  |   4  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.7300  |  2.0000  |  1.7000  |  2.2700  |  1.8000  |  1.9100  |  1.8500  |  2.2800  |  1.7100  |  2.0800  |\n     6  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |      Falla A.  |     Levine J.  |     54  |    104  |   809  |   530  |   6  |   1  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  2.2000  |  1.6100  |  2.0800  |  1.6700  |  1.9100  |  1.8000  |  2.2600  |  1.7000  |  2.0000  |  1.7300  |  2.3200  |  1.8300  |  2.0800  |  1.7000  |\n     7  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |     Melzer J.  |      Kudla D.  |     29  |    137  |  1177  |   402  |   2  |   6  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.4400  |  2.6200  |  1.5500  |  2.3500  |  1.4400  |  2.6200  |  1.6000  |  2.4700  |  1.5000  |  2.5000  |  1.6300  |  2.8200  |  1.5200  |  2.4600  |\n     8  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Robredo T.  |   Harrison R.  |    114  |     69  |   495  |   710  |   6  |   4  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  3.0000  |  1.3600  |  2.5000  |  1.5000  |  2.3800  |  1.5300  |  2.9300  |  1.4500  |  2.5000  |  1.5000  |  3.2500  |  1.5300  |  2.6600  |  1.4700  |\n     9  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |   Dimitrov G.  |      Baker B.  |     48  |     61  |   866  |   756  |   6  |   3  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.3600  |  3.0000  |  1.4000  |  2.8000  |  1.4400  |  2.6200  |  1.3800  |  3.3000  |  1.4000  |  2.7500  |  1.4500  |  3.5500  |  1.3900  |  2.8700  |",
            "title": "Example Data"
        },
        {
            "location": "/frame/sorting/#row-sorting",
            "text": "",
            "title": "Row Sorting"
        },
        {
            "location": "/frame/sorting/#key-sort",
            "text": "There are several overloaded  sort()  methods on the  DataFrameAxis  interface, the most basic of which takes a  boolean  to signal ascending ( true ) or descending ( false ) order. The example below illustrates how to sort the ATP\n2013 match frame by descending row key order. In the unsorted frame, the row keys are monotonically increasing \nintegers so the sort effectively reverses the order of the frame revealing the last matches at the top of the \nframe.   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Sort rows by row keys in descending order\nframe.rows().sort(false);\n//Print first ten rows\nframe.out().print(10);  \n Index  |  Location  |  Tournament   |     Date     |    Series     |  Court   |  Surface  |     Round     |  Best of  |    Winner     |      Loser       |  WRank  |  LRank  |  WPts   |  LPts   |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |   B365L   |   EXW    |   EXL    |   LBW    |    LBL    |   PSW    |    PSL    |   SJW    |   SJL    |   MaxW   |   MaxL    |   AvgW   |   AvgL   |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  2630  |    London  |  Masters Cup  |  2013-11-11  |  Masters Cup  |  Indoor  |     Hard  |    The Final  |        3  |  Djokovic N.  |        Nadal R.  |      2  |      1  |  10610  |  12030  |   6  |   3  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.5700  |   2.3700  |  1.5500  |  2.4000  |  1.5700  |   2.3800  |  1.5800  |   2.5800  |  1.5700  |  2.3800  |  1.6500  |   2.5800  |  1.5700  |  2.4300  |\n  2629  |    London  |  Masters Cup  |  2013-11-10  |  Masters Cup  |  Indoor  |     Hard  |   Semifinals  |        3  |  Djokovic N.  |     Wawrinka S.  |      2  |      8  |  10610  |   3330  |   6  |   3  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.1400  |   5.5000  |  1.1300  |  5.5000  |  1.1400  |   5.5000  |  1.1900  |   5.6100  |  1.1500  |  5.5000  |  1.1900  |   6.3000  |  1.1600  |  5.2500  |\n  2628  |    London  |  Masters Cup  |  2013-11-10  |  Masters Cup  |  Indoor  |     Hard  |   Semifinals  |        3  |     Nadal R.  |      Federer R.  |      1  |      7  |  12030  |   3805  |   7  |   5  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.3000  |   3.5000  |  1.3300  |  3.2000  |  1.3000  |   3.5000  |  1.3400  |   3.6700  |  1.3300  |  3.4000  |  1.3700  |   4.0500  |  1.3300  |  3.3400  |\n  2627  |    London  |  Masters Cup  |  2013-11-09  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |  Djokovic N.  |      Gasquet R.  |      2  |      9  |  10610  |   3300  |   7  |   6  |   4  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.0600  |  10.0000  |  1.0500  |  9.0000  |  1.0500  |  10.0000  |  1.0700  |  10.7500  |  1.0500  |  9.0000  |  1.1000  |  10.7500  |  1.0600  |  9.2200  |\n  2626  |    London  |  Masters Cup  |  2013-11-09  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |   Federer R.  |  Del Potro J.M.  |      7  |      5  |   3805  |   5055  |   4  |   6  |   7  |   6  |   7  |   5  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  2.0000  |   1.8000  |  2.0000  |  1.8000  |  2.0000  |   1.8000  |  2.0700  |   1.8500  |  2.0000  |  1.8000  |  2.1000  |   1.9500  |  1.9800  |  1.8100  |\n  2625  |    London  |  Masters Cup  |  2013-11-08  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |     Nadal R.  |      Berdych T.  |      1  |      6  |  12030  |   3980  |   6  |   4  |   1  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.2000  |   4.5000  |  1.2200  |  4.0000  |  1.2000  |   4.5000  |  1.2700  |   4.2600  |  1.2200  |  4.0000  |  1.3000  |   4.6000  |  1.2300  |  4.1200  |\n  2624  |    London  |  Masters Cup  |  2013-11-08  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |  Wawrinka S.  |       Ferrer D.  |      8  |      3  |   3330  |   5800  |   6  |   7  |   6  |   4  |   6  |   1  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.4400  |   2.7500  |  1.4500  |  2.7000  |  1.4400  |   2.7500  |  1.5600  |   2.6200  |  1.4400  |  2.7500  |  1.5700  |   2.8000  |  1.4800  |  2.6300  |\n  2623  |    London  |  Masters Cup  |  2013-11-07  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |  Djokovic N.  |  Del Potro J.M.  |      2  |      5  |  10610  |   5055  |   6  |   3  |   3  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.2800  |   3.7500  |  1.2800  |  3.5000  |  1.3000  |   3.5000  |  1.3000  |   3.8900  |  1.2900  |  3.7500  |  1.3300  |   4.0500  |  1.2800  |  3.6100  |\n  2622  |    London  |  Masters Cup  |  2013-11-07  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |   Federer R.  |      Gasquet R.  |      7  |      9  |   3805  |   3300  |   6  |   4  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.2800  |   3.7500  |  1.3000  |  3.4000  |  1.2900  |   3.7500  |  1.3400  |   3.6000  |  1.3000  |  3.5000  |  1.3500  |   4.2000  |  1.2900  |  3.5900  |\n  2621  |    London  |  Masters Cup  |  2013-11-06  |  Masters Cup  |  Indoor  |     Hard  |  Round Robin  |        3  |   Berdych T.  |       Ferrer D.  |      6  |      3  |   3980  |   5800  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  2.1000  |   1.7200  |  2.0000  |  1.8000  |  2.0000  |   1.8000  |  2.0600  |   1.8500  |  2.0000  |  1.8000  |  2.2000  |   1.8500  |  2.0300  |  1.7800  |",
            "title": "Key Sort"
        },
        {
            "location": "/frame/sorting/#value-sort",
            "text": "A more common scenario may be to sort rows according to values in a specific column. Any data type that implements the Comparable  interface can be sorted in this way (the same applies to sorting by keys as described in the previous section). \nIf the  DataFrame  happens to have a column of data whose type does not implement  Comparable  and sorting according to\nthis data is required, a user provided  Comparator  will have to be applied, which is discussed later.  The example below demonstrates how to sort the 2013 ATP match results by the rank of the eventual winner of the match ( WRank ). \nOddly, there appears to be a player ranked zero (namely Khachanov K.), and thus he appears first. Beyond this anomaly, you find \nthe expected names in the Winner column, namely Djokovic and Nadal.   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Sort rows by the WRank (winner rank) column values\nframe.rows().sort(true, \"WRank\");\n//Print first ten rows\nframe.out().print(10);  \n Index  |     Location     |          Tournament           |     Date     |     Series     |   Court   |  Surface  |      Round      |  Best of  |     Winner     |      Loser       |  WRank  |  LRank  |  WPts   |  LPts  |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |   B365L   |   EXW    |    EXL    |   LBW    |    LBL    |   PSW    |    PSL    |   SJW    |    SJL    |   MaxW   |   MaxL    |   AvgW   |   AvgL    |\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  2224  |  St. Petersburg  |          St. Petersburg Open  |  2013-09-16  |        ATP250  |   Indoor  |     Hard  |      1st Round  |        3  |  Khachanov K.  |      Hanescu V.  |      0  |     63  |      0  |   771  |   7  |   6  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  7.0000  |   1.1000  |  5.2500  |   1.1200  |  5.5000  |   1.1200  |  7.7400  |   1.1100  |  5.5000  |   1.1400  |  8.2000  |   1.1500  |  6.1400  |   1.1200  |\n   152  |       Melbourne  |              Australian Open  |  2013-01-14  |    Grand Slam  |  Outdoor  |     Hard  |      1st Round  |        5  |   Djokovic N.  |    Mathieu P.H.  |      1  |     60  |  12920  |   763  |   6  |   2  |   6  |   4  |   7  |   5  |   0  |   0  |   0  |   0  |      3  |      0  |  Completed  |  1.0020  |  29.0000  |  1.0200  |  15.0000  |  1.0200  |  13.0000  |  1.0100  |  41.0000  |  1.0100  |  19.0000  |  1.0200  |  46.0000  |  1.0100  |  21.7000  |\n  1121  |            Rome  |  Internazionali BNL d'Italia  |  2013-05-14  |  Masters 1000  |  Outdoor  |     Clay  |      2nd Round  |        3  |   Djokovic N.  |     Montanes A.  |      1  |     89  |  12730  |   573  |   6  |   2  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.0100  |  21.0000  |  1.0200  |  12.0000  |  1.0200  |  13.0000  |  1.0500  |  13.0000  |  1.0200  |  13.0000  |  1.0500  |  24.5000  |  1.0200  |  14.5500  |\n  1137  |            Rome  |  Internazionali BNL d'Italia  |  2013-05-16  |  Masters 1000  |  Outdoor  |     Clay  |      3rd Round  |        3  |   Djokovic N.  |   Dolgopolov O.  |      1  |     23  |  12730  |  1420  |   6  |   1  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.0600  |  10.0000  |  1.0800  |   7.0000  |  1.0600  |   9.0000  |  1.0800  |  10.7500  |  1.0800  |   7.5000  |  1.1000  |  11.5000  |  1.0700  |   8.5500  |\n  1592  |          London  |                    Wimbledon  |  2013-07-03  |    Grand Slam  |  Outdoor  |    Grass  |  Quarterfinals  |        5  |   Djokovic N.  |      Berdych T.  |      1  |      6  |  11830  |  4515  |   7  |   6  |   6  |   4  |   6  |   3  |   0  |   0  |   0  |   0  |      3  |      0  |  Completed  |  1.1000  |   7.0000  |  1.1200  |   6.0000  |  1.1200  |   6.0000  |  1.1300  |   7.9400  |     NaN  |      NaN  |  1.1500  |   7.9400  |  1.1200  |   6.4100  |\n   704  |    Indian Wells  |             BNP Paribas Open  |  2013-03-15  |  Masters 1000  |  Outdoor  |     Hard  |  Quarterfinals  |        3  |   Djokovic N.  |     Tsonga J.W.  |      1  |      8  |  13280  |  3660  |   6  |   3  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.0800  |   8.0000  |  8.0000  |   6.7100  |  1.1100  |   6.5000  |  1.1100  |   8.5000  |  1.1300  |   6.5000  |  1.1400  |   8.7000  |  1.1000  |   6.8800  |\n  1596  |          London  |                    Wimbledon  |  2013-07-05  |    Grand Slam  |  Outdoor  |    Grass  |     Semifinals  |        5  |   Djokovic N.  |  Del Potro J.M.  |      1  |      8  |  11830  |  3960  |   7  |   5  |   4  |   6  |   7  |   6  |   6  |   7  |   6  |   3  |      3  |      2  |  Completed  |  1.1600  |   5.0000  |  1.1200  |   6.0000  |  1.1400  |   5.5000  |  1.1600  |   6.6000  |  1.1400  |   6.0000  |  1.2000  |   6.6000  |  1.1400  |   5.6200  |\n   674  |    Indian Wells  |             BNP Paribas Open  |  2013-03-11  |  Masters 1000  |  Outdoor  |     Hard  |      2nd Round  |        3  |   Djokovic N.  |      Fognini F.  |      1  |     36  |  13280  |  1065  |   6  |   0  |   5  |   7  |   6  |   2  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.0020  |  29.0000  |  1.0100  |  15.0000  |  1.0100  |  17.0000  |  1.0100  |  33.8900  |  1.0100  |  15.0000  |  1.0400  |  41.0000  |  1.0100  |  20.1700  |\n   690  |    Indian Wells  |             BNP Paribas Open  |  2013-03-12  |  Masters 1000  |  Outdoor  |     Hard  |      3rd Round  |        3  |   Djokovic N.  |     Dimitrov G.  |      1  |     31  |  13280  |  1137  |   7  |   6  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.0200  |  17.0000  |  1.0300  |  10.0000  |  1.0400  |   9.0000  |  1.0200  |  23.0000  |  1.0500  |   9.0000  |  1.0500  |  23.0000  |  1.0300  |  12.7200  |\n  2611  |           Paris  |          BNP Paribas Masters  |  2013-11-01  |  Masters 1000  |   Indoor  |     Hard  |  Quarterfinals  |        3  |      Nadal R.  |      Gasquet R.  |      1  |     10  |  11670  |  3130  |   6  |   4  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.1400  |   5.5000  |  1.1300  |   5.5000  |  1.1400  |   5.5000  |  1.2000  |   5.3700  |  1.1400  |   5.5000  |  1.2000  |   6.3000  |  1.1500  |   5.2400  |",
            "title": "Value Sort"
        },
        {
            "location": "/frame/sorting/#multidimensional-sort",
            "text": "The  DataFrameAxis  interface also supports a  multidimensional  sort where the row order is first sorted by the values in one column \nand then by a second, third and so on. The example below again uses the 2013 ATP match results to sort first by the match  Date  column \n(because we expect to find many matches occurring on the same day), and then subsequently by the rank of the eventual winner ( WRank ). \nThe output below shows the first 10 rows of the sorted frame, with the earliest matches occurring on 2012-10-01 (it appears the ATP tour \ndoes not follow a calendar year), followed by a WRank sort which is also ascending.   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Multidimensional row sort (ascending) first by Date, then WRank\nframe.rows().sort(true, Collect.asList(\"Date\", \"WRank\"));\n//Print first ten rows\nframe.out().print(10);  \n Index  |  Location  |                Tournament                 |     Date     |  Series  |   Court   |  Surface  |    Round    |  Best of  |       Winner       |     Loser     |  WRank  |  LRank  |  WPts  |  LPts  |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  2342  |     Tokyo  |  Rakuten Japan Open Tennis Championships  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |       Tsonga J.W.  |   Monfils G.  |      8  |     42  |  3325  |  1030  |   6  |   3  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.4000  |  2.7500  |  1.4200  |  2.6500  |  1.4400  |  2.6200  |  1.5300  |  2.6600  |  1.4400  |  2.7500  |  1.5500  |  2.8500  |  1.4700  |  2.6300  |\n  2315  |   Beijing  |                               China Open  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |        Gasquet R.  |     Mayer F.  |     10  |     41  |  3005  |  1030  |   6  |   3  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.2500  |  3.7500  |  1.2800  |  3.3000  |  1.3000  |  3.4000  |  1.3600  |  3.4300  |  1.2900  |  3.5000  |  1.3600  |  3.8000  |  1.2900  |  3.4600  |\n  2340  |     Tokyo  |  Rakuten Japan Open Tennis Championships  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |        Almagro N.  |    Becker B.  |     17  |     73  |  1940  |   672  |   7  |   6  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.6100  |  2.2000  |  1.6200  |  2.1500  |  1.6700  |  2.1000  |  1.5700  |  2.5400  |  1.6200  |  2.2500  |  1.7100  |  2.5500  |  1.6000  |  2.2800  |\n  2310  |   Beijing  |                               China Open  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |        Fognini F.  |   Robredo T.  |     19  |     18  |  1840  |  1855  |   7  |   5  |   4  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  3.0000  |  1.3600  |  2.6500  |  1.4200  |  2.6200  |  1.4400  |  2.9300  |  1.4500  |  2.7500  |  1.4400  |  3.6500  |  1.4600  |  2.7900  |  1.4200  |\n  2312  |   Beijing  |                               China Open  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |  Kohlschreiber P.  |  Montanes A.  |     25  |     58  |  1445  |   836  |   7  |   5  |   1  |   6  |   7  |   6  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.1600  |  5.0000  |  1.1500  |  4.7500  |  1.1700  |  4.5000  |  1.1700  |  5.6500  |  1.1800  |  4.5000  |  1.2200  |  5.6500  |  1.1700  |  4.7500  |\n  2314  |   Beijing  |                               China Open  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |        Querrey S.  |   Youzhny M.  |     30  |     20  |  1265  |  1780  |   7  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  2.3700  |  1.5300  |  2.3000  |  1.5500  |  2.2500  |  1.5700  |  2.7000  |  1.5200  |  2.3800  |  1.5700  |  2.7500  |  1.6200  |  2.4900  |  1.5100  |\n  2341  |     Tokyo  |  Rakuten Japan Open Tennis Championships  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |     Dolgopolov O.  |    Brands D.  |     39  |     61  |  1080  |   813  |   6  |   3  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.5000  |  2.5000  |  1.5000  |  2.4000  |  1.5000  |  2.5000  |  1.5300  |  2.6600  |  1.5700  |  2.3800  |  1.6000  |  2.6600  |  1.5200  |  2.4700  |\n  2343  |     Tokyo  |  Rakuten Japan Open Tennis Championships  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |       Nieminen J.  |    Monaco J.  |     44  |     36  |   980  |  1115  |   7  |   6  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.6100  |  2.2000  |  1.6000  |  2.2000  |  1.6200  |  2.2000  |  1.7000  |  2.2700  |  1.6200  |  2.2500  |  1.7300  |  2.3800  |  1.6400  |  2.2000  |\n  2309  |   Beijing  |                               China Open  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |          Tomic B.  |     Zhang Z.  |     55  |    193  |   855  |   251  |   7  |   6  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.1400  |  5.0000  |  1.1500  |  4.7500  |  1.1700  |  4.5000  |  1.1800  |  5.4700  |  1.1400  |  5.5000  |  1.2200  |  5.5000  |  1.1600  |  4.9500  |\n  2313  |   Beijing  |                               China Open  |  2012-10-01  |  ATP500  |  Outdoor  |     Hard  |  1st Round  |        3  |         Hewitt L.  |      Haas T.  |     59  |     12  |   825  |  2265  |   7  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  2.3700  |  1.5300  |  2.3000  |  1.5500  |  2.2500  |  1.5700  |  2.4100  |  1.6200  |  2.3800  |  1.5700  |  2.5000  |  1.6200  |  2.3500  |  1.5700  |",
            "title": "Multidimensional Sort"
        },
        {
            "location": "/frame/sorting/#comparator-sort",
            "text": "If the two row sort mechanisms discussed above are functionally insufficient for your needs, a final  sort()  method \non the  DataFrameAxis  interface is provided which accepts a user implemented  Comparator , allowing for sorting logic \nof arbitrary complexity.   Depending on the implementation of the  Comparator , performance may not be quite as good as the two prior use cases as \nthose mechanisms can leverage private access to the internal data structures of the frame. If that proves to be the case, \nusing parallel sort on a multi-core machine is likely to boost performance significantly (see section below which provides\nsome hard numbers on this topic).  The example below shows how to sort the 2013 ATP match results by a derived value (i.e. one where a column representing\nthe sort quantity does not exist on the frame). In this setup we apply a row sort such that rows that have the smallest \nabsolute difference between the betting odds on the eventual winner ( AvgW ) and eventual loser ( AvgL ) appear first.\nThis helps identity matches that as far as the bookmakers are concerned, are simply too close to call.   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Sort rows so that matches smallest difference in betting odds between winner and looser.\nframe.rows().sort((row1, row2) -> {\n    double diff1 = Math.abs(row1.getDouble(\"AvgW\") - row1.getDouble(\"AvgL\"));\n    double diff2 = Math.abs(row2.getDouble(\"AvgW\") - row2.getDouble(\"AvgL\"));\n    return Double.compare(diff1, diff2);\n});\n//Print first ten rows\nframe.out().print(10);  \n Index  |    Location     |                   Tournament                   |     Date     |     Series     |   Court   |  Surface  |      Round      |  Best of  |       Winner       |      Loser      |  WRank  |  LRank  |  WPts  |  LPts  |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n   640  |   Indian Wells  |                              BNP Paribas Open  |  2013-03-09  |  Masters 1000  |  Outdoor  |     Hard  |      1st Round  |        3  |          Blake J.  |       Haase R.  |     99  |     47  |   538  |   845  |   6  |   3  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.8300  |  1.8300  |  1.9000  |  1.8500  |  1.8300  |  1.8300  |  1.9300  |  1.9700  |  1.8300  |  1.9100  |  2.0800  |  1.9700  |  1.8800  |  1.8700  |\n  1981  |     Cincinnati  |    Western & Southern Financial Group Masters  |  2013-08-13  |  Masters 1000  |  Outdoor  |     Hard  |      1st Round  |        3  |           Haas T.  |    Anderson K.  |     13  |     19  |  2140  |  1740  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.9000  |  1.8000  |  1.8500  |  1.8500  |  1.9100  |  1.8000  |  1.9200  |  1.9900  |  1.9100  |  1.9100  |  1.9700  |  2.0400  |  1.8700  |  1.8800  |\n  2050  |  Winston-Salem  |  Winston-Salem Open at Wake Forest University  |  2013-08-20  |        ATP250  |  Outdoor  |     Hard  |      2nd Round  |        3  |         Melzer J.  |   De Bakker T.  |     32  |    101  |  1220  |   547  |   7  |   5  |   6  |   7  |   4  |   2  |   0  |   0  |   0  |   0  |      1  |      1  |    Retired  |  1.8300  |  1.8300  |  1.8500  |  1.8500  |  1.8300  |  1.8300  |  1.9300  |  1.9800  |  1.8300  |  1.9100  |  1.9300  |  1.9800  |  1.8700  |  1.8800  |\n   341  |         Zagreb  |                            PBZ Zagreb Indoors  |  2013-02-08  |        ATP250  |   Indoor  |     Hard  |  Quarterfinals  |        3  |          Haase R.  |  Petzschner P.  |     58  |    122  |   755  |   436  |   6  |   4  |   3  |   6  |   6  |   0  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.8300  |  1.8300  |  1.8500  |  1.9000  |  1.8300  |  1.8300  |  1.9500  |  1.9700  |  1.8000  |  2.0000  |  2.0000  |  2.1000  |  1.8700  |  1.8800  |\n   937  |      Barcelona  |                           Open Banco Sabadell  |  2013-04-24  |        ATP500  |  Outdoor  |     Clay  |      2nd Round  |        3  |         Klizan M.  |    Montanes A.  |     31  |     80  |  1205  |   630  |   6  |   3  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.8300  |  1.8300  |  1.8500  |  1.9000  |  1.7300  |  2.0000  |  2.0400  |  1.8700  |  1.8000  |  2.0000  |  2.0400  |  2.0000  |  1.8800  |  1.8700  |\n  2545  |       Valencia  |                             Valencia Open 500  |  2013-10-22  |        ATP500  |   Indoor  |     Hard  |      1st Round  |        3  |      Benneteau J.  |       Lopez F.  |     33  |     30  |  1195  |  1275  |   6  |   3  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.9000  |  1.8000  |  1.9000  |  1.8500  |  1.9100  |  1.8000  |  1.9900  |  1.9000  |  1.9000  |  1.9000  |  2.0400  |  1.9500  |  1.8800  |  1.8700  |\n   729  |          Miami  |                            Sony Ericsson Open  |  2013-03-21  |  Masters 1000  |  Outdoor  |     Hard  |      1st Round  |        3  |          Falla A.  |       Soeda G.  |     60  |     83  |   776  |   600  |   7  |   5  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.8000  |  1.9000  |  1.8500  |  1.9000  |  1.8300  |  1.8300  |  1.9100  |  2.0000  |  2.0000  |  1.8000  |  2.0000  |  2.0500  |  1.8800  |  1.8700  |\n   858  |    Monte Carlo  |                           Monte Carlo Masters  |  2013-04-14  |  Masters 1000  |  Outdoor  |     Clay  |      1st Round  |        3  |  Kohlschreiber P.  |    Bellucci T.  |     21  |     39  |  1670  |   987  |   6  |   4  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.8300  |  1.8300  |  1.9000  |  1.8500  |  1.9100  |  1.8000  |  1.9300  |  1.9700  |  1.9000  |  1.9000  |  2.0000  |  2.0000  |  1.8700  |  1.8800  |\n  2000  |     Cincinnati  |    Western & Southern Financial Group Masters  |  2013-08-14  |  Masters 1000  |  Outdoor  |     Hard  |      2nd Round  |        3  |          Isner J.  |     Gasquet R.  |     22  |     11  |  1585  |  2625  |   7  |   6  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.8000  |  1.9000  |  1.8500  |  1.8500  |  1.7300  |  2.0000  |  2.0200  |  1.8700  |  1.9100  |  1.8300  |  2.0200  |  2.0800  |  1.8700  |  1.8800  |\n   632  |   Indian Wells  |                              BNP Paribas Open  |  2013-03-08  |  Masters 1000  |  Outdoor  |     Hard  |      1st Round  |        3  |      Matosevic M.  |     Robredo T.  |     53  |     69  |   813  |   680  |   7  |   5  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.8300  |  1.8300  |  1.8500  |  1.9000  |  1.8300  |  1.8300  |  1.9300  |  1.9800  |  1.8300  |  1.9100  |  1.9500  |  2.1000  |  1.8800  |  1.8700  |",
            "title": "Comparator Sort"
        },
        {
            "location": "/frame/sorting/#column-sorting",
            "text": "",
            "title": "Column Sorting"
        },
        {
            "location": "/frame/sorting/#key-sort_1",
            "text": "The Morpheus API is almost entirely symmetrical in the row and column dimension, so the sorting mechanisms presented \nin the previous sections can also be used to sort columns. The first code example below sorts the columns of the 2013 \nATP match results in ascending order by calling the  sort(ascending=true)  method on the  DataFrameAxis  returned \nfrom  DataFrame.cols() .   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//Sort columns by column keys in ascending order\nframe.cols().sort(true);\n//Print first ten rows\nframe.out().print(10);  \n Index  |   AvgL   |   AvgW   |  B365L   |  B365W   |  Best of  |   Comment   |   Court   |     Date     |   EXL    |   EXW    |  L1  |  L2  |  L3  |  L4  |  L5  |   LBL    |   LBW    |  LPts  |  LRank  |  Location  |     Loser      |  Lsets  |   MaxL   |   MaxW   |   PSL    |   PSW    |    Round    |   SJL    |   SJW    |  Series  |  Surface  |        Tournament        |  W1  |  W2  |  W3  |  W4  |  W5  |  WPts  |  WRank  |     Winner     |  Wsets  |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     0  |  2.7800  |  1.4200  |  3.0000  |  1.3600  |        3  |  Completed  |  Outdoor  |  2012-12-31  |  2.6500  |  1.4500  |   4  |   4  |   0  |   0  |   0  |  2.6200  |  1.4400  |   778  |     57  |  Brisbane  |    Giraldo S.  |      0  |  3.2000  |  1.4700  |  2.8500  |  1.4700  |  1st Round  |  2.6300  |  1.4400  |  ATP250  |     Hard  |  Brisbane International  |   6  |   6  |   0  |   0  |   0  |  1215  |     28  |      Mayer F.  |      2  |\n     1  |  2.0500  |  1.7300  |  2.2000  |  1.6100  |        3  |  Completed  |  Outdoor  |  2012-12-31  |  2.0000  |  1.7500  |   3  |   6  |   1  |   0  |   0  |  1.9100  |  1.8000  |  1075  |     35  |  Brisbane  |  Benneteau J.  |      1  |  2.2600  |  1.8000  |  2.1000  |  1.8000  |  1st Round  |  2.0000  |  1.7300  |  ATP250  |     Hard  |  Brisbane International  |   6  |   2  |   6  |   0  |   0  |   927  |     41  |   Nieminen J.  |      2  |\n     2  |  3.5800  |  1.2800  |  3.7500  |  1.2500  |        3  |  Completed  |  Outdoor  |  2012-12-31  |  3.7500  |  1.2500  |   5  |   2  |   0  |   0  |   0  |  3.5000  |  1.2900  |   845  |     49  |  Brisbane  |  Matosevic M.  |      0  |  4.2000  |  1.3000  |  3.8500  |  1.3000  |  1st Round  |  3.2000  |  1.3000  |  ATP250  |     Hard  |  Brisbane International  |   7  |   6  |   0  |   0  |   0  |  1830  |     19  |  Nishikori K.  |      2  |\n     3  |  7.7600  |  1.0800  |  9.0000  |  1.0700  |        3  |  Completed  |  Outdoor  |  2012-12-31  |  8.0000  |  1.0600  |   4  |   4  |   0  |   0  |   0  |  7.0000  |  1.0800  |   137  |    326  |  Brisbane  |   Mitchell B.  |      0  |  9.5000  |  1.1000  |  9.4300  |  1.0800  |  1st Round  |  7.0000  |  1.0700  |  ATP250  |     Hard  |  Brisbane International  |   6  |   6  |   0  |   0  |   0  |  1070  |     36  |  Baghdatis M.  |      2  |\n     4  |  1.8500  |  1.8800  |  1.8000  |  1.9000  |        3  |  Completed  |  Outdoor  |  2013-01-01  |  1.8700  |  1.8700  |   1  |   2  |   0  |   0  |   0  |  1.8000  |  1.9100  |  1175  |     30  |  Brisbane  |     Klizan M.  |      0  |  2.0000  |  2.0500  |  2.0000  |  1.8800  |  1st Round  |  1.8000  |  1.9100  |  ATP250  |     Hard  |  Brisbane International  |   6  |   6  |   0  |   0  |   0  |   897  |     43  |    Istomin D.  |      2  |\n     5  |  2.0800  |  1.7100  |  2.2000  |  1.6100  |        3  |  Completed  |  Outdoor  |  2013-01-01  |  2.0000  |  1.7500  |   4  |   1  |   0  |   0  |   0  |  2.0000  |  1.7300  |   655  |     79  |  Brisbane  |        Ito T.  |      0  |  2.2800  |  1.8500  |  2.2700  |  1.7000  |  1st Round  |  1.9100  |  1.8000  |  ATP250  |     Hard  |  Brisbane International  |   6  |   6  |   0  |   0  |   0  |   239  |    199  |    Millman J.  |      2  |\n     6  |  1.7000  |  2.0800  |  1.6100  |  2.2000  |        3  |  Completed  |  Outdoor  |  2013-01-01  |  1.6700  |  2.0800  |   1  |   6  |   0  |   0  |   0  |  1.8000  |  1.9100  |   530  |    104  |  Brisbane  |     Levine J.  |      0  |  1.8300  |  2.3200  |  1.7000  |  2.2600  |  1st Round  |  1.7300  |  2.0000  |  ATP250  |     Hard  |  Brisbane International  |   6  |   7  |   0  |   0  |   0  |   809  |     54  |      Falla A.  |      2  |\n     7  |  2.4600  |  1.5200  |  2.6200  |  1.4400  |        3  |  Completed  |  Outdoor  |  2013-01-01  |  2.3500  |  1.5500  |   6  |   4  |   4  |   0  |   0  |  2.6200  |  1.4400  |   402  |    137  |  Brisbane  |      Kudla D.  |      1  |  2.8200  |  1.6300  |  2.4700  |  1.6000  |  1st Round  |  2.5000  |  1.5000  |  ATP250  |     Hard  |  Brisbane International  |   2  |   6  |   6  |   0  |   0  |  1177  |     29  |     Melzer J.  |      2  |\n     8  |  1.4700  |  2.6600  |  1.3600  |  3.0000  |        3  |  Completed  |  Outdoor  |  2013-01-01  |  1.5000  |  2.5000  |   4  |   6  |   0  |   0  |   0  |  1.5300  |  2.3800  |   710  |     69  |  Brisbane  |   Harrison R.  |      0  |  1.5300  |  3.2500  |  1.4500  |  2.9300  |  1st Round  |  1.5000  |  2.5000  |  ATP250  |     Hard  |  Brisbane International  |   6  |   7  |   0  |   0  |   0  |   495  |    114  |    Robredo T.  |      2  |\n     9  |  2.8700  |  1.3900  |  3.0000  |  1.3600  |        3  |  Completed  |  Outdoor  |  2013-01-01  |  2.8000  |  1.4000  |   3  |   6  |   0  |   0  |   0  |  2.6200  |  1.4400  |   756  |     61  |  Brisbane  |      Baker B.  |      0  |  3.5500  |  1.4500  |  3.3000  |  1.3800  |  1st Round  |  2.7500  |  1.4000  |  ATP250  |     Hard  |  Brisbane International  |   6  |   7  |   0  |   0  |   0  |   866  |     48  |   Dimitrov G.  |      2  |",
            "title": "Key Sort"
        },
        {
            "location": "/frame/sorting/#value-sort_1",
            "text": "To sort columns according to values in one or more rows, the values need to be of a homogeneous type. It doesn't make\nmuch sense to sort the 2013 ATP match results in this way, and in fact you will get a  ClassCastException  when a  String \nis compared to a  Double  value by calling its  compareTo()  method. You could of course implement a custom  Comparator \nand sort the columns based on the  toString()  of all the values, but it's hard to see what the value is in that.  To demonstrate a column sort based on row data, the following example creates a  10x10  frame initialized with random double \nprecision values. Given the homogeneity of the types along the rows in this frame, a column sort is a reasonable proposition. \nThe column keys in the unsorted frame are  C0 ,  C1 ,  C2  to  C9 , and the new order post sort is shown below.    //Create a 10x10 frame initialized with random doubles\nDataFrame<String,String> frame = DataFrame.ofDoubles(\n    Range.of(0, 10).map(i -> \"R\" + i),\n    Range.of(0, 10).map(i -> \"C\" + i), \n    value -> Math.random() * 100d\n);\n//Sort columns by the data in first row in ascending order\nframe.cols().sort(true, \"R0\");\n//Print first ten rows\nframe.out().print(10);  \n Index  |      C3       |      C1       |      C8       |      C7       |      C4       |      C6       |      C9       |      C2       |      C0       |      C5       |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    R0  |  11.70306561  |  29.51025702  |  44.56451853  |  49.14276083  |  49.30009688  |  49.66602735  |  59.72386052  |  64.69244099  |  83.13030538  |  98.60728081  |\n    R1  |  96.31980655  |  85.87996651  |  79.06930692  |  48.08819333  |  84.33796355  |   8.83992696  |  73.19731538  |  65.38285662  |  71.82216915  |   80.3000506  |\n    R2  |  49.53677506  |   9.45207021  |   1.54748873  |  57.99794713  |  20.02291114  |   8.96502162  |   46.1696664  |  77.33926623  |  34.21577299  |  94.85120102  |\n    R3  |   46.3516546  |  74.97630129  |  73.49083701  |  27.68089003  |  33.72130957  |  52.23839606  |  65.37188146  |  48.36073597  |  51.23686989  |  95.92785717  |\n    R4  |  72.01951644  |  39.75814373  |  69.23633177  |  46.96902638  |  70.82327915  |  91.53783289  |  57.33616312  |  38.94393019  |   1.68280501  |   87.2170405  |\n    R5  |  62.49904384  |   7.52041005  |  34.26879199  |  47.23321809  |   99.3061582  |  98.79472512  |    4.5477916  |  65.23264192  |   3.96089302  |  24.99717992  |\n    R6  |  42.63425518  |  26.61492898  |  16.50642183  |  15.04284983  |   67.1590594  |  44.72721238  |  77.61992004  |  53.96632198  |  60.65017717  |  31.01324133  |\n    R7  |   76.7178214  |   11.1839268  |  93.51531804  |  66.05863202  |  46.75806412  |   7.82498305  |  17.40931894  |  94.65796849  |  55.14862369  |  28.87315085  |\n    R8  |   48.7670682  |   1.44288709  |  51.16369446  |   32.3675847  |  93.95447717  |  10.09004133  |    6.6242657  |  69.63114684  |  37.92679795  |    2.1543318  |\n    R9  |  42.25961018  |  55.62679387  |   1.01423147  |  22.75711435  |  85.05398313  |  92.11731992  |  79.68760819  |  29.04198229  |  97.24438511  |  17.33341345  |  We can of course sort by data in both the row and column dimension as follows:   //Create a 10x10 frame initialized with random doubles\nDataFrame<String,String> frame = DataFrame.ofDoubles(\n    Range.of(0, 10).map(i -> \"R\" + i),\n    Range.of(0, 10).map(i -> \"C\" + i),\n    value -> Math.random() * 100d\n);\n//Sort columns by the data in first row\nframe.cols().sort(true, \"R0\");\n//Sort rows by the data that is now in the first column\nframe.rows().sort(true, frame.cols().key(0));\n//Print first ten rows\nframe.out().print(10);  \n Index  |      C8       |      C2       |      C1       |      C6       |      C7       |      C9       |      C4       |      C3       |      C0       |      C5       |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    R6  |  10.79109008  |   98.6524288  |  84.86273268  |  94.70606475  |  53.38375256  |  61.10043302  |  15.12273981  |  69.89835361  |  57.66658584  |  57.27005464  |\n    R0  |  11.43931147  |  20.54256763  |  23.79181572  |  27.47288193  |  29.95263252  |  75.42157524  |  76.42720119  |  77.22820182  |  77.67104703  |  95.18532196  |\n    R5  |  36.90300534  |  37.55942619  |  73.84924663  |  96.27509348  |  89.51587605  |  56.83564479  |   8.43705948  |  65.56679789  |  26.18719945  |  19.35039888  |\n    R8  |  48.92084538  |  74.04830396  |  15.13429492  |  79.55867974  |  69.78011977  |  92.60505236  |   0.90437748  |  66.04955254  |   4.24454634  |  16.08194424  |\n    R1  |  49.72933364  |  23.54721145  |  13.76578354  |  36.82903111  |  89.45754038  |  88.36190839  |  88.35626403  |   57.3339813  |   5.07104083  |   64.8812482  |\n    R3  |  50.05896919  |  20.99271329  |   19.3462031  |  84.85646161  |  18.70819105  |  36.02397058  |  39.60181063  |  97.98311929  |  23.15751737  |  11.19755199  |\n    R2  |  70.91560241  |  48.22161882  |  36.04857549  |  17.43486886  |  53.59280303  |  24.17841927  |   7.32760917  |    93.077888  |  95.69590224  |   70.1948771  |\n    R7  |  71.66128613  |  25.04195311  |  25.24586372  |  45.55143352  |   1.95783573  |  56.47302595  |  84.33080973  |  47.08666346  |  21.68500396  |  63.38464116  |\n    R9  |   72.5569825  |   3.69441849  |  40.53699445  |  58.65763148  |   3.58546753  |  73.53537208  |   7.97752157  |  15.53034669  |    0.8829291  |  44.36481597  |\n    R4  |  75.16120178  |  42.44021922  |  72.00614468  |  79.09004161  |   4.43077647  |  43.89845937  |  44.84293408  |  62.85029955  |  77.98911497  |  48.24598168  |",
            "title": "Value Sort"
        },
        {
            "location": "/frame/sorting/#filters",
            "text": "Slicing or filtering a  DataFrame  is a common operation and is discussed in some detail  here . There are \nstructural immutability constraints on filtered frames, however sorting in both the row and column dimensions is \nsupported.  The example below first filters the 2013 ATP match results so as to capture all rows in which Novak Djokovic was the victor, \nand then sorts these rows in ascending order according to the rank of the match loser ( LRank ). In that way, the highest\nrank players he beat appear first, so it is no surprise to see names like Nadal, Murray, Federer and so on.   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\n//First filter frame to include only rows where Novak Djokovic was the victor\nDataFrame<Integer,String> filter = frame.rows().select(row -> row.getValue(\"Winner\").equals(\"Djokovic N.\"));\n//Sort rows so that the highest rank players he beat come first\nfilter.rows().sort(true, \"LRank\");\n//Print first ten rows\nfilter.out().print(10);  \n Index  |   Location    |          Tournament          |     Date     |     Series     |   Court   |  Surface  |     Round     |  Best of  |    Winner     |      Loser       |  WRank  |  LRank  |  WPts   |  LPts   |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  2630  |       London  |                 Masters Cup  |  2013-11-11  |   Masters Cup  |   Indoor  |     Hard  |    The Final  |        3  |  Djokovic N.  |        Nadal R.  |      2  |      1  |  10610  |  12030  |   6  |   3  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.5700  |  2.3700  |  1.5500  |  2.4000  |  1.5700  |  2.3800  |  1.5800  |  2.5800  |  1.5700  |  2.3800  |  1.6500  |  2.5800  |  1.5700  |  2.4300  |\n  2339  |      Beijing  |                  China Open  |  2012-10-07  |        ATP500  |  Outdoor  |     Hard  |    The Final  |        3  |  Djokovic N.  |        Nadal R.  |      1  |      2  |  11120  |  10860  |   6  |   3  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.7200  |  2.1000  |  1.7000  |  2.0500  |  1.7300  |  2.1000  |  1.8000  |  2.1500  |  1.6700  |  2.2000  |  1.8300  |  2.2000  |  1.7400  |  2.0600  |\n   265  |    Melbourne  |             Australian Open  |  2013-01-27  |    Grand Slam  |  Outdoor  |     Hard  |    The Final  |        5  |  Djokovic N.  |       Murray A.  |      1  |      3  |  12920  |   8000  |   6  |   7  |   7  |   6  |   6  |   3  |   6  |   2  |   0  |   0  |      3  |      1  |  Completed  |  1.5300  |  2.6200  |  1.5000  |  2.5500  |  1.5300  |  2.5000  |  1.5400  |  2.7400  |  1.5300  |  2.6300  |  1.5700  |  2.9000  |  1.5000  |  2.6100  |\n  2615  |        Paris  |         BNP Paribas Masters  |  2013-11-03  |  Masters 1000  |   Indoor  |     Hard  |    The Final  |        3  |  Djokovic N.  |       Ferrer D.  |      2  |      3  |  11120  |   6600  |   7  |   5  |   7  |   5  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.1200  |  6.0000  |  1.1500  |  5.2500  |  1.1400  |  5.5000  |  1.1700  |  6.0000  |  1.1500  |  5.5000  |  1.1500  |  5.4200  |  1.1800  |  6.5000  |\n  2623  |       London  |                 Masters Cup  |  2013-11-07  |   Masters Cup  |   Indoor  |     Hard  |  Round Robin  |        3  |  Djokovic N.  |  Del Potro J.M.  |      2  |      5  |  10610  |   5055  |   6  |   3  |   3  |   6  |   6  |   3  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.2800  |  3.7500  |  1.2800  |  3.5000  |  1.3000  |  3.5000  |  1.3000  |  3.8900  |  1.2900  |  3.7500  |  1.3300  |  4.0500  |  1.2800  |  3.6100  |\n   263  |    Melbourne  |             Australian Open  |  2013-01-24  |    Grand Slam  |  Outdoor  |     Hard  |   Semifinals  |        5  |  Djokovic N.  |       Ferrer D.  |      1  |      5  |  12920  |   6505  |   6  |   2  |   6  |   2  |   6  |   1  |   0  |   0  |   0  |   0  |      3  |      0  |  Completed  |  1.0800  |  8.0000  |  1.1000  |  6.5000  |  1.1100  |  6.5000  |  1.1200  |  7.9400  |  1.1000  |  8.0000  |  1.1400  |  9.4000  |  1.1000  |  6.9100  |\n  2425  |     Shanghai  |            Shanghai Masters  |  2013-10-13  |  Masters 1000  |  Outdoor  |     Hard  |    The Final  |        3  |  Djokovic N.  |  Del Potro J.M.  |      2  |      5  |  11120  |   4925  |   6  |   1  |   3  |   6  |   7  |   6  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.3300  |  3.4000  |  1.3500  |  2.9000  |  1.3600  |  3.2500  |  1.3800  |  3.4100  |  1.3600  |  3.2500  |  1.4000  |  3.5000  |  1.3600  |  3.1500  |\n   912  |  Monte Carlo  |         Monte Carlo Masters  |  2013-04-21  |  Masters 1000  |  Outdoor  |     Clay  |    The Final  |        3  |  Djokovic N.  |        Nadal R.  |      1  |      5  |  12500  |   6385  |   6  |   2  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  2.8700  |  1.4000  |  2.6500  |  1.4500  |  2.7500  |  1.4400  |  2.9700  |  1.4600  |  2.7500  |  1.4400  |  3.0000  |  1.5000  |  2.7600  |  1.4400  |\n  2613  |        Paris  |         BNP Paribas Masters  |  2013-11-02  |  Masters 1000  |   Indoor  |     Hard  |   Semifinals  |        3  |  Djokovic N.  |      Federer R.  |      2  |      6  |  11120  |   4245  |   4  |   6  |   6  |   3  |   6  |   2  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.2800  |  3.7500  |  1.2500  |  3.7500  |  1.2900  |  3.7500  |  1.3100  |  3.9400  |  1.2900  |  3.7500  |  1.3200  |  4.4000  |  1.2800  |  3.6200  |\n   613  |        Dubai  |  Dubai Tennis Championships  |  2013-03-02  |        ATP500  |  Outdoor  |     Hard  |    The Final  |        3  |  Djokovic N.  |      Berdych T.  |      1  |      6  |  12960  |   4545  |   7  |   5  |   6  |   3  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.1100  |  6.5000  |  1.1400  |  5.2000  |  1.1100  |  6.0000  |  1.1500  |  6.5400  |  1.1300  |  6.5000  |  1.1500  |  8.7000  |  1.1200  |  6.1700  |",
            "title": "Filters"
        },
        {
            "location": "/frame/sorting/#performance",
            "text": "Morpheus has been designed specifically to deal with very large data, and the section on general performance\nwhich is discussed  here  and  here  describes some of the implementation choices and their rationale. \nMorpheus is not a traditional Big Data solution in that it is focused on addressing a class of problem that \ncan fit in the memory space of a single operating system process. These days it is not uncommon to find machines \nwith 250GB of physical memory, which can certainly be used to analyze substantial data. A distributed solution\nusing Morpheus while possible, is not addressed as part of the framework, at least not at the time of writing.  Great care has been spent designing efficient data structures that scale well on the Java Virtual Machine, and\nmany of the common  DataFrame  operations can execute in  parallel  based on internal algorithms that leverage\nthe fork and join framework introduced in Java 7. Sorting is one such operation.",
            "title": "Performance"
        },
        {
            "location": "/frame/sorting/#sequential-sorting",
            "text": "By default,  DataFrame  functions implement sequential routines unless a concurrent implementation is requested\nby calling the  parallel()  method on the interface in question. This design has been influenced by the \nJava 8 Stream API, which is not only fluid to work with, but should also serve to reduce the learning curve\nsomewhat (assuming you are familiar with some of these new APIs introduced in JDK 8).  The test results presented below were performed on a late 2013 Macbook Pro with a 2.6Ghz Core i7 Quad Core \nprocessor. Like all benchmarks, they should be taken with a pinch of salt, but the key point to convey here is \nthe substantial gain in performance of the  parallel  sort over the  sequential  while adding no implementation \ncomplexity to the end user.  In all cases, these benchmarks execute 10 runs of each scenario, and compute summary statistics on the timimg results,\nnamely a  min ,  max ,  mean ,  median  and  standard deviation . This should give adequate time for the Hotspot \nto perform its magic, and should also give us a sense on the level of dispersion we see across the runs. Obviously \nthe lower the dispersion, the more confidence we can have in the results.  The code below is the full benchmark. Before each test run, we call  DataFrame.rows().sort(null)  which is effectively\npassing a null comparator, and thereby resetting the frame to its original unsorted state. If we didn't do this\nthe first test would be much slower as subsequent tests would be asked to sort an already sorted frame. We would\nsee a very big difference between the  min  and  max  sort times if that was the case.   //Define range of row counts we want to test, from 1M to 5M inclusive\nRange<Integer> rowCounts = Range.of(1, 6).map(i -> i * 1000000);\n\n//Time DataFrame sort operations on frame of random doubles with row counts ranging from 1M to 6M\nDataFrame<String,String> results = DataFrame.combineFirst(rowCounts.map(rowCount -> {\n    Range<Integer> rowKeys = Range.of(0, rowCount.intValue());\n    Range<String> colKeys = Range.of(0, 5).map(i -> \"C\" + i);\n    //Create frame initialized with random double values\n    DataFrame<Integer,String> frame = DataFrame.ofDoubles(rowKeys, colKeys, v -> Math.random() * 100d);\n    String label = \"Rows(\" + (rowCount / 1000000) + \"M)\";\n    //Run each test 10 times, clear the sort before running the test with sort(null)\n    return PerfStat.run(10, TimeUnit.MILLISECONDS, false, tasks -> {\n        tasks.beforeEach(() -> frame.rows().sort(null));\n        tasks.put(label, () -> frame.rows().sort(true, \"C1\"));\n    });\n}));\n\n//Plot the results of the combined DataFrame with timings\nChart.create().withBarPlot(results, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time In Milliseconds\");\n    chart.title().withText(\"DataFrame Sorting Performance (Sequential)\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.subtitle().withText(\"Row Sort with counts from 1M to 5M rows\");\n    chart.legend().on().bottom();\n    chart.show();\n});  The results in the chart below show a fairly linear increase in sort times as we progress from a frame with 1 million \nto 5 million rows. The dispersion in the 10 iterations as measured by the standard deviation is fairly low in all cases, \nat least as a percentage of the time taken.",
            "title": "Sequential Sorting"
        },
        {
            "location": "/frame/sorting/#parallel-sorting",
            "text": "The sequential benchmark code presented in the previous section can be ever so slightly modified to leverage a parallel \nsort by ensuring that we call  DataFrame.rows().parallel().sort() . The modified code is as follows:   //Define range of row counts we want to test, from 1M to 5M inclusive\nRange<Integer> rowCounts = Range.of(1, 6).map(i -> i * 1000000);\n\n//Time DataFrame sort operations on frame of random doubles with row counts ranging from 1M to 6M\nDataFrame<String,String> results = DataFrame.combineFirst(rowCounts.map(rowCount -> {\n    Range<Integer> rowKeys = Range.of(0, rowCount.intValue());\n    Range<String> colKeys = Range.of(0, 5).map(i -> \"C\" + i);\n    //Create frame initialized with random double values\n    DataFrame<Integer,String> frame = DataFrame.ofDoubles(rowKeys, colKeys, v -> Math.random() * 100d);\n    String label = \"Rows(\" + (rowCount / 1000000) + \"M)\";\n    //Run each test 10 times, clear the sort before running the test with sort(null)\n    return PerfStat.run(10, TimeUnit.MILLISECONDS, false, tasks -> {\n        tasks.beforeEach(() -> frame.rows().sort(null));\n        tasks.put(label, () -> frame.rows().parallel().sort(true, \"C1\"));\n    });\n}));\n\n//Plot the results of the combined DataFrame with timings\nChart.create().withBarPlot(results, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time In Milliseconds\");\n    chart.title().withText(\"DataFrame Sorting Performance (Parallel)\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.subtitle().withText(\"Row Sort with counts from 1M to 5M rows\");\n    chart.legend().on().bottom();\n    chart.show();\n});  The plot with the parallel execution results below shows a very significant improvement, and also has lower \ndispersion in the results. For the largest frame with 5 million rows the  median  sort time went from around \n1400 milliseconds in the sequential case to just over 500 milliseconds with the parallel execution. As processor \ncache sizes increase and ever more cores become available, we can only expect this spread to widen as Moore's \nLaw begins to plateau.",
            "title": "Parallel Sorting"
        },
        {
            "location": "/frame/sorting/#comparator-performance",
            "text": "A final performance comparison that may be of interest is to consider the relative cost of sorting\nwith a user provided  Comparator  as opposed to using the  sort()  functions that just take a row\nor column key. The example below addresses this question by sorting a frame with 1 million rows\nand using identical logic where in one case we simply sort the rows based on a column key, and\nin the second we provide a  Comparator  implementation that effectively does the same thing. We \nalso run a sequential and parallel setup for these two cases, so 4 test scenarios in all. For each\nscenario, we run 10 iterations so we can collect some summary statistics. The benchmark code is as\nfollows:   //Create frame initialized with random double values\nRange<Integer> rowKeys = Range.of(0, 1000000);\nRange<String> colKeys = Range.of(0, 5).map(i -> \"C\" + i);\nDataFrame<Integer,String> frame = DataFrame.ofDoubles(rowKeys, colKeys, v -> Math.random() * 100d);\n//Define comparator to sort rows by column C1, which is ordinal 1\nComparator<DataFrameRow<Integer,String>> comparator = (row1, row2) -> {\n    double v1 = row1.getDouble(1);\n    double v2 = row2.getDouble(1);\n    return Double.compare(v1, v2);\n};\n\n//Time sorting in various modes (with & without comparator in both sequential & parallel mode)\nDataFrame<String,String> results = PerfStat.run(10, TimeUnit.MILLISECONDS, false, tasks -> {\n    tasks.beforeEach(() -> frame.rows().sort(null));\n    tasks.put(\"W/O Comparator (seq)\", () -> frame.rows().sort(true, \"C1\"));\n    tasks.put(\"W/O Comparator (par)\", () -> frame.rows().parallel().sort(true, \"C1\"));\n    tasks.put(\"W/ Comparator (seq)\", () -> frame.rows().sort(comparator));\n    tasks.put(\"W/ Comparator (par)\", () -> frame.rows().parallel().sort(comparator));\n});\n\n//Plot the results of the combined DataFrame with timings\nChart.create().withBarPlot(results, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time In Milliseconds\");\n    chart.title().withText(\"DataFrame Sorting Performance With & Without Comparator\");\n    chart.subtitle().withText(\"1 Million rows of random double precision values\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.legend().on().bottom();\n    chart.show();\n});  The test results presented below are not entirely surprising, and seem to suggest that the user  Comparator \nbased sort is more than twice as slow as the built in sort. As might be expected, the built in sort\nhas private access to the internal structures of the  DataFrame , and therefore has to make fewer calls\nto get to the data in question. An interesting outcome in the sequential versus parallel battle\nis that the parallel sort with the  Comparator  is still faster than the sequential sort without\nthe  Comparator .   \n       These performance benchmarks are obviously very controlled test cases, and real world performance is likely\nto differ for all kinds of reasons. In addition, at the time of writing the Morpheus Library is at version 1.0, \nso performance improvements are likely to come in follow up releases.",
            "title": "Comparator Performance"
        },
        {
            "location": "/frame/grouping/",
            "text": "Grouping\n\n\nIntroduction\n\n\nThe contents of a Morpheus \nDataFrame\n can be grouped in either the row or column dimension making it easy \nto aggregate and compute summary statistics on these groups. A group is nothing more than a \nDataFrame\n filter, \nso grouping even large frames has fairly modest memory requirements. Grouping functions can operate in\neither sequential mode or parallel mode for improved performance on large frames. Finally, rows or columns \ncan be grouped based on derived data, not just from data that exists in the frame. For example, if a \nDataFrame\n \ncontains a column with dates, it is possible to group the data into months without adding an explicit column \nto capture the month associated with each date (see examples below).\n\n\nExample Data\n\n\nIn this section, we will continue to use the ATP 2013 dataset that was introduced earlier in the filtering \ndiscussion \nhere\n. This dataset makes for an ideal grouping candidate given the large \nnumber of categorical items such as \nSurface\n, \nTournament\n, \nRound\n and so on. As a reminder, here are the first \n10 rows of the 2013 ATP match \nDataFrame\n.\n\n\n\n Index  |  Location  |        Tournament        |     Date     |  Series  |   Court   |  Surface  |    Round    |  Best of  |     Winner     |     Loser      |  WRank  |  LRank  |  WPts  |  LPts  |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     0  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |      Mayer F.  |    Giraldo S.  |     28  |     57  |  1215  |   778  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.3600  |  3.0000  |  1.4500  |  2.6500  |  1.4400  |  2.6200  |  1.4700  |  2.8500  |  1.4400  |  2.6300  |  1.4700  |  3.2000  |  1.4200  |  2.7800  |\n     1  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |   Nieminen J.  |  Benneteau J.  |     41  |     35  |   927  |  1075  |   6  |   3  |   2  |   6  |   6  |   1  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.8000  |  1.9100  |  1.8000  |  2.1000  |  1.7300  |  2.0000  |  1.8000  |  2.2600  |  1.7300  |  2.0500  |\n     2  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |  Nishikori K.  |  Matosevic M.  |     19  |     49  |  1830  |   845  |   7  |   5  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.2500  |  3.7500  |  1.2500  |  3.7500  |  1.2900  |  3.5000  |  1.3000  |  3.8500  |  1.3000  |  3.2000  |  1.3000  |  4.2000  |  1.2800  |  3.5800  |\n     3  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |  Baghdatis M.  |   Mitchell B.  |     36  |    326  |  1070  |   137  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.0700  |  9.0000  |  1.0600  |  8.0000  |  1.0800  |  7.0000  |  1.0800  |  9.4300  |  1.0700  |  7.0000  |  1.1000  |  9.5000  |  1.0800  |  7.7600  |\n     4  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Istomin D.  |     Klizan M.  |     43  |     30  |   897  |  1175  |   6  |   1  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.9000  |  1.8000  |  1.8700  |  1.8700  |  1.9100  |  1.8000  |  1.8800  |  2.0000  |  1.9100  |  1.8000  |  2.0500  |  2.0000  |  1.8800  |  1.8500  |\n     5  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Millman J.  |        Ito T.  |    199  |     79  |   239  |   655  |   6  |   4  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.7300  |  2.0000  |  1.7000  |  2.2700  |  1.8000  |  1.9100  |  1.8500  |  2.2800  |  1.7100  |  2.0800  |\n     6  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |      Falla A.  |     Levine J.  |     54  |    104  |   809  |   530  |   6  |   1  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  2.2000  |  1.6100  |  2.0800  |  1.6700  |  1.9100  |  1.8000  |  2.2600  |  1.7000  |  2.0000  |  1.7300  |  2.3200  |  1.8300  |  2.0800  |  1.7000  |\n     7  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |     Melzer J.  |      Kudla D.  |     29  |    137  |  1177  |   402  |   2  |   6  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.4400  |  2.6200  |  1.5500  |  2.3500  |  1.4400  |  2.6200  |  1.6000  |  2.4700  |  1.5000  |  2.5000  |  1.6300  |  2.8200  |  1.5200  |  2.4600  |\n     8  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Robredo T.  |   Harrison R.  |    114  |     69  |   495  |   710  |   6  |   4  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  3.0000  |  1.3600  |  2.5000  |  1.5000  |  2.3800  |  1.5300  |  2.9300  |  1.4500  |  2.5000  |  1.5000  |  3.2500  |  1.5300  |  2.6600  |  1.4700  |\n     9  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |   Dimitrov G.  |      Baker B.  |     48  |     61  |   866  |   756  |   6  |   3  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.3600  |  3.0000  |  1.4000  |  2.8000  |  1.4400  |  2.6200  |  1.3800  |  3.3000  |  1.4000  |  2.7500  |  1.4500  |  3.5500  |  1.3900  |  2.8700  |Groups for depth 0...\n\n\n\n\nGrouping Rows\n\n\nTwo \ngroupBy()\n methods exist on the \nDataFrameAxis\n interface which can be accessed by calling \nDataFrame.rows()\n as \nshown below. The first \ngroupBy()\n function takes one or more column keys from which to extract data to generate the\nrelavant groups, and the second takes a lambda expression which must return a \nTuple\n representing the group for the \nrow presented to it. The following code groups the frame by \nSurface\n and then by \nRound\n:\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\nDataFrameGrouping.Rows<Integer,String> grouping = frame.rows().groupBy(\"Surface\", \"Round\");\n\n\n\n\nThe same grouping can be achieved using the lambda \ngroupBy()\n method, albeit more verbosely, as follows:\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\nDataFrameGrouping.Rows<Integer,String> grouping = frame.rows().groupBy(row -> {\n    String surface = row.getValue(\"Surface\");\n    String round = row.getValue(\"Round\");\n    return Tuple.of(surface, round);\n});\n\n\n\n\nThis second example demonstrates to some degree what is going on under the hood in the first example, where\na \nTuple\n that is used to represent the group key is explicitly created in user code. This latter method is useful \nbecause it allows for an arbitrary group to be created based on data that may not be present in the \nDataFrame\n. \nConsider the scenario below where we group the ATP tournament data into months by extracting the month from the \ntournament date as follows:\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\nframe.rows().groupBy(row -> {\n    LocalDate date = row.getValue(\"Date\");\n    Month month = date.getMonth();\n    return Tuple.of(month);\n}).forEach(0, (groupKey, group) -> {\n    System.out.printf(\"There are %s rows for group %s\\n\", group.rowCount(), groupKey);\n});\n\n\n\n\nThere are 252 rows for group (JANUARY)\nThere are 332 rows for group (FEBRUARY)\nThere are 206 rows for group (MARCH)\nThere are 207 rows for group (APRIL)\nThere are 298 rows for group (MAY)\nThere are 275 rows for group (JUNE)\nThere are 303 rows for group (JULY)\nThere are 286 rows for group (AUGUST)\nThere are 136 rows for group (SEPTEMBER)\nThere are 300 rows for group (OCTOBER)\nThere are 22 rows for group (NOVEMBER)\nThere are 14 rows for group (DECEMBER)\n\n\n\nGrouping Depth\n\n\nThe \nDataFrameGrouping\n interface, an instance of which is returned from the \ngroupBy()\n methods, provides a versatile\nAPI to query and analyze the grouped data. The previous example demonstrates how to iterate over groups at a given\ndepth using a user provided lambda \nBiConsumer\n. The depth equals the number of dimensions in the grouping, so in the\nexamples where we perform a 2-dimensional group-by, the depth equals 2.  The Morpheus API allows groups at different\ndepths to be analyzed independently for maximum versatility. For example, consider a 3-dimensional group-by first by\n\nCourt\n, \nSurface\n and then \nRound\n as follows:\n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\nDataFrameGrouping.Rows<Integer,String> grouping = frame.rows().groupBy(\"Court\", \"Surface\", \"Round\");\nfor (int depth=0; depth<grouping.getDepth(); ++depth) {\n    System.out.printf(\"Groups for depth %s...\\n\", depth);\n    grouping.getGroupKeys(depth).sorted().forEach(groupKey -> {\n        DataFrame<Integer,String> group = grouping.getGroup(groupKey);\n        System.out.printf(\"There are %s rows for group %s\\n\", group.rowCount(), groupKey);\n    });\n}\n\n\n\n\nGroups for depth 0...\nThere are 514 rows for group (Indoor)\nThere are 2117 rows for group (Outdoor)\nGroups for depth 1...\nThere are 27 rows for group (Indoor,Clay)\nThere are 487 rows for group (Indoor,Hard)\nThere are 826 rows for group (Outdoor,Clay)\nThere are 298 rows for group (Outdoor,Grass)\nThere are 993 rows for group (Outdoor,Hard)\nGroups for depth 2...\nThere are 12 rows for group (Indoor,Clay,1st Round)\nThere are 8 rows for group (Indoor,Clay,2nd Round)\nThere are 4 rows for group (Indoor,Clay,Quarterfinals)\nThere are 2 rows for group (Indoor,Clay,Semifinals)\nThere are 1 rows for group (Indoor,Clay,The Final)\nThere are 216 rows for group (Indoor,Hard,1st Round)\nThere are 136 rows for group (Indoor,Hard,2nd Round)\nThere are 8 rows for group (Indoor,Hard,3rd Round)\nThere are 64 rows for group (Indoor,Hard,Quarterfinals)\nThere are 12 rows for group (Indoor,Hard,Round Robin)\nThere are 34 rows for group (Indoor,Hard,Semifinals)\nThere are 17 rows for group (Indoor,Hard,The Final)\nThere are 368 rows for group (Outdoor,Clay,1st Round)\nThere are 240 rows for group (Outdoor,Clay,2nd Round)\nThere are 56 rows for group (Outdoor,Clay,3rd Round)\nThere are 8 rows for group (Outdoor,Clay,4th Round)\nThere are 88 rows for group (Outdoor,Clay,Quarterfinals)\nThere are 44 rows for group (Outdoor,Clay,Semifinals)\nThere are 22 rows for group (Outdoor,Clay,The Final)\nThere are 144 rows for group (Outdoor,Grass,1st Round)\nThere are 80 rows for group (Outdoor,Grass,2nd Round)\nThere are 24 rows for group (Outdoor,Grass,3rd Round)\nThere are 8 rows for group (Outdoor,Grass,4th Round)\nThere are 24 rows for group (Outdoor,Grass,Quarterfinals)\nThere are 12 rows for group (Outdoor,Grass,Semifinals)\nThere are 6 rows for group (Outdoor,Grass,The Final)\nThere are 436 rows for group (Outdoor,Hard,1st Round)\nThere are 288 rows for group (Outdoor,Hard,2nd Round)\nThere are 104 rows for group (Outdoor,Hard,3rd Round)\nThere are 32 rows for group (Outdoor,Hard,4th Round)\nThere are 76 rows for group (Outdoor,Hard,Quarterfinals)\nThere are 38 rows for group (Outdoor,Hard,Semifinals)\nThere are 19 rows for group (Outdoor,Hard,The Final)\n\n\n\nSummary Statistics\n\n\nThe primary motivation for grouping data is very often for aggregation purposes or for computing summary statistics\non the groups. The \nstats()\n method on the \nDataFrameGrouping\n interface takes the grouping depth to operate on and \nreturns a standard Morpheus \nStats\n reference that itself produces \nDataFrames\n of summary statistics. The stats frames \nwill only include numeric columns on which stats can be calculated. The examples below illustrate the results of computing \nthe \nmean()\n on groups at different depths. \n\n\n\n\n\nDataFrame<Integer,String> frame = loadTennisMatchData(2013);\nDataFrameGrouping.Rows<Integer,String> grouping = frame.rows().groupBy(\"Court\", \"Surface\", \"Round\");\n//Computes means for top level groups\ngrouping.stats(0).mean().rows().sort(true).out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});\n\n\n\n\n\n   Index    |  Best of  |  WRank  |  LRank  |   WPts    |   LPts    |   W1   |   L1   |   W2   |   L2   |   W3   |   L3   |   W4   |   L4   |   W5   |   L5   |  Wsets  |  Lsets  |  B365W  |  B365L  |  EXW   |  EXL   |  LBW   |  LBL   |  PSW   |  PSL   |  SJW   |  SJL   |  MaxW  |  MaxL  |  AvgW  |  AvgL  |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  (Indoor)  |     3.00  |  55.80  |  89.98  |  1908.29  |  1163.28  |  5.79  |  4.14  |  5.67  |  3.91  |  2.16  |  1.19  |  0.00  |  0.00  |  0.00  |  0.00  |   1.94  |   0.36  |   1.90  |   3.26  |  1.86  |  2.95  |  1.86  |  2.94  |  1.99  |  3.39  |  1.88  |  2.99  |  2.09  |  3.66  |  1.90  |  3.07  |\n (Outdoor)  |     3.48  |  52.63  |  84.87  |  2147.69  |  1144.18  |  5.76  |  4.00  |  5.65  |  3.80  |  3.01  |  1.78  |  0.61  |  0.43  |  0.30  |  0.18  |   2.18  |   0.42  |   1.85  |   4.39  |  1.79  |  3.58  |  1.80  |  3.76  |  1.93  |  4.71  |  1.82  |  3.78  |  2.04  |  5.30  |  1.83  |  3.94  |\n\n\n\n\nExtending this to groups one level below the top level, so therefore with depth = 1:\n\n\n\n\n\n//Computes means for second level, with depth = 1\ngrouping.stats(1).mean().rows().sort(true).out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});\n\n\n\n\n\n      Index       |  Best of  |  WRank  |  LRank  |   WPts    |   LPts    |   W1   |   L1   |   W2   |   L2   |   W3   |   L3   |   W4   |   L4   |   W5   |   L5   |  Wsets  |  Lsets  |  B365W  |  B365L  |  EXW   |  EXL   |  LBW   |  LBL   |  PSW   |  PSL   |  SJW   |  SJL   |  MaxW  |  MaxL  |  AvgW  |  AvgL  |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n   (Indoor,Clay)  |     3.00  |  74.67  |  91.37  |  1411.93  |   785.89  |  6.22  |  4.30  |  5.37  |  4.33  |  2.26  |  1.30  |  0.00  |  0.00  |  0.00  |  0.00  |   1.93  |   0.37  |   1.96  |   3.47  |  1.93  |  3.16  |  1.91  |  3.14  |  2.10  |  3.65  |  1.89  |  3.13  |  2.13  |  3.93  |  1.94  |  3.28  |\n   (Indoor,Hard)  |     3.00  |  54.76  |  89.91  |  1935.81  |  1184.21  |  5.77  |  4.13  |  5.69  |  3.89  |  2.16  |  1.18  |  0.00  |  0.00  |  0.00  |  0.00  |   1.94  |   0.36  |   1.90  |   3.25  |  1.86  |  2.94  |  1.85  |  2.93  |  1.99  |  3.37  |  1.88  |  2.98  |  2.09  |  3.65  |  1.90  |  3.06  |\n  (Outdoor,Clay)  |     3.31  |  55.96  |  92.25  |  1824.49  |  1091.88  |  5.73  |  3.88  |  5.62  |  3.75  |  2.74  |  1.51  |  0.35  |  0.26  |  0.20  |  0.13  |   2.10  |   0.40  |   1.84  |   4.15  |  1.80  |  3.48  |  1.80  |  3.58  |  1.91  |  4.37  |  1.82  |  3.58  |  2.02  |  4.86  |  1.83  |  3.75  |\n (Outdoor,Grass)  |     3.85  |  63.94  |  95.93  |  1898.03  |  1016.78  |  5.85  |  4.08  |  5.85  |  3.96  |  3.62  |  2.39  |  0.88  |  0.58  |  0.37  |  0.22  |   2.35  |   0.40  |   1.98  |   4.36  |  1.89  |  3.72  |  1.93  |  3.80  |  2.11  |  4.73  |  1.89  |  3.74  |  2.31  |  5.34  |  1.93  |  3.97  |\n  (Outdoor,Hard)  |     3.51  |  46.47  |  75.41  |  2491.45  |  1225.92  |  5.76  |  4.08  |  5.61  |  3.81  |  3.05  |  1.83  |  0.74  |  0.52  |  0.37  |  0.21  |   2.19  |   0.45  |   1.81  |   4.61  |  1.76  |  3.62  |  1.77  |  3.90  |  1.89  |  4.98  |  1.80  |  3.95  |  1.98  |  5.65  |  1.80  |  4.08  |\n\n\n\n\nFinally the most granular grouping can be accessed with depth = 2:\n\n\n\n\n\n//Computes means for third level, with depth = 2\ngrouping.stats(2).mean().rows().sort(true).out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});\n\n\n\n\n\n\n            Index             |  Best of  |  WRank  |  LRank   |   WPts    |   LPts    |   W1   |   L1   |   W2   |   L2   |   W3   |   L3   |   W4   |   L4   |   W5   |   L5   |  Wsets  |  Lsets  |  B365W  |  B365L  |  EXW   |  EXL   |  LBW   |  LBL   |  PSW   |  PSL   |  SJW   |  SJL   |  MaxW  |  MaxL  |  AvgW  |  AvgL  |\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     (Indoor,Clay,1st Round)  |     3.00  |  88.50  |  112.83  |   630.58  |   613.67  |  6.33  |  4.42  |  4.83  |  4.42  |  2.42  |  1.33  |  0.00  |  0.00  |  0.00  |  0.00  |   1.83  |   0.42  |   1.95  |   2.55  |  1.92  |  2.55  |  1.89  |  2.46  |  2.04  |  2.69  |  1.89  |  2.43  |  2.05  |  2.93  |  1.90  |  2.58  |\n     (Indoor,Clay,2nd Round)  |     3.00  |  70.25  |   69.50  |  1458.88  |   966.75  |  6.38  |  4.25  |  5.75  |  3.62  |  1.63  |  1.00  |  0.00  |  0.00  |  0.00  |  0.00  |   2.00  |   0.25  |   2.09  |   4.22  |  2.05  |  3.48  |  2.04  |  3.38  |  2.33  |  4.38  |  1.98  |  3.05  |  2.38  |  4.69  |  2.09  |  3.76  |\n (Indoor,Clay,Quarterfinals)  |     3.00  |  72.25  |   68.25  |  1808.50  |  1109.25  |  5.75  |  5.25  |  5.50  |  5.00  |  3.25  |  2.50  |  0.00  |  0.00  |  0.00  |  0.00  |   2.00  |   0.50  |   2.22  |   3.48  |  2.13  |  3.23  |  2.15  |  3.50  |  2.33  |  3.26  |  2.12  |  3.49  |  2.37  |  3.70  |  2.17  |  3.24  |\n    (Indoor,Clay,Semifinals)  |     3.00  |  49.00  |   95.50  |  3050.00  |   567.00  |  6.00  |  3.00  |  6.50  |  6.00  |  3.00  |  0.50  |  0.00  |  0.00  |  0.00  |  0.00  |   2.00  |   0.50  |   1.28  |   6.25  |  1.31  |  5.18  |  1.29  |  5.44  |  1.35  |  7.23  |  1.28  |  6.75  |  1.35  |  7.82  |  1.30  |  5.94  |\n     (Indoor,Clay,The Final)  |     3.00  |   5.00  |   93.00  |  5550.00  |   550.00  |  6.00  |  2.00  |  6.00  |  3.00  |  0.00  |  0.00  |  0.00  |  0.00  |  0.00  |  0.00  |   2.00  |   0.00  |   1.44  |   2.75  |  1.38  |  2.90  |  1.40  |  2.75  |  1.52  |  2.78  |  1.44  |  2.75  |  1.57  |  2.90  |  1.44  |  2.73  |\n     (Indoor,Hard,1st Round)  |     3.00  |  78.47  |  119.85  |   983.54  |   791.82  |  5.74  |  4.19  |  5.80  |  3.74  |  1.89  |  1.06  |  0.00  |  0.00  |  0.00  |  0.00  |   1.92  |   0.32  |   2.06  |   3.05  |  2.00  |  2.72  |  1.97  |  2.70  |  2.16  |  3.15  |  2.01  |  2.73  |  2.30  |  3.38  |  2.07  |  2.85  |\n     (Indoor,Hard,2nd Round)  |     3.00  |  47.49  |   85.58  |  1933.18  |   876.24  |  5.86  |  3.90  |  5.66  |  4.09  |  2.14  |  1.20  |  0.00  |  0.00  |  0.00  |  0.00  |   1.96  |   0.35  |   1.76  |   3.51  |  1.75  |  3.18  |  1.75  |  3.16  |  1.83  |  3.58  |  1.76  |  3.26  |  1.91  |  4.02  |  1.76  |  3.31  |\n     (Indoor,Hard,3rd Round)  |     3.00  |   5.25  |   16.88  |  6193.75  |  2036.25  |  5.88  |  4.38  |  6.00  |  2.88  |  1.50  |  0.75  |  0.00  |  0.00  |  0.00  |  0.00  |   2.00  |   0.25  |   1.37  |   3.57  |  1.41  |  3.32  |  1.42  |  3.38  |  1.44  |  3.85  |  1.42  |  3.28  |  1.47  |  4.00  |  1.41  |  3.37  |\n (Indoor,Hard,Quarterfinals)  |     3.00  |  32.70  |   66.11  |  2366.33  |  1487.16  |  5.87  |  4.25  |  5.44  |  3.89  |  2.41  |  1.28  |  0.00  |  0.00  |  0.00  |  0.00  |   1.92  |   0.39  |   1.89  |   3.23  |  1.81  |  2.99  |  1.84  |  2.95  |  1.98  |  3.49  |  1.87  |  3.01  |  2.09  |  3.67  |  1.87  |  3.05  |\n   (Indoor,Hard,Round Robin)  |     3.00  |   4.17  |    6.08  |  7602.08  |  4375.42  |  6.00  |  4.75  |  5.33  |  5.00  |  4.17  |  2.08  |  0.00  |  0.00  |  0.00  |  0.00  |   2.00  |   0.67  |   1.44  |   4.03  |  1.44  |  3.75  |  1.44  |  3.98  |  1.49  |  4.08  |  1.45  |  3.87  |  1.53  |  4.30  |  1.45  |  3.81  |\n\n\n\n\nGrouping Columns\n\n\nGrouping columns in a \nDataFrame\n is entirely analogous to grouping rows as the API is completely symmetrical in the\nrow and column dimension. Instead of operating on the \nDataFrameAxis\n returned by a call to \nDataFrame.rows()\n, you operate \non the same interface returned from \nDataFrame.cols()\n. The ATP match results served as a good candidate for grouping rows, \nbut is not appropriate for grouping columns as there are few if any repetitions within a given row. Rather than introduce \na new dataset, we can simply transpose the ATP dataset and then group the columns.\n\n\n\n\n\n//Transpose the ATP match data, select 20 left columns, print first ten rows\nloadTennisMatchData(2013).transpose().left(20).out().print(10);\n\n\n\n\n\n   Index     |            0             |            1             |            2             |            3             |            4             |            5             |            6             |            7             |            8             |            9             |            10            |            11            |            12            |            13            |            14            |            15            |            16            |            17            |            18            |            19            |\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n   Location  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |\n Tournament  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |\n       Date  |              2012-12-31  |              2012-12-31  |              2012-12-31  |              2012-12-31  |              2013-01-01  |              2013-01-01  |              2013-01-01  |              2013-01-01  |              2013-01-01  |              2013-01-01  |              2013-01-01  |              2013-01-01  |              2013-01-02  |              2013-01-02  |              2013-01-02  |              2013-01-02  |              2013-01-03  |              2013-01-03  |              2013-01-03  |              2013-01-03  |\n     Series  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |\n      Court  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |\n    Surface  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |\n      Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               2nd Round  |               2nd Round  |               2nd Round  |               2nd Round  |               2nd Round  |               2nd Round  |               2nd Round  |               2nd Round  |\n    Best of  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |\n     Winner  |                Mayer F.  |             Nieminen J.  |            Nishikori K.  |            Baghdatis M.  |              Istomin D.  |              Millman J.  |                Falla A.  |               Melzer J.  |              Robredo T.  |             Dimitrov G.  |               Goffin D.  |               Hewitt L.  |                Simon G.  |            Baghdatis M.  |            Nishikori K.  |           Dolgopolov O.  |             Dimitrov G.  |               Melzer J.  |              Istomin D.  |               Murray A.  |\n      Loser  |              Giraldo S.  |            Benneteau J.  |            Matosevic M.  |             Mitchell B.  |               Klizan M.  |                  Ito T.  |               Levine J.  |                Kudla D.  |             Harrison R.  |                Baker B.  |                Ebden M.  |             Kunitsyn I.  |                Falla A.  |                Mayer F.  |              Robredo T.  |             Nieminen J.  |               Raonic M.  |               Goffin D.  |               Hewitt L.  |              Millman J.  |Groups for depth 0...\n\n\n\n\nWith the transposed ATP match results \nDataFrame\n, we can now group the columns in much the same way as we grouped the \nrows. The code below performs a 2-dimensional column grouping based on data in the rows identified by \nCourt\n and \nSurface\n,\nand subsequently prints out the number of columns per group at level 0 and 1.\n\n\n\n\n\nDataFrame<String,Integer> frame = loadTennisMatchData(2013).transpose();\nDataFrameGrouping.Cols<String,Integer> grouping = frame.cols().groupBy(\"Court\", \"Surface\");\nfor (int depth=0; depth<grouping.getDepth(); ++depth) {\n    System.out.printf(\"Groups for depth %s...\\n\", depth);\n    grouping.getGroupKeys(depth).sorted().forEach(groupKey -> {\n        DataFrame<String,Integer> group = grouping.getGroup(groupKey);\n        System.out.printf(\"There are %s columns for group %s\\n\", group.colCount(), groupKey);\n    });\n}\n\n\n\n\nGroups for depth 0...\nThere are 514 columns for group (Indoor)\nThere are 2117 columns for group (Outdoor)\nGroups for depth 1...\nThere are 27 columns for group (Indoor,Clay)\nThere are 487 columns for group (Indoor,Hard)\nThere are 826 columns for group (Outdoor,Clay)\nThere are 298 columns for group (Outdoor,Grass)\nThere are 993 columns for group (Outdoor,Hard)\n\n\n\nTo avoid repetition, further column grouping examples are omitted as they are identical to the row grouping scenarios \nillustrated in prior sections. The next section presents and discusses some data on grouping performance of large \n\nDataFrames\n.\n\n\nPerformance\n\n\nThe performance of the group-by functions on a \nDataFrame\n will vary depending on many factors, including \nbut not limited to the number of resulting groups, the depth of the grouping and the cost of extracting the \nrelevant data from rows or columns. As with many \nDataFrame\n operations, a \nparallel\n implementation of grouping \ncan be used to speed up scenarios which involve a large number of records and/or ones in which the code to \nassemble a group is somewhat costly. This section provides examples of how sequential and parallel grouping \nperformance might be expected to compare in the most common scenarios. The absolute times published below are \nobviously vary machine specific, and in this case relate to a Macbook Pro with Quad Core-i7 processor.\n\n\nExample Data\n\n\nThe ATP match data only holds a few thousand records per year which is a little small to gather reasonable group-by \nperformance statistics against (the times will be small and thus sensitive to measurement noise). We will therefore \nuse the the UK house price transaction data made available from the UK Land Registry as introduced in the Morpeus \nOverview section. This dataset, which contains approximately 1.35 records in 2006, can be loaded using the code below. \nWhile the raw data is in CSV format, it does not include a header as the first row, so for convenience we map the \ndefault numbered columns to more meaningful names.\n\n\n\n\n\n/**\n * Returns a DataFrame of UK house prices from the Land Registry of the UK\n * @param year      the year to load prices for\n * @return          the house price DataFrame\n */\nstatic DataFrame<Integer,String> loadHousePrices(int year) {\n    return DataFrame.read().csv(options -> {\n        options.setHeader(false);\n        options.setParallel(true);\n        options.setExcludeColumns(\"Column-0\");\n        options.setResource(\"/uk-house-prices-\" + year + \".csv\");\n        options.getFormats().setDateFormat(\"Date\", \"yyyy-MM-dd HH:mm\");\n        options.setColumnNameMapping((colName, colOrdinal) -> {\n            switch (colOrdinal) {\n                case 0:  return \"Price\";\n                case 1:  return \"Date\";\n                case 2:  return \"PostCode\";\n                case 3:  return \"PropertyType\";\n                case 4:  return \"Old/New\";\n                case 5:  return \"Duration\";\n                case 6:  return \"PAON\";\n                case 7:  return \"SAON\";\n                case 8:  return \"Street\";\n                case 9:  return \"Locality\";\n                case 10: return \"Town/City\";\n                case 11: return \"District\";\n                case 12: return \"County\";\n                case 13: return \"PPDType\";\n                case 14: return \"RecordStatus\";\n                default: return colName;\n            }\n        });\n    });\n}\n\n\n\n\n\n Index  |  Price   |         Date          |  PostCode  |  PropertyType  |  Old/New  |  Duration  |  PAON  |  SAON  |        Street        |   Locality   |  Town/City   |       District       |        County        |  PPDType  |  RecordStatus  |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     0  |  180000  |  2006-06-30T00:00:00  |   PR2 3SP  |             S  |        N  |         F  |    30  |  null  |    CLEVELEYS AVENUE  |     FULWOOD  |     PRESTON  |             PRESTON  |          LANCASHIRE  |        A  |             A  |\n     1  |   99750  |  2006-09-15T00:00:00  |  EX23 8TA  |             F  |        N  |         L  |    25  |  null  |         BULLEID WAY  |        BUDE  |        BUDE  |      NORTH CORNWALL  |            CORNWALL  |        A  |             A  |\n     2  |   65000  |  2006-10-06T00:00:00  |  BB12 7ER  |             T  |        N  |         L  |    70  |  null  |  SHAKESPEARE STREET  |     PADIHAM  |     BURNLEY  |             BURNLEY  |          LANCASHIRE  |        A  |             A  |\n     3  |  120000  |  2006-02-13T00:00:00  |   LE2 1PB  |             T  |        N  |         F  |    35  |  null  |       CHEPSTOW ROAD  |   LEICESTER  |   LEICESTER  |           LEICESTER  |           LEICESTER  |        A  |             A  |\n     4  |  112000  |  2006-04-07T00:00:00  |  CF38 2JF  |             T  |        N  |         F  |   127  |  null  |         MANOR CHASE  |      BEDDAU  |  PONTYPRIDD  |  RHONDDA CYNON TAFF  |  RHONDDA CYNON TAFF  |        A  |             A  |\n     5  |   84950  |  2006-02-24T00:00:00  |   CV6 4AS  |             T  |        N  |         F  |   381  |  null  |        BURNABY ROAD  |        null  |    COVENTRY  |            COVENTRY  |       WEST MIDLANDS  |        A  |             A  |\n     6  |  124950  |  2006-10-06T00:00:00  |  ST20 0HZ  |             S  |        N  |         F  |    38  |  null  |     ST LAWRENCE WAY  |     GNOSALL  |    STAFFORD  |            STAFFORD  |       STAFFORDSHIRE  |        A  |             A  |\n     7  |   30000  |  2006-11-03T00:00:00  |    M8 0QL  |             S  |        Y  |         F  |    32  |  null  |          CHIME BANK  |  MANCHESTER  |  MANCHESTER  |          MANCHESTER  |  GREATER MANCHESTER  |        A  |             A  |\n     8  |  380000  |  2006-09-25T00:00:00  |    N7 9SY  |             F  |        N  |         F  |     8  |  null  |    HEDDINGTON GROVE  |      LONDON  |      LONDON  |           ISLINGTON  |      GREATER LONDON  |        A  |             A  |\n     9  |  451000  |  2006-02-08T00:00:00  |   SW9 9AL  |             F  |        Y  |         L  |    29  |  null  |         STANE GROVE  |        null  |      LONDON  |             LAMBETH  |      GREATER LONDON  |        A  |             A  |\n\n\n\n\nGroup Count\n\n\nThe first example attempts to demonstrate the performance impact of an increasing number of groups created by a the \ngroup-by operation. In this case we group the rows of the UK property transaction data for 2006 by 5 different columns \nindependently, namely \nPropertyType\n, \nCounty\n, \nMonth\n, \nDistrict\n, \nTown/City\n and \nLocality\n. The group count for these \ncolumns is listed below. The expectation is that execution times will increase as the number of groups generated by a group-by \nfunction increases, which is exactly what we see in the subsequent plot.\n\n\n\n\nProperty Type\n: 5 Groups\n\n\nMonth\n: 12 Groups\n\n\nCounty\n: 115 Groups\n\n\nDistrict\n: 387 Groups\n\n\nTown/City\n: 1161 Groups\n\n\nLocality\n: 16800 Groups\n\n\n\n\n\n    \n\n\n\n\n\nRunning this same example with \nparallel execution\n yields a similar trend, but the absolute times are much smaller.\n\n\n\n    \n\n\n\n\n\nThe following code is used to generate the above results, for the sequential execution.\n\n\n\n\n\n//Load UK house prices for 2006\nDataFrame<Integer,String> frame = loadHousePrices(2006);\n\n//Run 10 iterations of sequential and parallel group by Town/City\nDataFrame<String,String> results = PerfStat.run(5, TimeUnit.MILLISECONDS, false, tasks -> {\n    tasks.put(\"PropertyType\", () -> frame.rows().groupBy(\"PropertyType\"));\n    tasks.put(\"Month\", () -> frame.rows().groupBy(row -> {\n        final LocalDateTime date = row.getValue(\"Date\");\n        return Tuple.of(date.getMonth());\n    }));\n    tasks.put(\"County\", () -> frame.rows().groupBy(\"County\"));\n    tasks.put(\"District\", () -> frame.rows().groupBy(\"District\"));\n    tasks.put(\"Town/City\", () -> frame.rows().groupBy(\"Town/City\"));\n    tasks.put(\"Locality\", () -> frame.rows().groupBy(\"Locality\"));\n});\n\n//Plot the results of the combined DataFrame with timings\nChart.create().withBarPlot(results, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time In Milliseconds\");\n    chart.title().withText(\"1-Dimensional Grouping of 1.35 Million Rows (Sequential)\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.subtitle().withText(\"Grouping of UK House Price Transactions in 2006 by various columns\");\n    chart.legend().on().bottom();\n    chart.show();\n});\n\n\n\n\nSequential vs Parallel\n\n\nThe following example plots the results of a standard 1-dimensional group-by operation on the same chart. The 1.35 \nmillion rows in the UK house price dataset is grouped by \nCounty\n, which results in 115 distinct groups as indicated\nin the previous section. The timing statistics below seem to indicate the parallel grouping in this scenario was a little\nover twice as fast as the sequential algorithm. This result bodes well as this is the simplest possible group-by scenatio, \nand we would expect the parallel algorithm to perform even better as the grouping complexity increases.\n\n\n\n    \n\n\n\n\n\nThe following code is used to generate the above results.\n\n\n\n\n\n//Load UK house prices for 2006\nDataFrame<Integer,String> frame = loadHousePrices(2006);\n\n//Run 10 iterations of sequential and parallel group by County\nDataFrame<String,String> results = PerfStat.run(10, TimeUnit.MILLISECONDS, false, tasks -> {\n    tasks.put(\"Sequential\", () -> frame.rows().sequential().groupBy(\"County\"));\n    tasks.put(\"Parallel\", () -> frame.rows().parallel().groupBy(\"County\"));\n});\n\n//Plot the results of the combined DataFrame with timings\nChart.create().withBarPlot(results, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time In Milliseconds\");\n    chart.title().withText(\"1-Dimensional Grouping of 1.35 Million Rows\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.subtitle().withText(\"Grouping of UK House Price Transactions in 2006 by County\");\n    chart.legend().on().bottom();\n    chart.show();\n});\n\n\n\n\nMulti-Dimensional\n\n\nThe prior examples use 1-dimensional grouping only, so this section looks at the cost of a multi-dimensioal\ngroup-by on the rows of the UK property transaction data. In order to remove group count as a factor in the second\ndimension, we simply group by the same column twice. This is not particularly useful in reality other than to assess \nthe relative cost of adding the second dimension. Naturally, the group count in the second dimension will always\nbe 1.\n\n\nThe plot below indicates adding another dimension is significant, although using parallel execution can mostly\ndiscount the additional cost, at least in this scenario.\n\n\n\n    \n\n\n\n\n\nThe following code is used to generate the above results.\n\n\n\n\n\n//Load UK house prices for 2006\nDataFrame<Integer,String> frame = loadHousePrices(2006);\n\n//Run 10 iterations of sequential and parallel group by County and Town/City\nDataFrame<String,String> results = PerfStat.run(10, TimeUnit.MILLISECONDS, false, tasks -> {\n    tasks.put(\"Sequential(1-D)\", () -> frame.rows().sequential().groupBy(\"County\"));\n    tasks.put(\"Parallel(1-D)\", () -> frame.rows().parallel().groupBy(\"County\"));\n    tasks.put(\"Sequential(2-D)\", () -> frame.rows().sequential().groupBy(\"County\", \"County\"));\n    tasks.put(\"Parallel(2-D)\", () -> frame.rows().parallel().groupBy(\"County\", \"County\"));\n});\n\n//Plot the results of the combined DataFrame with timings\nChart.create().withBarPlot(results, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time In Milliseconds\");\n    chart.title().withText(\"1-Dimension vs 2-Dimensional Grouping of 1.35 Million Rows\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.subtitle().withText(\"Grouping of UK House Price Transactions in 2006 by County x 2\");\n    chart.legend().on().bottom();\n    chart.show();\n});\n\n\n\n\nGrouping Function\n\n\nThe Morpheus API provides two styles of grouping function, the first simply takes the column keys or row\nkeys depending if you are grouping rows or columns respectively. The second style uses a lambda expression\nthat consumes either the row or column, and returns a \nTuple\n that represents the group for that entry. The\nlatter is more flexible in that it can produce a group that is based on some derived calculation on the\nrow or column vector it is presented with. This section assess whether there is any perfomance difference\nto using either of these approaches.\n\n\nIn this example we perform a two-dimensional group by using the \nCounty\n first, and the \nTown/City\n second.\nThe results in the subsequent plot suggests there is a very small performance cost to using the second style,\nhowever it is fairly neglible in this case.\n\n\n\n    \n\n\n\n\n\nThe following code is used to generate the above results.\n\n\n\n\n\n//Load UK house prices for 2006\nDataFrame<Integer,String> frame = loadHousePrices(2006);\n\n//Run 10 iterations of sequential and parallel group by County and Town/City\nDataFrame<String,String> results = PerfStat.run(5, TimeUnit.MILLISECONDS, false, tasks -> {\n    tasks.put(\"Method-1\", () -> frame.rows().groupBy(\"County\", \"Town/City\"));\n    tasks.put(\"Method-2\", () -> frame.rows().groupBy(row -> Tuple.of(\n        row.<String>getValue(\"County\"),\n        row.<String>getValue(\"Town/City\"))\n    ));\n});\n\n//Plot the results of the combined DataFrame with timings\nChart.create().withBarPlot(results, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time In Milliseconds\");\n    chart.title().withText(\"2-Dimensional Grouping of 1.35 Million Rows\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.subtitle().withText(\"Grouping of UK House Price Transactions in 2006 by County & Town/City\");\n    chart.legend().on().bottom();\n    chart.show();\n});",
            "title": "Grouping"
        },
        {
            "location": "/frame/grouping/#grouping",
            "text": "",
            "title": "Grouping"
        },
        {
            "location": "/frame/grouping/#introduction",
            "text": "The contents of a Morpheus  DataFrame  can be grouped in either the row or column dimension making it easy \nto aggregate and compute summary statistics on these groups. A group is nothing more than a  DataFrame  filter, \nso grouping even large frames has fairly modest memory requirements. Grouping functions can operate in\neither sequential mode or parallel mode for improved performance on large frames. Finally, rows or columns \ncan be grouped based on derived data, not just from data that exists in the frame. For example, if a  DataFrame  \ncontains a column with dates, it is possible to group the data into months without adding an explicit column \nto capture the month associated with each date (see examples below).",
            "title": "Introduction"
        },
        {
            "location": "/frame/grouping/#example-data",
            "text": "In this section, we will continue to use the ATP 2013 dataset that was introduced earlier in the filtering \ndiscussion  here . This dataset makes for an ideal grouping candidate given the large \nnumber of categorical items such as  Surface ,  Tournament ,  Round  and so on. As a reminder, here are the first \n10 rows of the 2013 ATP match  DataFrame .  \n Index  |  Location  |        Tournament        |     Date     |  Series  |   Court   |  Surface  |    Round    |  Best of  |     Winner     |     Loser      |  WRank  |  LRank  |  WPts  |  LPts  |  W1  |  L1  |  W2  |  L2  |  W3  |  L3  |  W4  |  L4  |  W5  |  L5  |  Wsets  |  Lsets  |   Comment   |  B365W   |  B365L   |   EXW    |   EXL    |   LBW    |   LBL    |   PSW    |   PSL    |   SJW    |   SJL    |   MaxW   |   MaxL   |   AvgW   |   AvgL   |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     0  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |      Mayer F.  |    Giraldo S.  |     28  |     57  |  1215  |   778  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.3600  |  3.0000  |  1.4500  |  2.6500  |  1.4400  |  2.6200  |  1.4700  |  2.8500  |  1.4400  |  2.6300  |  1.4700  |  3.2000  |  1.4200  |  2.7800  |\n     1  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |   Nieminen J.  |  Benneteau J.  |     41  |     35  |   927  |  1075  |   6  |   3  |   2  |   6  |   6  |   1  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.8000  |  1.9100  |  1.8000  |  2.1000  |  1.7300  |  2.0000  |  1.8000  |  2.2600  |  1.7300  |  2.0500  |\n     2  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |  Nishikori K.  |  Matosevic M.  |     19  |     49  |  1830  |   845  |   7  |   5  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.2500  |  3.7500  |  1.2500  |  3.7500  |  1.2900  |  3.5000  |  1.3000  |  3.8500  |  1.3000  |  3.2000  |  1.3000  |  4.2000  |  1.2800  |  3.5800  |\n     3  |  Brisbane  |  Brisbane International  |  2012-12-31  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |  Baghdatis M.  |   Mitchell B.  |     36  |    326  |  1070  |   137  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.0700  |  9.0000  |  1.0600  |  8.0000  |  1.0800  |  7.0000  |  1.0800  |  9.4300  |  1.0700  |  7.0000  |  1.1000  |  9.5000  |  1.0800  |  7.7600  |\n     4  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Istomin D.  |     Klizan M.  |     43  |     30  |   897  |  1175  |   6  |   1  |   6  |   2  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.9000  |  1.8000  |  1.8700  |  1.8700  |  1.9100  |  1.8000  |  1.8800  |  2.0000  |  1.9100  |  1.8000  |  2.0500  |  2.0000  |  1.8800  |  1.8500  |\n     5  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Millman J.  |        Ito T.  |    199  |     79  |   239  |   655  |   6  |   4  |   6  |   1  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.6100  |  2.2000  |  1.7500  |  2.0000  |  1.7300  |  2.0000  |  1.7000  |  2.2700  |  1.8000  |  1.9100  |  1.8500  |  2.2800  |  1.7100  |  2.0800  |\n     6  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |      Falla A.  |     Levine J.  |     54  |    104  |   809  |   530  |   6  |   1  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  2.2000  |  1.6100  |  2.0800  |  1.6700  |  1.9100  |  1.8000  |  2.2600  |  1.7000  |  2.0000  |  1.7300  |  2.3200  |  1.8300  |  2.0800  |  1.7000  |\n     7  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |     Melzer J.  |      Kudla D.  |     29  |    137  |  1177  |   402  |   2  |   6  |   6  |   4  |   6  |   4  |   0  |   0  |   0  |   0  |      2  |      1  |  Completed  |  1.4400  |  2.6200  |  1.5500  |  2.3500  |  1.4400  |  2.6200  |  1.6000  |  2.4700  |  1.5000  |  2.5000  |  1.6300  |  2.8200  |  1.5200  |  2.4600  |\n     8  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |    Robredo T.  |   Harrison R.  |    114  |     69  |   495  |   710  |   6  |   4  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  3.0000  |  1.3600  |  2.5000  |  1.5000  |  2.3800  |  1.5300  |  2.9300  |  1.4500  |  2.5000  |  1.5000  |  3.2500  |  1.5300  |  2.6600  |  1.4700  |\n     9  |  Brisbane  |  Brisbane International  |  2013-01-01  |  ATP250  |  Outdoor  |     Hard  |  1st Round  |        3  |   Dimitrov G.  |      Baker B.  |     48  |     61  |   866  |   756  |   6  |   3  |   7  |   6  |   0  |   0  |   0  |   0  |   0  |   0  |      2  |      0  |  Completed  |  1.3600  |  3.0000  |  1.4000  |  2.8000  |  1.4400  |  2.6200  |  1.3800  |  3.3000  |  1.4000  |  2.7500  |  1.4500  |  3.5500  |  1.3900  |  2.8700  |Groups for depth 0...",
            "title": "Example Data"
        },
        {
            "location": "/frame/grouping/#grouping-rows",
            "text": "Two  groupBy()  methods exist on the  DataFrameAxis  interface which can be accessed by calling  DataFrame.rows()  as \nshown below. The first  groupBy()  function takes one or more column keys from which to extract data to generate the\nrelavant groups, and the second takes a lambda expression which must return a  Tuple  representing the group for the \nrow presented to it. The following code groups the frame by  Surface  and then by  Round :   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\nDataFrameGrouping.Rows<Integer,String> grouping = frame.rows().groupBy(\"Surface\", \"Round\");  The same grouping can be achieved using the lambda  groupBy()  method, albeit more verbosely, as follows:   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\nDataFrameGrouping.Rows<Integer,String> grouping = frame.rows().groupBy(row -> {\n    String surface = row.getValue(\"Surface\");\n    String round = row.getValue(\"Round\");\n    return Tuple.of(surface, round);\n});  This second example demonstrates to some degree what is going on under the hood in the first example, where\na  Tuple  that is used to represent the group key is explicitly created in user code. This latter method is useful \nbecause it allows for an arbitrary group to be created based on data that may not be present in the  DataFrame . \nConsider the scenario below where we group the ATP tournament data into months by extracting the month from the \ntournament date as follows:   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\nframe.rows().groupBy(row -> {\n    LocalDate date = row.getValue(\"Date\");\n    Month month = date.getMonth();\n    return Tuple.of(month);\n}).forEach(0, (groupKey, group) -> {\n    System.out.printf(\"There are %s rows for group %s\\n\", group.rowCount(), groupKey);\n});  There are 252 rows for group (JANUARY)\nThere are 332 rows for group (FEBRUARY)\nThere are 206 rows for group (MARCH)\nThere are 207 rows for group (APRIL)\nThere are 298 rows for group (MAY)\nThere are 275 rows for group (JUNE)\nThere are 303 rows for group (JULY)\nThere are 286 rows for group (AUGUST)\nThere are 136 rows for group (SEPTEMBER)\nThere are 300 rows for group (OCTOBER)\nThere are 22 rows for group (NOVEMBER)\nThere are 14 rows for group (DECEMBER)",
            "title": "Grouping Rows"
        },
        {
            "location": "/frame/grouping/#grouping-depth",
            "text": "The  DataFrameGrouping  interface, an instance of which is returned from the  groupBy()  methods, provides a versatile\nAPI to query and analyze the grouped data. The previous example demonstrates how to iterate over groups at a given\ndepth using a user provided lambda  BiConsumer . The depth equals the number of dimensions in the grouping, so in the\nexamples where we perform a 2-dimensional group-by, the depth equals 2.  The Morpheus API allows groups at different\ndepths to be analyzed independently for maximum versatility. For example, consider a 3-dimensional group-by first by Court ,  Surface  and then  Round  as follows:   DataFrame<Integer,String> frame = loadTennisMatchData(2013);\nDataFrameGrouping.Rows<Integer,String> grouping = frame.rows().groupBy(\"Court\", \"Surface\", \"Round\");\nfor (int depth=0; depth<grouping.getDepth(); ++depth) {\n    System.out.printf(\"Groups for depth %s...\\n\", depth);\n    grouping.getGroupKeys(depth).sorted().forEach(groupKey -> {\n        DataFrame<Integer,String> group = grouping.getGroup(groupKey);\n        System.out.printf(\"There are %s rows for group %s\\n\", group.rowCount(), groupKey);\n    });\n}  Groups for depth 0...\nThere are 514 rows for group (Indoor)\nThere are 2117 rows for group (Outdoor)\nGroups for depth 1...\nThere are 27 rows for group (Indoor,Clay)\nThere are 487 rows for group (Indoor,Hard)\nThere are 826 rows for group (Outdoor,Clay)\nThere are 298 rows for group (Outdoor,Grass)\nThere are 993 rows for group (Outdoor,Hard)\nGroups for depth 2...\nThere are 12 rows for group (Indoor,Clay,1st Round)\nThere are 8 rows for group (Indoor,Clay,2nd Round)\nThere are 4 rows for group (Indoor,Clay,Quarterfinals)\nThere are 2 rows for group (Indoor,Clay,Semifinals)\nThere are 1 rows for group (Indoor,Clay,The Final)\nThere are 216 rows for group (Indoor,Hard,1st Round)\nThere are 136 rows for group (Indoor,Hard,2nd Round)\nThere are 8 rows for group (Indoor,Hard,3rd Round)\nThere are 64 rows for group (Indoor,Hard,Quarterfinals)\nThere are 12 rows for group (Indoor,Hard,Round Robin)\nThere are 34 rows for group (Indoor,Hard,Semifinals)\nThere are 17 rows for group (Indoor,Hard,The Final)\nThere are 368 rows for group (Outdoor,Clay,1st Round)\nThere are 240 rows for group (Outdoor,Clay,2nd Round)\nThere are 56 rows for group (Outdoor,Clay,3rd Round)\nThere are 8 rows for group (Outdoor,Clay,4th Round)\nThere are 88 rows for group (Outdoor,Clay,Quarterfinals)\nThere are 44 rows for group (Outdoor,Clay,Semifinals)\nThere are 22 rows for group (Outdoor,Clay,The Final)\nThere are 144 rows for group (Outdoor,Grass,1st Round)\nThere are 80 rows for group (Outdoor,Grass,2nd Round)\nThere are 24 rows for group (Outdoor,Grass,3rd Round)\nThere are 8 rows for group (Outdoor,Grass,4th Round)\nThere are 24 rows for group (Outdoor,Grass,Quarterfinals)\nThere are 12 rows for group (Outdoor,Grass,Semifinals)\nThere are 6 rows for group (Outdoor,Grass,The Final)\nThere are 436 rows for group (Outdoor,Hard,1st Round)\nThere are 288 rows for group (Outdoor,Hard,2nd Round)\nThere are 104 rows for group (Outdoor,Hard,3rd Round)\nThere are 32 rows for group (Outdoor,Hard,4th Round)\nThere are 76 rows for group (Outdoor,Hard,Quarterfinals)\nThere are 38 rows for group (Outdoor,Hard,Semifinals)\nThere are 19 rows for group (Outdoor,Hard,The Final)",
            "title": "Grouping Depth"
        },
        {
            "location": "/frame/grouping/#summary-statistics",
            "text": "The primary motivation for grouping data is very often for aggregation purposes or for computing summary statistics\non the groups. The  stats()  method on the  DataFrameGrouping  interface takes the grouping depth to operate on and \nreturns a standard Morpheus  Stats  reference that itself produces  DataFrames  of summary statistics. The stats frames \nwill only include numeric columns on which stats can be calculated. The examples below illustrate the results of computing \nthe  mean()  on groups at different depths.    DataFrame<Integer,String> frame = loadTennisMatchData(2013);\nDataFrameGrouping.Rows<Integer,String> grouping = frame.rows().groupBy(\"Court\", \"Surface\", \"Round\");\n//Computes means for top level groups\ngrouping.stats(0).mean().rows().sort(true).out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});  \n   Index    |  Best of  |  WRank  |  LRank  |   WPts    |   LPts    |   W1   |   L1   |   W2   |   L2   |   W3   |   L3   |   W4   |   L4   |   W5   |   L5   |  Wsets  |  Lsets  |  B365W  |  B365L  |  EXW   |  EXL   |  LBW   |  LBL   |  PSW   |  PSL   |  SJW   |  SJL   |  MaxW  |  MaxL  |  AvgW  |  AvgL  |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  (Indoor)  |     3.00  |  55.80  |  89.98  |  1908.29  |  1163.28  |  5.79  |  4.14  |  5.67  |  3.91  |  2.16  |  1.19  |  0.00  |  0.00  |  0.00  |  0.00  |   1.94  |   0.36  |   1.90  |   3.26  |  1.86  |  2.95  |  1.86  |  2.94  |  1.99  |  3.39  |  1.88  |  2.99  |  2.09  |  3.66  |  1.90  |  3.07  |\n (Outdoor)  |     3.48  |  52.63  |  84.87  |  2147.69  |  1144.18  |  5.76  |  4.00  |  5.65  |  3.80  |  3.01  |  1.78  |  0.61  |  0.43  |  0.30  |  0.18  |   2.18  |   0.42  |   1.85  |   4.39  |  1.79  |  3.58  |  1.80  |  3.76  |  1.93  |  4.71  |  1.82  |  3.78  |  2.04  |  5.30  |  1.83  |  3.94  |  Extending this to groups one level below the top level, so therefore with depth = 1:   //Computes means for second level, with depth = 1\ngrouping.stats(1).mean().rows().sort(true).out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});  \n      Index       |  Best of  |  WRank  |  LRank  |   WPts    |   LPts    |   W1   |   L1   |   W2   |   L2   |   W3   |   L3   |   W4   |   L4   |   W5   |   L5   |  Wsets  |  Lsets  |  B365W  |  B365L  |  EXW   |  EXL   |  LBW   |  LBL   |  PSW   |  PSL   |  SJW   |  SJL   |  MaxW  |  MaxL  |  AvgW  |  AvgL  |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n   (Indoor,Clay)  |     3.00  |  74.67  |  91.37  |  1411.93  |   785.89  |  6.22  |  4.30  |  5.37  |  4.33  |  2.26  |  1.30  |  0.00  |  0.00  |  0.00  |  0.00  |   1.93  |   0.37  |   1.96  |   3.47  |  1.93  |  3.16  |  1.91  |  3.14  |  2.10  |  3.65  |  1.89  |  3.13  |  2.13  |  3.93  |  1.94  |  3.28  |\n   (Indoor,Hard)  |     3.00  |  54.76  |  89.91  |  1935.81  |  1184.21  |  5.77  |  4.13  |  5.69  |  3.89  |  2.16  |  1.18  |  0.00  |  0.00  |  0.00  |  0.00  |   1.94  |   0.36  |   1.90  |   3.25  |  1.86  |  2.94  |  1.85  |  2.93  |  1.99  |  3.37  |  1.88  |  2.98  |  2.09  |  3.65  |  1.90  |  3.06  |\n  (Outdoor,Clay)  |     3.31  |  55.96  |  92.25  |  1824.49  |  1091.88  |  5.73  |  3.88  |  5.62  |  3.75  |  2.74  |  1.51  |  0.35  |  0.26  |  0.20  |  0.13  |   2.10  |   0.40  |   1.84  |   4.15  |  1.80  |  3.48  |  1.80  |  3.58  |  1.91  |  4.37  |  1.82  |  3.58  |  2.02  |  4.86  |  1.83  |  3.75  |\n (Outdoor,Grass)  |     3.85  |  63.94  |  95.93  |  1898.03  |  1016.78  |  5.85  |  4.08  |  5.85  |  3.96  |  3.62  |  2.39  |  0.88  |  0.58  |  0.37  |  0.22  |   2.35  |   0.40  |   1.98  |   4.36  |  1.89  |  3.72  |  1.93  |  3.80  |  2.11  |  4.73  |  1.89  |  3.74  |  2.31  |  5.34  |  1.93  |  3.97  |\n  (Outdoor,Hard)  |     3.51  |  46.47  |  75.41  |  2491.45  |  1225.92  |  5.76  |  4.08  |  5.61  |  3.81  |  3.05  |  1.83  |  0.74  |  0.52  |  0.37  |  0.21  |   2.19  |   0.45  |   1.81  |   4.61  |  1.76  |  3.62  |  1.77  |  3.90  |  1.89  |  4.98  |  1.80  |  3.95  |  1.98  |  5.65  |  1.80  |  4.08  |  Finally the most granular grouping can be accessed with depth = 2:   //Computes means for third level, with depth = 2\ngrouping.stats(2).mean().rows().sort(true).out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});  \n            Index             |  Best of  |  WRank  |  LRank   |   WPts    |   LPts    |   W1   |   L1   |   W2   |   L2   |   W3   |   L3   |   W4   |   L4   |   W5   |   L5   |  Wsets  |  Lsets  |  B365W  |  B365L  |  EXW   |  EXL   |  LBW   |  LBL   |  PSW   |  PSL   |  SJW   |  SJL   |  MaxW  |  MaxL  |  AvgW  |  AvgL  |\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     (Indoor,Clay,1st Round)  |     3.00  |  88.50  |  112.83  |   630.58  |   613.67  |  6.33  |  4.42  |  4.83  |  4.42  |  2.42  |  1.33  |  0.00  |  0.00  |  0.00  |  0.00  |   1.83  |   0.42  |   1.95  |   2.55  |  1.92  |  2.55  |  1.89  |  2.46  |  2.04  |  2.69  |  1.89  |  2.43  |  2.05  |  2.93  |  1.90  |  2.58  |\n     (Indoor,Clay,2nd Round)  |     3.00  |  70.25  |   69.50  |  1458.88  |   966.75  |  6.38  |  4.25  |  5.75  |  3.62  |  1.63  |  1.00  |  0.00  |  0.00  |  0.00  |  0.00  |   2.00  |   0.25  |   2.09  |   4.22  |  2.05  |  3.48  |  2.04  |  3.38  |  2.33  |  4.38  |  1.98  |  3.05  |  2.38  |  4.69  |  2.09  |  3.76  |\n (Indoor,Clay,Quarterfinals)  |     3.00  |  72.25  |   68.25  |  1808.50  |  1109.25  |  5.75  |  5.25  |  5.50  |  5.00  |  3.25  |  2.50  |  0.00  |  0.00  |  0.00  |  0.00  |   2.00  |   0.50  |   2.22  |   3.48  |  2.13  |  3.23  |  2.15  |  3.50  |  2.33  |  3.26  |  2.12  |  3.49  |  2.37  |  3.70  |  2.17  |  3.24  |\n    (Indoor,Clay,Semifinals)  |     3.00  |  49.00  |   95.50  |  3050.00  |   567.00  |  6.00  |  3.00  |  6.50  |  6.00  |  3.00  |  0.50  |  0.00  |  0.00  |  0.00  |  0.00  |   2.00  |   0.50  |   1.28  |   6.25  |  1.31  |  5.18  |  1.29  |  5.44  |  1.35  |  7.23  |  1.28  |  6.75  |  1.35  |  7.82  |  1.30  |  5.94  |\n     (Indoor,Clay,The Final)  |     3.00  |   5.00  |   93.00  |  5550.00  |   550.00  |  6.00  |  2.00  |  6.00  |  3.00  |  0.00  |  0.00  |  0.00  |  0.00  |  0.00  |  0.00  |   2.00  |   0.00  |   1.44  |   2.75  |  1.38  |  2.90  |  1.40  |  2.75  |  1.52  |  2.78  |  1.44  |  2.75  |  1.57  |  2.90  |  1.44  |  2.73  |\n     (Indoor,Hard,1st Round)  |     3.00  |  78.47  |  119.85  |   983.54  |   791.82  |  5.74  |  4.19  |  5.80  |  3.74  |  1.89  |  1.06  |  0.00  |  0.00  |  0.00  |  0.00  |   1.92  |   0.32  |   2.06  |   3.05  |  2.00  |  2.72  |  1.97  |  2.70  |  2.16  |  3.15  |  2.01  |  2.73  |  2.30  |  3.38  |  2.07  |  2.85  |\n     (Indoor,Hard,2nd Round)  |     3.00  |  47.49  |   85.58  |  1933.18  |   876.24  |  5.86  |  3.90  |  5.66  |  4.09  |  2.14  |  1.20  |  0.00  |  0.00  |  0.00  |  0.00  |   1.96  |   0.35  |   1.76  |   3.51  |  1.75  |  3.18  |  1.75  |  3.16  |  1.83  |  3.58  |  1.76  |  3.26  |  1.91  |  4.02  |  1.76  |  3.31  |\n     (Indoor,Hard,3rd Round)  |     3.00  |   5.25  |   16.88  |  6193.75  |  2036.25  |  5.88  |  4.38  |  6.00  |  2.88  |  1.50  |  0.75  |  0.00  |  0.00  |  0.00  |  0.00  |   2.00  |   0.25  |   1.37  |   3.57  |  1.41  |  3.32  |  1.42  |  3.38  |  1.44  |  3.85  |  1.42  |  3.28  |  1.47  |  4.00  |  1.41  |  3.37  |\n (Indoor,Hard,Quarterfinals)  |     3.00  |  32.70  |   66.11  |  2366.33  |  1487.16  |  5.87  |  4.25  |  5.44  |  3.89  |  2.41  |  1.28  |  0.00  |  0.00  |  0.00  |  0.00  |   1.92  |   0.39  |   1.89  |   3.23  |  1.81  |  2.99  |  1.84  |  2.95  |  1.98  |  3.49  |  1.87  |  3.01  |  2.09  |  3.67  |  1.87  |  3.05  |\n   (Indoor,Hard,Round Robin)  |     3.00  |   4.17  |    6.08  |  7602.08  |  4375.42  |  6.00  |  4.75  |  5.33  |  5.00  |  4.17  |  2.08  |  0.00  |  0.00  |  0.00  |  0.00  |   2.00  |   0.67  |   1.44  |   4.03  |  1.44  |  3.75  |  1.44  |  3.98  |  1.49  |  4.08  |  1.45  |  3.87  |  1.53  |  4.30  |  1.45  |  3.81  |",
            "title": "Summary Statistics"
        },
        {
            "location": "/frame/grouping/#grouping-columns",
            "text": "Grouping columns in a  DataFrame  is entirely analogous to grouping rows as the API is completely symmetrical in the\nrow and column dimension. Instead of operating on the  DataFrameAxis  returned by a call to  DataFrame.rows() , you operate \non the same interface returned from  DataFrame.cols() . The ATP match results served as a good candidate for grouping rows, \nbut is not appropriate for grouping columns as there are few if any repetitions within a given row. Rather than introduce \na new dataset, we can simply transpose the ATP dataset and then group the columns.   //Transpose the ATP match data, select 20 left columns, print first ten rows\nloadTennisMatchData(2013).transpose().left(20).out().print(10);  \n   Index     |            0             |            1             |            2             |            3             |            4             |            5             |            6             |            7             |            8             |            9             |            10            |            11            |            12            |            13            |            14            |            15            |            16            |            17            |            18            |            19            |\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n   Location  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |                Brisbane  |\n Tournament  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |  Brisbane International  |\n       Date  |              2012-12-31  |              2012-12-31  |              2012-12-31  |              2012-12-31  |              2013-01-01  |              2013-01-01  |              2013-01-01  |              2013-01-01  |              2013-01-01  |              2013-01-01  |              2013-01-01  |              2013-01-01  |              2013-01-02  |              2013-01-02  |              2013-01-02  |              2013-01-02  |              2013-01-03  |              2013-01-03  |              2013-01-03  |              2013-01-03  |\n     Series  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |                  ATP250  |\n      Court  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |                 Outdoor  |\n    Surface  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |                    Hard  |\n      Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               1st Round  |               2nd Round  |               2nd Round  |               2nd Round  |               2nd Round  |               2nd Round  |               2nd Round  |               2nd Round  |               2nd Round  |\n    Best of  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |                       3  |\n     Winner  |                Mayer F.  |             Nieminen J.  |            Nishikori K.  |            Baghdatis M.  |              Istomin D.  |              Millman J.  |                Falla A.  |               Melzer J.  |              Robredo T.  |             Dimitrov G.  |               Goffin D.  |               Hewitt L.  |                Simon G.  |            Baghdatis M.  |            Nishikori K.  |           Dolgopolov O.  |             Dimitrov G.  |               Melzer J.  |              Istomin D.  |               Murray A.  |\n      Loser  |              Giraldo S.  |            Benneteau J.  |            Matosevic M.  |             Mitchell B.  |               Klizan M.  |                  Ito T.  |               Levine J.  |                Kudla D.  |             Harrison R.  |                Baker B.  |                Ebden M.  |             Kunitsyn I.  |                Falla A.  |                Mayer F.  |              Robredo T.  |             Nieminen J.  |               Raonic M.  |               Goffin D.  |               Hewitt L.  |              Millman J.  |Groups for depth 0...  With the transposed ATP match results  DataFrame , we can now group the columns in much the same way as we grouped the \nrows. The code below performs a 2-dimensional column grouping based on data in the rows identified by  Court  and  Surface ,\nand subsequently prints out the number of columns per group at level 0 and 1.   DataFrame<String,Integer> frame = loadTennisMatchData(2013).transpose();\nDataFrameGrouping.Cols<String,Integer> grouping = frame.cols().groupBy(\"Court\", \"Surface\");\nfor (int depth=0; depth<grouping.getDepth(); ++depth) {\n    System.out.printf(\"Groups for depth %s...\\n\", depth);\n    grouping.getGroupKeys(depth).sorted().forEach(groupKey -> {\n        DataFrame<String,Integer> group = grouping.getGroup(groupKey);\n        System.out.printf(\"There are %s columns for group %s\\n\", group.colCount(), groupKey);\n    });\n}  Groups for depth 0...\nThere are 514 columns for group (Indoor)\nThere are 2117 columns for group (Outdoor)\nGroups for depth 1...\nThere are 27 columns for group (Indoor,Clay)\nThere are 487 columns for group (Indoor,Hard)\nThere are 826 columns for group (Outdoor,Clay)\nThere are 298 columns for group (Outdoor,Grass)\nThere are 993 columns for group (Outdoor,Hard)  To avoid repetition, further column grouping examples are omitted as they are identical to the row grouping scenarios \nillustrated in prior sections. The next section presents and discusses some data on grouping performance of large  DataFrames .",
            "title": "Grouping Columns"
        },
        {
            "location": "/frame/grouping/#performance",
            "text": "The performance of the group-by functions on a  DataFrame  will vary depending on many factors, including \nbut not limited to the number of resulting groups, the depth of the grouping and the cost of extracting the \nrelevant data from rows or columns. As with many  DataFrame  operations, a  parallel  implementation of grouping \ncan be used to speed up scenarios which involve a large number of records and/or ones in which the code to \nassemble a group is somewhat costly. This section provides examples of how sequential and parallel grouping \nperformance might be expected to compare in the most common scenarios. The absolute times published below are \nobviously vary machine specific, and in this case relate to a Macbook Pro with Quad Core-i7 processor.",
            "title": "Performance"
        },
        {
            "location": "/frame/grouping/#example-data_1",
            "text": "The ATP match data only holds a few thousand records per year which is a little small to gather reasonable group-by \nperformance statistics against (the times will be small and thus sensitive to measurement noise). We will therefore \nuse the the UK house price transaction data made available from the UK Land Registry as introduced in the Morpeus \nOverview section. This dataset, which contains approximately 1.35 records in 2006, can be loaded using the code below. \nWhile the raw data is in CSV format, it does not include a header as the first row, so for convenience we map the \ndefault numbered columns to more meaningful names.   /**\n * Returns a DataFrame of UK house prices from the Land Registry of the UK\n * @param year      the year to load prices for\n * @return          the house price DataFrame\n */\nstatic DataFrame<Integer,String> loadHousePrices(int year) {\n    return DataFrame.read().csv(options -> {\n        options.setHeader(false);\n        options.setParallel(true);\n        options.setExcludeColumns(\"Column-0\");\n        options.setResource(\"/uk-house-prices-\" + year + \".csv\");\n        options.getFormats().setDateFormat(\"Date\", \"yyyy-MM-dd HH:mm\");\n        options.setColumnNameMapping((colName, colOrdinal) -> {\n            switch (colOrdinal) {\n                case 0:  return \"Price\";\n                case 1:  return \"Date\";\n                case 2:  return \"PostCode\";\n                case 3:  return \"PropertyType\";\n                case 4:  return \"Old/New\";\n                case 5:  return \"Duration\";\n                case 6:  return \"PAON\";\n                case 7:  return \"SAON\";\n                case 8:  return \"Street\";\n                case 9:  return \"Locality\";\n                case 10: return \"Town/City\";\n                case 11: return \"District\";\n                case 12: return \"County\";\n                case 13: return \"PPDType\";\n                case 14: return \"RecordStatus\";\n                default: return colName;\n            }\n        });\n    });\n}  \n Index  |  Price   |         Date          |  PostCode  |  PropertyType  |  Old/New  |  Duration  |  PAON  |  SAON  |        Street        |   Locality   |  Town/City   |       District       |        County        |  PPDType  |  RecordStatus  |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     0  |  180000  |  2006-06-30T00:00:00  |   PR2 3SP  |             S  |        N  |         F  |    30  |  null  |    CLEVELEYS AVENUE  |     FULWOOD  |     PRESTON  |             PRESTON  |          LANCASHIRE  |        A  |             A  |\n     1  |   99750  |  2006-09-15T00:00:00  |  EX23 8TA  |             F  |        N  |         L  |    25  |  null  |         BULLEID WAY  |        BUDE  |        BUDE  |      NORTH CORNWALL  |            CORNWALL  |        A  |             A  |\n     2  |   65000  |  2006-10-06T00:00:00  |  BB12 7ER  |             T  |        N  |         L  |    70  |  null  |  SHAKESPEARE STREET  |     PADIHAM  |     BURNLEY  |             BURNLEY  |          LANCASHIRE  |        A  |             A  |\n     3  |  120000  |  2006-02-13T00:00:00  |   LE2 1PB  |             T  |        N  |         F  |    35  |  null  |       CHEPSTOW ROAD  |   LEICESTER  |   LEICESTER  |           LEICESTER  |           LEICESTER  |        A  |             A  |\n     4  |  112000  |  2006-04-07T00:00:00  |  CF38 2JF  |             T  |        N  |         F  |   127  |  null  |         MANOR CHASE  |      BEDDAU  |  PONTYPRIDD  |  RHONDDA CYNON TAFF  |  RHONDDA CYNON TAFF  |        A  |             A  |\n     5  |   84950  |  2006-02-24T00:00:00  |   CV6 4AS  |             T  |        N  |         F  |   381  |  null  |        BURNABY ROAD  |        null  |    COVENTRY  |            COVENTRY  |       WEST MIDLANDS  |        A  |             A  |\n     6  |  124950  |  2006-10-06T00:00:00  |  ST20 0HZ  |             S  |        N  |         F  |    38  |  null  |     ST LAWRENCE WAY  |     GNOSALL  |    STAFFORD  |            STAFFORD  |       STAFFORDSHIRE  |        A  |             A  |\n     7  |   30000  |  2006-11-03T00:00:00  |    M8 0QL  |             S  |        Y  |         F  |    32  |  null  |          CHIME BANK  |  MANCHESTER  |  MANCHESTER  |          MANCHESTER  |  GREATER MANCHESTER  |        A  |             A  |\n     8  |  380000  |  2006-09-25T00:00:00  |    N7 9SY  |             F  |        N  |         F  |     8  |  null  |    HEDDINGTON GROVE  |      LONDON  |      LONDON  |           ISLINGTON  |      GREATER LONDON  |        A  |             A  |\n     9  |  451000  |  2006-02-08T00:00:00  |   SW9 9AL  |             F  |        Y  |         L  |    29  |  null  |         STANE GROVE  |        null  |      LONDON  |             LAMBETH  |      GREATER LONDON  |        A  |             A  |",
            "title": "Example Data"
        },
        {
            "location": "/frame/grouping/#group-count",
            "text": "The first example attempts to demonstrate the performance impact of an increasing number of groups created by a the \ngroup-by operation. In this case we group the rows of the UK property transaction data for 2006 by 5 different columns \nindependently, namely  PropertyType ,  County ,  Month ,  District ,  Town/City  and  Locality . The group count for these \ncolumns is listed below. The expectation is that execution times will increase as the number of groups generated by a group-by \nfunction increases, which is exactly what we see in the subsequent plot.   Property Type : 5 Groups  Month : 12 Groups  County : 115 Groups  District : 387 Groups  Town/City : 1161 Groups  Locality : 16800 Groups   \n       Running this same example with  parallel execution  yields a similar trend, but the absolute times are much smaller.  \n       The following code is used to generate the above results, for the sequential execution.   //Load UK house prices for 2006\nDataFrame<Integer,String> frame = loadHousePrices(2006);\n\n//Run 10 iterations of sequential and parallel group by Town/City\nDataFrame<String,String> results = PerfStat.run(5, TimeUnit.MILLISECONDS, false, tasks -> {\n    tasks.put(\"PropertyType\", () -> frame.rows().groupBy(\"PropertyType\"));\n    tasks.put(\"Month\", () -> frame.rows().groupBy(row -> {\n        final LocalDateTime date = row.getValue(\"Date\");\n        return Tuple.of(date.getMonth());\n    }));\n    tasks.put(\"County\", () -> frame.rows().groupBy(\"County\"));\n    tasks.put(\"District\", () -> frame.rows().groupBy(\"District\"));\n    tasks.put(\"Town/City\", () -> frame.rows().groupBy(\"Town/City\"));\n    tasks.put(\"Locality\", () -> frame.rows().groupBy(\"Locality\"));\n});\n\n//Plot the results of the combined DataFrame with timings\nChart.create().withBarPlot(results, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time In Milliseconds\");\n    chart.title().withText(\"1-Dimensional Grouping of 1.35 Million Rows (Sequential)\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.subtitle().withText(\"Grouping of UK House Price Transactions in 2006 by various columns\");\n    chart.legend().on().bottom();\n    chart.show();\n});",
            "title": "Group Count"
        },
        {
            "location": "/frame/grouping/#sequential-vs-parallel",
            "text": "The following example plots the results of a standard 1-dimensional group-by operation on the same chart. The 1.35 \nmillion rows in the UK house price dataset is grouped by  County , which results in 115 distinct groups as indicated\nin the previous section. The timing statistics below seem to indicate the parallel grouping in this scenario was a little\nover twice as fast as the sequential algorithm. This result bodes well as this is the simplest possible group-by scenatio, \nand we would expect the parallel algorithm to perform even better as the grouping complexity increases.  \n       The following code is used to generate the above results.   //Load UK house prices for 2006\nDataFrame<Integer,String> frame = loadHousePrices(2006);\n\n//Run 10 iterations of sequential and parallel group by County\nDataFrame<String,String> results = PerfStat.run(10, TimeUnit.MILLISECONDS, false, tasks -> {\n    tasks.put(\"Sequential\", () -> frame.rows().sequential().groupBy(\"County\"));\n    tasks.put(\"Parallel\", () -> frame.rows().parallel().groupBy(\"County\"));\n});\n\n//Plot the results of the combined DataFrame with timings\nChart.create().withBarPlot(results, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time In Milliseconds\");\n    chart.title().withText(\"1-Dimensional Grouping of 1.35 Million Rows\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.subtitle().withText(\"Grouping of UK House Price Transactions in 2006 by County\");\n    chart.legend().on().bottom();\n    chart.show();\n});",
            "title": "Sequential vs Parallel"
        },
        {
            "location": "/frame/grouping/#multi-dimensional",
            "text": "The prior examples use 1-dimensional grouping only, so this section looks at the cost of a multi-dimensioal\ngroup-by on the rows of the UK property transaction data. In order to remove group count as a factor in the second\ndimension, we simply group by the same column twice. This is not particularly useful in reality other than to assess \nthe relative cost of adding the second dimension. Naturally, the group count in the second dimension will always\nbe 1.  The plot below indicates adding another dimension is significant, although using parallel execution can mostly\ndiscount the additional cost, at least in this scenario.  \n       The following code is used to generate the above results.   //Load UK house prices for 2006\nDataFrame<Integer,String> frame = loadHousePrices(2006);\n\n//Run 10 iterations of sequential and parallel group by County and Town/City\nDataFrame<String,String> results = PerfStat.run(10, TimeUnit.MILLISECONDS, false, tasks -> {\n    tasks.put(\"Sequential(1-D)\", () -> frame.rows().sequential().groupBy(\"County\"));\n    tasks.put(\"Parallel(1-D)\", () -> frame.rows().parallel().groupBy(\"County\"));\n    tasks.put(\"Sequential(2-D)\", () -> frame.rows().sequential().groupBy(\"County\", \"County\"));\n    tasks.put(\"Parallel(2-D)\", () -> frame.rows().parallel().groupBy(\"County\", \"County\"));\n});\n\n//Plot the results of the combined DataFrame with timings\nChart.create().withBarPlot(results, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time In Milliseconds\");\n    chart.title().withText(\"1-Dimension vs 2-Dimensional Grouping of 1.35 Million Rows\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.subtitle().withText(\"Grouping of UK House Price Transactions in 2006 by County x 2\");\n    chart.legend().on().bottom();\n    chart.show();\n});",
            "title": "Multi-Dimensional"
        },
        {
            "location": "/frame/grouping/#grouping-function",
            "text": "The Morpheus API provides two styles of grouping function, the first simply takes the column keys or row\nkeys depending if you are grouping rows or columns respectively. The second style uses a lambda expression\nthat consumes either the row or column, and returns a  Tuple  that represents the group for that entry. The\nlatter is more flexible in that it can produce a group that is based on some derived calculation on the\nrow or column vector it is presented with. This section assess whether there is any perfomance difference\nto using either of these approaches.  In this example we perform a two-dimensional group by using the  County  first, and the  Town/City  second.\nThe results in the subsequent plot suggests there is a very small performance cost to using the second style,\nhowever it is fairly neglible in this case.  \n       The following code is used to generate the above results.   //Load UK house prices for 2006\nDataFrame<Integer,String> frame = loadHousePrices(2006);\n\n//Run 10 iterations of sequential and parallel group by County and Town/City\nDataFrame<String,String> results = PerfStat.run(5, TimeUnit.MILLISECONDS, false, tasks -> {\n    tasks.put(\"Method-1\", () -> frame.rows().groupBy(\"County\", \"Town/City\"));\n    tasks.put(\"Method-2\", () -> frame.rows().groupBy(row -> Tuple.of(\n        row.<String>getValue(\"County\"),\n        row.<String>getValue(\"Town/City\"))\n    ));\n});\n\n//Plot the results of the combined DataFrame with timings\nChart.create().withBarPlot(results, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time In Milliseconds\");\n    chart.title().withText(\"2-Dimensional Grouping of 1.35 Million Rows\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.subtitle().withText(\"Grouping of UK House Price Transactions in 2006 by County & Town/City\");\n    chart.legend().on().bottom();\n    chart.show();\n});",
            "title": "Grouping Function"
        },
        {
            "location": "/frame/finding/",
            "text": "Finding Data\n\n\nIntroduction\n\n\nA \nDataFrame\n represents tablular data framed by a row and column axis that define a coordindate space by which entries \ncan be read or written. As previously discussed in the section on \naccess\n, it is possible to read/write data items \neither by keys, ordinals or a combination of the two. Finding data in a \nDataFrame\n often means identifying the row and or \ncolumn coordinates associated with the data being searched. The Morpheus API provides a versatile set of functions to locate \ndata in various ways, often providing both sequential and parallel search algorithms. This section provides some useful \nexamples of how to use these APIs.\n\n\nExample Data\n\n\nIn this section we will use a dataset introduced earlier, namely population data made available by the Office for National \nStatistics (ONS) in the United Kingdom. This dataset provides population counts between 1999 and 2014 decomposed into male and \nfemale age buckets ranging from 0 to 90 years old, over a range of UK Boroughs including the UK as a whole. The code to load \nthis CSV formatted dataset is shown below, as well as a rendering of the first 10 rows. Note that the raw dataset is expressed \nas counts, so we convert these to a percentage of the total population for that row in order to standardize the values.\n\n\n\n\n\n/**\n * Returns the ONS population dataset for UK boroughs, and convert counts to weights\n * @return  the ONS population dataset, expressed as population weights\n */\nstatic DataFrame<Tuple,String> loadPopulationDatasetWeights() {\n    return DataFrame.read().<Tuple>csv(options -> {\n        options.setResource(\"http://tinyurl.com/ons-population-year\");\n        options.setRowKeyParser(Tuple.class, row -> Tuple.of(Integer.parseInt(row[1]), row[2]));\n        options.setExcludeColumns(\"Code\");\n        options.getFormats().setNullValues(\"-\");\n        options.setColumnType(\"All Males\", Double.class);\n        options.setColumnType(\"All Females\", Double.class);\n        options.setColumnType(\"All Persons\", Double.class);\n        options.setColumnType(\"[MF]\\\\s+\\\\d+\", Double.class);\n    }).applyValues(v -> {\n       if (v.colKey().matches(\"[MF]\\\\s+\\\\d+\")) {\n           final double total = v.row().getDouble(\"All Persons\");\n           final double count = v.getDouble();\n           return count / total;\n       } else {\n           return v.getValue();\n       }\n    });\n}\n\n\n\n\n\n         Index          |  Year  |     Borough      |  All Persons  |  All Males  |   M 0   |   M 1   |   M 2   |   M 3   |   M 4   |   M 5   |   M 6   |   M 7   |   M 8   |   M 9   |  M 10   |  M 11   |  M 12   |  M 13   |  M 14   |  M 15   |  M 16   |  M 17   |  M 18   |  M 19   |  M 20   |  M 21   |  M 22   |  M 23   |  M 24   |  M 25   |  M 26   |  M 27   |  M 28   |  M 29   |  M 30   |  M 31   |  M 32   |  M 33   |  M 34   |  M 35   |  M 36   |  M 37   |  M 38   |  M 39   |  M 40   |  M 41   |  M 42   |  M 43   |  M 44   |  M 45   |  M 46   |  M 47   |  M 48   |  M 49   |  M 50   |  M 51   |  M 52   |  M 53   |  M 54   |  M 55   |  M 56   |  M 57   |  M 58   |  M 59   |  M 60   |  M 61   |  M 62   |  M 63   |  M 64   |  M 65   |  M 66   |  M 67   |  M 68   |  M 69   |  M 70   |  M 71   |  M 72   |  M 73   |  M 74   |  M 75   |  M 76   |  M 77   |  M 78   |  M 79   |  M 80   |  M 81   |  M 82   |  M 83   |  M 84   |  M 85   |  M 86   |  M 87   |  M 88   |  M 89   |  M 90+  |  All Females  |   F 0   |   F 1   |   F 2   |   F 3   |   F 4   |   F 5   |   F 6   |   F 7   |   F 8   |   F 9   |  F 10   |  F 11   |  F 12   |  F 13   |  F 14   |  F 15   |  F 16   |  F 17   |  F 18   |  F 19   |  F 20   |  F 21   |  F 22   |  F 23   |  F 24   |  F 25   |  F 26   |  F 27   |  F 28   |  F 29   |  F 30   |  F 31   |  F 32   |  F 33   |  F 34   |  F 35   |  F 36   |  F 37   |  F 38   |  F 39   |  F 40   |  F 41   |  F 42   |  F 43   |  F 44   |  F 45   |  F 46   |  F 47   |  F 48   |  F 49   |  F 50   |  F 51   |  F 52   |  F 53   |  F 54   |  F 55   |  F 56   |  F 57   |  F 58   |  F 59   |  F 60   |  F 61   |  F 62   |  F 63   |  F 64   |  F 65   |  F 66   |  F 67   |  F 68   |  F 69   |  F 70   |  F 71   |  F 72   |  F 73   |  F 74   |  F 75   |  F 76   |  F 77   |  F 78   |  F 79   |  F 80   |  F 81   |  F 82   |  F 83   |  F 84   |  F 85   |  F 86   |  F 87   |  F 88   |  F 89   |  F 90+  |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n (1999,City of London)  |  1999  |  City of London  |         6581  |       3519  |  0.36%  |  0.35%  |  0.33%  |  0.32%  |  0.30%  |  0.32%  |  0.30%  |  0.32%  |  0.30%  |  0.33%  |  0.33%  |  0.32%  |  0.29%  |  0.27%  |  0.26%  |  0.26%  |  0.26%  |  0.32%  |  0.40%  |  0.49%  |  0.61%  |  0.71%  |  0.79%  |  0.91%  |  0.99%  |  1.06%  |  1.11%  |  1.11%  |  1.11%  |  1.19%  |  1.19%  |  1.17%  |  1.14%  |  1.09%  |  1.08%  |  1.08%  |  0.99%  |  0.91%  |  0.88%  |  0.88%  |  0.87%  |  0.84%  |  0.77%  |  0.81%  |  0.81%  |  0.90%  |  0.90%  |  0.91%  |  0.99%  |  1.03%  |  1.03%  |  1.06%  |  0.96%  |  0.94%  |  0.93%  |  0.82%  |  0.74%  |  0.71%  |  0.65%  |  0.58%  |  0.53%  |  0.52%  |  0.52%  |  0.50%  |  0.49%  |  0.47%  |  0.49%  |  0.47%  |  0.46%  |  0.46%  |  0.44%  |  0.41%  |  0.40%  |  0.36%  |  0.35%  |  0.30%  |  0.30%  |  0.27%  |  0.26%  |  0.23%  |  0.21%  |  0.18%  |  0.18%  |  0.17%  |  0.17%  |  0.65%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |         3062  |  0.36%  |  0.36%  |  0.33%  |  0.33%  |  0.32%  |  0.32%  |  0.29%  |  0.29%  |  0.27%  |  0.27%  |  0.27%  |  0.24%  |  0.26%  |  0.24%  |  0.24%  |  0.24%  |  0.30%  |  0.38%  |  0.40%  |  0.46%  |  0.59%  |  0.71%  |  0.76%  |  0.82%  |  0.93%  |  1.03%  |  1.08%  |  1.09%  |  1.08%  |  1.11%  |  1.08%  |  1.03%  |  1.00%  |  0.94%  |  0.84%  |  0.77%  |  0.71%  |  0.65%  |  0.64%  |  0.59%  |  0.59%  |  0.62%  |  0.64%  |  0.65%  |  0.67%  |  0.61%  |  0.67%  |  0.68%  |  0.71%  |  0.73%  |  0.74%  |  0.74%  |  0.76%  |  0.70%  |  0.67%  |  0.59%  |  0.55%  |  0.52%  |  0.50%  |  0.47%  |  0.46%  |  0.44%  |  0.44%  |  0.46%  |  0.47%  |  0.46%  |  0.46%  |  0.44%  |  0.44%  |  0.43%  |  0.40%  |  0.40%  |  0.38%  |  0.35%  |  0.32%  |  0.30%  |  0.30%  |  0.29%  |  0.26%  |  0.26%  |  0.26%  |  0.24%  |  0.23%  |  0.21%  |  0.20%  |  1.15%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |\n (2000,City of London)  |  2000  |  City of London  |         7014  |       3775  |  0.36%  |  0.36%  |  0.34%  |  0.33%  |  0.31%  |  0.30%  |  0.31%  |  0.30%  |  0.31%  |  0.30%  |  0.31%  |  0.29%  |  0.26%  |  0.23%  |  0.23%  |  0.21%  |  0.24%  |  0.27%  |  0.34%  |  0.43%  |  0.56%  |  0.68%  |  0.81%  |  0.91%  |  1.03%  |  1.15%  |  1.25%  |  1.28%  |  1.28%  |  1.30%  |  1.34%  |  1.34%  |  1.28%  |  1.23%  |  1.15%  |  1.14%  |  1.08%  |  1.00%  |  0.93%  |  0.88%  |  0.88%  |  0.88%  |  0.84%  |  0.80%  |  0.80%  |  0.80%  |  0.88%  |  0.88%  |  0.88%  |  0.96%  |  1.01%  |  1.01%  |  1.06%  |  0.94%  |  0.91%  |  0.90%  |  0.80%  |  0.71%  |  0.67%  |  0.61%  |  0.56%  |  0.48%  |  0.46%  |  0.44%  |  0.43%  |  0.41%  |  0.40%  |  0.40%  |  0.38%  |  0.40%  |  0.38%  |  0.37%  |  0.36%  |  0.33%  |  0.31%  |  0.31%  |  0.27%  |  0.27%  |  0.24%  |  0.23%  |  0.21%  |  0.20%  |  0.17%  |  0.17%  |  0.16%  |  0.56%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |         3239  |  0.37%  |  0.34%  |  0.34%  |  0.31%  |  0.30%  |  0.30%  |  0.29%  |  0.27%  |  0.27%  |  0.27%  |  0.27%  |  0.26%  |  0.24%  |  0.26%  |  0.24%  |  0.26%  |  0.29%  |  0.33%  |  0.40%  |  0.46%  |  0.54%  |  0.70%  |  0.83%  |  0.87%  |  0.98%  |  1.07%  |  1.17%  |  1.18%  |  1.20%  |  1.15%  |  1.14%  |  1.08%  |  1.04%  |  0.98%  |  0.91%  |  0.81%  |  0.76%  |  0.68%  |  0.63%  |  0.61%  |  0.58%  |  0.60%  |  0.58%  |  0.61%  |  0.61%  |  0.63%  |  0.58%  |  0.64%  |  0.64%  |  0.67%  |  0.70%  |  0.71%  |  0.73%  |  0.73%  |  0.67%  |  0.64%  |  0.58%  |  0.53%  |  0.50%  |  0.46%  |  0.43%  |  0.41%  |  0.40%  |  0.38%  |  0.40%  |  0.41%  |  0.40%  |  0.40%  |  0.40%  |  0.40%  |  0.38%  |  0.36%  |  0.36%  |  0.34%  |  0.31%  |  0.30%  |  0.27%  |  0.29%  |  0.27%  |  0.24%  |  0.23%  |  0.21%  |  0.21%  |  0.21%  |  0.19%  |  1.15%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |\n (2001,City of London)  |  2001  |  City of London  |         7359  |       3984  |  0.23%  |  0.41%  |  0.34%  |  0.26%  |  0.29%  |  0.34%  |  0.27%  |  0.22%  |  0.26%  |  0.37%  |  0.35%  |  0.22%  |  0.39%  |  0.26%  |  0.15%  |  0.19%  |  0.16%  |  0.18%  |  0.16%  |  0.37%  |  0.52%  |  0.53%  |  0.72%  |  0.86%  |  1.09%  |  1.49%  |  1.49%  |  1.36%  |  1.47%  |  1.70%  |  1.55%  |  1.29%  |  1.26%  |  1.43%  |  1.35%  |  1.14%  |  1.21%  |  0.96%  |  0.86%  |  1.02%  |  0.88%  |  0.71%  |  0.64%  |  1.03%  |  0.99%  |  0.73%  |  0.67%  |  0.61%  |  0.82%  |  0.67%  |  1.51%  |  0.99%  |  0.75%  |  1.10%  |  1.09%  |  0.83%  |  0.86%  |  0.76%  |  0.87%  |  0.65%  |  0.38%  |  0.46%  |  0.61%  |  0.49%  |  0.34%  |  0.38%  |  0.29%  |  0.38%  |  0.29%  |  0.37%  |  0.37%  |  0.29%  |  0.33%  |  0.39%  |  0.29%  |  0.26%  |  0.31%  |  0.27%  |  0.15%  |  0.26%  |  0.20%  |  0.19%  |  0.11%  |  0.20%  |  0.14%  |  0.08%  |  0.14%  |  0.07%  |  0.05%  |  0.04%  |     13  |         3375  |  0.43%  |  0.41%  |  0.31%  |  0.29%  |  0.35%  |  0.22%  |  0.29%  |  0.20%  |  0.24%  |  0.33%  |  0.27%  |  0.27%  |  0.14%  |  0.35%  |  0.18%  |  0.18%  |  0.19%  |  0.53%  |  0.38%  |  0.48%  |  0.64%  |  0.53%  |  0.67%  |  0.73%  |  1.01%  |  1.26%  |  1.21%  |  1.09%  |  1.14%  |  1.45%  |  1.16%  |  1.13%  |  1.37%  |  0.90%  |  0.82%  |  0.73%  |  0.90%  |  0.82%  |  0.65%  |  0.61%  |  0.50%  |  0.38%  |  0.42%  |  0.86%  |  0.58%  |  0.72%  |  0.68%  |  0.64%  |  0.48%  |  0.57%  |  0.50%  |  0.88%  |  0.65%  |  0.77%  |  0.77%  |  0.52%  |  0.61%  |  0.52%  |  0.52%  |  0.45%  |  0.43%  |  0.43%  |  0.35%  |  0.34%  |  0.33%  |  0.39%  |  0.38%  |  0.37%  |  0.46%  |  0.37%  |  0.27%  |  0.35%  |  0.33%  |  0.42%  |  0.24%  |  0.33%  |  0.37%  |  0.18%  |  0.18%  |  0.20%  |  0.24%  |  0.30%  |  0.24%  |  0.15%  |  0.14%  |  0.14%  |  0.19%  |  0.35%  |  0.22%  |  0.10%  |     15  |\n (2002,City of London)  |  2002  |  City of London  |         7280  |       3968  |  0.40%  |  0.25%  |  0.38%  |  0.37%  |  0.27%  |  0.27%  |  0.33%  |  0.27%  |  0.23%  |  0.27%  |  0.34%  |  0.34%  |  0.19%  |  0.37%  |  0.12%  |  0.07%  |  0.11%  |  0.14%  |  0.16%  |  0.14%  |  0.27%  |  0.54%  |  0.69%  |  0.84%  |  0.93%  |  1.36%  |  1.55%  |  1.68%  |  1.40%  |  1.57%  |  1.57%  |  1.47%  |  1.36%  |  1.22%  |  1.39%  |  1.28%  |  1.15%  |  1.06%  |  1.00%  |  0.84%  |  1.07%  |  0.89%  |  0.78%  |  0.69%  |  1.03%  |  0.99%  |  0.77%  |  0.69%  |  0.62%  |  0.84%  |  0.63%  |  1.55%  |  0.96%  |  0.67%  |  1.17%  |  1.04%  |  0.81%  |  0.84%  |  0.76%  |  0.82%  |  0.62%  |  0.38%  |  0.43%  |  0.63%  |  0.55%  |  0.32%  |  0.36%  |  0.27%  |  0.34%  |  0.29%  |  0.33%  |  0.41%  |  0.29%  |  0.34%  |  0.40%  |  0.27%  |  0.25%  |  0.29%  |  0.23%  |  0.15%  |  0.23%  |  0.19%  |  0.18%  |  0.11%  |  0.21%  |  0.12%  |  0.08%  |  0.11%  |  0.05%  |  0.03%  |     14  |         3312  |  0.38%  |  0.48%  |  0.41%  |  0.26%  |  0.29%  |  0.36%  |  0.21%  |  0.27%  |  0.18%  |  0.25%  |  0.23%  |  0.21%  |  0.23%  |  0.08%  |  0.26%  |  0.18%  |  0.07%  |  0.21%  |  0.48%  |  0.40%  |  0.48%  |  0.66%  |  0.71%  |  0.65%  |  0.92%  |  1.02%  |  1.15%  |  1.15%  |  1.14%  |  0.96%  |  1.58%  |  1.15%  |  1.29%  |  1.39%  |  0.87%  |  0.84%  |  0.78%  |  0.80%  |  0.85%  |  0.60%  |  0.65%  |  0.51%  |  0.38%  |  0.45%  |  0.82%  |  0.49%  |  0.65%  |  0.71%  |  0.62%  |  0.49%  |  0.52%  |  0.56%  |  0.87%  |  0.70%  |  0.74%  |  0.76%  |  0.51%  |  0.56%  |  0.55%  |  0.49%  |  0.41%  |  0.40%  |  0.41%  |  0.37%  |  0.32%  |  0.32%  |  0.38%  |  0.38%  |  0.34%  |  0.47%  |  0.34%  |  0.26%  |  0.33%  |  0.32%  |  0.43%  |  0.26%  |  0.34%  |  0.36%  |  0.14%  |  0.16%  |  0.19%  |  0.23%  |  0.30%  |  0.21%  |  0.12%  |  0.14%  |  0.10%  |  0.15%  |  0.36%  |  0.19%  |     20  |\n (2003,City of London)  |  2003  |  City of London  |         7115  |       3892  |  0.49%  |  0.34%  |  0.22%  |  0.31%  |  0.37%  |  0.24%  |  0.27%  |  0.34%  |  0.30%  |  0.30%  |  0.34%  |  0.35%  |  0.30%  |  0.14%  |  0.22%  |  0.14%  |  0.06%  |  0.07%  |  0.17%  |  0.17%  |  0.15%  |  0.27%  |  0.69%  |  0.84%  |  0.93%  |  1.12%  |  1.41%  |  1.66%  |  1.60%  |  1.48%  |  1.52%  |  1.48%  |  1.39%  |  1.32%  |  1.19%  |  1.36%  |  1.25%  |  1.15%  |  1.07%  |  0.97%  |  0.89%  |  1.04%  |  0.90%  |  0.80%  |  0.72%  |  1.05%  |  1.04%  |  0.79%  |  0.72%  |  0.55%  |  0.90%  |  0.62%  |  1.52%  |  0.91%  |  0.66%  |  1.12%  |  1.04%  |  0.79%  |  0.86%  |  0.77%  |  0.77%  |  0.62%  |  0.37%  |  0.42%  |  0.59%  |  0.51%  |  0.32%  |  0.34%  |  0.31%  |  0.31%  |  0.25%  |  0.32%  |  0.44%  |  0.25%  |  0.34%  |  0.38%  |  0.27%  |  0.24%  |  0.31%  |  0.21%  |  0.14%  |  0.22%  |  0.17%  |  0.14%  |  0.11%  |  0.20%  |  0.11%  |  0.08%  |  0.10%  |  0.03%  |     13  |         3223  |  0.25%  |  0.35%  |  0.44%  |  0.41%  |  0.25%  |  0.30%  |  0.37%  |  0.20%  |  0.27%  |  0.17%  |  0.27%  |  0.18%  |  0.15%  |  0.17%  |  0.03%  |  0.22%  |  0.10%  |  0.17%  |  0.31%  |  0.44%  |  0.48%  |  0.52%  |  0.73%  |  0.86%  |  0.82%  |  1.19%  |  1.03%  |  1.21%  |  1.15%  |  1.19%  |  1.05%  |  1.39%  |  1.19%  |  1.25%  |  1.29%  |  0.84%  |  0.82%  |  0.63%  |  0.72%  |  0.74%  |  0.59%  |  0.66%  |  0.62%  |  0.41%  |  0.49%  |  0.83%  |  0.53%  |  0.58%  |  0.69%  |  0.55%  |  0.49%  |  0.56%  |  0.52%  |  0.86%  |  0.69%  |  0.74%  |  0.70%  |  0.49%  |  0.56%  |  0.48%  |  0.51%  |  0.42%  |  0.41%  |  0.42%  |  0.38%  |  0.34%  |  0.32%  |  0.39%  |  0.42%  |  0.35%  |  0.45%  |  0.34%  |  0.24%  |  0.32%  |  0.30%  |  0.42%  |  0.22%  |  0.32%  |  0.34%  |  0.14%  |  0.15%  |  0.20%  |  0.21%  |  0.22%  |  0.22%  |  0.13%  |  0.03%  |  0.07%  |  0.14%  |  0.31%  |     26  |\n (2004,City of London)  |  2004  |  City of London  |         7118  |       3875  |  0.39%  |  0.38%  |  0.35%  |  0.22%  |  0.28%  |  0.38%  |  0.24%  |  0.28%  |  0.44%  |  0.25%  |  0.31%  |  0.30%  |  0.31%  |  0.22%  |  0.06%  |  0.18%  |  0.11%  |  0.00%  |  0.03%  |  0.18%  |  0.13%  |  0.35%  |  0.37%  |  0.89%  |  0.98%  |  0.97%  |  1.25%  |  1.57%  |  1.62%  |  1.63%  |  1.40%  |  1.31%  |  1.43%  |  1.31%  |  1.21%  |  1.11%  |  1.39%  |  1.29%  |  1.24%  |  1.14%  |  1.03%  |  0.83%  |  0.91%  |  0.94%  |  0.81%  |  0.69%  |  1.07%  |  0.98%  |  0.77%  |  0.70%  |  0.62%  |  0.86%  |  0.63%  |  1.39%  |  0.91%  |  0.65%  |  1.08%  |  1.07%  |  0.77%  |  0.80%  |  0.74%  |  0.80%  |  0.60%  |  0.39%  |  0.45%  |  0.55%  |  0.48%  |  0.31%  |  0.35%  |  0.30%  |  0.31%  |  0.25%  |  0.28%  |  0.42%  |  0.22%  |  0.34%  |  0.39%  |  0.27%  |  0.20%  |  0.28%  |  0.13%  |  0.17%  |  0.20%  |  0.14%  |  0.11%  |  0.11%  |  0.21%  |  0.11%  |  0.07%  |  0.07%  |     12  |         3243  |  0.44%  |  0.25%  |  0.28%  |  0.37%  |  0.35%  |  0.28%  |  0.35%  |  0.39%  |  0.20%  |  0.27%  |  0.10%  |  0.27%  |  0.14%  |  0.13%  |  0.13%  |  0.01%  |  0.18%  |  0.24%  |  0.13%  |  0.31%  |  0.53%  |  0.58%  |  0.66%  |  0.83%  |  1.11%  |  1.12%  |  1.28%  |  1.21%  |  1.11%  |  1.14%  |  1.11%  |  1.10%  |  1.28%  |  1.12%  |  1.22%  |  1.17%  |  0.79%  |  0.86%  |  0.66%  |  0.67%  |  0.77%  |  0.56%  |  0.63%  |  0.55%  |  0.42%  |  0.48%  |  0.83%  |  0.52%  |  0.55%  |  0.79%  |  0.53%  |  0.46%  |  0.53%  |  0.51%  |  0.84%  |  0.72%  |  0.73%  |  0.72%  |  0.48%  |  0.56%  |  0.49%  |  0.48%  |  0.39%  |  0.45%  |  0.39%  |  0.38%  |  0.35%  |  0.28%  |  0.41%  |  0.39%  |  0.34%  |  0.42%  |  0.34%  |  0.21%  |  0.30%  |  0.31%  |  0.41%  |  0.21%  |  0.32%  |  0.31%  |  0.17%  |  0.15%  |  0.17%  |  0.15%  |  0.17%  |  0.17%  |  0.10%  |  0.01%  |  0.04%  |  0.13%  |     42  |\n (2005,City of London)  |  2005  |  City of London  |         7131  |       3869  |  0.48%  |  0.35%  |  0.31%  |  0.32%  |  0.17%  |  0.27%  |  0.31%  |  0.27%  |  0.27%  |  0.43%  |  0.28%  |  0.29%  |  0.28%  |  0.27%  |  0.06%  |  0.04%  |  0.17%  |  0.14%  |  0.00%  |  0.10%  |  0.13%  |  0.22%  |  0.60%  |  0.56%  |  1.01%  |  1.30%  |  1.04%  |  1.25%  |  1.64%  |  1.58%  |  1.46%  |  1.23%  |  1.29%  |  1.53%  |  1.37%  |  1.08%  |  1.07%  |  1.44%  |  1.25%  |  1.09%  |  1.12%  |  0.95%  |  0.86%  |  0.97%  |  0.87%  |  0.81%  |  0.72%  |  1.15%  |  0.94%  |  0.81%  |  0.73%  |  0.59%  |  0.83%  |  0.70%  |  1.25%  |  0.94%  |  0.66%  |  1.02%  |  0.98%  |  0.77%  |  0.81%  |  0.70%  |  0.74%  |  0.59%  |  0.38%  |  0.42%  |  0.59%  |  0.43%  |  0.32%  |  0.34%  |  0.28%  |  0.24%  |  0.24%  |  0.27%  |  0.42%  |  0.21%  |  0.29%  |  0.38%  |  0.24%  |  0.20%  |  0.29%  |  0.11%  |  0.20%  |  0.17%  |  0.10%  |  0.10%  |  0.08%  |  0.20%  |  0.10%  |  0.03%  |     12  |         3262  |  0.41%  |  0.41%  |  0.21%  |  0.21%  |  0.32%  |  0.35%  |  0.24%  |  0.32%  |  0.35%  |  0.20%  |  0.25%  |  0.10%  |  0.28%  |  0.10%  |  0.10%  |  0.08%  |  0.03%  |  0.10%  |  0.27%  |  0.21%  |  0.43%  |  0.60%  |  0.69%  |  0.70%  |  1.15%  |  1.07%  |  1.25%  |  1.25%  |  1.19%  |  1.07%  |  1.26%  |  1.07%  |  1.19%  |  1.28%  |  1.04%  |  1.12%  |  1.04%  |  0.84%  |  0.77%  |  0.73%  |  0.63%  |  0.73%  |  0.60%  |  0.66%  |  0.53%  |  0.48%  |  0.48%  |  0.77%  |  0.46%  |  0.53%  |  0.80%  |  0.53%  |  0.50%  |  0.55%  |  0.52%  |  0.81%  |  0.73%  |  0.77%  |  0.70%  |  0.48%  |  0.52%  |  0.49%  |  0.52%  |  0.36%  |  0.46%  |  0.42%  |  0.35%  |  0.34%  |  0.25%  |  0.38%  |  0.36%  |  0.28%  |  0.39%  |  0.31%  |  0.25%  |  0.29%  |  0.34%  |  0.36%  |  0.22%  |  0.29%  |  0.31%  |  0.14%  |  0.17%  |  0.15%  |  0.14%  |  0.17%  |  0.13%  |  0.03%  |  0.03%  |  0.04%  |     50  |\n (2006,City of London)  |  2006  |  City of London  |         7254  |       3959  |  0.36%  |  0.34%  |  0.36%  |  0.28%  |  0.25%  |  0.18%  |  0.22%  |  0.32%  |  0.30%  |  0.25%  |  0.41%  |  0.25%  |  0.28%  |  0.23%  |  0.10%  |  0.07%  |  0.07%  |  0.21%  |  0.17%  |  0.00%  |  0.17%  |  0.26%  |  0.44%  |  0.87%  |  0.87%  |  1.13%  |  1.50%  |  1.14%  |  1.36%  |  1.71%  |  1.57%  |  1.42%  |  1.14%  |  1.17%  |  1.60%  |  1.35%  |  1.06%  |  1.02%  |  1.48%  |  1.20%  |  1.03%  |  1.13%  |  0.94%  |  0.87%  |  0.95%  |  0.81%  |  0.85%  |  0.73%  |  1.14%  |  0.91%  |  0.83%  |  0.72%  |  0.66%  |  0.87%  |  0.66%  |  1.17%  |  0.92%  |  0.65%  |  0.94%  |  0.85%  |  0.77%  |  0.77%  |  0.66%  |  0.74%  |  0.55%  |  0.39%  |  0.36%  |  0.57%  |  0.43%  |  0.30%  |  0.32%  |  0.28%  |  0.19%  |  0.22%  |  0.26%  |  0.40%  |  0.23%  |  0.28%  |  0.39%  |  0.23%  |  0.19%  |  0.26%  |  0.08%  |  0.14%  |  0.17%  |  0.07%  |  0.06%  |  0.08%  |  0.17%  |  0.10%  |     13  |         3295  |  0.39%  |  0.36%  |  0.43%  |  0.17%  |  0.23%  |  0.34%  |  0.32%  |  0.28%  |  0.30%  |  0.34%  |  0.19%  |  0.25%  |  0.07%  |  0.26%  |  0.06%  |  0.10%  |  0.06%  |  0.17%  |  0.10%  |  0.37%  |  0.33%  |  0.45%  |  0.70%  |  0.80%  |  0.98%  |  1.21%  |  1.10%  |  1.19%  |  1.19%  |  1.27%  |  1.16%  |  1.19%  |  1.08%  |  1.16%  |  1.10%  |  1.06%  |  1.02%  |  0.94%  |  0.85%  |  0.62%  |  0.73%  |  0.62%  |  0.70%  |  0.62%  |  0.63%  |  0.54%  |  0.52%  |  0.45%  |  0.73%  |  0.43%  |  0.51%  |  0.79%  |  0.54%  |  0.50%  |  0.52%  |  0.50%  |  0.77%  |  0.69%  |  0.73%  |  0.65%  |  0.41%  |  0.57%  |  0.43%  |  0.47%  |  0.33%  |  0.41%  |  0.40%  |  0.32%  |  0.36%  |  0.23%  |  0.37%  |  0.36%  |  0.28%  |  0.36%  |  0.33%  |  0.25%  |  0.28%  |  0.30%  |  0.37%  |  0.23%  |  0.28%  |  0.26%  |  0.15%  |  0.15%  |  0.14%  |  0.12%  |  0.15%  |  0.12%  |  0.03%  |  0.00%  |     45  |\n (2007,City of London)  |  2007  |  City of London  |         7607  |       4197  |  0.41%  |  0.33%  |  0.32%  |  0.29%  |  0.30%  |  0.21%  |  0.17%  |  0.18%  |  0.35%  |  0.30%  |  0.20%  |  0.43%  |  0.21%  |  0.25%  |  0.09%  |  0.09%  |  0.04%  |  0.12%  |  0.21%  |  0.35%  |  0.00%  |  0.39%  |  0.46%  |  0.84%  |  1.20%  |  1.18%  |  1.49%  |  1.62%  |  1.30%  |  1.41%  |  1.62%  |  1.34%  |  1.39%  |  1.10%  |  1.14%  |  1.50%  |  1.37%  |  1.13%  |  1.09%  |  1.54%  |  1.16%  |  1.04%  |  0.97%  |  0.92%  |  0.99%  |  0.97%  |  0.83%  |  0.84%  |  0.75%  |  1.05%  |  0.85%  |  0.82%  |  0.72%  |  0.67%  |  0.84%  |  0.70%  |  1.03%  |  0.84%  |  0.63%  |  0.83%  |  0.78%  |  0.71%  |  0.76%  |  0.60%  |  0.71%  |  0.47%  |  0.34%  |  0.39%  |  0.51%  |  0.39%  |  0.29%  |  0.30%  |  0.22%  |  0.21%  |  0.21%  |  0.25%  |  0.35%  |  0.20%  |  0.20%  |  0.38%  |  0.18%  |  0.18%  |  0.24%  |  0.08%  |  0.08%  |  0.13%  |  0.05%  |  0.05%  |  0.08%  |  0.12%  |     20  |         3410  |  0.32%  |  0.32%  |  0.39%  |  0.33%  |  0.22%  |  0.22%  |  0.22%  |  0.28%  |  0.21%  |  0.28%  |  0.33%  |  0.20%  |  0.24%  |  0.05%  |  0.25%  |  0.04%  |  0.14%  |  0.32%  |  0.20%  |  0.32%  |  0.51%  |  0.45%  |  0.74%  |  0.92%  |  0.99%  |  1.20%  |  1.31%  |  1.12%  |  1.33%  |  1.09%  |  1.05%  |  1.00%  |  1.05%  |  0.95%  |  0.95%  |  0.95%  |  1.00%  |  0.96%  |  0.82%  |  0.85%  |  0.62%  |  0.74%  |  0.63%  |  0.72%  |  0.66%  |  0.64%  |  0.57%  |  0.47%  |  0.39%  |  0.63%  |  0.38%  |  0.50%  |  0.71%  |  0.54%  |  0.51%  |  0.51%  |  0.51%  |  0.74%  |  0.68%  |  0.70%  |  0.60%  |  0.35%  |  0.54%  |  0.46%  |  0.42%  |  0.35%  |  0.39%  |  0.42%  |  0.29%  |  0.32%  |  0.21%  |  0.35%  |  0.34%  |  0.24%  |  0.33%  |  0.30%  |  0.22%  |  0.24%  |  0.29%  |  0.30%  |  0.22%  |  0.22%  |  0.25%  |  0.14%  |  0.13%  |  0.13%  |  0.09%  |  0.12%  |  0.11%  |  0.01%  |     36  |\n (2008,City of London)  |  2008  |  City of London  |         7429  |       4131  |  0.39%  |  0.35%  |  0.30%  |  0.32%  |  0.24%  |  0.30%  |  0.17%  |  0.16%  |  0.23%  |  0.40%  |  0.27%  |  0.20%  |  0.40%  |  0.23%  |  0.13%  |  0.11%  |  0.08%  |  0.09%  |  0.15%  |  0.24%  |  0.38%  |  0.15%  |  0.58%  |  0.52%  |  1.01%  |  1.35%  |  1.14%  |  1.66%  |  1.64%  |  1.37%  |  1.25%  |  1.49%  |  1.29%  |  1.36%  |  1.01%  |  1.08%  |  1.39%  |  1.24%  |  1.12%  |  1.06%  |  1.63%  |  1.16%  |  1.06%  |  0.93%  |  0.94%  |  0.89%  |  1.04%  |  0.77%  |  0.90%  |  0.90%  |  1.09%  |  0.86%  |  0.82%  |  0.69%  |  0.70%  |  0.82%  |  0.74%  |  0.98%  |  0.82%  |  0.62%  |  0.81%  |  0.82%  |  0.71%  |  0.74%  |  0.57%  |  0.71%  |  0.46%  |  0.35%  |  0.39%  |  0.52%  |  0.39%  |  0.32%  |  0.30%  |  0.23%  |  0.20%  |  0.19%  |  0.26%  |  0.32%  |  0.19%  |  0.22%  |  0.36%  |  0.17%  |  0.16%  |  0.23%  |  0.08%  |  0.08%  |  0.09%  |  0.05%  |  0.04%  |  0.07%  |     25  |         3298  |  0.16%  |  0.23%  |  0.31%  |  0.31%  |  0.32%  |  0.22%  |  0.23%  |  0.22%  |  0.26%  |  0.20%  |  0.28%  |  0.32%  |  0.20%  |  0.26%  |  0.01%  |  0.26%  |  0.00%  |  0.30%  |  0.35%  |  0.34%  |  0.47%  |  0.58%  |  0.47%  |  0.77%  |  0.98%  |  1.10%  |  1.20%  |  1.28%  |  1.18%  |  1.27%  |  1.02%  |  1.04%  |  0.92%  |  0.93%  |  0.77%  |  0.81%  |  0.82%  |  0.86%  |  0.92%  |  0.77%  |  0.85%  |  0.55%  |  0.70%  |  0.62%  |  0.70%  |  0.67%  |  0.67%  |  0.50%  |  0.47%  |  0.43%  |  0.57%  |  0.39%  |  0.51%  |  0.71%  |  0.54%  |  0.57%  |  0.57%  |  0.51%  |  0.70%  |  0.69%  |  0.69%  |  0.65%  |  0.36%  |  0.55%  |  0.46%  |  0.39%  |  0.40%  |  0.42%  |  0.39%  |  0.31%  |  0.30%  |  0.22%  |  0.38%  |  0.36%  |  0.27%  |  0.32%  |  0.30%  |  0.22%  |  0.20%  |  0.30%  |  0.32%  |  0.20%  |  0.23%  |  0.24%  |  0.13%  |  0.12%  |  0.12%  |  0.09%  |  0.12%  |  0.11%  |     30  |Male weight: 55.76%, Female weight: 44.24% for 2014 in City of London\n\n\n\n\nFirst & Last\n\n\nRow / Column\n\n\nIt is often useful to locate the first or last row / column in a \nDataFrame\n that satisfies some condition. The\n\nDataFrameAxis\n interface includes overloaded \nfirst()\n and \nlast()\n methods, one which takes no arguments and \nanother which takes a predicate. In the example below, we use this API to find the first row in the ONS population \ndata where the male / female population weights differ by more than 10%, which is a very significant difference. \n\n\nThe results indicate that in 2007, males made up \n55.17%\n and females made up \n44.83%\n of the population in \nthe \nCity of London\n, also know as the financial district. Perhaps no surprise there.\n\n\n\n\n\nDataFrame<Tuple,String> frame = DemoData.loadPopulationDatasetWeights();\nframe.rows().first(row -> {\n    double total = row.getDouble(\"All Persons\");\n    double maleWeight = row.getDouble(\"All Males\") / total;\n    double femaleWeight = row.getDouble(\"All Females\") / total;\n    return Math.abs(maleWeight - femaleWeight) > 0.1;\n}).ifPresent(row -> {\n    int year = row.key().item(0);\n    String borough = row.key().item(1);\n    double total = row.getDouble(\"All Persons\");\n    double males = (row.getDouble(\"All Males\") / total) * 100d;\n    double females = (row.getDouble(\"All Females\") / total) * 100d;\n    IO.printf(\"Male weight: %.2f%%, Female weight: %.2f%% for %s in %s\", males, females, year, borough);\n});\n\n\n\n\nMale weight: 55.17%, Female weight: 44.83% for 2007 in City of London\n\n\n\nWe can also search the frame from bottom to top by calling \nDataFrame.rows().last()\n which yields another hit\nfor the \nCity of London\n, but in this case in 2014. The weights are \n55.76%\n and \n44.24%\n for males and\nfemales respectively. So much for progress in gender equality in the financial district!\n\n\nMale weight: 55.76%, Female weight: 44.24% for 2014 in City of London\n\n\n\nRow / Column Value\n\n\nThe above example demonstrates the use of the \nfirst()\n and \nlast()\n methods on \nDataFrameAxis\n to locate the \nfirst and last row or column that satisfies some predicate. The same functionality is provided on the \nDataFrameVector\n\ninterface to locate the first and last entries in a specific row or column.\n\n\nIn this example, we iterate over all rows in the \nDataFrame\n that correspond to the Borough of \nKensington and\nChelsea\n, and for each row we attempt to find the first age group which has a population weight that exceeds\n1% of the total population for that year. We then simply print the results to standard out.\n\n\n\n\n\nDataFrame<Tuple,String> frame = DemoData.loadPopulationDatasetWeights();\nframe.rows().filter(r -> r.getValue(\"Borough\").equals(\"Kensington and Chelsea\")).forEach(row -> {\n    row.first(v -> v.colKey().matches(\"[MF]\\\\s+\\\\d+\") && v.getDouble() > 0.01).ifPresent(v -> {\n        Tuple rowKey = v.rowKey();\n        String group = v.colKey();\n        double weight = v.getDouble() * 100d;\n        IO.printf(\"Age group %s has a population of %.2f%% for %s\\n\", group, weight, rowKey);\n    });\n});\n\n\n\n\nAge group M 28 has a population of 1.05% for (1999,Kensington and Chelsea)\nAge group M 27 has a population of 1.02% for (2000,Kensington and Chelsea)\nAge group M 26 has a population of 1.04% for (2001,Kensington and Chelsea)\nAge group M 26 has a population of 1.13% for (2002,Kensington and Chelsea)\nAge group M 26 has a population of 1.09% for (2003,Kensington and Chelsea)\nAge group M 26 has a population of 1.02% for (2004,Kensington and Chelsea)\nAge group M 25 has a population of 1.07% for (2005,Kensington and Chelsea)\nAge group M 25 has a population of 1.07% for (2006,Kensington and Chelsea)\nAge group M 26 has a population of 1.14% for (2007,Kensington and Chelsea)\nAge group M 25 has a population of 1.02% for (2008,Kensington and Chelsea)\nAge group M 26 has a population of 1.07% for (2009,Kensington and Chelsea)\nAge group M 27 has a population of 1.10% for (2010,Kensington and Chelsea)\nAge group M 27 has a population of 1.02% for (2011,Kensington and Chelsea)\nAge group M 28 has a population of 1.00% for (2012,Kensington and Chelsea)\nAge group M 29 has a population of 1.04% for (2013,Kensington and Chelsea)\nAge group M 30 has a population of 1.00% for (2014,Kensington and Chelsea)\n\n\n\nPerforming similar analysis but this time searching for the last entry in each row which exceeds a population \nweight of 1.0% we get the following results.  The code for this is identical to above, except we call\n\nrow.last()\n passing in the same predicate.\n\n\nAge group F 85 has a population of 1.13% for (1999,Kensington and Chelsea)\nAge group F 85 has a population of 1.10% for (2000,Kensington and Chelsea)\nAge group F 35 has a population of 1.01% for (2001,Kensington and Chelsea)\nAge group F 35 has a population of 1.01% for (2002,Kensington and Chelsea)\nAge group F 35 has a population of 1.01% for (2003,Kensington and Chelsea)\nAge group F 35 has a population of 1.03% for (2004,Kensington and Chelsea)\nAge group F 34 has a population of 1.03% for (2005,Kensington and Chelsea)\nAge group F 34 has a population of 1.04% for (2006,Kensington and Chelsea)\nAge group F 35 has a population of 1.01% for (2007,Kensington and Chelsea)\nAge group F 34 has a population of 1.01% for (2008,Kensington and Chelsea)\nAge group F 34 has a population of 1.01% for (2009,Kensington and Chelsea)\nAge group F 34 has a population of 1.02% for (2010,Kensington and Chelsea)\nAge group F 34 has a population of 1.02% for (2011,Kensington and Chelsea)\nAge group F 33 has a population of 1.12% for (2012,Kensington and Chelsea)\nAge group F 34 has a population of 1.07% for (2013,Kensington and Chelsea)\nAge group F 35 has a population of 1.03% for (2014,Kensington and Chelsea)\n\n\n\nMin & Max\n\n\nFrame Min/Max\n\n\nFinding the \nmininum\n or \nmaximum\n value across the entire frame or within a given row or column subject\nto some condition is trivial. Consider the example below whereby we wish to find the \ngender\n, \nage group\n \nand \nyear\n that contains the largest population weight, but only in the Borough of \nIslington\n. The code \nbelow achieves this by calling the \nmax()\n function on the ONS dataset with an appropriate predicate. The \npredicate defines the condition that we only want to consider values associated with the Islington Borough, \nand represent a male or female population weight, and which has a value > 0. This example also demonstrates \nthe useful accessor methods on the \nDataFrameValue\n interface, such as efficiently accessing the row or column \nobject associated  with the value.\n\n\n\n\n\nDataFrame<Tuple,String> frame = DemoData.loadPopulationDatasetWeights();\nframe.max(v ->\n    v.row().getValue(\"Borough\").equals(\"Islington\") &&\n    v.colKey().matches(\"[MF]\\\\s+\\\\d+\") &&\n    v.getDouble() > 0\n).ifPresent(max -> {\n    int year = max.rowKey().item(0);\n    String group = max.colKey();\n    double weight = max.getDouble() * 100;\n    String borough = max.row().getValue(\"Borough\");\n    System.out.printf(\"Max population is %.2f%% for age group %s in %s, %s\", weight, group, borough, year);\n});\n\n\n\n\nMax population is 1.57% for age group F 27 in Islington, 2012\n\n\n\nThe analog of the above example to find the minimim value would simply involve a call to the \nDataFrame.min()\n method.\n\n\nRow/Column Vector Min/Max\n\n\nFinding the row or column that represents some minimum or maximum given some user specified \nComparator\n is also\nsupported by the \nDataFrameAxis\n API. While this capability requires a linear search, it is much faster than sorting\nthe entire \nDataFrame\n according to the \nComparator\n. In addition, this method supports \nparallel\n execution which\ncan boost performance on very large frames, or when the \nComparator\n is expensive to execute.\n\n\nThe example below uses the ONS population dataset to find the row that has the highest ratio of female to male\nbirths, and prints the results to standard out. The \nmax()\n method returns an \nOptional<DataFrameRow>\n or an\n\nOptional<DataFrameColumn>\n when operating on rows or columns respectively, which provides access to the\ncoordinates and data associated with the result. \n\n\n\n\n\nfinal DataFrame<Tuple,String> frame = DemoData.loadPopulationDatasetWeights();\nframe.rows().max((row1, row2) -> {\n    double ratio1 = row1.getDouble(\"F 0\") / row1.getDouble(\"M 0\");\n    double ratio2 = row2.getDouble(\"F 0\") / row2.getDouble(\"M 0\");\n    return Double.compare(ratio1, ratio2);\n}).ifPresent(row -> {\n    Tuple rowKey = row.key();\n    double males = row.getDouble(\"M 0\") * 100d;\n    double females = row.getDouble(\"F 0\") * 100d;\n    IO.printf(\"Largest female / male births = %.2f%%/%.2f%% for %s\", females, males, rowKey);\n});\n\n\n\n\nLargest female / male births = 0.46%/0.19% for (2009,City of London)\n\n\n\nThe smallest ratio between female / male births can be found simply by calling \nDataFrame.rows().min()\n with\nthe same \nComparator\n as above. Furthermore, this API is completely symmetrical in the row and column dimension,\nso finding the minimum or maximum column follows the same approach by call \nDataFrame.cols()\n.\n\n\nRow/Column Value Min/Max\n\n\nIt is often useful to be able to locate the minimum or maximum value in a specific row or column. While you\ncould use the frame level \nmin()\n and \nmax()\n methods with a predicate that restricts the search space to a \nspecific row or column, this would not be particuarly efficient as you would still need to iterate over all \nvalues in the frame.\n\n\nThe following examples show how one could select a specific row or column using the \nrowAt()\n or \ncolAt()\n \nfunctions, and then use the \nmin()\n or \nmax()\n functions to search just within the desired vector. Overloaded \nversions of these methods are provided, one which takes no arguments, and the other takes a predicate. The code \nbelow demonstrates how to find the maximum population bucket for the Borough of Islingtom in year 2000. \n\n\n\n\n\nTuple rowKey = Tuple.of(2000, \"Islington\");\nDataFrame<Tuple,String> frame = DemoData.loadPopulationDatasetWeights();\nframe.rowAt(rowKey).max(v -> v.colKey().matches(\"[MF]\\\\s+\\\\d+\") && v.getDouble() > 0).ifPresent(max -> {\n    String group = max.colKey();\n    int year = max.rowKey().item(0);\n    String borough = max.rowKey().item(1);\n    double weight = max.getDouble() * 100d;\n    System.out.printf(\"Max population weight for %s in %s is %.2f%% for %s\", borough, year, weight, group);\n});\n\n\n\n\nMax population weight for Islington in 2000 is 1.30% for F 26\n\n\n\nSimilarly, the following code illustrates how to find which Borough out of \nIslington\n, \nWandsworth\n and \n\nKensington and Chelsea\n has the largest female population aged 30 and in what year this occurred. In this\ncase we select the column named \nF 30\n and find the max value subject to the condition that the Borough is \npart of our constrained universe.\n\n\n\n\n\nSet<String> boroughs = Collect.asSet(\"Islington\", \"Wandsworth\", \"Kensington and Chelsea\");\nDataFrame<Tuple,String> frame = DemoData.loadPopulationDatasetWeights();\nframe.colAt(\"F 30\").max(v -> boroughs.contains((String)v.row().getValue(\"Borough\"))).ifPresent(max -> {\n    int year = max.rowKey().item(0);\n    double weight = max.getDouble() * 100d;\n    String borough = max.row().getValue(\"Borough\");\n    System.out.printf(\"Max female population weight aged 30 is %.2f%% in %s, %s\", weight, borough, year);\n});\n\n\n\n\nMax female population weight aged 30 is 1.69% in Wandsworth, 2012\n\n\n\nBinary Search\n\n\nThe \nDataFrameVector\n interface exposes 3 overloaded \nbinarySearch()\n methods useful in locating an entry that\nmatches a specific value. The search can be optionally restricted to a subset of the vector by providing an offset \nordinal and length, and an optional \nComparator\n if the data is sorted according to some custom logic. \n\n\nNaturally the data in the vector must be sorted in ascending order, otherwise the behaviour of these methods is undefined. \nThe \nbinarySearch()\n methods return an \nOptional<DataFrameValue<R,C>>\n containing the matched value, or an empty\noptional if no match. On a successful match, the \nDataFrameValue<R,C>\n interace can be used to access the coordinates\nof the matched value.\n\n\nIn the example below, we perform a binary search for a pre-selected value in the \nF 30\n column, and then assert\nthat to located value and row key matches what we expect. Note that we sort the \nDataFrame\n rows so that data in the\n\nF 30\n column is sorted in ascending order first.\n\n\n\n\n\nfinal Tuple expected = Tuple.of(2000, \"Islington\");\nfinal DataFrame<Tuple,String> frame = DemoData.loadPopulationDatasetWeights();\nfinal double weight = frame.data().getDouble(expected, \"F 30\");\nframe.rows().sort(true, \"F 30\"); //Ensure data is sorted\nframe.colAt(\"F 30\").binarySearch(weight).ifPresent(value -> {\n    assert(value.rowKey().equals(expected));\n    assert(value.getDouble() == weight);\n});\n\n\n\n\nThe same functionality is available in the row dimension as these methods are exposed on the \nDataFrameVector\n interface\nwhich is the super interface of \nDataFrameRow\n and \nDataFrameColumn\n.\n\n\nLower & Higher\n\n\nMorpheus also provides a useful API for locating the nearest row or column \nkey\n before or after some specified value, \neven if that value does not exist in the axis in question. This functionality is contingent on the data in the axis being \nsorted in ascending order as it is implemented internally using a binary search, but is therefore also fast even for a \nvery large axis. Consider a \nDataFrame\n of random values constructed with a row axis representing month end \nLocalDate\n\nvalues, which is built as follows:\n\n\n\n\n\nLocalDate start = LocalDate.of(2014, 1, 1);\nRange<String> columns = Range.of(0, 10).map(i -> \"Column-\" + i);\nRange<LocalDate> monthEnds = Range.of(0, 12).map(i -> start.plusMonths(i+1).minusDays(1));\nDataFrame<LocalDate,String> frame = DataFrame.ofDoubles(monthEnds, columns, v -> Math.random() * 100d);\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});\n\n\n\n\n\n   Index     |  Column-0  |  Column-1  |  Column-2  |  Column-3  |  Column-4  |  Column-5  |  Column-6  |  Column-7  |  Column-8  |  Column-9  |\n------------------------------------------------------------------------------------------------------------------------------------------------\n 2014-01-31  |     19.77  |     79.79  |     54.33  |     72.15  |     32.47  |     48.82  |     40.26  |     85.37  |     33.05  |     38.18  |\n 2014-02-28  |     37.88  |     62.38  |      5.54  |     11.75  |     19.53  |     71.03  |      6.66  |     33.03  |     74.29  |     11.47  |\n 2014-03-31  |      3.33  |     12.06  |     67.68  |     22.46  |     30.11  |     11.07  |     73.94  |     43.57  |     44.48  |     33.34  |\n 2014-04-30  |     57.28  |     99.35  |     51.99  |     53.83  |     74.58  |     14.18  |     85.94  |     91.67  |     52.23  |     34.07  |\n 2014-05-31  |     81.53  |     82.24  |     71.24  |     89.30  |      7.24  |     75.58  |     74.63  |     18.16  |      7.31  |     71.79  |\n 2014-06-30  |     98.89  |     34.17  |     92.35  |     44.58  |      6.92  |     53.81  |     30.58  |     30.46  |     14.15  |     34.93  |\n 2014-07-31  |     23.99  |     75.76  |     93.12  |     65.59  |     23.22  |     83.26  |     10.02  |     99.18  |     11.73  |     43.27  |\n 2014-08-31  |     43.93  |      9.30  |      6.24  |     18.98  |     28.80  |     42.37  |     37.73  |     74.29  |      8.99  |     78.51  |\n 2014-09-30  |     98.85  |     45.26  |     34.00  |     35.86  |     78.51  |     49.17  |     56.03  |     61.06  |      5.50  |     55.80  |\n 2014-10-31  |      5.48  |     10.46  |     65.42  |     20.80  |     49.96  |     21.22  |     76.53  |     56.08  |     11.83  |     28.27  |\n 2014-11-30  |     84.84  |     37.78  |     95.47  |     42.30  |     20.87  |     89.41  |     12.20  |     27.29  |     64.71  |     65.12  |\n 2014-12-31  |     22.34  |     93.66  |     97.76  |     45.17  |     39.16  |     71.14  |     31.19  |     65.24  |     46.66  |     88.43  |\n\n\n\n\nThis kind of dataset may be typical of an experiment that captures monthly values and reports them at month end. Some \nhypothetical analysis involving this dataset may assume that for any date in February 2014 for example, the prior row applies,\nand therefore we need to access the entry for 2014-01-31 (to avoid peak ahead). In this scenario, we know the dataset is \nreported at month end in all cases, so we could code up a rule such that when requesting a value for any given date, we adjust \nthat date to the prior month-end. What if the data is reported at some random dates within the month rather than always at month \nend? In that case we would have to perform a linear search in the worst case to find an appropriate row.\n\n\nThe \nDataFrameAxis\n interface exposes a \nlowerKey()\n and \nhigherKey()\n method that provides a fast mechanism to solve\nthis problem. On the assumption that the keys are sorted, calling \nlowerKey()\n with some date will yield an \nOptional\n \nmatch to the previous key. So in the context of the frame above, requesting the lower key of \n2014-02-15\n would yield\n\n2014-01-31\n. Similarly, calling \nhigherKey()\n with the same argument would yield \n2014-02-28\n.\n\n\nThe example below iterates over the first 35 days of the \nDataFrame\n row axis at a \ndaily\n frequency, and attempts\nto locate the previous row key. Since we know what to expect in this example, we can make some assertions on the resulting\nkeys returned from a call to \nlowerKey()\n, which serves as a usuful unit test.\n\n\n\n\n\n//Iterate over first 35 days of row axis at daily frequency\nRange<LocalDate> dates = Range.of(monthEnds.start(), monthEnds.start().plusDays(35), Period.ofDays(1));\ndates.forEach(date -> {\n    if (frame.rows().contains(date)) {\n        IO.println(\"Exact match for: \" + date);\n    } else {\n        Optional<LocalDate> lowerKey = frame.rows().lowerKey(date);\n        assert(lowerKey.isPresent());\n        assert(lowerKey.get().equals(date.withDayOfMonth(1).minusDays(1)));\n        IO.printf(\"Lower match for %s is %s%n\", date, lowerKey.get());\n    }\n});\n\n\n\n\n\nExact match for: 2014-01-31\nLower match for 2014-02-01 is 2014-01-31\nLower match for 2014-02-02 is 2014-01-31\nLower match for 2014-02-03 is 2014-01-31\nLower match for 2014-02-04 is 2014-01-31\nLower match for 2014-02-05 is 2014-01-31\nLower match for 2014-02-06 is 2014-01-31\nLower match for 2014-02-07 is 2014-01-31\nLower match for 2014-02-08 is 2014-01-31\nLower match for 2014-02-09 is 2014-01-31\nLower match for 2014-02-10 is 2014-01-31\nLower match for 2014-02-11 is 2014-01-31\nLower match for 2014-02-12 is 2014-01-31\nLower match for 2014-02-13 is 2014-01-31\nLower match for 2014-02-14 is 2014-01-31\nLower match for 2014-02-15 is 2014-01-31\nLower match for 2014-02-16 is 2014-01-31\nLower match for 2014-02-17 is 2014-01-31\nLower match for 2014-02-18 is 2014-01-31\nLower match for 2014-02-19 is 2014-01-31\nLower match for 2014-02-20 is 2014-01-31\nLower match for 2014-02-21 is 2014-01-31\nLower match for 2014-02-22 is 2014-01-31\nLower match for 2014-02-23 is 2014-01-31\nLower match for 2014-02-24 is 2014-01-31\nLower match for 2014-02-25 is 2014-01-31\nLower match for 2014-02-26 is 2014-01-31\nLower match for 2014-02-27 is 2014-01-31\nExact match for: 2014-02-28\nLower match for 2014-03-01 is 2014-02-28\nLower match for 2014-03-02 is 2014-02-28\nLower match for 2014-03-03 is 2014-02-28\nLower match for 2014-03-04 is 2014-02-28\nLower match for 2014-03-05 is 2014-02-28\nLower match for 2014-03-06 is 2014-02-28",
            "title": "Finding"
        },
        {
            "location": "/frame/finding/#finding-data",
            "text": "",
            "title": "Finding Data"
        },
        {
            "location": "/frame/finding/#introduction",
            "text": "A  DataFrame  represents tablular data framed by a row and column axis that define a coordindate space by which entries \ncan be read or written. As previously discussed in the section on  access , it is possible to read/write data items \neither by keys, ordinals or a combination of the two. Finding data in a  DataFrame  often means identifying the row and or \ncolumn coordinates associated with the data being searched. The Morpheus API provides a versatile set of functions to locate \ndata in various ways, often providing both sequential and parallel search algorithms. This section provides some useful \nexamples of how to use these APIs.",
            "title": "Introduction"
        },
        {
            "location": "/frame/finding/#example-data",
            "text": "In this section we will use a dataset introduced earlier, namely population data made available by the Office for National \nStatistics (ONS) in the United Kingdom. This dataset provides population counts between 1999 and 2014 decomposed into male and \nfemale age buckets ranging from 0 to 90 years old, over a range of UK Boroughs including the UK as a whole. The code to load \nthis CSV formatted dataset is shown below, as well as a rendering of the first 10 rows. Note that the raw dataset is expressed \nas counts, so we convert these to a percentage of the total population for that row in order to standardize the values.   /**\n * Returns the ONS population dataset for UK boroughs, and convert counts to weights\n * @return  the ONS population dataset, expressed as population weights\n */\nstatic DataFrame<Tuple,String> loadPopulationDatasetWeights() {\n    return DataFrame.read().<Tuple>csv(options -> {\n        options.setResource(\"http://tinyurl.com/ons-population-year\");\n        options.setRowKeyParser(Tuple.class, row -> Tuple.of(Integer.parseInt(row[1]), row[2]));\n        options.setExcludeColumns(\"Code\");\n        options.getFormats().setNullValues(\"-\");\n        options.setColumnType(\"All Males\", Double.class);\n        options.setColumnType(\"All Females\", Double.class);\n        options.setColumnType(\"All Persons\", Double.class);\n        options.setColumnType(\"[MF]\\\\s+\\\\d+\", Double.class);\n    }).applyValues(v -> {\n       if (v.colKey().matches(\"[MF]\\\\s+\\\\d+\")) {\n           final double total = v.row().getDouble(\"All Persons\");\n           final double count = v.getDouble();\n           return count / total;\n       } else {\n           return v.getValue();\n       }\n    });\n}  \n         Index          |  Year  |     Borough      |  All Persons  |  All Males  |   M 0   |   M 1   |   M 2   |   M 3   |   M 4   |   M 5   |   M 6   |   M 7   |   M 8   |   M 9   |  M 10   |  M 11   |  M 12   |  M 13   |  M 14   |  M 15   |  M 16   |  M 17   |  M 18   |  M 19   |  M 20   |  M 21   |  M 22   |  M 23   |  M 24   |  M 25   |  M 26   |  M 27   |  M 28   |  M 29   |  M 30   |  M 31   |  M 32   |  M 33   |  M 34   |  M 35   |  M 36   |  M 37   |  M 38   |  M 39   |  M 40   |  M 41   |  M 42   |  M 43   |  M 44   |  M 45   |  M 46   |  M 47   |  M 48   |  M 49   |  M 50   |  M 51   |  M 52   |  M 53   |  M 54   |  M 55   |  M 56   |  M 57   |  M 58   |  M 59   |  M 60   |  M 61   |  M 62   |  M 63   |  M 64   |  M 65   |  M 66   |  M 67   |  M 68   |  M 69   |  M 70   |  M 71   |  M 72   |  M 73   |  M 74   |  M 75   |  M 76   |  M 77   |  M 78   |  M 79   |  M 80   |  M 81   |  M 82   |  M 83   |  M 84   |  M 85   |  M 86   |  M 87   |  M 88   |  M 89   |  M 90+  |  All Females  |   F 0   |   F 1   |   F 2   |   F 3   |   F 4   |   F 5   |   F 6   |   F 7   |   F 8   |   F 9   |  F 10   |  F 11   |  F 12   |  F 13   |  F 14   |  F 15   |  F 16   |  F 17   |  F 18   |  F 19   |  F 20   |  F 21   |  F 22   |  F 23   |  F 24   |  F 25   |  F 26   |  F 27   |  F 28   |  F 29   |  F 30   |  F 31   |  F 32   |  F 33   |  F 34   |  F 35   |  F 36   |  F 37   |  F 38   |  F 39   |  F 40   |  F 41   |  F 42   |  F 43   |  F 44   |  F 45   |  F 46   |  F 47   |  F 48   |  F 49   |  F 50   |  F 51   |  F 52   |  F 53   |  F 54   |  F 55   |  F 56   |  F 57   |  F 58   |  F 59   |  F 60   |  F 61   |  F 62   |  F 63   |  F 64   |  F 65   |  F 66   |  F 67   |  F 68   |  F 69   |  F 70   |  F 71   |  F 72   |  F 73   |  F 74   |  F 75   |  F 76   |  F 77   |  F 78   |  F 79   |  F 80   |  F 81   |  F 82   |  F 83   |  F 84   |  F 85   |  F 86   |  F 87   |  F 88   |  F 89   |  F 90+  |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n (1999,City of London)  |  1999  |  City of London  |         6581  |       3519  |  0.36%  |  0.35%  |  0.33%  |  0.32%  |  0.30%  |  0.32%  |  0.30%  |  0.32%  |  0.30%  |  0.33%  |  0.33%  |  0.32%  |  0.29%  |  0.27%  |  0.26%  |  0.26%  |  0.26%  |  0.32%  |  0.40%  |  0.49%  |  0.61%  |  0.71%  |  0.79%  |  0.91%  |  0.99%  |  1.06%  |  1.11%  |  1.11%  |  1.11%  |  1.19%  |  1.19%  |  1.17%  |  1.14%  |  1.09%  |  1.08%  |  1.08%  |  0.99%  |  0.91%  |  0.88%  |  0.88%  |  0.87%  |  0.84%  |  0.77%  |  0.81%  |  0.81%  |  0.90%  |  0.90%  |  0.91%  |  0.99%  |  1.03%  |  1.03%  |  1.06%  |  0.96%  |  0.94%  |  0.93%  |  0.82%  |  0.74%  |  0.71%  |  0.65%  |  0.58%  |  0.53%  |  0.52%  |  0.52%  |  0.50%  |  0.49%  |  0.47%  |  0.49%  |  0.47%  |  0.46%  |  0.46%  |  0.44%  |  0.41%  |  0.40%  |  0.36%  |  0.35%  |  0.30%  |  0.30%  |  0.27%  |  0.26%  |  0.23%  |  0.21%  |  0.18%  |  0.18%  |  0.17%  |  0.17%  |  0.65%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |         3062  |  0.36%  |  0.36%  |  0.33%  |  0.33%  |  0.32%  |  0.32%  |  0.29%  |  0.29%  |  0.27%  |  0.27%  |  0.27%  |  0.24%  |  0.26%  |  0.24%  |  0.24%  |  0.24%  |  0.30%  |  0.38%  |  0.40%  |  0.46%  |  0.59%  |  0.71%  |  0.76%  |  0.82%  |  0.93%  |  1.03%  |  1.08%  |  1.09%  |  1.08%  |  1.11%  |  1.08%  |  1.03%  |  1.00%  |  0.94%  |  0.84%  |  0.77%  |  0.71%  |  0.65%  |  0.64%  |  0.59%  |  0.59%  |  0.62%  |  0.64%  |  0.65%  |  0.67%  |  0.61%  |  0.67%  |  0.68%  |  0.71%  |  0.73%  |  0.74%  |  0.74%  |  0.76%  |  0.70%  |  0.67%  |  0.59%  |  0.55%  |  0.52%  |  0.50%  |  0.47%  |  0.46%  |  0.44%  |  0.44%  |  0.46%  |  0.47%  |  0.46%  |  0.46%  |  0.44%  |  0.44%  |  0.43%  |  0.40%  |  0.40%  |  0.38%  |  0.35%  |  0.32%  |  0.30%  |  0.30%  |  0.29%  |  0.26%  |  0.26%  |  0.26%  |  0.24%  |  0.23%  |  0.21%  |  0.20%  |  1.15%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |\n (2000,City of London)  |  2000  |  City of London  |         7014  |       3775  |  0.36%  |  0.36%  |  0.34%  |  0.33%  |  0.31%  |  0.30%  |  0.31%  |  0.30%  |  0.31%  |  0.30%  |  0.31%  |  0.29%  |  0.26%  |  0.23%  |  0.23%  |  0.21%  |  0.24%  |  0.27%  |  0.34%  |  0.43%  |  0.56%  |  0.68%  |  0.81%  |  0.91%  |  1.03%  |  1.15%  |  1.25%  |  1.28%  |  1.28%  |  1.30%  |  1.34%  |  1.34%  |  1.28%  |  1.23%  |  1.15%  |  1.14%  |  1.08%  |  1.00%  |  0.93%  |  0.88%  |  0.88%  |  0.88%  |  0.84%  |  0.80%  |  0.80%  |  0.80%  |  0.88%  |  0.88%  |  0.88%  |  0.96%  |  1.01%  |  1.01%  |  1.06%  |  0.94%  |  0.91%  |  0.90%  |  0.80%  |  0.71%  |  0.67%  |  0.61%  |  0.56%  |  0.48%  |  0.46%  |  0.44%  |  0.43%  |  0.41%  |  0.40%  |  0.40%  |  0.38%  |  0.40%  |  0.38%  |  0.37%  |  0.36%  |  0.33%  |  0.31%  |  0.31%  |  0.27%  |  0.27%  |  0.24%  |  0.23%  |  0.21%  |  0.20%  |  0.17%  |  0.17%  |  0.16%  |  0.56%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |         3239  |  0.37%  |  0.34%  |  0.34%  |  0.31%  |  0.30%  |  0.30%  |  0.29%  |  0.27%  |  0.27%  |  0.27%  |  0.27%  |  0.26%  |  0.24%  |  0.26%  |  0.24%  |  0.26%  |  0.29%  |  0.33%  |  0.40%  |  0.46%  |  0.54%  |  0.70%  |  0.83%  |  0.87%  |  0.98%  |  1.07%  |  1.17%  |  1.18%  |  1.20%  |  1.15%  |  1.14%  |  1.08%  |  1.04%  |  0.98%  |  0.91%  |  0.81%  |  0.76%  |  0.68%  |  0.63%  |  0.61%  |  0.58%  |  0.60%  |  0.58%  |  0.61%  |  0.61%  |  0.63%  |  0.58%  |  0.64%  |  0.64%  |  0.67%  |  0.70%  |  0.71%  |  0.73%  |  0.73%  |  0.67%  |  0.64%  |  0.58%  |  0.53%  |  0.50%  |  0.46%  |  0.43%  |  0.41%  |  0.40%  |  0.38%  |  0.40%  |  0.41%  |  0.40%  |  0.40%  |  0.40%  |  0.40%  |  0.38%  |  0.36%  |  0.36%  |  0.34%  |  0.31%  |  0.30%  |  0.27%  |  0.29%  |  0.27%  |  0.24%  |  0.23%  |  0.21%  |  0.21%  |  0.21%  |  0.19%  |  1.15%  |    NaN  |    NaN  |    NaN  |    NaN  |      0  |\n (2001,City of London)  |  2001  |  City of London  |         7359  |       3984  |  0.23%  |  0.41%  |  0.34%  |  0.26%  |  0.29%  |  0.34%  |  0.27%  |  0.22%  |  0.26%  |  0.37%  |  0.35%  |  0.22%  |  0.39%  |  0.26%  |  0.15%  |  0.19%  |  0.16%  |  0.18%  |  0.16%  |  0.37%  |  0.52%  |  0.53%  |  0.72%  |  0.86%  |  1.09%  |  1.49%  |  1.49%  |  1.36%  |  1.47%  |  1.70%  |  1.55%  |  1.29%  |  1.26%  |  1.43%  |  1.35%  |  1.14%  |  1.21%  |  0.96%  |  0.86%  |  1.02%  |  0.88%  |  0.71%  |  0.64%  |  1.03%  |  0.99%  |  0.73%  |  0.67%  |  0.61%  |  0.82%  |  0.67%  |  1.51%  |  0.99%  |  0.75%  |  1.10%  |  1.09%  |  0.83%  |  0.86%  |  0.76%  |  0.87%  |  0.65%  |  0.38%  |  0.46%  |  0.61%  |  0.49%  |  0.34%  |  0.38%  |  0.29%  |  0.38%  |  0.29%  |  0.37%  |  0.37%  |  0.29%  |  0.33%  |  0.39%  |  0.29%  |  0.26%  |  0.31%  |  0.27%  |  0.15%  |  0.26%  |  0.20%  |  0.19%  |  0.11%  |  0.20%  |  0.14%  |  0.08%  |  0.14%  |  0.07%  |  0.05%  |  0.04%  |     13  |         3375  |  0.43%  |  0.41%  |  0.31%  |  0.29%  |  0.35%  |  0.22%  |  0.29%  |  0.20%  |  0.24%  |  0.33%  |  0.27%  |  0.27%  |  0.14%  |  0.35%  |  0.18%  |  0.18%  |  0.19%  |  0.53%  |  0.38%  |  0.48%  |  0.64%  |  0.53%  |  0.67%  |  0.73%  |  1.01%  |  1.26%  |  1.21%  |  1.09%  |  1.14%  |  1.45%  |  1.16%  |  1.13%  |  1.37%  |  0.90%  |  0.82%  |  0.73%  |  0.90%  |  0.82%  |  0.65%  |  0.61%  |  0.50%  |  0.38%  |  0.42%  |  0.86%  |  0.58%  |  0.72%  |  0.68%  |  0.64%  |  0.48%  |  0.57%  |  0.50%  |  0.88%  |  0.65%  |  0.77%  |  0.77%  |  0.52%  |  0.61%  |  0.52%  |  0.52%  |  0.45%  |  0.43%  |  0.43%  |  0.35%  |  0.34%  |  0.33%  |  0.39%  |  0.38%  |  0.37%  |  0.46%  |  0.37%  |  0.27%  |  0.35%  |  0.33%  |  0.42%  |  0.24%  |  0.33%  |  0.37%  |  0.18%  |  0.18%  |  0.20%  |  0.24%  |  0.30%  |  0.24%  |  0.15%  |  0.14%  |  0.14%  |  0.19%  |  0.35%  |  0.22%  |  0.10%  |     15  |\n (2002,City of London)  |  2002  |  City of London  |         7280  |       3968  |  0.40%  |  0.25%  |  0.38%  |  0.37%  |  0.27%  |  0.27%  |  0.33%  |  0.27%  |  0.23%  |  0.27%  |  0.34%  |  0.34%  |  0.19%  |  0.37%  |  0.12%  |  0.07%  |  0.11%  |  0.14%  |  0.16%  |  0.14%  |  0.27%  |  0.54%  |  0.69%  |  0.84%  |  0.93%  |  1.36%  |  1.55%  |  1.68%  |  1.40%  |  1.57%  |  1.57%  |  1.47%  |  1.36%  |  1.22%  |  1.39%  |  1.28%  |  1.15%  |  1.06%  |  1.00%  |  0.84%  |  1.07%  |  0.89%  |  0.78%  |  0.69%  |  1.03%  |  0.99%  |  0.77%  |  0.69%  |  0.62%  |  0.84%  |  0.63%  |  1.55%  |  0.96%  |  0.67%  |  1.17%  |  1.04%  |  0.81%  |  0.84%  |  0.76%  |  0.82%  |  0.62%  |  0.38%  |  0.43%  |  0.63%  |  0.55%  |  0.32%  |  0.36%  |  0.27%  |  0.34%  |  0.29%  |  0.33%  |  0.41%  |  0.29%  |  0.34%  |  0.40%  |  0.27%  |  0.25%  |  0.29%  |  0.23%  |  0.15%  |  0.23%  |  0.19%  |  0.18%  |  0.11%  |  0.21%  |  0.12%  |  0.08%  |  0.11%  |  0.05%  |  0.03%  |     14  |         3312  |  0.38%  |  0.48%  |  0.41%  |  0.26%  |  0.29%  |  0.36%  |  0.21%  |  0.27%  |  0.18%  |  0.25%  |  0.23%  |  0.21%  |  0.23%  |  0.08%  |  0.26%  |  0.18%  |  0.07%  |  0.21%  |  0.48%  |  0.40%  |  0.48%  |  0.66%  |  0.71%  |  0.65%  |  0.92%  |  1.02%  |  1.15%  |  1.15%  |  1.14%  |  0.96%  |  1.58%  |  1.15%  |  1.29%  |  1.39%  |  0.87%  |  0.84%  |  0.78%  |  0.80%  |  0.85%  |  0.60%  |  0.65%  |  0.51%  |  0.38%  |  0.45%  |  0.82%  |  0.49%  |  0.65%  |  0.71%  |  0.62%  |  0.49%  |  0.52%  |  0.56%  |  0.87%  |  0.70%  |  0.74%  |  0.76%  |  0.51%  |  0.56%  |  0.55%  |  0.49%  |  0.41%  |  0.40%  |  0.41%  |  0.37%  |  0.32%  |  0.32%  |  0.38%  |  0.38%  |  0.34%  |  0.47%  |  0.34%  |  0.26%  |  0.33%  |  0.32%  |  0.43%  |  0.26%  |  0.34%  |  0.36%  |  0.14%  |  0.16%  |  0.19%  |  0.23%  |  0.30%  |  0.21%  |  0.12%  |  0.14%  |  0.10%  |  0.15%  |  0.36%  |  0.19%  |     20  |\n (2003,City of London)  |  2003  |  City of London  |         7115  |       3892  |  0.49%  |  0.34%  |  0.22%  |  0.31%  |  0.37%  |  0.24%  |  0.27%  |  0.34%  |  0.30%  |  0.30%  |  0.34%  |  0.35%  |  0.30%  |  0.14%  |  0.22%  |  0.14%  |  0.06%  |  0.07%  |  0.17%  |  0.17%  |  0.15%  |  0.27%  |  0.69%  |  0.84%  |  0.93%  |  1.12%  |  1.41%  |  1.66%  |  1.60%  |  1.48%  |  1.52%  |  1.48%  |  1.39%  |  1.32%  |  1.19%  |  1.36%  |  1.25%  |  1.15%  |  1.07%  |  0.97%  |  0.89%  |  1.04%  |  0.90%  |  0.80%  |  0.72%  |  1.05%  |  1.04%  |  0.79%  |  0.72%  |  0.55%  |  0.90%  |  0.62%  |  1.52%  |  0.91%  |  0.66%  |  1.12%  |  1.04%  |  0.79%  |  0.86%  |  0.77%  |  0.77%  |  0.62%  |  0.37%  |  0.42%  |  0.59%  |  0.51%  |  0.32%  |  0.34%  |  0.31%  |  0.31%  |  0.25%  |  0.32%  |  0.44%  |  0.25%  |  0.34%  |  0.38%  |  0.27%  |  0.24%  |  0.31%  |  0.21%  |  0.14%  |  0.22%  |  0.17%  |  0.14%  |  0.11%  |  0.20%  |  0.11%  |  0.08%  |  0.10%  |  0.03%  |     13  |         3223  |  0.25%  |  0.35%  |  0.44%  |  0.41%  |  0.25%  |  0.30%  |  0.37%  |  0.20%  |  0.27%  |  0.17%  |  0.27%  |  0.18%  |  0.15%  |  0.17%  |  0.03%  |  0.22%  |  0.10%  |  0.17%  |  0.31%  |  0.44%  |  0.48%  |  0.52%  |  0.73%  |  0.86%  |  0.82%  |  1.19%  |  1.03%  |  1.21%  |  1.15%  |  1.19%  |  1.05%  |  1.39%  |  1.19%  |  1.25%  |  1.29%  |  0.84%  |  0.82%  |  0.63%  |  0.72%  |  0.74%  |  0.59%  |  0.66%  |  0.62%  |  0.41%  |  0.49%  |  0.83%  |  0.53%  |  0.58%  |  0.69%  |  0.55%  |  0.49%  |  0.56%  |  0.52%  |  0.86%  |  0.69%  |  0.74%  |  0.70%  |  0.49%  |  0.56%  |  0.48%  |  0.51%  |  0.42%  |  0.41%  |  0.42%  |  0.38%  |  0.34%  |  0.32%  |  0.39%  |  0.42%  |  0.35%  |  0.45%  |  0.34%  |  0.24%  |  0.32%  |  0.30%  |  0.42%  |  0.22%  |  0.32%  |  0.34%  |  0.14%  |  0.15%  |  0.20%  |  0.21%  |  0.22%  |  0.22%  |  0.13%  |  0.03%  |  0.07%  |  0.14%  |  0.31%  |     26  |\n (2004,City of London)  |  2004  |  City of London  |         7118  |       3875  |  0.39%  |  0.38%  |  0.35%  |  0.22%  |  0.28%  |  0.38%  |  0.24%  |  0.28%  |  0.44%  |  0.25%  |  0.31%  |  0.30%  |  0.31%  |  0.22%  |  0.06%  |  0.18%  |  0.11%  |  0.00%  |  0.03%  |  0.18%  |  0.13%  |  0.35%  |  0.37%  |  0.89%  |  0.98%  |  0.97%  |  1.25%  |  1.57%  |  1.62%  |  1.63%  |  1.40%  |  1.31%  |  1.43%  |  1.31%  |  1.21%  |  1.11%  |  1.39%  |  1.29%  |  1.24%  |  1.14%  |  1.03%  |  0.83%  |  0.91%  |  0.94%  |  0.81%  |  0.69%  |  1.07%  |  0.98%  |  0.77%  |  0.70%  |  0.62%  |  0.86%  |  0.63%  |  1.39%  |  0.91%  |  0.65%  |  1.08%  |  1.07%  |  0.77%  |  0.80%  |  0.74%  |  0.80%  |  0.60%  |  0.39%  |  0.45%  |  0.55%  |  0.48%  |  0.31%  |  0.35%  |  0.30%  |  0.31%  |  0.25%  |  0.28%  |  0.42%  |  0.22%  |  0.34%  |  0.39%  |  0.27%  |  0.20%  |  0.28%  |  0.13%  |  0.17%  |  0.20%  |  0.14%  |  0.11%  |  0.11%  |  0.21%  |  0.11%  |  0.07%  |  0.07%  |     12  |         3243  |  0.44%  |  0.25%  |  0.28%  |  0.37%  |  0.35%  |  0.28%  |  0.35%  |  0.39%  |  0.20%  |  0.27%  |  0.10%  |  0.27%  |  0.14%  |  0.13%  |  0.13%  |  0.01%  |  0.18%  |  0.24%  |  0.13%  |  0.31%  |  0.53%  |  0.58%  |  0.66%  |  0.83%  |  1.11%  |  1.12%  |  1.28%  |  1.21%  |  1.11%  |  1.14%  |  1.11%  |  1.10%  |  1.28%  |  1.12%  |  1.22%  |  1.17%  |  0.79%  |  0.86%  |  0.66%  |  0.67%  |  0.77%  |  0.56%  |  0.63%  |  0.55%  |  0.42%  |  0.48%  |  0.83%  |  0.52%  |  0.55%  |  0.79%  |  0.53%  |  0.46%  |  0.53%  |  0.51%  |  0.84%  |  0.72%  |  0.73%  |  0.72%  |  0.48%  |  0.56%  |  0.49%  |  0.48%  |  0.39%  |  0.45%  |  0.39%  |  0.38%  |  0.35%  |  0.28%  |  0.41%  |  0.39%  |  0.34%  |  0.42%  |  0.34%  |  0.21%  |  0.30%  |  0.31%  |  0.41%  |  0.21%  |  0.32%  |  0.31%  |  0.17%  |  0.15%  |  0.17%  |  0.15%  |  0.17%  |  0.17%  |  0.10%  |  0.01%  |  0.04%  |  0.13%  |     42  |\n (2005,City of London)  |  2005  |  City of London  |         7131  |       3869  |  0.48%  |  0.35%  |  0.31%  |  0.32%  |  0.17%  |  0.27%  |  0.31%  |  0.27%  |  0.27%  |  0.43%  |  0.28%  |  0.29%  |  0.28%  |  0.27%  |  0.06%  |  0.04%  |  0.17%  |  0.14%  |  0.00%  |  0.10%  |  0.13%  |  0.22%  |  0.60%  |  0.56%  |  1.01%  |  1.30%  |  1.04%  |  1.25%  |  1.64%  |  1.58%  |  1.46%  |  1.23%  |  1.29%  |  1.53%  |  1.37%  |  1.08%  |  1.07%  |  1.44%  |  1.25%  |  1.09%  |  1.12%  |  0.95%  |  0.86%  |  0.97%  |  0.87%  |  0.81%  |  0.72%  |  1.15%  |  0.94%  |  0.81%  |  0.73%  |  0.59%  |  0.83%  |  0.70%  |  1.25%  |  0.94%  |  0.66%  |  1.02%  |  0.98%  |  0.77%  |  0.81%  |  0.70%  |  0.74%  |  0.59%  |  0.38%  |  0.42%  |  0.59%  |  0.43%  |  0.32%  |  0.34%  |  0.28%  |  0.24%  |  0.24%  |  0.27%  |  0.42%  |  0.21%  |  0.29%  |  0.38%  |  0.24%  |  0.20%  |  0.29%  |  0.11%  |  0.20%  |  0.17%  |  0.10%  |  0.10%  |  0.08%  |  0.20%  |  0.10%  |  0.03%  |     12  |         3262  |  0.41%  |  0.41%  |  0.21%  |  0.21%  |  0.32%  |  0.35%  |  0.24%  |  0.32%  |  0.35%  |  0.20%  |  0.25%  |  0.10%  |  0.28%  |  0.10%  |  0.10%  |  0.08%  |  0.03%  |  0.10%  |  0.27%  |  0.21%  |  0.43%  |  0.60%  |  0.69%  |  0.70%  |  1.15%  |  1.07%  |  1.25%  |  1.25%  |  1.19%  |  1.07%  |  1.26%  |  1.07%  |  1.19%  |  1.28%  |  1.04%  |  1.12%  |  1.04%  |  0.84%  |  0.77%  |  0.73%  |  0.63%  |  0.73%  |  0.60%  |  0.66%  |  0.53%  |  0.48%  |  0.48%  |  0.77%  |  0.46%  |  0.53%  |  0.80%  |  0.53%  |  0.50%  |  0.55%  |  0.52%  |  0.81%  |  0.73%  |  0.77%  |  0.70%  |  0.48%  |  0.52%  |  0.49%  |  0.52%  |  0.36%  |  0.46%  |  0.42%  |  0.35%  |  0.34%  |  0.25%  |  0.38%  |  0.36%  |  0.28%  |  0.39%  |  0.31%  |  0.25%  |  0.29%  |  0.34%  |  0.36%  |  0.22%  |  0.29%  |  0.31%  |  0.14%  |  0.17%  |  0.15%  |  0.14%  |  0.17%  |  0.13%  |  0.03%  |  0.03%  |  0.04%  |     50  |\n (2006,City of London)  |  2006  |  City of London  |         7254  |       3959  |  0.36%  |  0.34%  |  0.36%  |  0.28%  |  0.25%  |  0.18%  |  0.22%  |  0.32%  |  0.30%  |  0.25%  |  0.41%  |  0.25%  |  0.28%  |  0.23%  |  0.10%  |  0.07%  |  0.07%  |  0.21%  |  0.17%  |  0.00%  |  0.17%  |  0.26%  |  0.44%  |  0.87%  |  0.87%  |  1.13%  |  1.50%  |  1.14%  |  1.36%  |  1.71%  |  1.57%  |  1.42%  |  1.14%  |  1.17%  |  1.60%  |  1.35%  |  1.06%  |  1.02%  |  1.48%  |  1.20%  |  1.03%  |  1.13%  |  0.94%  |  0.87%  |  0.95%  |  0.81%  |  0.85%  |  0.73%  |  1.14%  |  0.91%  |  0.83%  |  0.72%  |  0.66%  |  0.87%  |  0.66%  |  1.17%  |  0.92%  |  0.65%  |  0.94%  |  0.85%  |  0.77%  |  0.77%  |  0.66%  |  0.74%  |  0.55%  |  0.39%  |  0.36%  |  0.57%  |  0.43%  |  0.30%  |  0.32%  |  0.28%  |  0.19%  |  0.22%  |  0.26%  |  0.40%  |  0.23%  |  0.28%  |  0.39%  |  0.23%  |  0.19%  |  0.26%  |  0.08%  |  0.14%  |  0.17%  |  0.07%  |  0.06%  |  0.08%  |  0.17%  |  0.10%  |     13  |         3295  |  0.39%  |  0.36%  |  0.43%  |  0.17%  |  0.23%  |  0.34%  |  0.32%  |  0.28%  |  0.30%  |  0.34%  |  0.19%  |  0.25%  |  0.07%  |  0.26%  |  0.06%  |  0.10%  |  0.06%  |  0.17%  |  0.10%  |  0.37%  |  0.33%  |  0.45%  |  0.70%  |  0.80%  |  0.98%  |  1.21%  |  1.10%  |  1.19%  |  1.19%  |  1.27%  |  1.16%  |  1.19%  |  1.08%  |  1.16%  |  1.10%  |  1.06%  |  1.02%  |  0.94%  |  0.85%  |  0.62%  |  0.73%  |  0.62%  |  0.70%  |  0.62%  |  0.63%  |  0.54%  |  0.52%  |  0.45%  |  0.73%  |  0.43%  |  0.51%  |  0.79%  |  0.54%  |  0.50%  |  0.52%  |  0.50%  |  0.77%  |  0.69%  |  0.73%  |  0.65%  |  0.41%  |  0.57%  |  0.43%  |  0.47%  |  0.33%  |  0.41%  |  0.40%  |  0.32%  |  0.36%  |  0.23%  |  0.37%  |  0.36%  |  0.28%  |  0.36%  |  0.33%  |  0.25%  |  0.28%  |  0.30%  |  0.37%  |  0.23%  |  0.28%  |  0.26%  |  0.15%  |  0.15%  |  0.14%  |  0.12%  |  0.15%  |  0.12%  |  0.03%  |  0.00%  |     45  |\n (2007,City of London)  |  2007  |  City of London  |         7607  |       4197  |  0.41%  |  0.33%  |  0.32%  |  0.29%  |  0.30%  |  0.21%  |  0.17%  |  0.18%  |  0.35%  |  0.30%  |  0.20%  |  0.43%  |  0.21%  |  0.25%  |  0.09%  |  0.09%  |  0.04%  |  0.12%  |  0.21%  |  0.35%  |  0.00%  |  0.39%  |  0.46%  |  0.84%  |  1.20%  |  1.18%  |  1.49%  |  1.62%  |  1.30%  |  1.41%  |  1.62%  |  1.34%  |  1.39%  |  1.10%  |  1.14%  |  1.50%  |  1.37%  |  1.13%  |  1.09%  |  1.54%  |  1.16%  |  1.04%  |  0.97%  |  0.92%  |  0.99%  |  0.97%  |  0.83%  |  0.84%  |  0.75%  |  1.05%  |  0.85%  |  0.82%  |  0.72%  |  0.67%  |  0.84%  |  0.70%  |  1.03%  |  0.84%  |  0.63%  |  0.83%  |  0.78%  |  0.71%  |  0.76%  |  0.60%  |  0.71%  |  0.47%  |  0.34%  |  0.39%  |  0.51%  |  0.39%  |  0.29%  |  0.30%  |  0.22%  |  0.21%  |  0.21%  |  0.25%  |  0.35%  |  0.20%  |  0.20%  |  0.38%  |  0.18%  |  0.18%  |  0.24%  |  0.08%  |  0.08%  |  0.13%  |  0.05%  |  0.05%  |  0.08%  |  0.12%  |     20  |         3410  |  0.32%  |  0.32%  |  0.39%  |  0.33%  |  0.22%  |  0.22%  |  0.22%  |  0.28%  |  0.21%  |  0.28%  |  0.33%  |  0.20%  |  0.24%  |  0.05%  |  0.25%  |  0.04%  |  0.14%  |  0.32%  |  0.20%  |  0.32%  |  0.51%  |  0.45%  |  0.74%  |  0.92%  |  0.99%  |  1.20%  |  1.31%  |  1.12%  |  1.33%  |  1.09%  |  1.05%  |  1.00%  |  1.05%  |  0.95%  |  0.95%  |  0.95%  |  1.00%  |  0.96%  |  0.82%  |  0.85%  |  0.62%  |  0.74%  |  0.63%  |  0.72%  |  0.66%  |  0.64%  |  0.57%  |  0.47%  |  0.39%  |  0.63%  |  0.38%  |  0.50%  |  0.71%  |  0.54%  |  0.51%  |  0.51%  |  0.51%  |  0.74%  |  0.68%  |  0.70%  |  0.60%  |  0.35%  |  0.54%  |  0.46%  |  0.42%  |  0.35%  |  0.39%  |  0.42%  |  0.29%  |  0.32%  |  0.21%  |  0.35%  |  0.34%  |  0.24%  |  0.33%  |  0.30%  |  0.22%  |  0.24%  |  0.29%  |  0.30%  |  0.22%  |  0.22%  |  0.25%  |  0.14%  |  0.13%  |  0.13%  |  0.09%  |  0.12%  |  0.11%  |  0.01%  |     36  |\n (2008,City of London)  |  2008  |  City of London  |         7429  |       4131  |  0.39%  |  0.35%  |  0.30%  |  0.32%  |  0.24%  |  0.30%  |  0.17%  |  0.16%  |  0.23%  |  0.40%  |  0.27%  |  0.20%  |  0.40%  |  0.23%  |  0.13%  |  0.11%  |  0.08%  |  0.09%  |  0.15%  |  0.24%  |  0.38%  |  0.15%  |  0.58%  |  0.52%  |  1.01%  |  1.35%  |  1.14%  |  1.66%  |  1.64%  |  1.37%  |  1.25%  |  1.49%  |  1.29%  |  1.36%  |  1.01%  |  1.08%  |  1.39%  |  1.24%  |  1.12%  |  1.06%  |  1.63%  |  1.16%  |  1.06%  |  0.93%  |  0.94%  |  0.89%  |  1.04%  |  0.77%  |  0.90%  |  0.90%  |  1.09%  |  0.86%  |  0.82%  |  0.69%  |  0.70%  |  0.82%  |  0.74%  |  0.98%  |  0.82%  |  0.62%  |  0.81%  |  0.82%  |  0.71%  |  0.74%  |  0.57%  |  0.71%  |  0.46%  |  0.35%  |  0.39%  |  0.52%  |  0.39%  |  0.32%  |  0.30%  |  0.23%  |  0.20%  |  0.19%  |  0.26%  |  0.32%  |  0.19%  |  0.22%  |  0.36%  |  0.17%  |  0.16%  |  0.23%  |  0.08%  |  0.08%  |  0.09%  |  0.05%  |  0.04%  |  0.07%  |     25  |         3298  |  0.16%  |  0.23%  |  0.31%  |  0.31%  |  0.32%  |  0.22%  |  0.23%  |  0.22%  |  0.26%  |  0.20%  |  0.28%  |  0.32%  |  0.20%  |  0.26%  |  0.01%  |  0.26%  |  0.00%  |  0.30%  |  0.35%  |  0.34%  |  0.47%  |  0.58%  |  0.47%  |  0.77%  |  0.98%  |  1.10%  |  1.20%  |  1.28%  |  1.18%  |  1.27%  |  1.02%  |  1.04%  |  0.92%  |  0.93%  |  0.77%  |  0.81%  |  0.82%  |  0.86%  |  0.92%  |  0.77%  |  0.85%  |  0.55%  |  0.70%  |  0.62%  |  0.70%  |  0.67%  |  0.67%  |  0.50%  |  0.47%  |  0.43%  |  0.57%  |  0.39%  |  0.51%  |  0.71%  |  0.54%  |  0.57%  |  0.57%  |  0.51%  |  0.70%  |  0.69%  |  0.69%  |  0.65%  |  0.36%  |  0.55%  |  0.46%  |  0.39%  |  0.40%  |  0.42%  |  0.39%  |  0.31%  |  0.30%  |  0.22%  |  0.38%  |  0.36%  |  0.27%  |  0.32%  |  0.30%  |  0.22%  |  0.20%  |  0.30%  |  0.32%  |  0.20%  |  0.23%  |  0.24%  |  0.13%  |  0.12%  |  0.12%  |  0.09%  |  0.12%  |  0.11%  |     30  |Male weight: 55.76%, Female weight: 44.24% for 2014 in City of London",
            "title": "Example Data"
        },
        {
            "location": "/frame/finding/#first-last",
            "text": "",
            "title": "First &amp; Last"
        },
        {
            "location": "/frame/finding/#row-column",
            "text": "It is often useful to locate the first or last row / column in a  DataFrame  that satisfies some condition. The DataFrameAxis  interface includes overloaded  first()  and  last()  methods, one which takes no arguments and \nanother which takes a predicate. In the example below, we use this API to find the first row in the ONS population \ndata where the male / female population weights differ by more than 10%, which is a very significant difference.   The results indicate that in 2007, males made up  55.17%  and females made up  44.83%  of the population in \nthe  City of London , also know as the financial district. Perhaps no surprise there.   DataFrame<Tuple,String> frame = DemoData.loadPopulationDatasetWeights();\nframe.rows().first(row -> {\n    double total = row.getDouble(\"All Persons\");\n    double maleWeight = row.getDouble(\"All Males\") / total;\n    double femaleWeight = row.getDouble(\"All Females\") / total;\n    return Math.abs(maleWeight - femaleWeight) > 0.1;\n}).ifPresent(row -> {\n    int year = row.key().item(0);\n    String borough = row.key().item(1);\n    double total = row.getDouble(\"All Persons\");\n    double males = (row.getDouble(\"All Males\") / total) * 100d;\n    double females = (row.getDouble(\"All Females\") / total) * 100d;\n    IO.printf(\"Male weight: %.2f%%, Female weight: %.2f%% for %s in %s\", males, females, year, borough);\n});  Male weight: 55.17%, Female weight: 44.83% for 2007 in City of London  We can also search the frame from bottom to top by calling  DataFrame.rows().last()  which yields another hit\nfor the  City of London , but in this case in 2014. The weights are  55.76%  and  44.24%  for males and\nfemales respectively. So much for progress in gender equality in the financial district!  Male weight: 55.76%, Female weight: 44.24% for 2014 in City of London",
            "title": "Row / Column"
        },
        {
            "location": "/frame/finding/#row-column-value",
            "text": "The above example demonstrates the use of the  first()  and  last()  methods on  DataFrameAxis  to locate the \nfirst and last row or column that satisfies some predicate. The same functionality is provided on the  DataFrameVector \ninterface to locate the first and last entries in a specific row or column.  In this example, we iterate over all rows in the  DataFrame  that correspond to the Borough of  Kensington and\nChelsea , and for each row we attempt to find the first age group which has a population weight that exceeds\n1% of the total population for that year. We then simply print the results to standard out.   DataFrame<Tuple,String> frame = DemoData.loadPopulationDatasetWeights();\nframe.rows().filter(r -> r.getValue(\"Borough\").equals(\"Kensington and Chelsea\")).forEach(row -> {\n    row.first(v -> v.colKey().matches(\"[MF]\\\\s+\\\\d+\") && v.getDouble() > 0.01).ifPresent(v -> {\n        Tuple rowKey = v.rowKey();\n        String group = v.colKey();\n        double weight = v.getDouble() * 100d;\n        IO.printf(\"Age group %s has a population of %.2f%% for %s\\n\", group, weight, rowKey);\n    });\n});  Age group M 28 has a population of 1.05% for (1999,Kensington and Chelsea)\nAge group M 27 has a population of 1.02% for (2000,Kensington and Chelsea)\nAge group M 26 has a population of 1.04% for (2001,Kensington and Chelsea)\nAge group M 26 has a population of 1.13% for (2002,Kensington and Chelsea)\nAge group M 26 has a population of 1.09% for (2003,Kensington and Chelsea)\nAge group M 26 has a population of 1.02% for (2004,Kensington and Chelsea)\nAge group M 25 has a population of 1.07% for (2005,Kensington and Chelsea)\nAge group M 25 has a population of 1.07% for (2006,Kensington and Chelsea)\nAge group M 26 has a population of 1.14% for (2007,Kensington and Chelsea)\nAge group M 25 has a population of 1.02% for (2008,Kensington and Chelsea)\nAge group M 26 has a population of 1.07% for (2009,Kensington and Chelsea)\nAge group M 27 has a population of 1.10% for (2010,Kensington and Chelsea)\nAge group M 27 has a population of 1.02% for (2011,Kensington and Chelsea)\nAge group M 28 has a population of 1.00% for (2012,Kensington and Chelsea)\nAge group M 29 has a population of 1.04% for (2013,Kensington and Chelsea)\nAge group M 30 has a population of 1.00% for (2014,Kensington and Chelsea)  Performing similar analysis but this time searching for the last entry in each row which exceeds a population \nweight of 1.0% we get the following results.  The code for this is identical to above, except we call row.last()  passing in the same predicate.  Age group F 85 has a population of 1.13% for (1999,Kensington and Chelsea)\nAge group F 85 has a population of 1.10% for (2000,Kensington and Chelsea)\nAge group F 35 has a population of 1.01% for (2001,Kensington and Chelsea)\nAge group F 35 has a population of 1.01% for (2002,Kensington and Chelsea)\nAge group F 35 has a population of 1.01% for (2003,Kensington and Chelsea)\nAge group F 35 has a population of 1.03% for (2004,Kensington and Chelsea)\nAge group F 34 has a population of 1.03% for (2005,Kensington and Chelsea)\nAge group F 34 has a population of 1.04% for (2006,Kensington and Chelsea)\nAge group F 35 has a population of 1.01% for (2007,Kensington and Chelsea)\nAge group F 34 has a population of 1.01% for (2008,Kensington and Chelsea)\nAge group F 34 has a population of 1.01% for (2009,Kensington and Chelsea)\nAge group F 34 has a population of 1.02% for (2010,Kensington and Chelsea)\nAge group F 34 has a population of 1.02% for (2011,Kensington and Chelsea)\nAge group F 33 has a population of 1.12% for (2012,Kensington and Chelsea)\nAge group F 34 has a population of 1.07% for (2013,Kensington and Chelsea)\nAge group F 35 has a population of 1.03% for (2014,Kensington and Chelsea)",
            "title": "Row / Column Value"
        },
        {
            "location": "/frame/finding/#min-max",
            "text": "",
            "title": "Min &amp; Max"
        },
        {
            "location": "/frame/finding/#frame-minmax",
            "text": "Finding the  mininum  or  maximum  value across the entire frame or within a given row or column subject\nto some condition is trivial. Consider the example below whereby we wish to find the  gender ,  age group  \nand  year  that contains the largest population weight, but only in the Borough of  Islington . The code \nbelow achieves this by calling the  max()  function on the ONS dataset with an appropriate predicate. The \npredicate defines the condition that we only want to consider values associated with the Islington Borough, \nand represent a male or female population weight, and which has a value > 0. This example also demonstrates \nthe useful accessor methods on the  DataFrameValue  interface, such as efficiently accessing the row or column \nobject associated  with the value.   DataFrame<Tuple,String> frame = DemoData.loadPopulationDatasetWeights();\nframe.max(v ->\n    v.row().getValue(\"Borough\").equals(\"Islington\") &&\n    v.colKey().matches(\"[MF]\\\\s+\\\\d+\") &&\n    v.getDouble() > 0\n).ifPresent(max -> {\n    int year = max.rowKey().item(0);\n    String group = max.colKey();\n    double weight = max.getDouble() * 100;\n    String borough = max.row().getValue(\"Borough\");\n    System.out.printf(\"Max population is %.2f%% for age group %s in %s, %s\", weight, group, borough, year);\n});  Max population is 1.57% for age group F 27 in Islington, 2012  The analog of the above example to find the minimim value would simply involve a call to the  DataFrame.min()  method.",
            "title": "Frame Min/Max"
        },
        {
            "location": "/frame/finding/#rowcolumn-vector-minmax",
            "text": "Finding the row or column that represents some minimum or maximum given some user specified  Comparator  is also\nsupported by the  DataFrameAxis  API. While this capability requires a linear search, it is much faster than sorting\nthe entire  DataFrame  according to the  Comparator . In addition, this method supports  parallel  execution which\ncan boost performance on very large frames, or when the  Comparator  is expensive to execute.  The example below uses the ONS population dataset to find the row that has the highest ratio of female to male\nbirths, and prints the results to standard out. The  max()  method returns an  Optional<DataFrameRow>  or an Optional<DataFrameColumn>  when operating on rows or columns respectively, which provides access to the\ncoordinates and data associated with the result.    final DataFrame<Tuple,String> frame = DemoData.loadPopulationDatasetWeights();\nframe.rows().max((row1, row2) -> {\n    double ratio1 = row1.getDouble(\"F 0\") / row1.getDouble(\"M 0\");\n    double ratio2 = row2.getDouble(\"F 0\") / row2.getDouble(\"M 0\");\n    return Double.compare(ratio1, ratio2);\n}).ifPresent(row -> {\n    Tuple rowKey = row.key();\n    double males = row.getDouble(\"M 0\") * 100d;\n    double females = row.getDouble(\"F 0\") * 100d;\n    IO.printf(\"Largest female / male births = %.2f%%/%.2f%% for %s\", females, males, rowKey);\n});  Largest female / male births = 0.46%/0.19% for (2009,City of London)  The smallest ratio between female / male births can be found simply by calling  DataFrame.rows().min()  with\nthe same  Comparator  as above. Furthermore, this API is completely symmetrical in the row and column dimension,\nso finding the minimum or maximum column follows the same approach by call  DataFrame.cols() .",
            "title": "Row/Column Vector Min/Max"
        },
        {
            "location": "/frame/finding/#rowcolumn-value-minmax",
            "text": "It is often useful to be able to locate the minimum or maximum value in a specific row or column. While you\ncould use the frame level  min()  and  max()  methods with a predicate that restricts the search space to a \nspecific row or column, this would not be particuarly efficient as you would still need to iterate over all \nvalues in the frame.  The following examples show how one could select a specific row or column using the  rowAt()  or  colAt()  \nfunctions, and then use the  min()  or  max()  functions to search just within the desired vector. Overloaded \nversions of these methods are provided, one which takes no arguments, and the other takes a predicate. The code \nbelow demonstrates how to find the maximum population bucket for the Borough of Islingtom in year 2000.    Tuple rowKey = Tuple.of(2000, \"Islington\");\nDataFrame<Tuple,String> frame = DemoData.loadPopulationDatasetWeights();\nframe.rowAt(rowKey).max(v -> v.colKey().matches(\"[MF]\\\\s+\\\\d+\") && v.getDouble() > 0).ifPresent(max -> {\n    String group = max.colKey();\n    int year = max.rowKey().item(0);\n    String borough = max.rowKey().item(1);\n    double weight = max.getDouble() * 100d;\n    System.out.printf(\"Max population weight for %s in %s is %.2f%% for %s\", borough, year, weight, group);\n});  Max population weight for Islington in 2000 is 1.30% for F 26  Similarly, the following code illustrates how to find which Borough out of  Islington ,  Wandsworth  and  Kensington and Chelsea  has the largest female population aged 30 and in what year this occurred. In this\ncase we select the column named  F 30  and find the max value subject to the condition that the Borough is \npart of our constrained universe.   Set<String> boroughs = Collect.asSet(\"Islington\", \"Wandsworth\", \"Kensington and Chelsea\");\nDataFrame<Tuple,String> frame = DemoData.loadPopulationDatasetWeights();\nframe.colAt(\"F 30\").max(v -> boroughs.contains((String)v.row().getValue(\"Borough\"))).ifPresent(max -> {\n    int year = max.rowKey().item(0);\n    double weight = max.getDouble() * 100d;\n    String borough = max.row().getValue(\"Borough\");\n    System.out.printf(\"Max female population weight aged 30 is %.2f%% in %s, %s\", weight, borough, year);\n});  Max female population weight aged 30 is 1.69% in Wandsworth, 2012",
            "title": "Row/Column Value Min/Max"
        },
        {
            "location": "/frame/finding/#binary-search",
            "text": "The  DataFrameVector  interface exposes 3 overloaded  binarySearch()  methods useful in locating an entry that\nmatches a specific value. The search can be optionally restricted to a subset of the vector by providing an offset \nordinal and length, and an optional  Comparator  if the data is sorted according to some custom logic.   Naturally the data in the vector must be sorted in ascending order, otherwise the behaviour of these methods is undefined. \nThe  binarySearch()  methods return an  Optional<DataFrameValue<R,C>>  containing the matched value, or an empty\noptional if no match. On a successful match, the  DataFrameValue<R,C>  interace can be used to access the coordinates\nof the matched value.  In the example below, we perform a binary search for a pre-selected value in the  F 30  column, and then assert\nthat to located value and row key matches what we expect. Note that we sort the  DataFrame  rows so that data in the F 30  column is sorted in ascending order first.   final Tuple expected = Tuple.of(2000, \"Islington\");\nfinal DataFrame<Tuple,String> frame = DemoData.loadPopulationDatasetWeights();\nfinal double weight = frame.data().getDouble(expected, \"F 30\");\nframe.rows().sort(true, \"F 30\"); //Ensure data is sorted\nframe.colAt(\"F 30\").binarySearch(weight).ifPresent(value -> {\n    assert(value.rowKey().equals(expected));\n    assert(value.getDouble() == weight);\n});  The same functionality is available in the row dimension as these methods are exposed on the  DataFrameVector  interface\nwhich is the super interface of  DataFrameRow  and  DataFrameColumn .",
            "title": "Binary Search"
        },
        {
            "location": "/frame/finding/#lower-higher",
            "text": "Morpheus also provides a useful API for locating the nearest row or column  key  before or after some specified value, \neven if that value does not exist in the axis in question. This functionality is contingent on the data in the axis being \nsorted in ascending order as it is implemented internally using a binary search, but is therefore also fast even for a \nvery large axis. Consider a  DataFrame  of random values constructed with a row axis representing month end  LocalDate \nvalues, which is built as follows:   LocalDate start = LocalDate.of(2014, 1, 1);\nRange<String> columns = Range.of(0, 10).map(i -> \"Column-\" + i);\nRange<LocalDate> monthEnds = Range.of(0, 12).map(i -> start.plusMonths(i+1).minusDays(1));\nDataFrame<LocalDate,String> frame = DataFrame.ofDoubles(monthEnds, columns, v -> Math.random() * 100d);\nframe.out().print(formats -> {\n    formats.withDecimalFormat(Double.class, \"0.00;-0.00\", 1);\n});  \n   Index     |  Column-0  |  Column-1  |  Column-2  |  Column-3  |  Column-4  |  Column-5  |  Column-6  |  Column-7  |  Column-8  |  Column-9  |\n------------------------------------------------------------------------------------------------------------------------------------------------\n 2014-01-31  |     19.77  |     79.79  |     54.33  |     72.15  |     32.47  |     48.82  |     40.26  |     85.37  |     33.05  |     38.18  |\n 2014-02-28  |     37.88  |     62.38  |      5.54  |     11.75  |     19.53  |     71.03  |      6.66  |     33.03  |     74.29  |     11.47  |\n 2014-03-31  |      3.33  |     12.06  |     67.68  |     22.46  |     30.11  |     11.07  |     73.94  |     43.57  |     44.48  |     33.34  |\n 2014-04-30  |     57.28  |     99.35  |     51.99  |     53.83  |     74.58  |     14.18  |     85.94  |     91.67  |     52.23  |     34.07  |\n 2014-05-31  |     81.53  |     82.24  |     71.24  |     89.30  |      7.24  |     75.58  |     74.63  |     18.16  |      7.31  |     71.79  |\n 2014-06-30  |     98.89  |     34.17  |     92.35  |     44.58  |      6.92  |     53.81  |     30.58  |     30.46  |     14.15  |     34.93  |\n 2014-07-31  |     23.99  |     75.76  |     93.12  |     65.59  |     23.22  |     83.26  |     10.02  |     99.18  |     11.73  |     43.27  |\n 2014-08-31  |     43.93  |      9.30  |      6.24  |     18.98  |     28.80  |     42.37  |     37.73  |     74.29  |      8.99  |     78.51  |\n 2014-09-30  |     98.85  |     45.26  |     34.00  |     35.86  |     78.51  |     49.17  |     56.03  |     61.06  |      5.50  |     55.80  |\n 2014-10-31  |      5.48  |     10.46  |     65.42  |     20.80  |     49.96  |     21.22  |     76.53  |     56.08  |     11.83  |     28.27  |\n 2014-11-30  |     84.84  |     37.78  |     95.47  |     42.30  |     20.87  |     89.41  |     12.20  |     27.29  |     64.71  |     65.12  |\n 2014-12-31  |     22.34  |     93.66  |     97.76  |     45.17  |     39.16  |     71.14  |     31.19  |     65.24  |     46.66  |     88.43  |  This kind of dataset may be typical of an experiment that captures monthly values and reports them at month end. Some \nhypothetical analysis involving this dataset may assume that for any date in February 2014 for example, the prior row applies,\nand therefore we need to access the entry for 2014-01-31 (to avoid peak ahead). In this scenario, we know the dataset is \nreported at month end in all cases, so we could code up a rule such that when requesting a value for any given date, we adjust \nthat date to the prior month-end. What if the data is reported at some random dates within the month rather than always at month \nend? In that case we would have to perform a linear search in the worst case to find an appropriate row.  The  DataFrameAxis  interface exposes a  lowerKey()  and  higherKey()  method that provides a fast mechanism to solve\nthis problem. On the assumption that the keys are sorted, calling  lowerKey()  with some date will yield an  Optional  \nmatch to the previous key. So in the context of the frame above, requesting the lower key of  2014-02-15  would yield 2014-01-31 . Similarly, calling  higherKey()  with the same argument would yield  2014-02-28 .  The example below iterates over the first 35 days of the  DataFrame  row axis at a  daily  frequency, and attempts\nto locate the previous row key. Since we know what to expect in this example, we can make some assertions on the resulting\nkeys returned from a call to  lowerKey() , which serves as a usuful unit test.   //Iterate over first 35 days of row axis at daily frequency\nRange<LocalDate> dates = Range.of(monthEnds.start(), monthEnds.start().plusDays(35), Period.ofDays(1));\ndates.forEach(date -> {\n    if (frame.rows().contains(date)) {\n        IO.println(\"Exact match for: \" + date);\n    } else {\n        Optional<LocalDate> lowerKey = frame.rows().lowerKey(date);\n        assert(lowerKey.isPresent());\n        assert(lowerKey.get().equals(date.withDayOfMonth(1).minusDays(1)));\n        IO.printf(\"Lower match for %s is %s%n\", date, lowerKey.get());\n    }\n});  \nExact match for: 2014-01-31\nLower match for 2014-02-01 is 2014-01-31\nLower match for 2014-02-02 is 2014-01-31\nLower match for 2014-02-03 is 2014-01-31\nLower match for 2014-02-04 is 2014-01-31\nLower match for 2014-02-05 is 2014-01-31\nLower match for 2014-02-06 is 2014-01-31\nLower match for 2014-02-07 is 2014-01-31\nLower match for 2014-02-08 is 2014-01-31\nLower match for 2014-02-09 is 2014-01-31\nLower match for 2014-02-10 is 2014-01-31\nLower match for 2014-02-11 is 2014-01-31\nLower match for 2014-02-12 is 2014-01-31\nLower match for 2014-02-13 is 2014-01-31\nLower match for 2014-02-14 is 2014-01-31\nLower match for 2014-02-15 is 2014-01-31\nLower match for 2014-02-16 is 2014-01-31\nLower match for 2014-02-17 is 2014-01-31\nLower match for 2014-02-18 is 2014-01-31\nLower match for 2014-02-19 is 2014-01-31\nLower match for 2014-02-20 is 2014-01-31\nLower match for 2014-02-21 is 2014-01-31\nLower match for 2014-02-22 is 2014-01-31\nLower match for 2014-02-23 is 2014-01-31\nLower match for 2014-02-24 is 2014-01-31\nLower match for 2014-02-25 is 2014-01-31\nLower match for 2014-02-26 is 2014-01-31\nLower match for 2014-02-27 is 2014-01-31\nExact match for: 2014-02-28\nLower match for 2014-03-01 is 2014-02-28\nLower match for 2014-03-02 is 2014-02-28\nLower match for 2014-03-03 is 2014-02-28\nLower match for 2014-03-04 is 2014-02-28\nLower match for 2014-03-05 is 2014-02-28\nLower match for 2014-03-06 is 2014-02-28",
            "title": "Lower &amp; Higher"
        },
        {
            "location": "/frame/writing/",
            "text": "Writing Data\n\n\nIntroduction\n\n\nThe Morpheus library provides a simple yet powerful API to write a \nDataFrame\n to an output device for persistent\nstorage, which can then be later read back into memory. Currently there is built in support for writing a frame out \nto CSV and JSON formats, as well as to SQL databases (current support is limited to MYSQL, Microsoft SQL Server, HSQLDB \nand Sqlite). It is easy to write a custom sink which integrates naturally with the Morpheus API, and an example of this \nis provided in a later section below.\n\n\nThe \nDataFrameWrite\n interface which can be accessed by calling \nwrite()\n on a frame instance, provides format specific \noutput methods, as well as a generalized method for user defined sinks. The following sections cover CSV, JSON and database\noutput, as well as a custom sink that records a frame as a Java serialized object. For the examples in this section, \nconsider the following 10x7 containing various data types for the columns to illustrate how these can be mapped \nto the output.\n\n\n\n\n\nfinal LocalDate start = LocalDate.of(2014, 1, 1);\nfinal Range<LocalDate> rowKeys = Range.of(start, start.plusDays(10));\nDataFrame<LocalDate,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    columns.add(\"Column-0\", Boolean.class, v -> Math.random() > 0.5d);\n    columns.add(\"Column-1\", Double.class, v -> Math.random() * 10d);\n    columns.add(\"Column-2\", LocalTime.class, v -> LocalTime.now().plusMinutes(v.rowOrdinal()));\n    columns.add(\"Column-3\", Month.class, v -> Month.values()[v.rowOrdinal()+1]);\n    columns.add(\"Column-4\", Integer.class, v -> (int)(Math.random() * 10));\n    columns.add(\"Column-5\", String.class, v -> String.format(\"(%s,%s)\", v.rowOrdinal(), v.colOrdinal()));\n    columns.add(\"Column-6\", LocalDateTime.class, v -> LocalDateTime.now().plusMinutes(v.rowOrdinal()));\n});\n\n\n\n\n\n   Index     |  Column-0  |   Column-1   |    Column-2    |  Column-3   |  Column-4  |  Column-5  |         Column-6          |\n-------------------------------------------------------------------------------------------------------------------------------\n 2014-01-01  |     false  |  1.25672365  |   21:00:13.73  |   FEBRUARY  |         4  |     (0,5)  |  2014-08-02T21:00:13.759  |\n 2014-01-02  |      true  |  4.41265583  |  21:01:13.732  |      MARCH  |         3  |     (1,5)  |  2014-08-02T21:01:13.759  |\n 2014-01-03  |     false  |  7.82861945  |  21:02:13.732  |      APRIL  |         4  |     (2,5)  |  2014-08-02T21:02:13.759  |\n 2014-01-04  |      true  |  1.32584627  |  21:03:13.732  |        MAY  |         4  |     (3,5)  |  2014-08-02T21:03:13.759  |\n 2014-01-05  |     false  |  0.55894277  |  21:04:13.732  |       JUNE  |         2  |     (4,5)  |  2014-08-02T21:04:13.759  |\n 2014-01-06  |     false  |  3.52911319  |  21:05:13.732  |       JULY  |         2  |     (5,5)  |  2014-08-02T21:05:13.759  |\n 2014-01-07  |      true  |  1.12132289  |  21:06:13.732  |     AUGUST  |         6  |     (6,5)  |  2014-08-02T21:06:13.759  |\n 2014-01-08  |     false  |  6.78505742  |  21:07:13.732  |  SEPTEMBER  |         8  |     (7,5)  |  2014-08-02T21:07:13.759  |\n 2014-01-09  |     false  |  2.79950377  |  21:08:13.732  |    OCTOBER  |         4  |     (8,5)  |  2014-08-02T21:08:13.759  |\n 2014-01-10  |     false  |  6.16985695  |  21:09:13.732  |   NOVEMBER  |         4  |     (9,5)  |  2014-08-02T21:09:13.759  |\n\n\n\n\nCSV\n\n\nStoring tabular data as CSV text is common practice, so it is only natural that a \nDataFrame\n can be written out in \nthis format. The Morpheus library ships with a default CSV output handler called \nCsvSink\n, however it is possible to \nwrite a custom CSV sink if necessary. The example below writes the \nDataFrame\n presented in the introduction to a CSV \nfile with various customizations applied via the \nCsvSinkOptions\n object. The customizations are as follows:\n\n\n1. Set the separator, which defulats to a comma.\n2. Set whether the row keys are included in output\n3. Set whether the column keys are included in output\n4. Set the text to print for null values\n5. Set the title for the row key column\n6. Set a custom Printer to format row keys (otherwise defaults to Object.toString())\n7. Set a custim Printer to format column keys (otherwise defaults to Object.toString())\n8. Set type specific and column specific Printers\n\n\n\n\n\n\nframe.write().csv(options -> {\n    options.setFile(\"DataFrame-1.csv\");\n    options.setSeparator(\",\");\n    options.setIncludeRowHeader(true);\n    options.setIncludeColumnHeader(true);\n    options.setNullText(\"null\");\n    options.setTitle(\"Date\");\n\n    DateTimeFormatter dateFormat = DateTimeFormatter.ofPattern(\"dd/MMM/yyyy\");\n    options.setRowKeyPrinter(Printer.ofLocalDate(dateFormat));\n\n    options.setFormats(formats -> {\n        DateTimeFormatter timeFormat = DateTimeFormatter.ofPattern(\"HH:mm\");\n        DateTimeFormatter dateTimeFormat = DateTimeFormatter.ofPattern(\"dd-MMM-yyyy HH:mm\");\n        formats.setDecimalFormat(Double.class, \"0.00##;-0.00##\", 1);\n        formats.setPrinter(\"Column-2\", Printer.ofLocalTime(timeFormat));\n        formats.<Month>setPrinter(\"Column-3\", Printer.forObject(m -> m.name().toLowerCase()));\n        formats.setPrinter(\"Column-6\", Printer.ofLocalDateTime(dateTimeFormat));\n    });\n});\n\n\n\n\nThe resulting file output is shown below. Note how custom column formats are configured above using either a data type, \nor with an explicit column name. The \nCsvSink\n will attempt to look up a printer for the column name first, and if no\nprinter is registered, it will fall back onto a printer for the data type associated with the column. In addition,\nan explicit printer can be configured for the row keys, and we can control whether the row header and column header \nare included in the output, which by default they are.\n\n\n\nDate,Column-0,Column-1,Column-2,Column-3,Column-4,Column-5,Column-6\n2014-01-01,false,1.2567,21:00,february,4,(0,5),02-Aug-2014 21:00\n2014-01-02,true,4.4127,21:01,march,3,(1,5),02-Aug-2014 21:01\n2014-01-03,false,7.8286,21:02,april,4,(2,5),02-Aug-2014 21:02\n2014-01-04,true,1.3258,21:03,may,4,(3,5),02-Aug-2014 21:03\n2014-01-05,false,0.5589,21:04,june,2,(4,5),02-Aug-2014 21:04\n2014-01-06,false,3.5291,21:05,july,2,(5,5),02-Aug-2014 21:05\n2014-01-07,true,1.1213,21:06,august,6,(6,5),02-Aug-2014 21:06\n2014-01-08,false,6.7851,21:07,september,8,(7,5),02-Aug-2014 21:07\n2014-01-09,false,2.7995,21:08,october,4,(8,5),02-Aug-2014 21:08\n2014-01-10,false,6.1699,21:09,november,4,(9,5),02-Aug-2014 21:09\n\n\n\n\nThe \nCsvSinkOptions\n allows the output resource to be specified as either a \nFile\n, \nURL\n or an \nOutputStream\n. \nThe latter is the most flexible in that a user has full control of how the output is written. For example, consider the \nsame frame as above, but in this case we wish to write to a URL endpoint via http POST, expressing the frame as GZIP \ncompressed CSV. In order to do this, we write the frame to a \nByteArrayOutputStream\n, and then perform the http POST with\nthe resulting bytes.\n\n\n\n\n\nByteArrayOutputStream baos = new ByteArrayOutputStream();\nframe.write().csv(options -> {\n    try {\n        options.setOutputStream(new GZIPOutputStream(baos));\n        options.setSeparator(\",\");\n        options.setIncludeRowHeader(true);\n        options.setIncludeColumnHeader(true);\n        options.setNullText(\"null\");\n        options.setTitle(\"Date\");\n        options.setRowKeyPrinter(LocalDate::toString);\n        options.setFormats(formats -> {\n            DateTimeFormatter timeFormat = DateTimeFormatter.ofPattern(\"HH:mm\");\n            DateTimeFormatter dateTimeFormat = DateTimeFormatter.ofPattern(\"dd-MMM-yyyy HH:mm\");\n            formats.setDecimalFormat(Double.class, \"0.00##;-0.00##\", 1);\n            formats.<Month>setPrinter(\"Column-2\", v -> v.name().toLowerCase());\n            formats.<LocalTime>setPrinter(\"Column-1\", timeFormat::format);\n            formats.<LocalDateTime>setPrinter(\"Column-5\", dateTimeFormat::format);\n        });\n    } catch (IOException ex) {\n        throw new RuntimeException(ex.getMessage(), ex);\n    }\n});\n\nfinal byte[] bytes = baos.toByteArray();\nHttpClient.getDefault().doPost(post -> {\n    post.setRetryCount(5);\n    post.setReadTimeout(5000);\n    post.setConnectTimeout(1000);\n    post.setUrl(\"http://www.domain.con/test\");\n    post.setContent(bytes);\n    post.setContentType(\"application/x-gzip\");\n    post.setContentLength(bytes.length);\n    post.setResponseHandler((status, stream) -> {\n        if (status.getCode() == 200) {\n            return Optional.empty();\n        } else {\n            throw new RuntimeException(\"Failed with response: \" + status);\n        }\n    });\n});\n\n\n\n\nJSON\n\n\nWriting a \nDataFrame\n out in JSON format works similar to the CSV example above, however the output options differ\nslightly. If we use the same 10x7 frame introduced earlier, we can write out JSON with custom formatting as shown below. \n\n\nMeta-data in the payload is included to allow a subsequent reader to re-constitute the frame with the appropriate data \ntypes. For example, \nInteger\n and \nLong\n types are not distinguishable in standard JSON output, but the \ndataType\n field \nwhich is written as a JSON property for each column object definition, provides the required information. Other types, \nlike \nLocalTime\n, and \nLocalDateTime\n in this example are formatted into a string which can be controlled via the options \n\nFormats\n entity, much the same way as CSV formatting works. The data type property for objects like these is mapped\nto the simple name of the class.\n\n\n\n\n\nframe.write().json(options -> {\n    options.setFile(\"DataFrame-1.json\");\n    options.setEncoding(\"UTF-8\");\n    options.setFormats(formats -> {\n        DateTimeFormatter timeFormat = DateTimeFormatter.ofPattern(\"HH:mm\");\n        DateTimeFormatter dateTimeFormat = DateTimeFormatter.ofPattern(\"dd-MMM-yyyy HH:mm\");\n        formats.setPrinter(\"Column-2\", Printer.ofLocalTime(timeFormat));\n        formats.<Month>setPrinter(\"Column-3\", Printer.forObject(m -> m.name().toLowerCase()));\n        formats.setPrinter(\"Column-6\", Printer.ofLocalDateTime(dateTimeFormat));\n    });\n});\n\n\n\n\nThe resulting JSON output is shown below. It should be noted that the \nJsonSink\n implementation that ships with Morpheus\nuses a stream based JsonWriter, making it very memory efficient even for extremely large frames. The same applies in reverse\nwith the \nJsonSource\n implementation. Finally, the JSON payload includes the row and column count for the frame which is\nimportant to enable the \nJsonSource\n to pre-allocate the arrays when reading, which again, improves performance for very \nlarge frames.\n\n\n{\n  \"DataFrame\": {\n    \"rowCount\": 10,\n    \"colCount\": 7,\n    \"rowKeys\": {\n      \"type\": \"LocalDate\",\n      \"values\": [\n        \"2014-01-01\",\n        \"2014-01-02\",\n        \"2014-01-03\",\n        \"2014-01-04\",\n        \"2014-01-05\",\n        \"2014-01-06\",\n        \"2014-01-07\",\n        \"2014-01-08\",\n        \"2014-01-09\",\n        \"2014-01-10\"\n      ]\n    },\n    \"columns\": [\n      {\n        \"key\": \"Column-0\",\n        \"keyType\": \"String\",\n        \"dataType\": \"Boolean\",\n        \"defaultValue\": false,\n        \"values\": [\n          true,\n          true,\n          true,\n          true,\n          false,\n          false,\n          true,\n          false,\n          true,\n          false\n        ]\n      },\n      {\n        \"key\": \"Column-1\",\n        \"keyType\": \"String\",\n        \"dataType\": \"Double\",\n        \"defaultValue\": null,\n        \"values\": [\n          3.4243295493435575,\n          1.1438780283716343,\n          4.316506413470317,\n          7.454588915346636,\n          6.950360875904566,\n          9.491531171790896,\n          5.204554855321635,\n          5.754177059739899,\n          0.8195736745281579,\n          6.988186292016216\n        ]\n      },\n      {\n        \"key\": \"Column-2\",\n        \"keyType\": \"String\",\n        \"dataType\": \"LocalTime\",\n        \"defaultValue\": null,\n        \"values\": [\n          \"21:24\",\n          \"21:25\",\n          \"21:26\",\n          \"21:27\",\n          \"21:28\",\n          \"21:29\",\n          \"21:30\",\n          \"21:31\",\n          \"21:32\",\n          \"21:33\"\n        ]\n      },\n      {\n        \"key\": \"Column-3\",\n        \"keyType\": \"String\",\n        \"dataType\": \"Month\",\n        \"defaultValue\": null,\n        \"values\": [\n          \"february\",\n          \"march\",\n          \"april\",\n          \"may\",\n          \"june\",\n          \"july\",\n          \"august\",\n          \"september\",\n          \"october\",\n          \"november\"\n        ]\n      },\n      {\n        \"key\": \"Column-4\",\n        \"keyType\": \"String\",\n        \"dataType\": \"Integer\",\n        \"defaultValue\": 0,\n        \"values\": [\n          9,\n          9,\n          5,\n          7,\n          0,\n          2,\n          8,\n          5,\n          5,\n          0\n        ]\n      },\n      {\n        \"key\": \"Column-5\",\n        \"keyType\": \"String\",\n        \"dataType\": \"String\",\n        \"defaultValue\": null,\n        \"values\": [\n          \"(0,5)\",\n          \"(1,5)\",\n          \"(2,5)\",\n          \"(3,5)\",\n          \"(4,5)\",\n          \"(5,5)\",\n          \"(6,5)\",\n          \"(7,5)\",\n          \"(8,5)\",\n          \"(9,5)\"\n        ]\n      },\n      {\n        \"key\": \"Column-6\",\n        \"keyType\": \"String\",\n        \"dataType\": \"LocalDateTime\",\n        \"defaultValue\": null,\n        \"values\": [\n          \"02-Aug-2014 21:24\",\n          \"02-Aug-2014 21:25\",\n          \"02-Aug-2014 21:26\",\n          \"02-Aug-2014 21:27\",\n          \"02-Aug-2014 21:28\",\n          \"02-Aug-2014 21:29\",\n          \"02-Aug-2014 21:30\",\n          \"02-Aug-2014 21:31\",\n          \"02-Aug-2014 21:32\",\n          \"02-Aug-2014 21:33\"\n        ]\n      }\n    ]\n  }\n}\n\n\n\n\nSQL\n\n\nThe third built-in sink that ships with the Morpheus library enables frames to be written out to a SQL database, with\nsupport for \nMYSQL\n, \nHSQLDB\n, \nSQLite\n, \n\nH2 Database\n and \nMicrosoft SQL Server\n.\n\n\nThe \nDbSink\n can be used to write a \nDataFrame\n to an existing table, or otherwise have a table automatically created if \nnone exists. Obviously the latter scenario requires that the authenticated user has adequate permissions to create database\nobjects, otherwise an exception will be raised. The \nDbSinkOptions\n class allows some control for how the frame columns are \nmapped and stored in the database, both in terms of column names and column types.\n\n\nConsider the code example below where we write the same 10x7 \nDataFrame\n presented in the introduction to an HSQLDB instance.\nIn this scenario, we pass the JDBC URL and credentials to the \nDbSinkOptions\n, however the connection can be passed via a\n\njava.sql.DataSource\n or a \njava.sql.Connection\n object. For large frames, it is important to leverage SQL batch inserts, and\nthe batch size can be controlled via \nDbSinkOptions\n. Finally, the code below also demonstrates how to map the \nDataFrame\n\ncolumn keys to database column names, and also how to map types. While built in support exists for \nLocalDate\n and \nEnum\n\ntypes, the example explicitly sets a custom mapping for illustration purposes.\n\n\n\n\n\nClass.forName(\"org.hsqldb.jdbcDriver\");\nframe.write().db(options -> {\n    options.setBatchSize(1000);\n    options.setTableName(\"TestTable\");\n    options.setConnection(\"jdbc:hsqldb:/Users/witdxav/morpheus/tests/DataFrame_3.db\", \"sa\", null);\n    options.setRowKeyMapping(\"Date\", Date.class, Function1.toValue(Date::valueOf));\n    options.setColumnMappings(mappings -> {\n        mappings.add(Month.class, String.class, Function1.toValue(v -> {\n            return v.<Month>getValue().name().toLowerCase();\n        }));\n        mappings.add(LocalDate.class, java.sql.Date.class, Function1.toValue(v -> {\n            return Date.valueOf(v.<LocalDate>getValue());\n        }));\n    });\n    options.setColumnNames(colKey -> {\n        switch (colKey) {\n            case \"Column-1\":    return \"Column-A\";\n            case \"Column-2\":    return \"Column-B\";\n            case \"Column-3\":    return \"Column-C\";\n            default:            return colKey;\n        }\n    });\n});\n\n\n\n\nCustom Sink\n\n\nWriting a custom sink is fairly trivial, and involves implementing two classes, namely an implementation of the \nDataFrameSink\n\ninterface, and a correponding options class that describes how the frame should be output. In this section, we show how one could\nwrite a custom sink to record a \nDataFrame\n as a serialized object using standard java object serialization. The code below shows\nan implementation of a \nDataFrameSink\n which has been called \nCustomSink\n for lack of a better name.\n\n\n\n\n\npublic class CustomSink<R,C> implements DataFrameSink<R,C,CustomSinkOptions> {\n    @Override\n    public void write(DataFrame<R,C> frame, Consumer<CustomSinkOptions> configurator) {\n        final CustomSinkOptions options = Initialiser.apply(new CustomSinkOptions(), configurator);\n        ObjectOutputStream os = null;\n        try {\n            if (options.isCompressed()) {\n                os = new ObjectOutputStream(new GZIPOutputStream(options.getOutput()));\n                os.writeObject(frame);\n            } else {\n                os = new ObjectOutputStream(options.getOutput());\n                os.writeObject(frame);\n            }\n        } catch (Exception ex) {\n            throw new DataFrameException(\"Failed to write DataFrame to serialized output\", ex);\n        } finally {\n            IO.close(os);\n        }\n    }\n}\n\n\n\n\nThis class implements a single \nwrite()\n method which takes the frame to record, and a \nConsumer\n that is used to configure\nthe output options. The options class is usually sink specific, and in this example we have written a class called \nCustomSinkOptions\n\nwhich carries the \nOutputStream\n to write to, and a flag to indicate whether output should be compressed. The options class\nlooks as follows.`\n\n\n\n\n\npublic class CustomSinkOptions {\n\n    private OutputStream output;\n    private boolean compressed;\n\n    /**\n     * Returns the output stream to write to\n     * @return      the output stream to write to\n     */\n    public OutputStream getOutput() {\n        return output;\n    }\n\n    /**\n     * Returns true if compression is enabled\n     * @return  true if compression is enabled\n     */\n    public boolean isCompressed() {\n        return compressed;\n    }\n\n    /**\n     * Sets the output for these options\n     * @param output    the output stream to write to\n     */\n    public void setOutput(OutputStream output) {\n        this.output = output;\n    }\n\n    /**\n     * Sets the file output for these options\n     * @param file  the output file\n     */\n    public void setFile(File file) {\n        this.output = Try.call(() -> new BufferedOutputStream(new FileOutputStream(file)));\n    }\n\n    /**\n     * Sets whether the output should be wrapped in a GZIP stream\n     * @param compressed    true to wrap output in GZIP compression\n     */\n    public void setCompressed(boolean compressed) {\n        this.compressed = compressed;\n    }\n}\n\n\n\n\nTo use this sink to write a frame, consider the following code using the same frame from the introduction. In this\ncase we create an instance of the sink to write to, and implement a \nConsumer\n to configure the sink specific options\nas shown. which includes setting the output file and enabling GZIP compression.\n\n\n\n\n\nframe.write().to(new CustomSink<>(), options -> {\n    options.setFile(new File(\"/Users/witdxav/test/DataFrame-1.gzip\"));\n    options.setCompressed(true);\n});",
            "title": "Writing"
        },
        {
            "location": "/frame/writing/#writing-data",
            "text": "",
            "title": "Writing Data"
        },
        {
            "location": "/frame/writing/#introduction",
            "text": "The Morpheus library provides a simple yet powerful API to write a  DataFrame  to an output device for persistent\nstorage, which can then be later read back into memory. Currently there is built in support for writing a frame out \nto CSV and JSON formats, as well as to SQL databases (current support is limited to MYSQL, Microsoft SQL Server, HSQLDB \nand Sqlite). It is easy to write a custom sink which integrates naturally with the Morpheus API, and an example of this \nis provided in a later section below.  The  DataFrameWrite  interface which can be accessed by calling  write()  on a frame instance, provides format specific \noutput methods, as well as a generalized method for user defined sinks. The following sections cover CSV, JSON and database\noutput, as well as a custom sink that records a frame as a Java serialized object. For the examples in this section, \nconsider the following 10x7 containing various data types for the columns to illustrate how these can be mapped \nto the output.   final LocalDate start = LocalDate.of(2014, 1, 1);\nfinal Range<LocalDate> rowKeys = Range.of(start, start.plusDays(10));\nDataFrame<LocalDate,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    columns.add(\"Column-0\", Boolean.class, v -> Math.random() > 0.5d);\n    columns.add(\"Column-1\", Double.class, v -> Math.random() * 10d);\n    columns.add(\"Column-2\", LocalTime.class, v -> LocalTime.now().plusMinutes(v.rowOrdinal()));\n    columns.add(\"Column-3\", Month.class, v -> Month.values()[v.rowOrdinal()+1]);\n    columns.add(\"Column-4\", Integer.class, v -> (int)(Math.random() * 10));\n    columns.add(\"Column-5\", String.class, v -> String.format(\"(%s,%s)\", v.rowOrdinal(), v.colOrdinal()));\n    columns.add(\"Column-6\", LocalDateTime.class, v -> LocalDateTime.now().plusMinutes(v.rowOrdinal()));\n});  \n   Index     |  Column-0  |   Column-1   |    Column-2    |  Column-3   |  Column-4  |  Column-5  |         Column-6          |\n-------------------------------------------------------------------------------------------------------------------------------\n 2014-01-01  |     false  |  1.25672365  |   21:00:13.73  |   FEBRUARY  |         4  |     (0,5)  |  2014-08-02T21:00:13.759  |\n 2014-01-02  |      true  |  4.41265583  |  21:01:13.732  |      MARCH  |         3  |     (1,5)  |  2014-08-02T21:01:13.759  |\n 2014-01-03  |     false  |  7.82861945  |  21:02:13.732  |      APRIL  |         4  |     (2,5)  |  2014-08-02T21:02:13.759  |\n 2014-01-04  |      true  |  1.32584627  |  21:03:13.732  |        MAY  |         4  |     (3,5)  |  2014-08-02T21:03:13.759  |\n 2014-01-05  |     false  |  0.55894277  |  21:04:13.732  |       JUNE  |         2  |     (4,5)  |  2014-08-02T21:04:13.759  |\n 2014-01-06  |     false  |  3.52911319  |  21:05:13.732  |       JULY  |         2  |     (5,5)  |  2014-08-02T21:05:13.759  |\n 2014-01-07  |      true  |  1.12132289  |  21:06:13.732  |     AUGUST  |         6  |     (6,5)  |  2014-08-02T21:06:13.759  |\n 2014-01-08  |     false  |  6.78505742  |  21:07:13.732  |  SEPTEMBER  |         8  |     (7,5)  |  2014-08-02T21:07:13.759  |\n 2014-01-09  |     false  |  2.79950377  |  21:08:13.732  |    OCTOBER  |         4  |     (8,5)  |  2014-08-02T21:08:13.759  |\n 2014-01-10  |     false  |  6.16985695  |  21:09:13.732  |   NOVEMBER  |         4  |     (9,5)  |  2014-08-02T21:09:13.759  |",
            "title": "Introduction"
        },
        {
            "location": "/frame/writing/#csv",
            "text": "Storing tabular data as CSV text is common practice, so it is only natural that a  DataFrame  can be written out in \nthis format. The Morpheus library ships with a default CSV output handler called  CsvSink , however it is possible to \nwrite a custom CSV sink if necessary. The example below writes the  DataFrame  presented in the introduction to a CSV \nfile with various customizations applied via the  CsvSinkOptions  object. The customizations are as follows:  1. Set the separator, which defulats to a comma.\n2. Set whether the row keys are included in output\n3. Set whether the column keys are included in output\n4. Set the text to print for null values\n5. Set the title for the row key column\n6. Set a custom Printer to format row keys (otherwise defaults to Object.toString())\n7. Set a custim Printer to format column keys (otherwise defaults to Object.toString())\n8. Set type specific and column specific Printers   frame.write().csv(options -> {\n    options.setFile(\"DataFrame-1.csv\");\n    options.setSeparator(\",\");\n    options.setIncludeRowHeader(true);\n    options.setIncludeColumnHeader(true);\n    options.setNullText(\"null\");\n    options.setTitle(\"Date\");\n\n    DateTimeFormatter dateFormat = DateTimeFormatter.ofPattern(\"dd/MMM/yyyy\");\n    options.setRowKeyPrinter(Printer.ofLocalDate(dateFormat));\n\n    options.setFormats(formats -> {\n        DateTimeFormatter timeFormat = DateTimeFormatter.ofPattern(\"HH:mm\");\n        DateTimeFormatter dateTimeFormat = DateTimeFormatter.ofPattern(\"dd-MMM-yyyy HH:mm\");\n        formats.setDecimalFormat(Double.class, \"0.00##;-0.00##\", 1);\n        formats.setPrinter(\"Column-2\", Printer.ofLocalTime(timeFormat));\n        formats.<Month>setPrinter(\"Column-3\", Printer.forObject(m -> m.name().toLowerCase()));\n        formats.setPrinter(\"Column-6\", Printer.ofLocalDateTime(dateTimeFormat));\n    });\n});  The resulting file output is shown below. Note how custom column formats are configured above using either a data type, \nor with an explicit column name. The  CsvSink  will attempt to look up a printer for the column name first, and if no\nprinter is registered, it will fall back onto a printer for the data type associated with the column. In addition,\nan explicit printer can be configured for the row keys, and we can control whether the row header and column header \nare included in the output, which by default they are.  \nDate,Column-0,Column-1,Column-2,Column-3,Column-4,Column-5,Column-6\n2014-01-01,false,1.2567,21:00,february,4,(0,5),02-Aug-2014 21:00\n2014-01-02,true,4.4127,21:01,march,3,(1,5),02-Aug-2014 21:01\n2014-01-03,false,7.8286,21:02,april,4,(2,5),02-Aug-2014 21:02\n2014-01-04,true,1.3258,21:03,may,4,(3,5),02-Aug-2014 21:03\n2014-01-05,false,0.5589,21:04,june,2,(4,5),02-Aug-2014 21:04\n2014-01-06,false,3.5291,21:05,july,2,(5,5),02-Aug-2014 21:05\n2014-01-07,true,1.1213,21:06,august,6,(6,5),02-Aug-2014 21:06\n2014-01-08,false,6.7851,21:07,september,8,(7,5),02-Aug-2014 21:07\n2014-01-09,false,2.7995,21:08,october,4,(8,5),02-Aug-2014 21:08\n2014-01-10,false,6.1699,21:09,november,4,(9,5),02-Aug-2014 21:09  The  CsvSinkOptions  allows the output resource to be specified as either a  File ,  URL  or an  OutputStream . \nThe latter is the most flexible in that a user has full control of how the output is written. For example, consider the \nsame frame as above, but in this case we wish to write to a URL endpoint via http POST, expressing the frame as GZIP \ncompressed CSV. In order to do this, we write the frame to a  ByteArrayOutputStream , and then perform the http POST with\nthe resulting bytes.   ByteArrayOutputStream baos = new ByteArrayOutputStream();\nframe.write().csv(options -> {\n    try {\n        options.setOutputStream(new GZIPOutputStream(baos));\n        options.setSeparator(\",\");\n        options.setIncludeRowHeader(true);\n        options.setIncludeColumnHeader(true);\n        options.setNullText(\"null\");\n        options.setTitle(\"Date\");\n        options.setRowKeyPrinter(LocalDate::toString);\n        options.setFormats(formats -> {\n            DateTimeFormatter timeFormat = DateTimeFormatter.ofPattern(\"HH:mm\");\n            DateTimeFormatter dateTimeFormat = DateTimeFormatter.ofPattern(\"dd-MMM-yyyy HH:mm\");\n            formats.setDecimalFormat(Double.class, \"0.00##;-0.00##\", 1);\n            formats.<Month>setPrinter(\"Column-2\", v -> v.name().toLowerCase());\n            formats.<LocalTime>setPrinter(\"Column-1\", timeFormat::format);\n            formats.<LocalDateTime>setPrinter(\"Column-5\", dateTimeFormat::format);\n        });\n    } catch (IOException ex) {\n        throw new RuntimeException(ex.getMessage(), ex);\n    }\n});\n\nfinal byte[] bytes = baos.toByteArray();\nHttpClient.getDefault().doPost(post -> {\n    post.setRetryCount(5);\n    post.setReadTimeout(5000);\n    post.setConnectTimeout(1000);\n    post.setUrl(\"http://www.domain.con/test\");\n    post.setContent(bytes);\n    post.setContentType(\"application/x-gzip\");\n    post.setContentLength(bytes.length);\n    post.setResponseHandler((status, stream) -> {\n        if (status.getCode() == 200) {\n            return Optional.empty();\n        } else {\n            throw new RuntimeException(\"Failed with response: \" + status);\n        }\n    });\n});",
            "title": "CSV"
        },
        {
            "location": "/frame/writing/#json",
            "text": "Writing a  DataFrame  out in JSON format works similar to the CSV example above, however the output options differ\nslightly. If we use the same 10x7 frame introduced earlier, we can write out JSON with custom formatting as shown below.   Meta-data in the payload is included to allow a subsequent reader to re-constitute the frame with the appropriate data \ntypes. For example,  Integer  and  Long  types are not distinguishable in standard JSON output, but the  dataType  field \nwhich is written as a JSON property for each column object definition, provides the required information. Other types, \nlike  LocalTime , and  LocalDateTime  in this example are formatted into a string which can be controlled via the options  Formats  entity, much the same way as CSV formatting works. The data type property for objects like these is mapped\nto the simple name of the class.   frame.write().json(options -> {\n    options.setFile(\"DataFrame-1.json\");\n    options.setEncoding(\"UTF-8\");\n    options.setFormats(formats -> {\n        DateTimeFormatter timeFormat = DateTimeFormatter.ofPattern(\"HH:mm\");\n        DateTimeFormatter dateTimeFormat = DateTimeFormatter.ofPattern(\"dd-MMM-yyyy HH:mm\");\n        formats.setPrinter(\"Column-2\", Printer.ofLocalTime(timeFormat));\n        formats.<Month>setPrinter(\"Column-3\", Printer.forObject(m -> m.name().toLowerCase()));\n        formats.setPrinter(\"Column-6\", Printer.ofLocalDateTime(dateTimeFormat));\n    });\n});  The resulting JSON output is shown below. It should be noted that the  JsonSink  implementation that ships with Morpheus\nuses a stream based JsonWriter, making it very memory efficient even for extremely large frames. The same applies in reverse\nwith the  JsonSource  implementation. Finally, the JSON payload includes the row and column count for the frame which is\nimportant to enable the  JsonSource  to pre-allocate the arrays when reading, which again, improves performance for very \nlarge frames.  {\n  \"DataFrame\": {\n    \"rowCount\": 10,\n    \"colCount\": 7,\n    \"rowKeys\": {\n      \"type\": \"LocalDate\",\n      \"values\": [\n        \"2014-01-01\",\n        \"2014-01-02\",\n        \"2014-01-03\",\n        \"2014-01-04\",\n        \"2014-01-05\",\n        \"2014-01-06\",\n        \"2014-01-07\",\n        \"2014-01-08\",\n        \"2014-01-09\",\n        \"2014-01-10\"\n      ]\n    },\n    \"columns\": [\n      {\n        \"key\": \"Column-0\",\n        \"keyType\": \"String\",\n        \"dataType\": \"Boolean\",\n        \"defaultValue\": false,\n        \"values\": [\n          true,\n          true,\n          true,\n          true,\n          false,\n          false,\n          true,\n          false,\n          true,\n          false\n        ]\n      },\n      {\n        \"key\": \"Column-1\",\n        \"keyType\": \"String\",\n        \"dataType\": \"Double\",\n        \"defaultValue\": null,\n        \"values\": [\n          3.4243295493435575,\n          1.1438780283716343,\n          4.316506413470317,\n          7.454588915346636,\n          6.950360875904566,\n          9.491531171790896,\n          5.204554855321635,\n          5.754177059739899,\n          0.8195736745281579,\n          6.988186292016216\n        ]\n      },\n      {\n        \"key\": \"Column-2\",\n        \"keyType\": \"String\",\n        \"dataType\": \"LocalTime\",\n        \"defaultValue\": null,\n        \"values\": [\n          \"21:24\",\n          \"21:25\",\n          \"21:26\",\n          \"21:27\",\n          \"21:28\",\n          \"21:29\",\n          \"21:30\",\n          \"21:31\",\n          \"21:32\",\n          \"21:33\"\n        ]\n      },\n      {\n        \"key\": \"Column-3\",\n        \"keyType\": \"String\",\n        \"dataType\": \"Month\",\n        \"defaultValue\": null,\n        \"values\": [\n          \"february\",\n          \"march\",\n          \"april\",\n          \"may\",\n          \"june\",\n          \"july\",\n          \"august\",\n          \"september\",\n          \"october\",\n          \"november\"\n        ]\n      },\n      {\n        \"key\": \"Column-4\",\n        \"keyType\": \"String\",\n        \"dataType\": \"Integer\",\n        \"defaultValue\": 0,\n        \"values\": [\n          9,\n          9,\n          5,\n          7,\n          0,\n          2,\n          8,\n          5,\n          5,\n          0\n        ]\n      },\n      {\n        \"key\": \"Column-5\",\n        \"keyType\": \"String\",\n        \"dataType\": \"String\",\n        \"defaultValue\": null,\n        \"values\": [\n          \"(0,5)\",\n          \"(1,5)\",\n          \"(2,5)\",\n          \"(3,5)\",\n          \"(4,5)\",\n          \"(5,5)\",\n          \"(6,5)\",\n          \"(7,5)\",\n          \"(8,5)\",\n          \"(9,5)\"\n        ]\n      },\n      {\n        \"key\": \"Column-6\",\n        \"keyType\": \"String\",\n        \"dataType\": \"LocalDateTime\",\n        \"defaultValue\": null,\n        \"values\": [\n          \"02-Aug-2014 21:24\",\n          \"02-Aug-2014 21:25\",\n          \"02-Aug-2014 21:26\",\n          \"02-Aug-2014 21:27\",\n          \"02-Aug-2014 21:28\",\n          \"02-Aug-2014 21:29\",\n          \"02-Aug-2014 21:30\",\n          \"02-Aug-2014 21:31\",\n          \"02-Aug-2014 21:32\",\n          \"02-Aug-2014 21:33\"\n        ]\n      }\n    ]\n  }\n}",
            "title": "JSON"
        },
        {
            "location": "/frame/writing/#sql",
            "text": "The third built-in sink that ships with the Morpheus library enables frames to be written out to a SQL database, with\nsupport for  MYSQL ,  HSQLDB ,  SQLite ,  H2 Database  and  Microsoft SQL Server .  The  DbSink  can be used to write a  DataFrame  to an existing table, or otherwise have a table automatically created if \nnone exists. Obviously the latter scenario requires that the authenticated user has adequate permissions to create database\nobjects, otherwise an exception will be raised. The  DbSinkOptions  class allows some control for how the frame columns are \nmapped and stored in the database, both in terms of column names and column types.  Consider the code example below where we write the same 10x7  DataFrame  presented in the introduction to an HSQLDB instance.\nIn this scenario, we pass the JDBC URL and credentials to the  DbSinkOptions , however the connection can be passed via a java.sql.DataSource  or a  java.sql.Connection  object. For large frames, it is important to leverage SQL batch inserts, and\nthe batch size can be controlled via  DbSinkOptions . Finally, the code below also demonstrates how to map the  DataFrame \ncolumn keys to database column names, and also how to map types. While built in support exists for  LocalDate  and  Enum \ntypes, the example explicitly sets a custom mapping for illustration purposes.   Class.forName(\"org.hsqldb.jdbcDriver\");\nframe.write().db(options -> {\n    options.setBatchSize(1000);\n    options.setTableName(\"TestTable\");\n    options.setConnection(\"jdbc:hsqldb:/Users/witdxav/morpheus/tests/DataFrame_3.db\", \"sa\", null);\n    options.setRowKeyMapping(\"Date\", Date.class, Function1.toValue(Date::valueOf));\n    options.setColumnMappings(mappings -> {\n        mappings.add(Month.class, String.class, Function1.toValue(v -> {\n            return v.<Month>getValue().name().toLowerCase();\n        }));\n        mappings.add(LocalDate.class, java.sql.Date.class, Function1.toValue(v -> {\n            return Date.valueOf(v.<LocalDate>getValue());\n        }));\n    });\n    options.setColumnNames(colKey -> {\n        switch (colKey) {\n            case \"Column-1\":    return \"Column-A\";\n            case \"Column-2\":    return \"Column-B\";\n            case \"Column-3\":    return \"Column-C\";\n            default:            return colKey;\n        }\n    });\n});",
            "title": "SQL"
        },
        {
            "location": "/frame/writing/#custom-sink",
            "text": "Writing a custom sink is fairly trivial, and involves implementing two classes, namely an implementation of the  DataFrameSink \ninterface, and a correponding options class that describes how the frame should be output. In this section, we show how one could\nwrite a custom sink to record a  DataFrame  as a serialized object using standard java object serialization. The code below shows\nan implementation of a  DataFrameSink  which has been called  CustomSink  for lack of a better name.   public class CustomSink<R,C> implements DataFrameSink<R,C,CustomSinkOptions> {\n    @Override\n    public void write(DataFrame<R,C> frame, Consumer<CustomSinkOptions> configurator) {\n        final CustomSinkOptions options = Initialiser.apply(new CustomSinkOptions(), configurator);\n        ObjectOutputStream os = null;\n        try {\n            if (options.isCompressed()) {\n                os = new ObjectOutputStream(new GZIPOutputStream(options.getOutput()));\n                os.writeObject(frame);\n            } else {\n                os = new ObjectOutputStream(options.getOutput());\n                os.writeObject(frame);\n            }\n        } catch (Exception ex) {\n            throw new DataFrameException(\"Failed to write DataFrame to serialized output\", ex);\n        } finally {\n            IO.close(os);\n        }\n    }\n}  This class implements a single  write()  method which takes the frame to record, and a  Consumer  that is used to configure\nthe output options. The options class is usually sink specific, and in this example we have written a class called  CustomSinkOptions \nwhich carries the  OutputStream  to write to, and a flag to indicate whether output should be compressed. The options class\nlooks as follows.`   public class CustomSinkOptions {\n\n    private OutputStream output;\n    private boolean compressed;\n\n    /**\n     * Returns the output stream to write to\n     * @return      the output stream to write to\n     */\n    public OutputStream getOutput() {\n        return output;\n    }\n\n    /**\n     * Returns true if compression is enabled\n     * @return  true if compression is enabled\n     */\n    public boolean isCompressed() {\n        return compressed;\n    }\n\n    /**\n     * Sets the output for these options\n     * @param output    the output stream to write to\n     */\n    public void setOutput(OutputStream output) {\n        this.output = output;\n    }\n\n    /**\n     * Sets the file output for these options\n     * @param file  the output file\n     */\n    public void setFile(File file) {\n        this.output = Try.call(() -> new BufferedOutputStream(new FileOutputStream(file)));\n    }\n\n    /**\n     * Sets whether the output should be wrapped in a GZIP stream\n     * @param compressed    true to wrap output in GZIP compression\n     */\n    public void setCompressed(boolean compressed) {\n        this.compressed = compressed;\n    }\n}  To use this sink to write a frame, consider the following code using the same frame from the introduction. In this\ncase we create an instance of the sink to write to, and implement a  Consumer  to configure the sink specific options\nas shown. which includes setting the output file and enabling GZIP compression.   frame.write().to(new CustomSink<>(), options -> {\n    options.setFile(new File(\"/Users/witdxav/test/DataFrame-1.gzip\"));\n    options.setCompressed(true);\n});",
            "title": "Custom Sink"
        },
        {
            "location": "/frame/performance/",
            "text": "Introduction\n\n\nThe following sections provide some micro-benchmark timing and memory statistics for various \nstyles of Morpheus \nDataFrames\n. These benchmarks where performed on a 2013 MacBook Pro with a \nCore i7 2.6Ghz Quad-core CPU, and 16GB of memory. As with all benchmarks, these figures should \nbe taken with a pinch of salt as real-world performance could differ substantially for all sorts \nof reasons. They are however a reasonable set of observations to compare the relative cost of \ndifferent operations, and also to get a sense of how the sequential versus parallel execution \nof these operations compares.\n\n\nDataFrame\n\n\nInitialization\n\n\nA \nDataFrame\n is a column store composed of Morpheus Arrays, so the performance statistics presented\n\nhere\n should serve as a rough guide to extrapolate some crude expectations regarding \n\nDataFrame\n performance & memory. Nevertheless, there are many idiosyncratic operations on a \nDataFrame\n\nfor which the performance characteristics are worthwhile understanding, and this section attempts to\ncover some of this ground.\n\n\nThe chart below presents the initialization times for \nDataFrames\n with 10 columns of double precision\nvalues and row counts increasing from 1 million to 5 million rows. The series on this chart represent \nframes with differing types for the row axis, including \nInteger\n, \nDate\n, \nInstant\n, \nLocalDateTime\n \nand \nZonedDateTime\n types. If you have read the \nsection\n on the comparative performance \nof Morpheus Arrays for these types, it is not surprising to see that the frame with a \nZonedDateTime\n \nrow axis is more expensive to create than one with an \nInteger\n axis. With that said, this chart\nprovides a comforting picture with regard to the fairly linear scalability of performance as the \nrow count increases, and the absolute times involved are also re-assuring.\n\n\n\n    \n\n\n\n\n\nFigure 1. Large DataFrame construction times can be affected by the complexity of the largest axis.\n\n\nMemory\n\n\nWhen constructing \nDataFrames\n that are entirely composed of Morpheus arrays that are themselves\ninternally represented as primitives, it is fairly easy to estimate how much memory the corresponding \nframe will consume. Consider the \nDataFrame\n example in the previous section, which included 10 columns\nof double precision values. For the case with 5 million rows, and therefore 50 million elements, one \ncan reason that it would require at least \n(((64 / 8) * 50,000,000) / 1024^2) = 381MB\n for just the \ndata alone. As it turns out, direct measurement of this frame using the Java \nInstrumentation\n API \nsuggests the entire object requires more like 500MB of RAM. \n\n\nThe additional 120MB in this case is likely to be almost entirely attributable to the row and column \naxis of the frame, and obviously much more the former than the latter. The row and column axis is \neach backed by a \nTrove\n map of an appropriate type which maintains \nthe index, as well as a Morpheus array that defines the order. A Trove \nTIntIntHashMap\n initialized with \na capacity of 5 million elements consumes 62MB according to Java Instrumentation, so together this \nexplains over 90% of the memory used by this frame. This implies very little object overhead by the \nsupporting classes that make up the \nDataFrame\n, and also suggests that garbage collection times \nshould be minimal, which we will discuss in a following section.\n\n\n\n    \n\n\n\n\n\nFigure 2. Memory usage for a DataFrame with 50 million 64-bit double precision values and various row key types.\n \n\n\nAs you would expect, a \nDataFrame\n is generally very friendly to the garbage collector, so even creating \na large frame will not result in excessively long GC times. The chart below illustrates the times to \ndeallocate the DataFrame in this test, and both the magnitude and the small sampling noise in these\nresults is comforting.\n\n\n\n    \n\n\n\n\n\nFigure 3. The GC times are small in magnitude and demonstrate low sampling noise in the results.\n \n\n\nRow Iteration\n\n\nRow iteration is generally very fast when using the \nforEach()\n function with a consumer to perform \nsome relevant analysis on each row. For example, consider a DataFrame with 50 million rows and 4 \ncolumns containing random double precision values. The chart below shows how performance compares \nbetween sequential and parallel execution for a routine that computes the arithmetic mean of each row. \n\n\n\n    \n\n\n\n\n\nFigure 4. The parallel version of the result shows up to 50 million rows processed per second.\n \n\n\nThe code to generate these results is as follows:\n\n\n\n\n\n//Sample size for timing statistics\nint sample = 10;\n\n//Create frame with 50 million rows of Random doubles\nRange<Integer> rowKeys = Range.of(0, 10000000);\nArray<String> colKeys = Array.of(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"H\");\nDataFrame<Integer,String> frame = DataFrame.ofDoubles(rowKeys, colKeys).applyDoubles(v -> Math.random());\n\n//Time sequential and parallel computation of mean over all rows\nDataFrame<String,String> timing = PerfStat.run(sample, TimeUnit.MILLISECONDS, false, tasks -> {\n    tasks.put(\"Sequential\", () -> {\n        frame.sequential().rows().forEach(row -> row.stats().mean());\n        return frame;\n    });\n    tasks.put(\"Parallel\", () -> {\n        frame.parallel().rows().forEach(row -> row.stats().mean());\n        return frame;\n    });\n});\n\n//Plot timing statistics as a bar chart\nChart.create().withBarPlot(timing, false, chart -> {\n    chart.title().withText(\"Time to Compute Arithmetic Mean of 50 Million rows (Sample 10 times)\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Total Time in Milliseconds\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\nUpdating Elements\n\n\nA common \nDataFrame\n operation is to apply a function to each element in order to update the value based on \nsome condition. The example below illustrates a performance comparison of sequential versus parallel execution for\nsuch a scenario. In this example, the \nDataFrame\n contains 200 million random double precision values which are\ninitialized with a random value between 0 and 1 using \nMath.random()\n. The test the times how long it takes\nto iterate over all these values, and capping values at 0.5 if they exceed that value.The results show a large \nperformance improvement through parallel execution, and one could expect even larger differences as the \ncomplexity of the apply function increases.\n\n\n\n    \n\n\n\n\n\nFigure 5. Inspecting 200 million DataFrame elements and capping values > 0.5 in a little over a second.\n \n\n\nThe code to generate these results is as follows:\n\n\n\n\n\n//Sample size for timing statistics\nint count = 10;\n\n//Create frame with 50 million rows of Random doubles\nRange<Integer> rowKeys = Range.of(0, 50000000);\nArray<String> colKeys = Array.of(\"A\", \"B\", \"C\", \"D\");\nDataFrame<Integer,String> frame = DataFrame.ofDoubles(rowKeys, colKeys).applyDoubles(v -> Math.random());\n\n//Time sequential and parallel capping of all elements in the DataFrame\nToDoubleFunction<DataFrameValue<Integer,String>> cap = (v) -> v.getDouble() > 0.5 ? 0.5 : v.getDouble();\nDataFrame<String,String> timing = PerfStat.run(count, TimeUnit.MILLISECONDS, true, tasks -> {\n    tasks.beforeEach(() -> frame.applyDoubles(v -> Math.random()));\n    tasks.put(\"Sequential\", () -> frame.sequential().applyDoubles(cap));\n    tasks.put(\"Parallel\", () -> frame.parallel().applyDoubles(cap));\n});\n\n//Plot timing statistics as a bar chart\nChart.create().withBarPlot(timing, false, chart -> {\n    chart.title().withText(\"Time to Cap 200 Million DataFrame Elements (Sample 10 times)\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Total Time in Milliseconds\");\n    chart.legend().on();\n    chart.show();\n});",
            "title": "Performance"
        },
        {
            "location": "/frame/performance/#introduction",
            "text": "The following sections provide some micro-benchmark timing and memory statistics for various \nstyles of Morpheus  DataFrames . These benchmarks where performed on a 2013 MacBook Pro with a \nCore i7 2.6Ghz Quad-core CPU, and 16GB of memory. As with all benchmarks, these figures should \nbe taken with a pinch of salt as real-world performance could differ substantially for all sorts \nof reasons. They are however a reasonable set of observations to compare the relative cost of \ndifferent operations, and also to get a sense of how the sequential versus parallel execution \nof these operations compares.",
            "title": "Introduction"
        },
        {
            "location": "/frame/performance/#dataframe",
            "text": "",
            "title": "DataFrame"
        },
        {
            "location": "/frame/performance/#initialization",
            "text": "A  DataFrame  is a column store composed of Morpheus Arrays, so the performance statistics presented here  should serve as a rough guide to extrapolate some crude expectations regarding  DataFrame  performance & memory. Nevertheless, there are many idiosyncratic operations on a  DataFrame \nfor which the performance characteristics are worthwhile understanding, and this section attempts to\ncover some of this ground.  The chart below presents the initialization times for  DataFrames  with 10 columns of double precision\nvalues and row counts increasing from 1 million to 5 million rows. The series on this chart represent \nframes with differing types for the row axis, including  Integer ,  Date ,  Instant ,  LocalDateTime  \nand  ZonedDateTime  types. If you have read the  section  on the comparative performance \nof Morpheus Arrays for these types, it is not surprising to see that the frame with a  ZonedDateTime  \nrow axis is more expensive to create than one with an  Integer  axis. With that said, this chart\nprovides a comforting picture with regard to the fairly linear scalability of performance as the \nrow count increases, and the absolute times involved are also re-assuring.  \n       Figure 1. Large DataFrame construction times can be affected by the complexity of the largest axis.",
            "title": "Initialization"
        },
        {
            "location": "/frame/performance/#memory",
            "text": "When constructing  DataFrames  that are entirely composed of Morpheus arrays that are themselves\ninternally represented as primitives, it is fairly easy to estimate how much memory the corresponding \nframe will consume. Consider the  DataFrame  example in the previous section, which included 10 columns\nof double precision values. For the case with 5 million rows, and therefore 50 million elements, one \ncan reason that it would require at least  (((64 / 8) * 50,000,000) / 1024^2) = 381MB  for just the \ndata alone. As it turns out, direct measurement of this frame using the Java  Instrumentation  API \nsuggests the entire object requires more like 500MB of RAM.   The additional 120MB in this case is likely to be almost entirely attributable to the row and column \naxis of the frame, and obviously much more the former than the latter. The row and column axis is \neach backed by a  Trove  map of an appropriate type which maintains \nthe index, as well as a Morpheus array that defines the order. A Trove  TIntIntHashMap  initialized with \na capacity of 5 million elements consumes 62MB according to Java Instrumentation, so together this \nexplains over 90% of the memory used by this frame. This implies very little object overhead by the \nsupporting classes that make up the  DataFrame , and also suggests that garbage collection times \nshould be minimal, which we will discuss in a following section.  \n       Figure 2. Memory usage for a DataFrame with 50 million 64-bit double precision values and various row key types.    As you would expect, a  DataFrame  is generally very friendly to the garbage collector, so even creating \na large frame will not result in excessively long GC times. The chart below illustrates the times to \ndeallocate the DataFrame in this test, and both the magnitude and the small sampling noise in these\nresults is comforting.  \n       Figure 3. The GC times are small in magnitude and demonstrate low sampling noise in the results.",
            "title": "Memory"
        },
        {
            "location": "/frame/performance/#row-iteration",
            "text": "Row iteration is generally very fast when using the  forEach()  function with a consumer to perform \nsome relevant analysis on each row. For example, consider a DataFrame with 50 million rows and 4 \ncolumns containing random double precision values. The chart below shows how performance compares \nbetween sequential and parallel execution for a routine that computes the arithmetic mean of each row.   \n       Figure 4. The parallel version of the result shows up to 50 million rows processed per second.    The code to generate these results is as follows:   //Sample size for timing statistics\nint sample = 10;\n\n//Create frame with 50 million rows of Random doubles\nRange<Integer> rowKeys = Range.of(0, 10000000);\nArray<String> colKeys = Array.of(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"H\");\nDataFrame<Integer,String> frame = DataFrame.ofDoubles(rowKeys, colKeys).applyDoubles(v -> Math.random());\n\n//Time sequential and parallel computation of mean over all rows\nDataFrame<String,String> timing = PerfStat.run(sample, TimeUnit.MILLISECONDS, false, tasks -> {\n    tasks.put(\"Sequential\", () -> {\n        frame.sequential().rows().forEach(row -> row.stats().mean());\n        return frame;\n    });\n    tasks.put(\"Parallel\", () -> {\n        frame.parallel().rows().forEach(row -> row.stats().mean());\n        return frame;\n    });\n});\n\n//Plot timing statistics as a bar chart\nChart.create().withBarPlot(timing, false, chart -> {\n    chart.title().withText(\"Time to Compute Arithmetic Mean of 50 Million rows (Sample 10 times)\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Total Time in Milliseconds\");\n    chart.legend().on();\n    chart.show();\n});",
            "title": "Row Iteration"
        },
        {
            "location": "/frame/performance/#updating-elements",
            "text": "A common  DataFrame  operation is to apply a function to each element in order to update the value based on \nsome condition. The example below illustrates a performance comparison of sequential versus parallel execution for\nsuch a scenario. In this example, the  DataFrame  contains 200 million random double precision values which are\ninitialized with a random value between 0 and 1 using  Math.random() . The test the times how long it takes\nto iterate over all these values, and capping values at 0.5 if they exceed that value.The results show a large \nperformance improvement through parallel execution, and one could expect even larger differences as the \ncomplexity of the apply function increases.  \n       Figure 5. Inspecting 200 million DataFrame elements and capping values > 0.5 in a little over a second.    The code to generate these results is as follows:   //Sample size for timing statistics\nint count = 10;\n\n//Create frame with 50 million rows of Random doubles\nRange<Integer> rowKeys = Range.of(0, 50000000);\nArray<String> colKeys = Array.of(\"A\", \"B\", \"C\", \"D\");\nDataFrame<Integer,String> frame = DataFrame.ofDoubles(rowKeys, colKeys).applyDoubles(v -> Math.random());\n\n//Time sequential and parallel capping of all elements in the DataFrame\nToDoubleFunction<DataFrameValue<Integer,String>> cap = (v) -> v.getDouble() > 0.5 ? 0.5 : v.getDouble();\nDataFrame<String,String> timing = PerfStat.run(count, TimeUnit.MILLISECONDS, true, tasks -> {\n    tasks.beforeEach(() -> frame.applyDoubles(v -> Math.random()));\n    tasks.put(\"Sequential\", () -> frame.sequential().applyDoubles(cap));\n    tasks.put(\"Parallel\", () -> frame.parallel().applyDoubles(cap));\n});\n\n//Plot timing statistics as a bar chart\nChart.create().withBarPlot(timing, false, chart -> {\n    chart.title().withText(\"Time to Cap 200 Million DataFrame Elements (Sample 10 times)\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Total Time in Milliseconds\");\n    chart.legend().on();\n    chart.show();\n});",
            "title": "Updating Elements"
        },
        {
            "location": "/analysis/statistics/",
            "text": "Descriptive Statistics\n\n\nIntroduction\n\n\nComputing descriptive statistics is a fundamental feature of the Morpheus API, and to this end, a package is devoted \nto providing a clean and consistent framework that is then integrated with the Morpheus \nArray\n and \nDataFrame\n interfaces. \nAll the statistic calculators implement \nonline algorithms\n, so they can be used to compute stats on infinitely large \ndatasets. In addition, the Morpheus statistics package can be used standalone, independent of the \nArray\n and \nDataFrame\n \nstructures.\n\n\nThe table below enumerates the uni-variate statistics currently supported. Computing descriptive statistics on sample \ndata reduces dimensionality in that an \nArray\n can be reduced to a scalar, or stats on \nDataFrame\n columns can be \nreduced to an array of values representing the statistic for each column. To that end, the \nStats\n interface, which \nexposes all the uni-variate statistics methods, is parameterized by the return type of the functions it declares so that \nit can either return a scalar value, or some other data structure aggregate.\n\n\n\n\n\n\n\n\nMethod\n\n\nDescription\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\ncount()\n\n\nThe number of observations, ignoring nulls\n\n\n\n\n\n\n\n\nmin()\n\n\nThe minimum value, ignoring nulls\n\n\nDetails\n\n\n\n\n\n\nmax()\n\n\nThe maximim value, ignorning nulls\n\n\nDetails\n\n\n\n\n\n\nmean()\n\n\nThe first moment, or the arithmetic mean or average, ignoring nulls\n\n\nDetails\n\n\n\n\n\n\nvariance()\n\n\nThe un-biased variance or second moment, a measure of dispersion\n\n\nDetails\n\n\n\n\n\n\nstdDev()\n\n\nThe un-biased standard deviation, a measure of dispersion\n\n\nDetails\n\n\n\n\n\n\nskew()\n\n\nThe third moment, or skewness, a measure of the asymmetry in the distribution\n\n\nDetails\n\n\n\n\n\n\nkurtosis()\n\n\nThe fourth moment, or Kurtosis, a measure of the \"tailedness\" of the probability distribution\n\n\nDetails\n\n\n\n\n\n\nmedian()\n\n\nThe value separating the higher half of the data, or 50th percentile\n\n\nDetails\n\n\n\n\n\n\nmad()\n\n\nThe Mean Absolute Deviation from a central point, another measure of dispersion\n\n\nDetails\n\n\n\n\n\n\nsem()\n\n\nThe standard error of the mean\n\n\nDetails\n\n\n\n\n\n\ngeoMean()\n\n\nThe geometric mean, another measure of central tendency\n\n\nDetails\n\n\n\n\n\n\nsum()\n\n\nThe summation of all values, ignoring nulls\n\n\nDetails\n\n\n\n\n\n\nsumOfSquares()\n\n\nThe sum, over non-null observations, of the squared differences from the mean\n\n\nDetails\n\n\n\n\n\n\nautocorr(int lag)\n\n\nThe autocorrelation, which is the correlation of a signal with a delayed copy of itself\n\n\nDetails\n\n\n\n\n\n\npercentile(double nth)\n\n\nThe percentile value below which n% of values fall, ignoring nulls\n\n\nDetails\n\n\n\n\n\n\n\n\nThe following sections mostly operate on a \nDataFrame\n of random double precision values with an arbitrary number of\nrows and columns, which can be manufactured with the code below. In other examples the motor vehicle dataset introduced \nin the \noverview\n will be used to demonstrate how statistics functions behave on frames with mixed data types.\n\n\n\n\n\n/**\n * Returns a frame of random double precision values\n * @param rowCount  the row count for frame\n * @param columns   the column keys\n * @return          the newly created frame\n */\nstatic DataFrame<LocalDate,String> random(int rowCount, String... columns) {\n    final LocalDate start = LocalDate.now().minusDays(rowCount);\n    return DataFrame.ofDoubles(\n        Range.of(0, rowCount).map(start::plusDays),\n        Array.of(columns),\n        value -> Math.random() * 100d\n    );\n}\n\n\n\n\nFor example:\n\n\n\n\n\nrandom(100, \"A\", \"B\", \"C\", \"D\", \"E\").out().print(10);\n\n\n\n\n\n   Index     |       A       |       B       |       C       |       D       |       E       |\n----------------------------------------------------------------------------------------------\n 2014-04-18  |  27.65695706  |  24.12541112  |  54.53927679  |  29.53942097  |  36.32857957  |\n 2014-04-19  |  77.17892101  |  91.65862134  |  31.89758283  |  75.86327696  |  65.28820416  |\n 2014-04-20  |  36.11837803  |   3.50787279  |  18.35033145  |  29.87153499  |  89.64915738  |\n 2014-04-21  |   6.11609249  |  61.52213182  |   76.0263036  |  91.45821122  |  15.68900718  |\n 2014-04-22  |  87.76836565  |   5.14224821  |  97.79979071  |  38.69432549  |  16.94097048  |\n 2014-04-23  |  72.72059023  |  89.50857903  |  23.38300753  |  71.09288654  |  20.66341413  |\n 2014-04-24  |  84.01645441  |   2.27975471  |    3.4054976  |  75.16447082  |  68.66698321  |\n 2014-04-25  |  34.07626388  |  60.48921016  |  12.06511821  |  55.99893352  |  51.31240247  |\n 2014-04-26  |  56.80751283  |  36.48578987  |  17.44059537  |  78.88660804  |  77.63219394  |\n 2014-04-27  |  92.76964851  |  56.95184403  |    7.3521691  |  27.01594691  |   0.02729372  |\n\n\n\n\nUni-Variate Statistics\n\n\nFrame Level Statistics\n\n\nThe \nstats()\n method on the \nDataFrame\n interface returns \nStats<Double>\n that can be used to compute descriptive\nstatistics over all values in the frame. The \nStats\n interface is parameterised in \nDouble\n to indicate that scalar \nvalues are generated by all the methods. The code below demonstrate this use case:\n\n\n\n\n\n//Create 100x5 DataFrame of random doubles\nDataFrame<Integer,String> frame = random(100, \"A\", \"B\", \"C\", \"D\", \"E\");\n\nSystem.out.printf(\"Count = %.4f\\n\", frame.stats().count());\nSystem.out.printf(\"Minimum = %.4f\\n\", frame.stats().min());\nSystem.out.printf(\"Maximum = %.4f\\n\", frame.stats().max());\nSystem.out.printf(\"Mean = %.4f\\n\", frame.stats().mean());\nSystem.out.printf(\"Median = %.4f\\n\", frame.stats().median());\nSystem.out.printf(\"Variance = %.4f\\n\", frame.stats().variance());\nSystem.out.printf(\"StdDev = %.4f\\n\", frame.stats().stdDev());\nSystem.out.printf(\"Skew = %.4f\\n\", frame.stats().skew());\nSystem.out.printf(\"Kurtosis = %.4f\\n\", frame.stats().kurtosis());\nSystem.out.printf(\"Mean Abs Deviation = %.4f\\n\", frame.stats().mad());\nSystem.out.printf(\"Sum = %.4f\\n\", frame.stats().sum());\nSystem.out.printf(\"Sum of Squares = %.4f\\n\", frame.stats().sumSquares());\nSystem.out.printf(\"Std Error of Mean = %.4f\\n\", frame.stats().sem());\nSystem.out.printf(\"Geometric Mean = %.4f\\n\", frame.stats().geoMean());\nSystem.out.printf(\"Percentile(75th) = %.4f\\n\", frame.stats().percentile(0.75d));\nSystem.out.printf(\"Autocorrelation(2) = %.4f\\n\", frame.stats().autocorr(2));\n\n\n\n\n\nCount = 500.0000\nMinimum = 0.0912\nMaximum = 99.2049\nMean = 50.4180\nMedian = 49.8189\nVariance = 803.8727\nStdDev = 28.3526\nSkew = -0.0670\nKurtosis = -1.1654\nMean Abs Deviation = 24.4590\nSum = 25209.0143\nSum of Squares = 1672121.2725\nStd Error of Mean = 1.2680\nGeometric Mean = 37.1386\nPercentile(75) = 75.4636\nAutocorrelation(2) = -0.0671\n\n\n\n\nSingle Row / Column Statistics\n\n\nThe \nStats<Double>\n interface can be accessed on individual rows and columns, namely by calling the \nstats()\n method on the \n\nDataFrameRow\n or \nDataFrameColumn\n interface. Individual rows or columns can be accessed via \nordinal\n or \nkey\n via the \n\nrowAt()\n and \ncolAt()\n methods on \nDataFrame\n, as illustrated below.\n\n\n\n\n\n//Create 100x5 DataFrame of random doubles\nDataFrame<LocalDate,String> frame = random(100, \"A\", \"B\", \"C\", \"D\", \"E\");\n\n//Capture stats interface for two rows independently sourced by ordinal and key\nLocalDate date = frame.rows().key(3);\nStats<Double> stats1 = frame.rowAt(3).stats();\nStats<Double> stats2 = frame.rowAt(date).stats();\n\nStatType.univariate().forEach(statType -> {\n    switch (statType) {\n        case COUNT:         assert(stats1.count().doubleValue() == stats2.count());                     break;\n        case MIN:           assert(stats1.min().doubleValue() == stats2.min());                         break;\n        case MAX:           assert(stats1.max().doubleValue() == stats2.max());                         break;\n        case MEAN:          assert(stats1.mean().doubleValue() == stats2.mean());                       break;\n        case MEDIAN:        assert(stats1.median().doubleValue() == stats2.median());                   break;\n        case VARIANCE:      assert(stats1.variance().doubleValue() == stats2.variance());               break;\n        case STD_DEV:       assert(stats1.stdDev().doubleValue() == stats2.stdDev());                   break;\n        case KURTOSIS:      assert(stats1.kurtosis().doubleValue() == stats2.kurtosis());               break;\n        case MAD:           assert(stats1.mad().doubleValue() == stats2.mad());                         break;\n        case SEM:           assert(stats1.sem().doubleValue() == stats2.sem());                         break;\n        case GEO_MEAN:      assert(stats1.geoMean().doubleValue() == stats2.geoMean());                 break;\n        case SUM:           assert(stats1.sum().doubleValue() == stats2.sum());                         break;\n        case SUM_SQUARES:   assert(stats1.sumSquares().doubleValue() == stats2.sumSquares());           break;\n        case AUTO_CORREL:   assert(stats1.autocorr(2).doubleValue() == stats2.autocorr(2));             break;\n        case PERCENTILE:    assert(stats1.percentile(0.75).doubleValue() == stats2.percentile(0.75));   break;\n    }\n});\n\n\n\n\nGiven the symmetrical nature of the Morpheus \nDataFrame\n API, the exact same approach follows in the column dimension.\n\n\n\n\n\n//Create 100x5 DataFrame of random doubles\nDataFrame<LocalDate,String> frame = random(100, \"A\", \"B\", \"C\", \"D\", \"E\");\n\n//Capture stats interface for two columns independently sourced by ordinal and key\nStats<Double> stats1 = frame.colAt(3).stats();\nStats<Double> stats2 = frame.colAt(\"D\").stats();\n\nStatType.univariate().forEach(statType -> {\n    switch (statType) {\n        case COUNT:         assert(stats1.count().doubleValue() == stats2.count());                     break;\n        case MIN:           assert(stats1.min().doubleValue() == stats2.min());                         break;\n        case MAX:           assert(stats1.max().doubleValue() == stats2.max());                         break;\n        case MEAN:          assert(stats1.mean().doubleValue() == stats2.mean());                       break;\n        case MEDIAN:        assert(stats1.median().doubleValue() == stats2.median());                   break;\n        case VARIANCE:      assert(stats1.variance().doubleValue() == stats2.variance());               break;\n        case STD_DEV:       assert(stats1.stdDev().doubleValue() == stats2.stdDev());                   break;\n        case KURTOSIS:      assert(stats1.kurtosis().doubleValue() == stats2.kurtosis());               break;\n        case MAD:           assert(stats1.mad().doubleValue() == stats2.mad());                         break;\n        case SEM:           assert(stats1.sem().doubleValue() == stats2.sem());                         break;\n        case GEO_MEAN:      assert(stats1.geoMean().doubleValue() == stats2.geoMean());                 break;\n        case SUM:           assert(stats1.sum().doubleValue() == stats2.sum());                         break;\n        case SUM_SQUARES:   assert(stats1.sumSquares().doubleValue() == stats2.sumSquares());           break;\n        case AUTO_CORREL:   assert(stats1.autocorr(2).doubleValue() == stats2.autocorr(2));             break;\n        case PERCENTILE:    assert(stats1.percentile(0.75).doubleValue() == stats2.percentile(0.75));   break;\n    }\n});\n\n\n\n\nDemean Rows\n\n\nConsider an example where we would like to demean the \nDataFrame\n in question along the row dimension, that is to say,\nwe wish to compute the mean for each row, and subtract that amount from each item in the row.  This could be achieveed\nas follows.\n\n\n\n\n\n//GOOD: Row iteration using forEach()\nframe.rows().forEach(row -> {\n    final double mean = row.stats().mean();\n    row.applyDoubles(v -> {\n        return v.getDouble() - mean;\n    });\n});\n\n\n\n\nIn this scenario, the row object which serves as the lambda argument is reused across each invocation, as is the \nStats\n object,\nmaking this extremely efficient. A much less efficient mechanism would be to create a new row and new \nStats\n object for each\niteration through the loop as shown below. To see how performance of these approaches compares, see section below on \n\nperformance\n.\n\n\n\n\n\n//BAD: Row iteration creating a new row each time\nfor (int i=0; i<frame.rowCount(); ++i) {\n    final DataFrameRow<LocalDate,String> row = frame.rowAt(i);\n    final double mean = row.stats().mean();\n    row.applyDoubles(v -> {\n        return v.getDouble() - mean;\n    });\n}\n\n\n\n\nMultiple Row / Column Statistics\n\n\nCalculating descriptive statistics in batch for all rows or columns is also supported by the Morpheus API. There are two\nmechanisms which can be used to achieve this, the first is via the \nstats()\n method on \nDataFrameRows\n or \nDataFrameColumns\n,\nand the second is via the \ndescribe()\n method.\n\n\nThe same \nStats<?>\n interface described earlier is returned from \nDataFrameRows.stats()\n but in this case it is parameterized\nas \nStats<DataFrame<R,C>>\n as this implementation yields a \nDataFrame\n of values rather than a scalar value. In our example,\n\nDataFrame<LocalDate,String>\n row statistics will be presented as \nDataFrame<LocalDate,StatType>\n and column statistics will \nbe presented as \nDataFrame<StatType,String>\n. Consider the following code for accessing bulk row statistics:\n\n\n\n\n\n//Create 10x5 DataFramem of random doubles\nDataFrame<LocalDate,String> frame = random(10, \"A\", \"B\", \"C\", \"D\", \"E\");\n\nDataFrame<LocalDate,StatType> count = frame.rows().stats().count();\nDataFrame<LocalDate,StatType> min = frame.rows().stats().min();\nDataFrame<LocalDate,StatType> max = frame.rows().stats().max();\nDataFrame<LocalDate,StatType> mean = frame.rows().stats().mean();\nDataFrame<LocalDate,StatType> median = frame.rows().stats().median();\nDataFrame<LocalDate,StatType> variance = frame.rows().stats().variance();\nDataFrame<LocalDate,StatType> stdDev = frame.rows().stats().stdDev();\nDataFrame<LocalDate,StatType> kurtosis = frame.rows().stats().kurtosis();\nDataFrame<LocalDate,StatType> mad = frame.rows().stats().mad();\nDataFrame<LocalDate,StatType> sum = frame.rows().stats().sum();\nDataFrame<LocalDate,StatType> sumLogs = frame.rows().stats().sumLogs();\nDataFrame<LocalDate,StatType> sumSquares = frame.rows().stats().sumSquares();\nDataFrame<LocalDate,StatType> sem = frame.rows().stats().sem();\nDataFrame<LocalDate,StatType> geoMean = frame.rows().stats().geoMean();\nDataFrame<LocalDate,StatType> autocorr = frame.rows().stats().autocorr(1);\nDataFrame<LocalDate,StatType> percentile = frame.rows().stats().percentile(0.75d);\n\n\n\n\nTo illustrate what one of these frames looks like, let us look at the kurtosis results:\n\n\n\n\n\nkurtosis.out().print();\n\n\n\n\n\n   Index     |   KURTOSIS    |\n------------------------------\n 2014-04-18  |    0.9224301  |\n 2014-04-19  |  -1.03762054  |\n 2014-04-20  |  -2.19408392  |\n 2014-04-21  |  -1.60940179  |\n 2014-04-22  |   0.33646256  |\n 2014-04-23  |  -2.79071762  |\n 2014-04-24  |  -2.34090553  |\n 2014-04-25  |  -2.76200702  |\n 2014-04-26  |  -1.62476724  |\n 2014-04-27  |   -1.6700161  |\n\n\n\n\nAlternatively, we can also create a single \nDataFrame\n with multiple row statistics using the \ndescribe()\n method as follows:\n\n\n\n\n\nDataFrame<LocalDate,StatType> rowStats = frame.rows().describe(\n    StatType.COUNT, StatType.MEAN, StatType.VARIANCE, StatType.SKEWNESS, StatType.SUM\n);\n\n\n\n\n\n   Index     |  COUNT   |     MEAN      |    VARIANCE     |   SKEWNESS    |      SUM       |\n--------------------------------------------------------------------------------------------\n 2014-04-18  |  5.0000  |   72.4181454  |   619.98499044  |   0.21026915  |  362.09072702  |\n 2014-04-19  |  5.0000  |  43.53540087  |  1224.58077656  |   1.31797636  |  217.67700434  |\n 2014-04-20  |  5.0000  |  72.50600374  |   1144.3222537  |  -1.42186326  |  362.53001871  |\n 2014-04-21  |  5.0000  |  76.15303235  |   398.36488382  |  -0.09378226  |  380.76516176  |\n 2014-04-22  |  5.0000  |  72.31809543  |   808.83023776  |   -1.2161613  |  361.59047717  |\n 2014-04-23  |  5.0000  |  57.73293757  |   746.28716947  |    0.0998814  |  288.66468783  |\n 2014-04-24  |  5.0000  |  55.14827133  |  1253.77051383  |  -0.48490569  |  275.74135666  |\n 2014-04-25  |  5.0000  |  27.59810985  |   857.21698695  |    1.7015654  |  137.99054927  |\n 2014-04-26  |  5.0000  |  50.02524398  |     7.73321175  |   0.74564028  |  250.12621991  |\n 2014-04-27  |  5.0000  |  33.56051921  |   843.19166011  |   1.55424455  |  167.80259607  |\n\n\n\n\nThe symmetrical nature of the Morpheus API means that bulk column statistics work in exactly the same way, except the resulting\n\nDataFrame\n is transposed and parameterized in this example as \nDataFrame<StatType,String>\n. For completeness, consider the\nanalogous column based code:\n\n\n\n\n\n//Create 100x5 DataFrame of random double precision values\nDataFrame<LocalDate,String> frame = random(100, \"A\", \"B\", \"C\", \"D\", \"E\");\n\nDataFrame<StatType,String> count = frame.cols().stats().count();\nDataFrame<StatType,String> min = frame.cols().stats().min();\nDataFrame<StatType,String> max = frame.cols().stats().max();\nDataFrame<StatType,String> mean = frame.cols().stats().mean();\nDataFrame<StatType,String> median = frame.cols().stats().median();\nDataFrame<StatType,String> variance = frame.cols().stats().variance();\nDataFrame<StatType,String> stdDev = frame.cols().stats().stdDev();\nDataFrame<StatType,String> kurtosis = frame.cols().stats().kurtosis();\nDataFrame<StatType,String> mad = frame.cols().stats().mad();\nDataFrame<StatType,String> sum = frame.cols().stats().sum();\nDataFrame<StatType,String> sumLogs = frame.cols().stats().sumLogs();\nDataFrame<StatType,String> sumSquares = frame.cols().stats().sumSquares();\nDataFrame<StatType,String> sem = frame.cols().stats().sem();\nDataFrame<StatType,String> geoMean = frame.cols().stats().geoMean();\nDataFrame<StatType,String> autocorr = frame.cols().stats().autocorr(1);\nDataFrame<StatType,String> percentile = frame.cols().stats().percentile(0.75d);\n\n\n\n\nTo illustrate the resulting structure, let us print the percentile \nDataFrame\n to standard out as follows:\n\n\n\n\n\npercentile.out().print();\n\n\n\n\n\n   Index     |       A       |       B       |       C       |       D       |       E       |\n----------------------------------------------------------------------------------------------\n PERCENTILE  |  84.53057879  |  69.39927582  |  71.70889746  |  77.60914534  |  77.43947475  |\n\n\n\n\nFinally, the \ndescribe()\n method on \nDataFrameColumns\n can be used in the same way as demonstrated earlier to yield multiple \ndescriptive statistics on all columns in the frame.\n\n\n\n\n\nDataFrame<StatType,String> colStats = frame.cols().describe(\n    StatType.COUNT, StatType.MEAN, StatType.VARIANCE, StatType.SKEWNESS, StatType.SUM\n);\n\n\n\n\n\n  Index    |        A        |        B        |        C        |        D        |        E        |\n------------------------------------------------------------------------------------------------------\n    COUNT  |       100.0000  |       100.0000  |       100.0000  |       100.0000  |       100.0000  |\n     MEAN  |    53.21193611  |    46.66050934  |    47.30513686  |    52.83602104  |     53.5188048  |\n VARIANCE  |   1008.5668979  |   710.21278243  |    860.7008831  |   803.54694593  |   886.26568961  |\n SKEWNESS  |    -0.18291634  |     0.08827897  |     0.02910395  |    -0.05554874  |    -0.05292841  |\n      SUM  |  5321.19361134  |  4666.05093406  |  4730.51368606  |  5283.60210399  |  5351.88047967  |\n\n\n\n\nExpanding Window Statistics\n\n\nIt is often useful to compute summary statistics over an \nexpanding window\n, and the Morpheus API provides a clean and\nconsistent mechanism for doing this by again leveraging the \nStats<?>\n interface. This means all the univariate statistics\ndiscussed above can be calculated in an expanding window fashion, in either the row or column dimension. The code below \ndemonstrates how to compute an expanding window mean of a \nDataFrame\n of random doubles, with a minimum window size of \n5 periods. Calling the \nexpanding()\n method on the \nDataFrameAxisStas<R,C>\n presents a \nStats<?>\n implementation with\nexpanding window semantics.\n\n\n\n\n\nDataFrame<LocalDate,String> frame = random(100, \"A\", \"B\", \"C\", \"D\", \"E\");\nDataFrame<LocalDate,String> expandingMean = frame.cols().stats().expanding(5).mean();\nexpandingMean.out().print(10);\n\n\n\n\nThe first 10 rows of the results are shown below. Given that we specified a \nminimum window size\n of 5 periods, the first\n4 rows of the result contains NaN values, with the expanding means beginning in the 5th row.\n\n\n\n   Index     |       A       |       B       |       C       |       D       |       E       |\n----------------------------------------------------------------------------------------------\n 2014-04-18  |          NaN  |          NaN  |          NaN  |          NaN  |          NaN  |\n 2014-04-19  |          NaN  |          NaN  |          NaN  |          NaN  |          NaN  |\n 2014-04-20  |          NaN  |          NaN  |          NaN  |          NaN  |          NaN  |\n 2014-04-21  |          NaN  |          NaN  |          NaN  |          NaN  |          NaN  |\n 2014-04-22  |  37.47079054  |  44.65618465  |  34.44764118  |  27.31206568  |  36.88510609  |\n 2014-04-23  |  37.90629169  |  41.12951198  |  37.81200548  |  32.72777564  |  37.89339758  |\n 2014-04-24  |  35.45602666  |  46.58345677  |  38.53475246  |  32.08935944  |   39.4862089  |\n 2014-04-25  |  34.89378526  |  47.23091383  |  41.02249931  |  32.98138908  |  35.84038259  |\n 2014-04-26  |  40.93359834  |  43.69633714  |  46.76153667  |  36.59786248  |  36.64151385  |\n 2014-04-27  |  38.62418036  |  48.01872857  |  48.72740658  |  39.21108838  |  40.22726792  |\n\n\n\n\nRolling Window Statistics\n\n\nIt is often useful to compute summary statistics over a \nrolling window\n, and the Morpheus API provides a clean and\nconsistent mechanism for doing this by again leveraging the \nStats<?>\n interface. This means all the univariate statistics\ndiscussed above can be calculated in an rolling window fashion, in either the row or column dimension. The code below \ndemonstrates how to compute an rolling window mean of a \nDataFrame\n of random doubles based on a window size of 5 periods. \nCalling the \nrolling()\n method on the \nDataFrameAxisStas<R,C>\n presents a \nStats<?>\n implementation with rolling window \nsemantics.\n\n\n\n\n\nDataFrame<LocalDate,String> frame = random(100, \"A\", \"B\", \"C\", \"D\", \"E\");\nDataFrame<LocalDate,String> rollingMean = frame.cols().stats().rolling(5).mean();\nrollingMean.out().print(10);\n\n\n\n\nThe first 10 rows of these results are shown below. As with the expanding window functionality, the first 4 rows are NaN\nbecause we require at least a window size of 5 periods to yield a statistic.\n\n\n\n   Index     |       A       |       B       |       C       |       D       |       E       |\n----------------------------------------------------------------------------------------------\n 2014-04-18  |          NaN  |          NaN  |          NaN  |          NaN  |          NaN  |\n 2014-04-19  |          NaN  |          NaN  |          NaN  |          NaN  |          NaN  |\n 2014-04-20  |          NaN  |          NaN  |          NaN  |          NaN  |          NaN  |\n 2014-04-21  |          NaN  |          NaN  |          NaN  |          NaN  |          NaN  |\n 2014-04-22  |  48.86861615  |  49.43076987  |   23.0890681  |  46.37806901  |  31.72173443  |\n 2014-04-23  |  52.82603467  |  48.62150116  |  25.05736597  |   56.4940296  |  36.89243443  |\n 2014-04-24  |  55.63736749  |  56.85719458  |  24.74022673  |  71.88998581  |  31.63059599  |\n 2014-04-25  |  49.84144271  |  66.44284853  |  24.44921746  |  68.55549636  |  37.57049639  |\n 2014-04-26  |  50.72266995  |  79.98150692  |  39.67256206  |  60.98038021  |  48.18298467  |\n 2014-04-27  |  44.80180275  |  85.26550794  |  29.06426259  |  55.54849227  |  61.27390254  |\n\n\n\n\nNon-Numeric Data Handling\n\n\nThe Morpheus statistics API always presents results as \ndouble\n precision values, however it can operate on \nint\n and \nlong\n\ninput values by making the appropriate casts. Attempting to compute statistics on non-numeric data will result in an \nException\n, \nhowever when computing bulk row or column statistics, Morpheus will automatically restrict its operations to numeric vectors.\n\n\nFor example, consider the motor vehicle dataset first introduced in the \noverview\n which has a number of\nnon-numeric columns such as \nManufacturer\n, \nModel\n, \nType\n and so on. The first 10 rows of the dataset are shown below.\n\n\n\n  Index  |  Manufacturer  |    Model     |   Type    |  Min.Price  |   Price   |  Max.Price  |  MPG.city  |  MPG.highway  |       AirBags        |  DriveTrain  |  Cylinders  |  EngineSize  |  Horsepower  |  RPM   |  Rev.per.mile  |  Man.trans.avail  |  Fuel.tank.capacity  |  Passengers  |  Length  |  Wheelbase  |  Width  |  Turn.circle  |  Rear.seat.room  |  Luggage.room  |  Weight  |  Origin   |        Make        |\n -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n      0  |         Acura  |     Integra  |    Small  |    12.9000  |  15.9000  |    18.8000  |        25  |           31  |                None  |       Front  |          4  |      1.8000  |         140  |  6300  |          2890  |              Yes  |             13.2000  |           5  |     177  |        102  |     68  |           37  |            26.5  |            11  |    2705  |  non-USA  |     Acura Integra  |\n      1  |         Acura  |      Legend  |  Midsize  |    29.2000  |  33.9000  |    38.7000  |        18  |           25  |  Driver & Passenger  |       Front  |          6  |      3.2000  |         200  |  5500  |          2335  |              Yes  |             18.0000  |           5  |     195  |        115  |     71  |           38  |              30  |            15  |    3560  |  non-USA  |      Acura Legend  |\n      2  |          Audi  |          90  |  Compact  |    25.9000  |  29.1000  |    32.3000  |        20  |           26  |         Driver only  |       Front  |          6  |      2.8000  |         172  |  5500  |          2280  |              Yes  |             16.9000  |           5  |     180  |        102  |     67  |           37  |              28  |            14  |    3375  |  non-USA  |           Audi 90  |\n      3  |          Audi  |         100  |  Midsize  |    30.8000  |  37.7000  |    44.6000  |        19  |           26  |  Driver & Passenger  |       Front  |          6  |      2.8000  |         172  |  5500  |          2535  |              Yes  |             21.1000  |           6  |     193  |        106  |     70  |           37  |              31  |            17  |    3405  |  non-USA  |          Audi 100  |\n      4  |           BMW  |        535i  |  Midsize  |    23.7000  |  30.0000  |    36.2000  |        22  |           30  |         Driver only  |        Rear  |          4  |      3.5000  |         208  |  5700  |          2545  |              Yes  |             21.1000  |           4  |     186  |        109  |     69  |           39  |              27  |            13  |    3640  |  non-USA  |          BMW 535i  |\n      5  |         Buick  |     Century  |  Midsize  |    14.2000  |  15.7000  |    17.3000  |        22  |           31  |         Driver only  |       Front  |          4  |      2.2000  |         110  |  5200  |          2565  |               No  |             16.4000  |           6  |     189  |        105  |     69  |           41  |              28  |            16  |    2880  |      USA  |     Buick Century  |\n      6  |         Buick  |     LeSabre  |    Large  |    19.9000  |  20.8000  |    21.7000  |        19  |           28  |         Driver only  |       Front  |          6  |      3.8000  |         170  |  4800  |          1570  |               No  |             18.0000  |           6  |     200  |        111  |     74  |           42  |            30.5  |            17  |    3470  |      USA  |     Buick LeSabre  |\n      7  |         Buick  |  Roadmaster  |    Large  |    22.6000  |  23.7000  |    24.9000  |        16  |           25  |         Driver only  |        Rear  |          6  |      5.7000  |         180  |  4000  |          1320  |               No  |             23.0000  |           6  |     216  |        116  |     78  |           45  |            30.5  |            21  |    4105  |      USA  |  Buick Roadmaster  |\n      8  |         Buick  |     Riviera  |  Midsize  |    26.3000  |  26.3000  |    26.3000  |        19  |           27  |         Driver only  |       Front  |          6  |      3.8000  |         170  |  4800  |          1690  |               No  |             18.8000  |           5  |     198  |        108  |     73  |           41  |            26.5  |            14  |    3495  |      USA  |     Buick Riviera  |\n      9  |      Cadillac  |     DeVille  |    Large  |    33.0000  |  34.7000  |    36.3000  |        16  |           25  |         Driver only  |       Front  |          8  |      4.9000  |         200  |  4100  |          1510  |               No  |             18.0000  |           6  |     206  |        114  |     73  |           43  |              35  |            18  |    3620  |      USA  |  Cadillac DeVille  |\n\n\n\n\nIf we explicitly try compute a statistic on the \nModel\n column for example, we will get an exception, but bulk column statistics\nyield the desired result as shown below (in this case we use the \ndescribe()\n method, but we could have equally used the \nstats().xxx()\n \nmethods to compute any one particular statistic).\n\n\n\n\n\nDataFrame<Integer,String> frame = loadCarDataset();\nDataFrame<StatType,String> colStats = frame.cols().describe(\n    StatType.COUNT, StatType.MEAN, StatType.VARIANCE, StatType.SKEWNESS, StatType.SUM\n);\n\n\n\n\n\n  Index    |   Min.Price   |     Price     |   Max.Price    |   MPG.city    |  MPG.highway  |  EngineSize  |   Horsepower    |        RPM        |   Rev.per.mile    |  Fuel.tank.capacity  |  Passengers  |     Length     |   Wheelbase    |     Width     |  Turn.circle  |      Weight       |\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    COUNT  |      93.0000  |      93.0000  |       93.0000  |      93.0000  |      93.0000  |     93.0000  |        93.0000  |          93.0000  |          93.0000  |             93.0000  |     93.0000  |       93.0000  |       93.0000  |      93.0000  |      93.0000  |          93.0000  |\n     MEAN  |  17.12580645  |  19.50967742  |   21.89892473  |   22.3655914  |  29.08602151  |  2.66774194  |   143.82795699  |    5280.64516129  |    2332.20430108  |         16.66451613  |  5.08602151  |  183.20430108  |  103.94623656  |  69.37634409  |  38.95698925  |    3072.90322581  |\n VARIANCE  |  76.49302244  |  93.30457924  |  121.67097709  |  31.58228144  |  28.42730248  |  1.07612202  |  2743.07877513  |  356088.70967742  |  246518.72954652  |         10.75427069  |  1.07947639  |  213.22954652  |   46.50794764  |  14.28073866  |  10.38943432  |  347977.89270687  |\n SKEWNESS  |    1.1829892  |   1.53308186  |    2.03385866  |   1.70443015  |   1.22989674  |  0.85941842  |     0.95172825  |      -0.25853269  |       0.28154602  |           0.1081462  |  0.06251685  |   -0.09009462  |    0.11372684  |   0.26402738  |  -0.13356858  |      -0.14366904  |\n      SUM  |    1592.7000  |    1814.4000  |     2036.6000  |    2080.0000  |    2705.0000  |    248.1000  |     13376.0000  |      491100.0000  |      216895.0000  |           1549.8000  |    473.0000  |    17038.0000  |     9667.0000  |    6452.0000  |    3623.0000  |      285780.0000  |\n\n\n\n\nNull Handling\n\n\nTo be completed...\n\n\nBi-Variate Statistics\n\n\nThe Morpheus API makes it easy to compute the covariance or correlation between any two rows or columns, or compute the covariance and\ncorrelation matrix for an entire dataset. The statistics package in Morpheus uses an \nonline algorithm\n enabling these calculations to\nbe performed on an infinitely large dataset. In addition, the calculation of the covariance and correlation matrix of a dataset is an\n\nembarrassingly parallel\n problem that is well suited to a Fork & Join decomposition \nin order to improve performance on current multi-core microprocessor architectures.\n\n\nCovariance\n\n\nThe \nDataFrameAxisStats<R,C>\n interface exposes two overloaded \ncovariance()\n methods, one of which can be used to compute the covariance of \nany two vectors in either the row or the column dimension, while the other yields the full covariance matrix for the dataset. Let us consider \nthe motor vehicle dataset first introduced in the \noverview\n and compute the covariance between \nPrice\n and \nHorsepower\n, \nwhich we expect to be positive, and the covariance between \nEngineSize\n and \nMPG.city\n which we expected to be negative.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadCarDataset();\ndouble covar1 = frame.cols().stats().covariance(\"Price\", \"Horsepower\");\ndouble covar2 = frame.cols().stats().covariance(\"EngineSize\", \"MPG.city\");\nSystem.out.printf(\"\\nCovariance between Price & Horsepower = %.2f\", covar1);\nSystem.out.printf(\"\\nCovariance between EngineSize and MPG.city = %.2f\", covar2);\n\n\n\n\n\nCovariance between Price & Horsepower = 398.76\nCovariance between EngineSize and MPG.city = -4.14\n\n\n\n\nIt is also easy to compute the covariance matrix for all numeric columns in the frame as follows:\n\n\n\n\n\nDataFrame<Integer,String> frame = loadCarDataset();\nDataFrame<String,String> covm = frame.cols().stats().covariance();\ncovm.out().print(100, formats -> {\n    formats.setDecimalFormat(\"0.000;-0.000\", 1);\n});\n\n\n\n\n\n       Index         |  Min.Price  |    Price    |  Max.Price  |  MPG.city   |  MPG.highway  |  EngineSize  |  Horsepower  |      RPM      |  Rev.per.mile  |  Fuel.tank.capacity  |  Passengers  |   Length    |  Wheelbase  |    Width    |  Turn.circle  |    Weight     |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n          Min.Price  |     76.493  |     81.998  |     87.477  |    -30.615  |      -27.045  |       5.856  |     367.574  |     -222.321  |     -2042.672  |              18.223  |       0.556  |     70.735  |     30.822  |     16.290  |       12.083  |     3438.919  |\n              Price  |     81.998  |     93.305  |    104.585  |    -32.275  |      -28.876  |       5.986  |     398.765  |      -28.561  |     -2044.978  |              19.623  |       0.581  |     71.037  |     32.994  |     16.646  |       12.223  |     3687.667  |\n          Max.Price  |     87.477  |    104.585  |    121.671  |    -33.958  |      -30.733  |       6.122  |     430.076  |      164.653  |     -2048.416  |              21.027  |       0.610  |     71.344  |     35.167  |     17.024  |       12.365  |     3937.552  |\n           MPG.city  |    -30.615  |    -32.275  |    -33.958  |     31.582  |       28.283  |      -4.139  |    -197.980  |     1217.479  |      1941.631  |             -14.986  |      -2.434  |    -54.673  |    -25.567  |    -15.302  |      -12.071  |    -2795.095  |\n        MPG.highway  |    -27.045  |    -28.876  |    -30.733  |     28.283  |       28.427  |      -3.467  |    -172.865  |      997.335  |      1555.243  |             -13.744  |      -2.584  |    -42.268  |    -22.376  |    -12.902  |      -10.203  |    -2549.655  |\n         EngineSize  |      5.856  |      5.986  |      6.122  |     -4.139  |       -3.467  |       1.076  |      39.777  |     -339.164  |      -424.412  |               2.583  |       0.402  |     11.820  |      5.182  |      3.399  |        2.603  |      517.133  |\n         Horsepower  |    367.574  |    398.765  |    430.076  |   -197.980  |     -172.865  |      39.777  |    2743.079  |     1146.634  |    -15610.704  |             122.254  |       0.504  |    421.296  |    173.893  |    127.544  |       94.743  |    22825.505  |\n                RPM  |   -222.321  |    -28.561  |    164.653  |   1217.479  |      997.335  |    -339.164  |    1146.634  |   356088.710  |    146589.323  |            -652.325  |    -289.621  |  -3844.916  |  -1903.769  |  -1217.093  |     -972.581  |  -150636.133  |\n       Rev.per.mile  |  -2042.672  |  -2044.978  |  -2048.416  |   1941.631  |     1555.243  |    -424.412  |  -15610.704  |   146589.323  |    246518.730  |            -992.747  |    -172.800  |  -5004.314  |  -2156.293  |  -1464.371  |    -1173.328  |  -215349.676  |\n Fuel.tank.capacity  |     18.223  |     19.623  |     21.027  |    -14.986  |      -13.744  |       2.583  |     122.254  |     -652.325  |      -992.747  |              10.754  |       1.609  |     33.064  |     16.945  |      9.898  |        7.096  |     1729.468  |\n         Passengers  |      0.556  |      0.581  |      0.610  |     -2.434  |       -2.584  |       0.402  |       0.504  |     -289.621  |      -172.800  |               1.609  |       1.079  |      7.363  |      4.918  |      1.924  |        1.504  |      339.095  |\n             Length  |     70.735  |     71.037  |     71.344  |    -54.673  |      -42.268  |      11.820  |     421.296  |    -3844.916  |     -5004.314  |              33.064  |       7.363  |    213.230  |     82.022  |     45.368  |       34.781  |     6945.161  |\n          Wheelbase  |     30.822  |     32.994  |     35.167  |    -25.567  |      -22.376  |       5.182  |     173.893  |    -1903.769  |     -2156.293  |              16.945  |       4.918  |     82.022  |     46.508  |     20.803  |       15.900  |     3507.549  |\n              Width  |     16.290  |     16.646  |     17.024  |    -15.302  |      -12.902  |       3.399  |     127.544  |    -1217.093  |     -1464.371  |               9.898  |       1.924  |     45.368  |     20.803  |     14.281  |        9.962  |     1950.472  |\n        Turn.circle  |     12.083  |     12.223  |     12.365  |    -12.071  |      -10.203  |       2.603  |      94.743  |     -972.581  |     -1173.328  |               7.096  |       1.504  |     34.781  |     15.900  |      9.962  |       10.389  |     1479.365  |\n             Weight  |   3438.919  |   3687.667  |   3937.552  |  -2795.095  |    -2549.655  |     517.133  |   22825.505  |  -150636.133  |   -215349.676  |            1729.468  |     339.095  |   6945.161  |   3507.549  |   1950.472  |     1479.365  |   347977.893  |\n\n\n\n\nComputing covariance in the row dimension is identical to the above example but operates on \nDataFrameAxisStats<R,C>\n returned \nfrom a call to the \nframe.rows().stats()\n method.\n\n\nCorrelation\n\n\nThe \nDataFrameAxisStats<R,C>\n interface exposes two overloaded \ncorrelation()\n methods, one of which can be used to compute the correlation of \nany two vectors in either the row or the column dimension, while the other yields the full correlation matrix for the dataset. Let us consider \nthe motor vehicle dataset first introduced in the \noverview\n and compute the correlation between \nPrice\n and \nHorsepower\n, \nwhich we expect to be positive, and the correlation between \nEngineSize\n and \nMPG.city\n which we expected to be negative.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadCarDataset();\ndouble correl1 = frame.cols().stats().correlation(\"Price\", \"Horsepower\");\ndouble correl2 = frame.cols().stats().correlation(\"EngineSize\", \"MPG.city\");\nSystem.out.printf(\"\\nCorrelation between Price & Horsepower = %.2f\", correl1);\nSystem.out.printf(\"\\nCorrelation between EngineSize and MPG.city = %.2f\", correl2);\n\n\n\n\n\nCorrelation between Price & Horsepower = 0.79\nCorrelation between EngineSize and MPG.city = -0.71\n\n\n\n\nIt is also easy to compute the correlation matrix for all numeric columns in the frame as shown below. As expected, the diagonal\nvalues of this \nDataFrame\n are equal to one as the correlation of a signal with itself is always one.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadCarDataset();\nDataFrame<String,String> correlm = frame.cols().stats().correlation();\ncorrelm.out().print(100, formats -> {\n    formats.setDecimalFormat(\"0.000;-0.000\", 1);\n});\n\n\n\n\n\n       Index         |  Min.Price  |  Price   |  Max.Price  |  MPG.city  |  MPG.highway  |  EngineSize  |  Horsepower  |   RPM    |  Rev.per.mile  |  Fuel.tank.capacity  |  Passengers  |  Length  |  Wheelbase  |  Width   |  Turn.circle  |  Weight  |\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n          Min.Price  |      1.000  |   0.971  |      0.907  |    -0.623  |       -0.580  |       0.645  |       0.802  |  -0.043  |        -0.470  |               0.635  |       0.061  |   0.554  |      0.517  |   0.493  |        0.429  |   0.667  |\n              Price  |      0.971  |   1.000  |      0.982  |    -0.595  |       -0.561  |       0.597  |       0.788  |  -0.005  |        -0.426  |               0.619  |       0.058  |   0.504  |      0.501  |   0.456  |        0.393  |   0.647  |\n          Max.Price  |      0.907  |   0.982  |      1.000  |    -0.548  |       -0.523  |       0.535  |       0.744  |   0.025  |        -0.374  |               0.581  |       0.053  |   0.443  |      0.468  |   0.408  |        0.348  |   0.605  |\n           MPG.city  |     -0.623  |  -0.595  |     -0.548  |     1.000  |        0.944  |      -0.710  |      -0.673  |   0.363  |         0.696  |              -0.813  |      -0.417  |  -0.666  |     -0.667  |  -0.721  |       -0.666  |  -0.843  |\n        MPG.highway  |     -0.580  |  -0.561  |     -0.523  |     0.944  |        1.000  |      -0.627  |      -0.619  |   0.313  |         0.587  |              -0.786  |      -0.466  |  -0.543  |     -0.615  |  -0.640  |       -0.594  |  -0.811  |\n         EngineSize  |      0.645  |   0.597  |      0.535  |    -0.710  |       -0.627  |       1.000  |       0.732  |  -0.548  |        -0.824  |               0.759  |       0.373  |   0.780  |      0.732  |   0.867  |        0.778  |   0.845  |\n         Horsepower  |      0.802  |   0.788  |      0.744  |    -0.673  |       -0.619  |       0.732  |       1.000  |   0.037  |        -0.600  |               0.712  |       0.009  |   0.551  |      0.487  |   0.644  |        0.561  |   0.739  |\n                RPM  |     -0.043  |  -0.005  |      0.025  |     0.363  |        0.313  |      -0.548  |       0.037  |   1.000  |         0.495  |              -0.333  |      -0.467  |  -0.441  |     -0.468  |  -0.540  |       -0.506  |  -0.428  |\n       Rev.per.mile  |     -0.470  |  -0.426  |     -0.374  |     0.696  |        0.587  |      -0.824  |      -0.600  |   0.495  |         1.000  |              -0.610  |      -0.335  |  -0.690  |     -0.637  |  -0.780  |       -0.733  |  -0.735  |\n Fuel.tank.capacity  |      0.635  |   0.619  |      0.581  |    -0.813  |       -0.786  |       0.759  |       0.712  |  -0.333  |        -0.610  |               1.000  |       0.472  |   0.690  |      0.758  |   0.799  |        0.671  |   0.894  |\n         Passengers  |      0.061  |   0.058  |      0.053  |    -0.417  |       -0.466  |       0.373  |       0.009  |  -0.467  |        -0.335  |               0.472  |       1.000  |   0.485  |      0.694  |   0.490  |        0.449  |   0.553  |\n             Length  |      0.554  |   0.504  |      0.443  |    -0.666  |       -0.543  |       0.780  |       0.551  |  -0.441  |        -0.690  |               0.690  |       0.485  |   1.000  |      0.824  |   0.822  |        0.739  |   0.806  |\n          Wheelbase  |      0.517  |   0.501  |      0.468  |    -0.667  |       -0.615  |       0.732  |       0.487  |  -0.468  |        -0.637  |               0.758  |       0.694  |   0.824  |      1.000  |   0.807  |        0.723  |   0.872  |\n              Width  |      0.493  |   0.456  |      0.408  |    -0.721  |       -0.640  |       0.867  |       0.644  |  -0.540  |        -0.780  |               0.799  |       0.490  |   0.822  |      0.807  |   1.000  |        0.818  |   0.875  |\n        Turn.circle  |      0.429  |   0.393  |      0.348  |    -0.666  |       -0.594  |       0.778  |       0.561  |  -0.506  |        -0.733  |               0.671  |       0.449  |   0.739  |      0.723  |   0.818  |        1.000  |   0.778  |\n             Weight  |      0.667  |   0.647  |      0.605  |    -0.843  |       -0.811  |       0.845  |       0.739  |  -0.428  |        -0.735  |               0.894  |       0.553  |   0.806  |      0.872  |   0.875  |        0.778  |   1.000  |\n\n\n\n\nComputing correlation in the row dimension is identical to the above example but operates on \nDataFrameAxisStats<R,C>\n returned \nfrom a call to the \nframe.rows().stats()\n method.\n\n\nPerformance\n\n\nDemean Example\n\n\nEarlier, two separate techniques were presented as a way of de-meaning the values of a \nDataFrame\n in the row dimension. The first\napproach used the \nforEach()\n method to iterate over all rows, while the second used a traditional for-loop creating a new row object\nfor each iteration in the loop. \n\n\nBelow we assess the relative performance of these techniques. For the former case, we use both \nsequential\n and \nparallel\n iteration \nwhich is natively supported in the Morpheus API. The latter approach cannot be parallelized without writing a lot of code. The results \nare as follows.\n\n\n\n    \n\n\n\n\n\nFirstly, kudos to the Java Virtual Machine in that the so called \nBad\n approach is not that much slower than the \nGood(sequential)\n approach,\neven given that far more objecs are created in the former scenario. They are all very short lived objects, and we know the current generation\ngarbage collection algorithms are heavily optimized to deal with that kind of trash. The \nforEach()\n approach is easily parallelizable \nsimply by calling the \nparallel()\n method on \nDataFrameRows\n before starting the iteration. This clearly makes a big difference in relative\nperformance. The code for this analysis is shown below.\n\n\n\n\n\n//Create a 1,000,000x10 DataFrame of random double precision values\nDataFrame<LocalDate,String> frame = random(1000000, \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\");\n\n//Run 10 performance samples, randomizing the frame before each test\nDataFrame<String,String> timing = PerfStat.run(10, TimeUnit.MILLISECONDS, false, tasks -> {\n\n    tasks.beforeEach(() -> frame.applyDoubles(v -> Math.random() * 100d));\n\n    tasks.put(\"Bad\", () -> {\n        for (int i=0; i<frame.rowCount(); ++i) {\n            final DataFrameRow<LocalDate,String> row = frame.rowAt(i);\n            final double mean = row.stats().mean();\n            row.applyDoubles(v -> v.getDouble() - mean);\n        }\n        return frame;\n    });\n\n    tasks.put(\"Good(sequential)\", () -> {\n        frame.rows().forEach(row -> {\n            final double mean = row.stats().mean();\n            row.applyDoubles(v -> v.getDouble() - mean);\n        });\n        return frame;\n    });\n\n    tasks.put(\"Good(parallel)\", () -> {\n        frame.rows().parallel().forEach(row -> {\n            final double mean = row.stats().mean();\n            row.applyDoubles(v -> v.getDouble() - mean);\n        });\n        return frame;\n    });\n});\n\n//Plot a chart of the results\nChart.create().withBarPlot(timing, false, chart -> {\n    chart.title().withText(\"DataFrame Row Demeaning Performance (10 Samples)\");\n    chart.subtitle().withText(\"DataFrame Dimension: 1 Million x 10\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time (Milliseconds)\");\n    chart.legend().on().bottom();\n    chart.show();\n});\n\n\n\n\n\nCorrelation Example\n\n\nIt was suggested that computing the covariance or correlation matrix for a dataset was an \nembarrassingly parallel\n \nproblem. The example below creates a \nDataFrame\n of 1 million rows by 10 columns, and computes the correlation in the column\ndimension using both \nsequential\n and \nparallel\n execution, and then plots the results.\n\n\n\n    \n\n\n\n\n\nThese results suggest a roughly 3 times improvement in performance in parallel execution for this example. The performance \ndifferential for such a scenario is likely to be sensitive to many factors, especially the shape of the \nDataFrame\n, so take \nthese results with a pinch of salt (as with all benchmark results). Nevertheless, this does demonstrate that parallel execution \ncan give a significant boost, so it is worth exploring in a performance critical scenario. The code for this example is as follows:\n\n\n\n\n\n//Create a 1,000,000x10 DataFrame of random double precision values\nDataFrame<LocalDate,String> frame = random(1000000, \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\");\n\n//Run 10 performance samples, randomizing the frame before each test\nDataFrame<String,String> timing = PerfStat.run(10, TimeUnit.MILLISECONDS, false, tasks -> {\n    tasks.beforeEach(() -> frame.applyDoubles(v -> Math.random() * 100d));\n    tasks.put(\"Sequential\", () -> frame.cols().stats().correlation());\n    tasks.put(\"Parallel\", () -> frame.cols().parallel().stats().correlation());\n});\n\n//Plot a chart of the results\nChart.create().withBarPlot(timing, false, chart -> {\n    chart.title().withText(\"DataFrame Correlation Matrix Performance (10 Samples)\");\n    chart.subtitle().withText(\"DataFrame Dimension: 1 Million x 10\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time (Milliseconds)\");\n    chart.legend().on().bottom();\n    chart.show();\n});",
            "title": "Descriptive Statistics"
        },
        {
            "location": "/analysis/statistics/#descriptive-statistics",
            "text": "",
            "title": "Descriptive Statistics"
        },
        {
            "location": "/analysis/statistics/#introduction",
            "text": "Computing descriptive statistics is a fundamental feature of the Morpheus API, and to this end, a package is devoted \nto providing a clean and consistent framework that is then integrated with the Morpheus  Array  and  DataFrame  interfaces. \nAll the statistic calculators implement  online algorithms , so they can be used to compute stats on infinitely large \ndatasets. In addition, the Morpheus statistics package can be used standalone, independent of the  Array  and  DataFrame  \nstructures.  The table below enumerates the uni-variate statistics currently supported. Computing descriptive statistics on sample \ndata reduces dimensionality in that an  Array  can be reduced to a scalar, or stats on  DataFrame  columns can be \nreduced to an array of values representing the statistic for each column. To that end, the  Stats  interface, which \nexposes all the uni-variate statistics methods, is parameterized by the return type of the functions it declares so that \nit can either return a scalar value, or some other data structure aggregate.     Method  Description  Details      count()  The number of observations, ignoring nulls     min()  The minimum value, ignoring nulls  Details    max()  The maximim value, ignorning nulls  Details    mean()  The first moment, or the arithmetic mean or average, ignoring nulls  Details    variance()  The un-biased variance or second moment, a measure of dispersion  Details    stdDev()  The un-biased standard deviation, a measure of dispersion  Details    skew()  The third moment, or skewness, a measure of the asymmetry in the distribution  Details    kurtosis()  The fourth moment, or Kurtosis, a measure of the \"tailedness\" of the probability distribution  Details    median()  The value separating the higher half of the data, or 50th percentile  Details    mad()  The Mean Absolute Deviation from a central point, another measure of dispersion  Details    sem()  The standard error of the mean  Details    geoMean()  The geometric mean, another measure of central tendency  Details    sum()  The summation of all values, ignoring nulls  Details    sumOfSquares()  The sum, over non-null observations, of the squared differences from the mean  Details    autocorr(int lag)  The autocorrelation, which is the correlation of a signal with a delayed copy of itself  Details    percentile(double nth)  The percentile value below which n% of values fall, ignoring nulls  Details     The following sections mostly operate on a  DataFrame  of random double precision values with an arbitrary number of\nrows and columns, which can be manufactured with the code below. In other examples the motor vehicle dataset introduced \nin the  overview  will be used to demonstrate how statistics functions behave on frames with mixed data types.   /**\n * Returns a frame of random double precision values\n * @param rowCount  the row count for frame\n * @param columns   the column keys\n * @return          the newly created frame\n */\nstatic DataFrame<LocalDate,String> random(int rowCount, String... columns) {\n    final LocalDate start = LocalDate.now().minusDays(rowCount);\n    return DataFrame.ofDoubles(\n        Range.of(0, rowCount).map(start::plusDays),\n        Array.of(columns),\n        value -> Math.random() * 100d\n    );\n}  For example:   random(100, \"A\", \"B\", \"C\", \"D\", \"E\").out().print(10);  \n   Index     |       A       |       B       |       C       |       D       |       E       |\n----------------------------------------------------------------------------------------------\n 2014-04-18  |  27.65695706  |  24.12541112  |  54.53927679  |  29.53942097  |  36.32857957  |\n 2014-04-19  |  77.17892101  |  91.65862134  |  31.89758283  |  75.86327696  |  65.28820416  |\n 2014-04-20  |  36.11837803  |   3.50787279  |  18.35033145  |  29.87153499  |  89.64915738  |\n 2014-04-21  |   6.11609249  |  61.52213182  |   76.0263036  |  91.45821122  |  15.68900718  |\n 2014-04-22  |  87.76836565  |   5.14224821  |  97.79979071  |  38.69432549  |  16.94097048  |\n 2014-04-23  |  72.72059023  |  89.50857903  |  23.38300753  |  71.09288654  |  20.66341413  |\n 2014-04-24  |  84.01645441  |   2.27975471  |    3.4054976  |  75.16447082  |  68.66698321  |\n 2014-04-25  |  34.07626388  |  60.48921016  |  12.06511821  |  55.99893352  |  51.31240247  |\n 2014-04-26  |  56.80751283  |  36.48578987  |  17.44059537  |  78.88660804  |  77.63219394  |\n 2014-04-27  |  92.76964851  |  56.95184403  |    7.3521691  |  27.01594691  |   0.02729372  |",
            "title": "Introduction"
        },
        {
            "location": "/analysis/statistics/#uni-variate-statistics",
            "text": "",
            "title": "Uni-Variate Statistics"
        },
        {
            "location": "/analysis/statistics/#frame-level-statistics",
            "text": "The  stats()  method on the  DataFrame  interface returns  Stats<Double>  that can be used to compute descriptive\nstatistics over all values in the frame. The  Stats  interface is parameterised in  Double  to indicate that scalar \nvalues are generated by all the methods. The code below demonstrate this use case:   //Create 100x5 DataFrame of random doubles\nDataFrame<Integer,String> frame = random(100, \"A\", \"B\", \"C\", \"D\", \"E\");\n\nSystem.out.printf(\"Count = %.4f\\n\", frame.stats().count());\nSystem.out.printf(\"Minimum = %.4f\\n\", frame.stats().min());\nSystem.out.printf(\"Maximum = %.4f\\n\", frame.stats().max());\nSystem.out.printf(\"Mean = %.4f\\n\", frame.stats().mean());\nSystem.out.printf(\"Median = %.4f\\n\", frame.stats().median());\nSystem.out.printf(\"Variance = %.4f\\n\", frame.stats().variance());\nSystem.out.printf(\"StdDev = %.4f\\n\", frame.stats().stdDev());\nSystem.out.printf(\"Skew = %.4f\\n\", frame.stats().skew());\nSystem.out.printf(\"Kurtosis = %.4f\\n\", frame.stats().kurtosis());\nSystem.out.printf(\"Mean Abs Deviation = %.4f\\n\", frame.stats().mad());\nSystem.out.printf(\"Sum = %.4f\\n\", frame.stats().sum());\nSystem.out.printf(\"Sum of Squares = %.4f\\n\", frame.stats().sumSquares());\nSystem.out.printf(\"Std Error of Mean = %.4f\\n\", frame.stats().sem());\nSystem.out.printf(\"Geometric Mean = %.4f\\n\", frame.stats().geoMean());\nSystem.out.printf(\"Percentile(75th) = %.4f\\n\", frame.stats().percentile(0.75d));\nSystem.out.printf(\"Autocorrelation(2) = %.4f\\n\", frame.stats().autocorr(2));  \nCount = 500.0000\nMinimum = 0.0912\nMaximum = 99.2049\nMean = 50.4180\nMedian = 49.8189\nVariance = 803.8727\nStdDev = 28.3526\nSkew = -0.0670\nKurtosis = -1.1654\nMean Abs Deviation = 24.4590\nSum = 25209.0143\nSum of Squares = 1672121.2725\nStd Error of Mean = 1.2680\nGeometric Mean = 37.1386\nPercentile(75) = 75.4636\nAutocorrelation(2) = -0.0671",
            "title": "Frame Level Statistics"
        },
        {
            "location": "/analysis/statistics/#single-row-column-statistics",
            "text": "The  Stats<Double>  interface can be accessed on individual rows and columns, namely by calling the  stats()  method on the  DataFrameRow  or  DataFrameColumn  interface. Individual rows or columns can be accessed via  ordinal  or  key  via the  rowAt()  and  colAt()  methods on  DataFrame , as illustrated below.   //Create 100x5 DataFrame of random doubles\nDataFrame<LocalDate,String> frame = random(100, \"A\", \"B\", \"C\", \"D\", \"E\");\n\n//Capture stats interface for two rows independently sourced by ordinal and key\nLocalDate date = frame.rows().key(3);\nStats<Double> stats1 = frame.rowAt(3).stats();\nStats<Double> stats2 = frame.rowAt(date).stats();\n\nStatType.univariate().forEach(statType -> {\n    switch (statType) {\n        case COUNT:         assert(stats1.count().doubleValue() == stats2.count());                     break;\n        case MIN:           assert(stats1.min().doubleValue() == stats2.min());                         break;\n        case MAX:           assert(stats1.max().doubleValue() == stats2.max());                         break;\n        case MEAN:          assert(stats1.mean().doubleValue() == stats2.mean());                       break;\n        case MEDIAN:        assert(stats1.median().doubleValue() == stats2.median());                   break;\n        case VARIANCE:      assert(stats1.variance().doubleValue() == stats2.variance());               break;\n        case STD_DEV:       assert(stats1.stdDev().doubleValue() == stats2.stdDev());                   break;\n        case KURTOSIS:      assert(stats1.kurtosis().doubleValue() == stats2.kurtosis());               break;\n        case MAD:           assert(stats1.mad().doubleValue() == stats2.mad());                         break;\n        case SEM:           assert(stats1.sem().doubleValue() == stats2.sem());                         break;\n        case GEO_MEAN:      assert(stats1.geoMean().doubleValue() == stats2.geoMean());                 break;\n        case SUM:           assert(stats1.sum().doubleValue() == stats2.sum());                         break;\n        case SUM_SQUARES:   assert(stats1.sumSquares().doubleValue() == stats2.sumSquares());           break;\n        case AUTO_CORREL:   assert(stats1.autocorr(2).doubleValue() == stats2.autocorr(2));             break;\n        case PERCENTILE:    assert(stats1.percentile(0.75).doubleValue() == stats2.percentile(0.75));   break;\n    }\n});  Given the symmetrical nature of the Morpheus  DataFrame  API, the exact same approach follows in the column dimension.   //Create 100x5 DataFrame of random doubles\nDataFrame<LocalDate,String> frame = random(100, \"A\", \"B\", \"C\", \"D\", \"E\");\n\n//Capture stats interface for two columns independently sourced by ordinal and key\nStats<Double> stats1 = frame.colAt(3).stats();\nStats<Double> stats2 = frame.colAt(\"D\").stats();\n\nStatType.univariate().forEach(statType -> {\n    switch (statType) {\n        case COUNT:         assert(stats1.count().doubleValue() == stats2.count());                     break;\n        case MIN:           assert(stats1.min().doubleValue() == stats2.min());                         break;\n        case MAX:           assert(stats1.max().doubleValue() == stats2.max());                         break;\n        case MEAN:          assert(stats1.mean().doubleValue() == stats2.mean());                       break;\n        case MEDIAN:        assert(stats1.median().doubleValue() == stats2.median());                   break;\n        case VARIANCE:      assert(stats1.variance().doubleValue() == stats2.variance());               break;\n        case STD_DEV:       assert(stats1.stdDev().doubleValue() == stats2.stdDev());                   break;\n        case KURTOSIS:      assert(stats1.kurtosis().doubleValue() == stats2.kurtosis());               break;\n        case MAD:           assert(stats1.mad().doubleValue() == stats2.mad());                         break;\n        case SEM:           assert(stats1.sem().doubleValue() == stats2.sem());                         break;\n        case GEO_MEAN:      assert(stats1.geoMean().doubleValue() == stats2.geoMean());                 break;\n        case SUM:           assert(stats1.sum().doubleValue() == stats2.sum());                         break;\n        case SUM_SQUARES:   assert(stats1.sumSquares().doubleValue() == stats2.sumSquares());           break;\n        case AUTO_CORREL:   assert(stats1.autocorr(2).doubleValue() == stats2.autocorr(2));             break;\n        case PERCENTILE:    assert(stats1.percentile(0.75).doubleValue() == stats2.percentile(0.75));   break;\n    }\n});",
            "title": "Single Row / Column Statistics"
        },
        {
            "location": "/analysis/statistics/#demean-rows",
            "text": "Consider an example where we would like to demean the  DataFrame  in question along the row dimension, that is to say,\nwe wish to compute the mean for each row, and subtract that amount from each item in the row.  This could be achieveed\nas follows.   //GOOD: Row iteration using forEach()\nframe.rows().forEach(row -> {\n    final double mean = row.stats().mean();\n    row.applyDoubles(v -> {\n        return v.getDouble() - mean;\n    });\n});  In this scenario, the row object which serves as the lambda argument is reused across each invocation, as is the  Stats  object,\nmaking this extremely efficient. A much less efficient mechanism would be to create a new row and new  Stats  object for each\niteration through the loop as shown below. To see how performance of these approaches compares, see section below on  performance .   //BAD: Row iteration creating a new row each time\nfor (int i=0; i<frame.rowCount(); ++i) {\n    final DataFrameRow<LocalDate,String> row = frame.rowAt(i);\n    final double mean = row.stats().mean();\n    row.applyDoubles(v -> {\n        return v.getDouble() - mean;\n    });\n}",
            "title": "Demean Rows"
        },
        {
            "location": "/analysis/statistics/#multiple-row-column-statistics",
            "text": "Calculating descriptive statistics in batch for all rows or columns is also supported by the Morpheus API. There are two\nmechanisms which can be used to achieve this, the first is via the  stats()  method on  DataFrameRows  or  DataFrameColumns ,\nand the second is via the  describe()  method.  The same  Stats<?>  interface described earlier is returned from  DataFrameRows.stats()  but in this case it is parameterized\nas  Stats<DataFrame<R,C>>  as this implementation yields a  DataFrame  of values rather than a scalar value. In our example, DataFrame<LocalDate,String>  row statistics will be presented as  DataFrame<LocalDate,StatType>  and column statistics will \nbe presented as  DataFrame<StatType,String> . Consider the following code for accessing bulk row statistics:   //Create 10x5 DataFramem of random doubles\nDataFrame<LocalDate,String> frame = random(10, \"A\", \"B\", \"C\", \"D\", \"E\");\n\nDataFrame<LocalDate,StatType> count = frame.rows().stats().count();\nDataFrame<LocalDate,StatType> min = frame.rows().stats().min();\nDataFrame<LocalDate,StatType> max = frame.rows().stats().max();\nDataFrame<LocalDate,StatType> mean = frame.rows().stats().mean();\nDataFrame<LocalDate,StatType> median = frame.rows().stats().median();\nDataFrame<LocalDate,StatType> variance = frame.rows().stats().variance();\nDataFrame<LocalDate,StatType> stdDev = frame.rows().stats().stdDev();\nDataFrame<LocalDate,StatType> kurtosis = frame.rows().stats().kurtosis();\nDataFrame<LocalDate,StatType> mad = frame.rows().stats().mad();\nDataFrame<LocalDate,StatType> sum = frame.rows().stats().sum();\nDataFrame<LocalDate,StatType> sumLogs = frame.rows().stats().sumLogs();\nDataFrame<LocalDate,StatType> sumSquares = frame.rows().stats().sumSquares();\nDataFrame<LocalDate,StatType> sem = frame.rows().stats().sem();\nDataFrame<LocalDate,StatType> geoMean = frame.rows().stats().geoMean();\nDataFrame<LocalDate,StatType> autocorr = frame.rows().stats().autocorr(1);\nDataFrame<LocalDate,StatType> percentile = frame.rows().stats().percentile(0.75d);  To illustrate what one of these frames looks like, let us look at the kurtosis results:   kurtosis.out().print();  \n   Index     |   KURTOSIS    |\n------------------------------\n 2014-04-18  |    0.9224301  |\n 2014-04-19  |  -1.03762054  |\n 2014-04-20  |  -2.19408392  |\n 2014-04-21  |  -1.60940179  |\n 2014-04-22  |   0.33646256  |\n 2014-04-23  |  -2.79071762  |\n 2014-04-24  |  -2.34090553  |\n 2014-04-25  |  -2.76200702  |\n 2014-04-26  |  -1.62476724  |\n 2014-04-27  |   -1.6700161  |  Alternatively, we can also create a single  DataFrame  with multiple row statistics using the  describe()  method as follows:   DataFrame<LocalDate,StatType> rowStats = frame.rows().describe(\n    StatType.COUNT, StatType.MEAN, StatType.VARIANCE, StatType.SKEWNESS, StatType.SUM\n);  \n   Index     |  COUNT   |     MEAN      |    VARIANCE     |   SKEWNESS    |      SUM       |\n--------------------------------------------------------------------------------------------\n 2014-04-18  |  5.0000  |   72.4181454  |   619.98499044  |   0.21026915  |  362.09072702  |\n 2014-04-19  |  5.0000  |  43.53540087  |  1224.58077656  |   1.31797636  |  217.67700434  |\n 2014-04-20  |  5.0000  |  72.50600374  |   1144.3222537  |  -1.42186326  |  362.53001871  |\n 2014-04-21  |  5.0000  |  76.15303235  |   398.36488382  |  -0.09378226  |  380.76516176  |\n 2014-04-22  |  5.0000  |  72.31809543  |   808.83023776  |   -1.2161613  |  361.59047717  |\n 2014-04-23  |  5.0000  |  57.73293757  |   746.28716947  |    0.0998814  |  288.66468783  |\n 2014-04-24  |  5.0000  |  55.14827133  |  1253.77051383  |  -0.48490569  |  275.74135666  |\n 2014-04-25  |  5.0000  |  27.59810985  |   857.21698695  |    1.7015654  |  137.99054927  |\n 2014-04-26  |  5.0000  |  50.02524398  |     7.73321175  |   0.74564028  |  250.12621991  |\n 2014-04-27  |  5.0000  |  33.56051921  |   843.19166011  |   1.55424455  |  167.80259607  |  The symmetrical nature of the Morpheus API means that bulk column statistics work in exactly the same way, except the resulting DataFrame  is transposed and parameterized in this example as  DataFrame<StatType,String> . For completeness, consider the\nanalogous column based code:   //Create 100x5 DataFrame of random double precision values\nDataFrame<LocalDate,String> frame = random(100, \"A\", \"B\", \"C\", \"D\", \"E\");\n\nDataFrame<StatType,String> count = frame.cols().stats().count();\nDataFrame<StatType,String> min = frame.cols().stats().min();\nDataFrame<StatType,String> max = frame.cols().stats().max();\nDataFrame<StatType,String> mean = frame.cols().stats().mean();\nDataFrame<StatType,String> median = frame.cols().stats().median();\nDataFrame<StatType,String> variance = frame.cols().stats().variance();\nDataFrame<StatType,String> stdDev = frame.cols().stats().stdDev();\nDataFrame<StatType,String> kurtosis = frame.cols().stats().kurtosis();\nDataFrame<StatType,String> mad = frame.cols().stats().mad();\nDataFrame<StatType,String> sum = frame.cols().stats().sum();\nDataFrame<StatType,String> sumLogs = frame.cols().stats().sumLogs();\nDataFrame<StatType,String> sumSquares = frame.cols().stats().sumSquares();\nDataFrame<StatType,String> sem = frame.cols().stats().sem();\nDataFrame<StatType,String> geoMean = frame.cols().stats().geoMean();\nDataFrame<StatType,String> autocorr = frame.cols().stats().autocorr(1);\nDataFrame<StatType,String> percentile = frame.cols().stats().percentile(0.75d);  To illustrate the resulting structure, let us print the percentile  DataFrame  to standard out as follows:   percentile.out().print();  \n   Index     |       A       |       B       |       C       |       D       |       E       |\n----------------------------------------------------------------------------------------------\n PERCENTILE  |  84.53057879  |  69.39927582  |  71.70889746  |  77.60914534  |  77.43947475  |  Finally, the  describe()  method on  DataFrameColumns  can be used in the same way as demonstrated earlier to yield multiple \ndescriptive statistics on all columns in the frame.   DataFrame<StatType,String> colStats = frame.cols().describe(\n    StatType.COUNT, StatType.MEAN, StatType.VARIANCE, StatType.SKEWNESS, StatType.SUM\n);  \n  Index    |        A        |        B        |        C        |        D        |        E        |\n------------------------------------------------------------------------------------------------------\n    COUNT  |       100.0000  |       100.0000  |       100.0000  |       100.0000  |       100.0000  |\n     MEAN  |    53.21193611  |    46.66050934  |    47.30513686  |    52.83602104  |     53.5188048  |\n VARIANCE  |   1008.5668979  |   710.21278243  |    860.7008831  |   803.54694593  |   886.26568961  |\n SKEWNESS  |    -0.18291634  |     0.08827897  |     0.02910395  |    -0.05554874  |    -0.05292841  |\n      SUM  |  5321.19361134  |  4666.05093406  |  4730.51368606  |  5283.60210399  |  5351.88047967  |",
            "title": "Multiple Row / Column Statistics"
        },
        {
            "location": "/analysis/statistics/#expanding-window-statistics",
            "text": "It is often useful to compute summary statistics over an  expanding window , and the Morpheus API provides a clean and\nconsistent mechanism for doing this by again leveraging the  Stats<?>  interface. This means all the univariate statistics\ndiscussed above can be calculated in an expanding window fashion, in either the row or column dimension. The code below \ndemonstrates how to compute an expanding window mean of a  DataFrame  of random doubles, with a minimum window size of \n5 periods. Calling the  expanding()  method on the  DataFrameAxisStas<R,C>  presents a  Stats<?>  implementation with\nexpanding window semantics.   DataFrame<LocalDate,String> frame = random(100, \"A\", \"B\", \"C\", \"D\", \"E\");\nDataFrame<LocalDate,String> expandingMean = frame.cols().stats().expanding(5).mean();\nexpandingMean.out().print(10);  The first 10 rows of the results are shown below. Given that we specified a  minimum window size  of 5 periods, the first\n4 rows of the result contains NaN values, with the expanding means beginning in the 5th row.  \n   Index     |       A       |       B       |       C       |       D       |       E       |\n----------------------------------------------------------------------------------------------\n 2014-04-18  |          NaN  |          NaN  |          NaN  |          NaN  |          NaN  |\n 2014-04-19  |          NaN  |          NaN  |          NaN  |          NaN  |          NaN  |\n 2014-04-20  |          NaN  |          NaN  |          NaN  |          NaN  |          NaN  |\n 2014-04-21  |          NaN  |          NaN  |          NaN  |          NaN  |          NaN  |\n 2014-04-22  |  37.47079054  |  44.65618465  |  34.44764118  |  27.31206568  |  36.88510609  |\n 2014-04-23  |  37.90629169  |  41.12951198  |  37.81200548  |  32.72777564  |  37.89339758  |\n 2014-04-24  |  35.45602666  |  46.58345677  |  38.53475246  |  32.08935944  |   39.4862089  |\n 2014-04-25  |  34.89378526  |  47.23091383  |  41.02249931  |  32.98138908  |  35.84038259  |\n 2014-04-26  |  40.93359834  |  43.69633714  |  46.76153667  |  36.59786248  |  36.64151385  |\n 2014-04-27  |  38.62418036  |  48.01872857  |  48.72740658  |  39.21108838  |  40.22726792  |",
            "title": "Expanding Window Statistics"
        },
        {
            "location": "/analysis/statistics/#rolling-window-statistics",
            "text": "It is often useful to compute summary statistics over a  rolling window , and the Morpheus API provides a clean and\nconsistent mechanism for doing this by again leveraging the  Stats<?>  interface. This means all the univariate statistics\ndiscussed above can be calculated in an rolling window fashion, in either the row or column dimension. The code below \ndemonstrates how to compute an rolling window mean of a  DataFrame  of random doubles based on a window size of 5 periods. \nCalling the  rolling()  method on the  DataFrameAxisStas<R,C>  presents a  Stats<?>  implementation with rolling window \nsemantics.   DataFrame<LocalDate,String> frame = random(100, \"A\", \"B\", \"C\", \"D\", \"E\");\nDataFrame<LocalDate,String> rollingMean = frame.cols().stats().rolling(5).mean();\nrollingMean.out().print(10);  The first 10 rows of these results are shown below. As with the expanding window functionality, the first 4 rows are NaN\nbecause we require at least a window size of 5 periods to yield a statistic.  \n   Index     |       A       |       B       |       C       |       D       |       E       |\n----------------------------------------------------------------------------------------------\n 2014-04-18  |          NaN  |          NaN  |          NaN  |          NaN  |          NaN  |\n 2014-04-19  |          NaN  |          NaN  |          NaN  |          NaN  |          NaN  |\n 2014-04-20  |          NaN  |          NaN  |          NaN  |          NaN  |          NaN  |\n 2014-04-21  |          NaN  |          NaN  |          NaN  |          NaN  |          NaN  |\n 2014-04-22  |  48.86861615  |  49.43076987  |   23.0890681  |  46.37806901  |  31.72173443  |\n 2014-04-23  |  52.82603467  |  48.62150116  |  25.05736597  |   56.4940296  |  36.89243443  |\n 2014-04-24  |  55.63736749  |  56.85719458  |  24.74022673  |  71.88998581  |  31.63059599  |\n 2014-04-25  |  49.84144271  |  66.44284853  |  24.44921746  |  68.55549636  |  37.57049639  |\n 2014-04-26  |  50.72266995  |  79.98150692  |  39.67256206  |  60.98038021  |  48.18298467  |\n 2014-04-27  |  44.80180275  |  85.26550794  |  29.06426259  |  55.54849227  |  61.27390254  |",
            "title": "Rolling Window Statistics"
        },
        {
            "location": "/analysis/statistics/#non-numeric-data-handling",
            "text": "The Morpheus statistics API always presents results as  double  precision values, however it can operate on  int  and  long \ninput values by making the appropriate casts. Attempting to compute statistics on non-numeric data will result in an  Exception , \nhowever when computing bulk row or column statistics, Morpheus will automatically restrict its operations to numeric vectors.  For example, consider the motor vehicle dataset first introduced in the  overview  which has a number of\nnon-numeric columns such as  Manufacturer ,  Model ,  Type  and so on. The first 10 rows of the dataset are shown below.  \n  Index  |  Manufacturer  |    Model     |   Type    |  Min.Price  |   Price   |  Max.Price  |  MPG.city  |  MPG.highway  |       AirBags        |  DriveTrain  |  Cylinders  |  EngineSize  |  Horsepower  |  RPM   |  Rev.per.mile  |  Man.trans.avail  |  Fuel.tank.capacity  |  Passengers  |  Length  |  Wheelbase  |  Width  |  Turn.circle  |  Rear.seat.room  |  Luggage.room  |  Weight  |  Origin   |        Make        |\n -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n      0  |         Acura  |     Integra  |    Small  |    12.9000  |  15.9000  |    18.8000  |        25  |           31  |                None  |       Front  |          4  |      1.8000  |         140  |  6300  |          2890  |              Yes  |             13.2000  |           5  |     177  |        102  |     68  |           37  |            26.5  |            11  |    2705  |  non-USA  |     Acura Integra  |\n      1  |         Acura  |      Legend  |  Midsize  |    29.2000  |  33.9000  |    38.7000  |        18  |           25  |  Driver & Passenger  |       Front  |          6  |      3.2000  |         200  |  5500  |          2335  |              Yes  |             18.0000  |           5  |     195  |        115  |     71  |           38  |              30  |            15  |    3560  |  non-USA  |      Acura Legend  |\n      2  |          Audi  |          90  |  Compact  |    25.9000  |  29.1000  |    32.3000  |        20  |           26  |         Driver only  |       Front  |          6  |      2.8000  |         172  |  5500  |          2280  |              Yes  |             16.9000  |           5  |     180  |        102  |     67  |           37  |              28  |            14  |    3375  |  non-USA  |           Audi 90  |\n      3  |          Audi  |         100  |  Midsize  |    30.8000  |  37.7000  |    44.6000  |        19  |           26  |  Driver & Passenger  |       Front  |          6  |      2.8000  |         172  |  5500  |          2535  |              Yes  |             21.1000  |           6  |     193  |        106  |     70  |           37  |              31  |            17  |    3405  |  non-USA  |          Audi 100  |\n      4  |           BMW  |        535i  |  Midsize  |    23.7000  |  30.0000  |    36.2000  |        22  |           30  |         Driver only  |        Rear  |          4  |      3.5000  |         208  |  5700  |          2545  |              Yes  |             21.1000  |           4  |     186  |        109  |     69  |           39  |              27  |            13  |    3640  |  non-USA  |          BMW 535i  |\n      5  |         Buick  |     Century  |  Midsize  |    14.2000  |  15.7000  |    17.3000  |        22  |           31  |         Driver only  |       Front  |          4  |      2.2000  |         110  |  5200  |          2565  |               No  |             16.4000  |           6  |     189  |        105  |     69  |           41  |              28  |            16  |    2880  |      USA  |     Buick Century  |\n      6  |         Buick  |     LeSabre  |    Large  |    19.9000  |  20.8000  |    21.7000  |        19  |           28  |         Driver only  |       Front  |          6  |      3.8000  |         170  |  4800  |          1570  |               No  |             18.0000  |           6  |     200  |        111  |     74  |           42  |            30.5  |            17  |    3470  |      USA  |     Buick LeSabre  |\n      7  |         Buick  |  Roadmaster  |    Large  |    22.6000  |  23.7000  |    24.9000  |        16  |           25  |         Driver only  |        Rear  |          6  |      5.7000  |         180  |  4000  |          1320  |               No  |             23.0000  |           6  |     216  |        116  |     78  |           45  |            30.5  |            21  |    4105  |      USA  |  Buick Roadmaster  |\n      8  |         Buick  |     Riviera  |  Midsize  |    26.3000  |  26.3000  |    26.3000  |        19  |           27  |         Driver only  |       Front  |          6  |      3.8000  |         170  |  4800  |          1690  |               No  |             18.8000  |           5  |     198  |        108  |     73  |           41  |            26.5  |            14  |    3495  |      USA  |     Buick Riviera  |\n      9  |      Cadillac  |     DeVille  |    Large  |    33.0000  |  34.7000  |    36.3000  |        16  |           25  |         Driver only  |       Front  |          8  |      4.9000  |         200  |  4100  |          1510  |               No  |             18.0000  |           6  |     206  |        114  |     73  |           43  |              35  |            18  |    3620  |      USA  |  Cadillac DeVille  |  If we explicitly try compute a statistic on the  Model  column for example, we will get an exception, but bulk column statistics\nyield the desired result as shown below (in this case we use the  describe()  method, but we could have equally used the  stats().xxx()  \nmethods to compute any one particular statistic).   DataFrame<Integer,String> frame = loadCarDataset();\nDataFrame<StatType,String> colStats = frame.cols().describe(\n    StatType.COUNT, StatType.MEAN, StatType.VARIANCE, StatType.SKEWNESS, StatType.SUM\n);  \n  Index    |   Min.Price   |     Price     |   Max.Price    |   MPG.city    |  MPG.highway  |  EngineSize  |   Horsepower    |        RPM        |   Rev.per.mile    |  Fuel.tank.capacity  |  Passengers  |     Length     |   Wheelbase    |     Width     |  Turn.circle  |      Weight       |\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    COUNT  |      93.0000  |      93.0000  |       93.0000  |      93.0000  |      93.0000  |     93.0000  |        93.0000  |          93.0000  |          93.0000  |             93.0000  |     93.0000  |       93.0000  |       93.0000  |      93.0000  |      93.0000  |          93.0000  |\n     MEAN  |  17.12580645  |  19.50967742  |   21.89892473  |   22.3655914  |  29.08602151  |  2.66774194  |   143.82795699  |    5280.64516129  |    2332.20430108  |         16.66451613  |  5.08602151  |  183.20430108  |  103.94623656  |  69.37634409  |  38.95698925  |    3072.90322581  |\n VARIANCE  |  76.49302244  |  93.30457924  |  121.67097709  |  31.58228144  |  28.42730248  |  1.07612202  |  2743.07877513  |  356088.70967742  |  246518.72954652  |         10.75427069  |  1.07947639  |  213.22954652  |   46.50794764  |  14.28073866  |  10.38943432  |  347977.89270687  |\n SKEWNESS  |    1.1829892  |   1.53308186  |    2.03385866  |   1.70443015  |   1.22989674  |  0.85941842  |     0.95172825  |      -0.25853269  |       0.28154602  |           0.1081462  |  0.06251685  |   -0.09009462  |    0.11372684  |   0.26402738  |  -0.13356858  |      -0.14366904  |\n      SUM  |    1592.7000  |    1814.4000  |     2036.6000  |    2080.0000  |    2705.0000  |    248.1000  |     13376.0000  |      491100.0000  |      216895.0000  |           1549.8000  |    473.0000  |    17038.0000  |     9667.0000  |    6452.0000  |    3623.0000  |      285780.0000  |",
            "title": "Non-Numeric Data Handling"
        },
        {
            "location": "/analysis/statistics/#null-handling",
            "text": "To be completed...",
            "title": "Null Handling"
        },
        {
            "location": "/analysis/statistics/#bi-variate-statistics",
            "text": "The Morpheus API makes it easy to compute the covariance or correlation between any two rows or columns, or compute the covariance and\ncorrelation matrix for an entire dataset. The statistics package in Morpheus uses an  online algorithm  enabling these calculations to\nbe performed on an infinitely large dataset. In addition, the calculation of the covariance and correlation matrix of a dataset is an embarrassingly parallel  problem that is well suited to a Fork & Join decomposition \nin order to improve performance on current multi-core microprocessor architectures.",
            "title": "Bi-Variate Statistics"
        },
        {
            "location": "/analysis/statistics/#covariance",
            "text": "The  DataFrameAxisStats<R,C>  interface exposes two overloaded  covariance()  methods, one of which can be used to compute the covariance of \nany two vectors in either the row or the column dimension, while the other yields the full covariance matrix for the dataset. Let us consider \nthe motor vehicle dataset first introduced in the  overview  and compute the covariance between  Price  and  Horsepower , \nwhich we expect to be positive, and the covariance between  EngineSize  and  MPG.city  which we expected to be negative.   DataFrame<Integer,String> frame = loadCarDataset();\ndouble covar1 = frame.cols().stats().covariance(\"Price\", \"Horsepower\");\ndouble covar2 = frame.cols().stats().covariance(\"EngineSize\", \"MPG.city\");\nSystem.out.printf(\"\\nCovariance between Price & Horsepower = %.2f\", covar1);\nSystem.out.printf(\"\\nCovariance between EngineSize and MPG.city = %.2f\", covar2);  \nCovariance between Price & Horsepower = 398.76\nCovariance between EngineSize and MPG.city = -4.14  It is also easy to compute the covariance matrix for all numeric columns in the frame as follows:   DataFrame<Integer,String> frame = loadCarDataset();\nDataFrame<String,String> covm = frame.cols().stats().covariance();\ncovm.out().print(100, formats -> {\n    formats.setDecimalFormat(\"0.000;-0.000\", 1);\n});  \n       Index         |  Min.Price  |    Price    |  Max.Price  |  MPG.city   |  MPG.highway  |  EngineSize  |  Horsepower  |      RPM      |  Rev.per.mile  |  Fuel.tank.capacity  |  Passengers  |   Length    |  Wheelbase  |    Width    |  Turn.circle  |    Weight     |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n          Min.Price  |     76.493  |     81.998  |     87.477  |    -30.615  |      -27.045  |       5.856  |     367.574  |     -222.321  |     -2042.672  |              18.223  |       0.556  |     70.735  |     30.822  |     16.290  |       12.083  |     3438.919  |\n              Price  |     81.998  |     93.305  |    104.585  |    -32.275  |      -28.876  |       5.986  |     398.765  |      -28.561  |     -2044.978  |              19.623  |       0.581  |     71.037  |     32.994  |     16.646  |       12.223  |     3687.667  |\n          Max.Price  |     87.477  |    104.585  |    121.671  |    -33.958  |      -30.733  |       6.122  |     430.076  |      164.653  |     -2048.416  |              21.027  |       0.610  |     71.344  |     35.167  |     17.024  |       12.365  |     3937.552  |\n           MPG.city  |    -30.615  |    -32.275  |    -33.958  |     31.582  |       28.283  |      -4.139  |    -197.980  |     1217.479  |      1941.631  |             -14.986  |      -2.434  |    -54.673  |    -25.567  |    -15.302  |      -12.071  |    -2795.095  |\n        MPG.highway  |    -27.045  |    -28.876  |    -30.733  |     28.283  |       28.427  |      -3.467  |    -172.865  |      997.335  |      1555.243  |             -13.744  |      -2.584  |    -42.268  |    -22.376  |    -12.902  |      -10.203  |    -2549.655  |\n         EngineSize  |      5.856  |      5.986  |      6.122  |     -4.139  |       -3.467  |       1.076  |      39.777  |     -339.164  |      -424.412  |               2.583  |       0.402  |     11.820  |      5.182  |      3.399  |        2.603  |      517.133  |\n         Horsepower  |    367.574  |    398.765  |    430.076  |   -197.980  |     -172.865  |      39.777  |    2743.079  |     1146.634  |    -15610.704  |             122.254  |       0.504  |    421.296  |    173.893  |    127.544  |       94.743  |    22825.505  |\n                RPM  |   -222.321  |    -28.561  |    164.653  |   1217.479  |      997.335  |    -339.164  |    1146.634  |   356088.710  |    146589.323  |            -652.325  |    -289.621  |  -3844.916  |  -1903.769  |  -1217.093  |     -972.581  |  -150636.133  |\n       Rev.per.mile  |  -2042.672  |  -2044.978  |  -2048.416  |   1941.631  |     1555.243  |    -424.412  |  -15610.704  |   146589.323  |    246518.730  |            -992.747  |    -172.800  |  -5004.314  |  -2156.293  |  -1464.371  |    -1173.328  |  -215349.676  |\n Fuel.tank.capacity  |     18.223  |     19.623  |     21.027  |    -14.986  |      -13.744  |       2.583  |     122.254  |     -652.325  |      -992.747  |              10.754  |       1.609  |     33.064  |     16.945  |      9.898  |        7.096  |     1729.468  |\n         Passengers  |      0.556  |      0.581  |      0.610  |     -2.434  |       -2.584  |       0.402  |       0.504  |     -289.621  |      -172.800  |               1.609  |       1.079  |      7.363  |      4.918  |      1.924  |        1.504  |      339.095  |\n             Length  |     70.735  |     71.037  |     71.344  |    -54.673  |      -42.268  |      11.820  |     421.296  |    -3844.916  |     -5004.314  |              33.064  |       7.363  |    213.230  |     82.022  |     45.368  |       34.781  |     6945.161  |\n          Wheelbase  |     30.822  |     32.994  |     35.167  |    -25.567  |      -22.376  |       5.182  |     173.893  |    -1903.769  |     -2156.293  |              16.945  |       4.918  |     82.022  |     46.508  |     20.803  |       15.900  |     3507.549  |\n              Width  |     16.290  |     16.646  |     17.024  |    -15.302  |      -12.902  |       3.399  |     127.544  |    -1217.093  |     -1464.371  |               9.898  |       1.924  |     45.368  |     20.803  |     14.281  |        9.962  |     1950.472  |\n        Turn.circle  |     12.083  |     12.223  |     12.365  |    -12.071  |      -10.203  |       2.603  |      94.743  |     -972.581  |     -1173.328  |               7.096  |       1.504  |     34.781  |     15.900  |      9.962  |       10.389  |     1479.365  |\n             Weight  |   3438.919  |   3687.667  |   3937.552  |  -2795.095  |    -2549.655  |     517.133  |   22825.505  |  -150636.133  |   -215349.676  |            1729.468  |     339.095  |   6945.161  |   3507.549  |   1950.472  |     1479.365  |   347977.893  |  Computing covariance in the row dimension is identical to the above example but operates on  DataFrameAxisStats<R,C>  returned \nfrom a call to the  frame.rows().stats()  method.",
            "title": "Covariance"
        },
        {
            "location": "/analysis/statistics/#correlation",
            "text": "The  DataFrameAxisStats<R,C>  interface exposes two overloaded  correlation()  methods, one of which can be used to compute the correlation of \nany two vectors in either the row or the column dimension, while the other yields the full correlation matrix for the dataset. Let us consider \nthe motor vehicle dataset first introduced in the  overview  and compute the correlation between  Price  and  Horsepower , \nwhich we expect to be positive, and the correlation between  EngineSize  and  MPG.city  which we expected to be negative.   DataFrame<Integer,String> frame = loadCarDataset();\ndouble correl1 = frame.cols().stats().correlation(\"Price\", \"Horsepower\");\ndouble correl2 = frame.cols().stats().correlation(\"EngineSize\", \"MPG.city\");\nSystem.out.printf(\"\\nCorrelation between Price & Horsepower = %.2f\", correl1);\nSystem.out.printf(\"\\nCorrelation between EngineSize and MPG.city = %.2f\", correl2);  \nCorrelation between Price & Horsepower = 0.79\nCorrelation between EngineSize and MPG.city = -0.71  It is also easy to compute the correlation matrix for all numeric columns in the frame as shown below. As expected, the diagonal\nvalues of this  DataFrame  are equal to one as the correlation of a signal with itself is always one.   DataFrame<Integer,String> frame = loadCarDataset();\nDataFrame<String,String> correlm = frame.cols().stats().correlation();\ncorrelm.out().print(100, formats -> {\n    formats.setDecimalFormat(\"0.000;-0.000\", 1);\n});  \n       Index         |  Min.Price  |  Price   |  Max.Price  |  MPG.city  |  MPG.highway  |  EngineSize  |  Horsepower  |   RPM    |  Rev.per.mile  |  Fuel.tank.capacity  |  Passengers  |  Length  |  Wheelbase  |  Width   |  Turn.circle  |  Weight  |\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n          Min.Price  |      1.000  |   0.971  |      0.907  |    -0.623  |       -0.580  |       0.645  |       0.802  |  -0.043  |        -0.470  |               0.635  |       0.061  |   0.554  |      0.517  |   0.493  |        0.429  |   0.667  |\n              Price  |      0.971  |   1.000  |      0.982  |    -0.595  |       -0.561  |       0.597  |       0.788  |  -0.005  |        -0.426  |               0.619  |       0.058  |   0.504  |      0.501  |   0.456  |        0.393  |   0.647  |\n          Max.Price  |      0.907  |   0.982  |      1.000  |    -0.548  |       -0.523  |       0.535  |       0.744  |   0.025  |        -0.374  |               0.581  |       0.053  |   0.443  |      0.468  |   0.408  |        0.348  |   0.605  |\n           MPG.city  |     -0.623  |  -0.595  |     -0.548  |     1.000  |        0.944  |      -0.710  |      -0.673  |   0.363  |         0.696  |              -0.813  |      -0.417  |  -0.666  |     -0.667  |  -0.721  |       -0.666  |  -0.843  |\n        MPG.highway  |     -0.580  |  -0.561  |     -0.523  |     0.944  |        1.000  |      -0.627  |      -0.619  |   0.313  |         0.587  |              -0.786  |      -0.466  |  -0.543  |     -0.615  |  -0.640  |       -0.594  |  -0.811  |\n         EngineSize  |      0.645  |   0.597  |      0.535  |    -0.710  |       -0.627  |       1.000  |       0.732  |  -0.548  |        -0.824  |               0.759  |       0.373  |   0.780  |      0.732  |   0.867  |        0.778  |   0.845  |\n         Horsepower  |      0.802  |   0.788  |      0.744  |    -0.673  |       -0.619  |       0.732  |       1.000  |   0.037  |        -0.600  |               0.712  |       0.009  |   0.551  |      0.487  |   0.644  |        0.561  |   0.739  |\n                RPM  |     -0.043  |  -0.005  |      0.025  |     0.363  |        0.313  |      -0.548  |       0.037  |   1.000  |         0.495  |              -0.333  |      -0.467  |  -0.441  |     -0.468  |  -0.540  |       -0.506  |  -0.428  |\n       Rev.per.mile  |     -0.470  |  -0.426  |     -0.374  |     0.696  |        0.587  |      -0.824  |      -0.600  |   0.495  |         1.000  |              -0.610  |      -0.335  |  -0.690  |     -0.637  |  -0.780  |       -0.733  |  -0.735  |\n Fuel.tank.capacity  |      0.635  |   0.619  |      0.581  |    -0.813  |       -0.786  |       0.759  |       0.712  |  -0.333  |        -0.610  |               1.000  |       0.472  |   0.690  |      0.758  |   0.799  |        0.671  |   0.894  |\n         Passengers  |      0.061  |   0.058  |      0.053  |    -0.417  |       -0.466  |       0.373  |       0.009  |  -0.467  |        -0.335  |               0.472  |       1.000  |   0.485  |      0.694  |   0.490  |        0.449  |   0.553  |\n             Length  |      0.554  |   0.504  |      0.443  |    -0.666  |       -0.543  |       0.780  |       0.551  |  -0.441  |        -0.690  |               0.690  |       0.485  |   1.000  |      0.824  |   0.822  |        0.739  |   0.806  |\n          Wheelbase  |      0.517  |   0.501  |      0.468  |    -0.667  |       -0.615  |       0.732  |       0.487  |  -0.468  |        -0.637  |               0.758  |       0.694  |   0.824  |      1.000  |   0.807  |        0.723  |   0.872  |\n              Width  |      0.493  |   0.456  |      0.408  |    -0.721  |       -0.640  |       0.867  |       0.644  |  -0.540  |        -0.780  |               0.799  |       0.490  |   0.822  |      0.807  |   1.000  |        0.818  |   0.875  |\n        Turn.circle  |      0.429  |   0.393  |      0.348  |    -0.666  |       -0.594  |       0.778  |       0.561  |  -0.506  |        -0.733  |               0.671  |       0.449  |   0.739  |      0.723  |   0.818  |        1.000  |   0.778  |\n             Weight  |      0.667  |   0.647  |      0.605  |    -0.843  |       -0.811  |       0.845  |       0.739  |  -0.428  |        -0.735  |               0.894  |       0.553  |   0.806  |      0.872  |   0.875  |        0.778  |   1.000  |  Computing correlation in the row dimension is identical to the above example but operates on  DataFrameAxisStats<R,C>  returned \nfrom a call to the  frame.rows().stats()  method.",
            "title": "Correlation"
        },
        {
            "location": "/analysis/statistics/#performance",
            "text": "",
            "title": "Performance"
        },
        {
            "location": "/analysis/statistics/#demean-example",
            "text": "Earlier, two separate techniques were presented as a way of de-meaning the values of a  DataFrame  in the row dimension. The first\napproach used the  forEach()  method to iterate over all rows, while the second used a traditional for-loop creating a new row object\nfor each iteration in the loop.   Below we assess the relative performance of these techniques. For the former case, we use both  sequential  and  parallel  iteration \nwhich is natively supported in the Morpheus API. The latter approach cannot be parallelized without writing a lot of code. The results \nare as follows.  \n       Firstly, kudos to the Java Virtual Machine in that the so called  Bad  approach is not that much slower than the  Good(sequential)  approach,\neven given that far more objecs are created in the former scenario. They are all very short lived objects, and we know the current generation\ngarbage collection algorithms are heavily optimized to deal with that kind of trash. The  forEach()  approach is easily parallelizable \nsimply by calling the  parallel()  method on  DataFrameRows  before starting the iteration. This clearly makes a big difference in relative\nperformance. The code for this analysis is shown below.   //Create a 1,000,000x10 DataFrame of random double precision values\nDataFrame<LocalDate,String> frame = random(1000000, \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\");\n\n//Run 10 performance samples, randomizing the frame before each test\nDataFrame<String,String> timing = PerfStat.run(10, TimeUnit.MILLISECONDS, false, tasks -> {\n\n    tasks.beforeEach(() -> frame.applyDoubles(v -> Math.random() * 100d));\n\n    tasks.put(\"Bad\", () -> {\n        for (int i=0; i<frame.rowCount(); ++i) {\n            final DataFrameRow<LocalDate,String> row = frame.rowAt(i);\n            final double mean = row.stats().mean();\n            row.applyDoubles(v -> v.getDouble() - mean);\n        }\n        return frame;\n    });\n\n    tasks.put(\"Good(sequential)\", () -> {\n        frame.rows().forEach(row -> {\n            final double mean = row.stats().mean();\n            row.applyDoubles(v -> v.getDouble() - mean);\n        });\n        return frame;\n    });\n\n    tasks.put(\"Good(parallel)\", () -> {\n        frame.rows().parallel().forEach(row -> {\n            final double mean = row.stats().mean();\n            row.applyDoubles(v -> v.getDouble() - mean);\n        });\n        return frame;\n    });\n});\n\n//Plot a chart of the results\nChart.create().withBarPlot(timing, false, chart -> {\n    chart.title().withText(\"DataFrame Row Demeaning Performance (10 Samples)\");\n    chart.subtitle().withText(\"DataFrame Dimension: 1 Million x 10\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time (Milliseconds)\");\n    chart.legend().on().bottom();\n    chart.show();\n});",
            "title": "Demean Example"
        },
        {
            "location": "/analysis/statistics/#correlation-example",
            "text": "It was suggested that computing the covariance or correlation matrix for a dataset was an  embarrassingly parallel  \nproblem. The example below creates a  DataFrame  of 1 million rows by 10 columns, and computes the correlation in the column\ndimension using both  sequential  and  parallel  execution, and then plots the results.  \n       These results suggest a roughly 3 times improvement in performance in parallel execution for this example. The performance \ndifferential for such a scenario is likely to be sensitive to many factors, especially the shape of the  DataFrame , so take \nthese results with a pinch of salt (as with all benchmark results). Nevertheless, this does demonstrate that parallel execution \ncan give a significant boost, so it is worth exploring in a performance critical scenario. The code for this example is as follows:   //Create a 1,000,000x10 DataFrame of random double precision values\nDataFrame<LocalDate,String> frame = random(1000000, \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\");\n\n//Run 10 performance samples, randomizing the frame before each test\nDataFrame<String,String> timing = PerfStat.run(10, TimeUnit.MILLISECONDS, false, tasks -> {\n    tasks.beforeEach(() -> frame.applyDoubles(v -> Math.random() * 100d));\n    tasks.put(\"Sequential\", () -> frame.cols().stats().correlation());\n    tasks.put(\"Parallel\", () -> frame.cols().parallel().stats().correlation());\n});\n\n//Plot a chart of the results\nChart.create().withBarPlot(timing, false, chart -> {\n    chart.title().withText(\"DataFrame Correlation Matrix Performance (10 Samples)\");\n    chart.subtitle().withText(\"DataFrame Dimension: 1 Million x 10\");\n    chart.title().withFont(new Font(\"Verdana\", Font.PLAIN, 15));\n    chart.plot().axes().domain().label().withText(\"Timing Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Time (Milliseconds)\");\n    chart.legend().on().bottom();\n    chart.show();\n});",
            "title": "Correlation Example"
        },
        {
            "location": "/regression/ols/",
            "text": "Ordinary Least Squares (OLS)\n\n\nIntroduction\n\n\nRegression analysis\n is a statistical technique used to fit a model expressed in \nterms of one or more variables to some data. In particular, it allows one to analyze the relationship of a dependent variable (also \nreferred to as the regressand) on one or more independent or predictor variables (also referred to as regressors), and assess how \ninfluential each of these are.  \n\n\nThere are many types of regression analysis techniques, however one of the most commonly used is based on fitting data to a linear model, \nand in particular using an approach called \nLeast Squares\n. The Morpheus API currently supports \n3 variations of Linear Least Squares regression, namely \nOrdinary Least Squares\n (OLS), \n\nWeighted Least Squares\n (WLS) and \nGeneralized Least Squares\n\n(GLS). The following section reviews some OLS regression theory, and provides an example of how to use the Morhpeus API to apply this technique.\n\n\nTheory\n\n\nA linear regression model in matrix form can be expressed as:\n\n\n$$ Y = X \\beta + \\epsilon \\ \\ \\ \\ where \\ E[\\epsilon]=0 \\ \\ \\ \\ Var[\\epsilon] = \\sigma^2 I  $$\n\n\nY represents an \nnx1\n vector of regressands, and X represents an \nnxp\n design matrix, where \nn\n is the number of observations in the data, \nand \np\n represents the number of parameters to estimate. If the model includes an intercept term, the first column in the design matrix is \npopulated with 1's and therefore the first entry in the \npx1\n \\(\\beta\\) vector would represent the intercept value. The \nnx1\n \\(\\epsilon\\) \nvector represents the error or disturbance term, which is assumed to have a conditional mean of 0 and also to be free of any serial correlation \n(see section below on the \nGauss Markov\n assumptions).\n\n\nA least squares regression model estimates \\(\\beta\\) so as to minimize the sum of the squared error, or \\(\\epsilon^T\\epsilon\\). We \ncan use the model equation above to express \\(\\epsilon^T\\epsilon\\) in terms of Y and X\\(\\beta\\), differentiate this by \\(\\beta\\), \nset the result to zero since we wish to minimize the squared error, and solve for \\(\\beta\\) as follows:  \n\n\n$$ \\begin{align}\n\\epsilon^T\\epsilon &= (Y - X \\beta)^T(Y - X \\beta) \\\\\n &= Y^TY - \\beta^TX^TY - Y^TX\\beta + \\beta^T X^T X \\beta \\\\\n &= Y^TY - 2\\beta^T X^T Y + \\beta^T X^T X \\beta\n\\end{align} $$\n\n\nWe now differentiate this expression with respect to \\(\\beta\\) and set it to zero which yields the following:\n\n\n$$ \\frac{d\\epsilon^T\\epsilon}{d\\beta} = -2X^T Y + 2X^T X \\beta = 0 $$\n\n\nThis expression can be re-arranged to solve for \\(\\beta\\) and yields the following equation: \n\n\n$$ \\beta = (X^T X)^{-1} X^T Y $$ \n\n\nThe value of \\(\\beta\\) can only be \nestimated\n given some sample data drawn from a population or data generating process, the \ntrue\n value is \nunknown. We usually refer to the estimate as \\(\\hat{\\beta}\\) (beta \"hat\") in order to distinguish it from the true population value. In addition, \nwhen expressing the model in terms of \\(\\hat{\\beta}\\), the stochastic term is referred to as \nresiduals\n rather than \nerrors\n, and while they \nare conceptually related, they are \nnot the same thing\n. Errors represent the deviation of \nobservations from the unknown population line, while residuals represent the deviations from the estimation line, a subtle difference and easily \nconfused. In terms of \\(\\hat{\\beta}\\) and residuals \nu\n, the model is expressed in the usual way. \n\n\n$$ Y = X \\hat{\\beta} + u \\ \\ \\ \\ where \\ E[u]=0 \\ \\ \\ \\ Var[u] = \\sigma^2 I  $$\n\n\nThe residuals are of course still assumed to be consistent with the \nGauss Markov\n \nassumptions, and so the estimator for \\(\\hat{\\beta}\\) is: \n\n\n$$ \\hat{\\beta} = (X^T X)^{-1} X^T Y $$ \n\n\nSolving\n\n\nWe can solve for \\(\\hat{\\beta}\\) directly by calculating the right hand side of the above equation, which is what the Morpheus library does\nby default. There are situations in which this can present numerical stability issues, in which case it may be preferable to solve for \\(\\hat{\\beta}\\) \nby factorizing the design matrix X using a \nQR decomposition\n where Q represents an orthogonal matrix \nsuch that \\(Q^T Q = I\\) and thus \\(Q^T = Q^{-1}\\), and where R is an upper triangular matrix. The QR decomposition approach is based on the \nfollowing reasoning.\n\n\n$$ \\begin{align}\nX^T Y & = X^T X \\hat{\\beta}  \\\\\n\\big( QR \\big)^T Y & = \\big( QR \\big)^T \\big( QR \\big) \\hat{\\beta} \\\\\nR^T Q^T Y & = R^T Q^T Q R \\hat{\\beta} \\\\\nR^T Q^T Y & = R^T R \\hat{\\beta} \\\\\n(R^T)^{-1} R^T Q^T Y & = \\big( R^T \\big)^{-1} R^T R \\hat{\\beta} \\\\ \nQ^T Y & = R \\hat{\\beta}\n\\end{align} $$\n\n\nThis can be solved efficiently by \nbackward substitution\n because the R \nmatrix is upper \ntriangular\n, and therefore no inversion of the design matrix is required. The Morpheus \nlibrary also supports estimating \\(\\hat{\\beta}\\) using this technique, which can be configured on a case by case basis via the \nwithSolver()\n method \non the \nDataFrameLeastSquares\n interface. \n\n\nDiagnostics\n\n\nJust like any other statistical technique, regression analysis is susceptible to \nsampling error\n, and \nit is therefore common to compute the variance of the parameter estimates, as well as their \nstandard error\n, \nwhich can then be used for \ninferential\n purposes. In the context of a linear regression, the \n\nnull hypotheses\n \\(H_{0}\\), is generally that the model parameters are zero. The parameter standard \nerrors can be used to calculate a \nt-statistic\n and corresponding \np-value\n \nin order to decide if \\(H_{0}\\) can be rejected in favour of the alternative hypothesis, and thus assert that the estimates are statistically \nsignificantly different from zero.\n\n\nThe variance of \\(\\hat{\\beta}\\) with respect to the true population value can be expressed as follows:\n\n\n$$ Var(\\hat{\\beta}) = E[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)^T] $$\n\n\nWe substitute our population model \\( Y = X \\beta + \\epsilon \\) in our sample estimator \\( \\hat{\\beta} = (X^T X)^{-1} X^T Y \\) as follows:  \n\n\n$$ \\begin{align}\n\\hat{\\beta} &= (X^T X)^{-1} X^T ( X \\beta + \\epsilon ) \\\\\n\\hat{\\beta} &= (X^T X)^{-1} X^T X \\beta + (X^T X)^{-1} X^T \\epsilon  \\\\\n\\hat{\\beta} &= \\beta + (X^T X)^{-1} X^T \\epsilon  \\\\\n\\hat{\\beta} - \\beta &= (X^T X)^{-1} X^T \\epsilon  \\\\\n\\end{align} $$\n\n\nWith the above expression for \\( \\hat{\\beta} - \\beta \\) we can now solve for the variance of the OLS estimator as follows: \n\n\n$$ \\begin{align}\nVar(\\hat{\\beta}) &= E[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)^T] \\\\\n & = E[((X^T X)^{-1} X^T \\epsilon) ((X^T X)^{-1} X^T \\epsilon)^T] \\\\\n & = E[(X^T X)^{-1} X^T \\epsilon \\epsilon^T X (X^T X)^{-1} ]\n\\end{align} $$\n\n\nGiven that the design matrix X is non-stochastic and the \\( E[\\epsilon \\epsilon^T] = \\sigma^2 I \\):\n\n\n$$ \\begin{align}\nVar(\\hat{\\beta}) & = (X^T X)^{-1} X^T E[\\epsilon \\epsilon^T] X (X^T X)^{-1} \\\\\n & = (X^T X)^{-1} X^T (\\sigma^2 I) X (X^T X)^{-1} \\\\\n & = \\sigma^2 I (X^T X)^{-1} X^T X (X^T X)^{-1} \\\\\n & = \\sigma^2 I (X^T X)^{-1} \\\\\n & = \\sigma^2 (X^T X)^{-1}\n\\end{align} $$\n\n\nOther regression diagnostics that are calculated include the \ncoefficient of determination\n or\n\\(R^2\\) which is a number that indicates the proportion of variance in the dependent variable that is explained by the independent variables, and is\ndocumented in the table below. The parameter variance estimates as calculated above are used to compute the standard errors and a corresponding\n\nt-statistic\n which can be used for statistical inference.\n\n\n\n\n\n\n\n\nQuantity\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nResidual Sum of Squares (RSS)\n\n\n$$ RSS = \\sum_{i=1}^n \\big(y_{i} - \\hat{y_{i}} \\big)^2 = \\sum_{i=1}^n \\epsilon_{i}^2 = \\epsilon^T \\epsilon $$\n\n\n\n\n\n\nTotal Sum of Squares (TSS)\n\n\n$$ TSS = \\sum_{i=1}^{n} \\big(y_{i} - \\overline{y}\\big)^2 \\, \\textrm{where} \\; \\overline{y} = \\frac{1}{n} \\sum_{i=1}^n y_{i} $$\n\n\n\n\n\n\nExplained Sum of Squares (ESS)\n\n\n$$ ESS = \\sum_{i=1}^{n} \\big(\\hat{y_{i}} - \\overline{y}\\big)^2 \\, \\textrm{where} \\; \\overline{y} = \\frac{1}{n} \\sum_{i=1}^n y_{i} $$\n\n\n\n\n\n\nR-Squared\n\n\n$$ R^2 = 1 - \\frac{RSS}{TSS}  $$\n\n\n\n\n\n\nR-Squared (Adjusted)\n\n\n$$ R^2_{adj} = 1 - \\frac{ RSS * \\big( n - 1 \\big) }{ TSS * \\big( n - p \\big)}  $$\n\n\n\n\n\n\nRegression Standard Error\n\n\n$$ SE = \\sqrt{ \\frac{RSS}{ n - p }}  $$\n\n\n\n\n\n\nParameter Variance\n\n\n$$ Var(\\hat{\\beta}) = SE^2( X^T X )^{-1}  $$\n\n\n\n\n\n\nParameter Std Errors\n\n\n$$ SE(\\hat{\\beta_{i}}) = \\sqrt{ Var(\\hat{\\beta_{i}})} $$\n\n\n\n\n\n\nParameter T-statistics\n\n\n$$ T_{\\beta_{i}} = \\frac{\\hat{ \\beta_{i}}}{ SE( \\hat{ \\beta_{i} } ) }    $$\n\n\n\n\n\n\n\n\nGauss Markov Assumptions\n\n\nThe \nGauss Markov Theorem\n states that Ordinary Least Squares is the Best Linear Unbiased \nand Efficient (BLUE) estimator of \\(\\beta\\), conditional on a certain set of assumptions being met. In this context, \"best\" means that there are\nno other unbiased estimators with a smaller sampling variance than OLS. Unbiased means that that the expectation of \\( \\hat{\\beta}\\) is equal to the\npopulation \\(\\beta\\), or otherwise stated \\( E[ \\hat{\\beta} ] = \\beta \\). The assumptions that must hold for OLS to be BLUE are as follows:\n\n\n\n\n\n\n\n\nAssumptions\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAssumption 1\n\n\nThe regression model is linear in parameters, and therefore well specified\n\n\n\n\n\n\nAssumption 2\n\n\nThe regressors are linearly independent, and therefore do not exhibit perfect \nmulticollinearity\n\n\n\n\n\n\nAssumption 3\n\n\nThe errors in the regression have a conditional mean of zero\n\n\n\n\n\n\nAssumption 4\n\n\nThe errors are \nhomoscedastic\n, which means they exhibit constant variance\n\n\n\n\n\n\nAssumption 5\n\n\nThe errors are uncorrelated between observations\n\n\n\n\n\n\nAssumption 6\n\n\nThe errors are normally distributed, and independent and identically distributed (iid)\n\n\n\n\n\n\n\n\n Linear in Parameters \n\n\nThe first assumption regarding linearity suggests that the dependent variable is a linear function of the independent variables. This does not imply that there\nis a linear relationship between the independent and dependent variables, it only states the the model is linear in parameters. For example, a model of the form\n\\(y = \\alpha + \\beta x^2 \\) qualifies as being linear in parameters, while \\(y = \\alpha + \\beta^2 x \\) does not. If the functional form of a  model under\ninvestigation is not linear in parameters, it can often be transformed so as to render it linear.\n\n\n Linearly Independent \n\n\nThe second assumption that there is no perfect \nmulticollinearity\n between the regressors is important, as if\nit exists, the OLS estimator cannot be calculated. Another way of expressing this condition is that one of the independent variables cannot be a function of any \nof the other independent variables, and therefore the design matrix X must be non-singular, and therefore have full rank.\n\n\n Strict exogeneity \n\n\nThe third assumption above states that the disturbance term averages out to zero for any given instance of X, which implies that no observations of the independent \nvariables convey any information about the error. Mathematically this is stated as \\( E[ \\epsilon | X ] = 0 \\). This assumption is violated if the independent\nvariables are \nstochastic\n in nature, which can arise as a result of \nmeasurement error\n, or if there is \n\nendogeneity\n in the model.\n\n\n Spherical Errors \n\n\nThe fourth and fifth assumptions relate to the \ncovariance matrix\n of the error term, and specifically states \nthat \\(E[ \\epsilon \\epsilon^T | X] = \\sigma^2 I \\). There are two key concepts embedded in this statement, the first is that the disturbance term has uniform \nvariance of \\(\\sigma^2\\) regardless of the values of the independent variables, and is referred to as homoscedasticity. In addition, the off diagonal terms of \nthe covariance matrix are assumed to be zero, which suggests there is no serial correlation between errors. Either or both of these assumptions may not hold in \nthe real world, in which case a WLS or GLS estimator may prove to be a better, unbiased linear estimator. \n\n\nExample\n\n\nThe Morpheus API defines an interface called \nDataFrameRegression\n which exposes a number of methods that support different linear regression techniques, namely \nOLS, WLS and GLS. There are overloaded methods that take one or more regressors in order to conveniently support simple and multiple linear regression. \n\n\nThe regression interface, which operates on the column data in a \nDataFrame\n, can be accessed by calling the \nregress()\n method on the frame. If  a regression in \nthe row dimension is required, simply call \ntranspose()\n on the frame before calling \nregress()\n.\n\n\nTo illustrate an example, consider the same motor vehicle dataset introduced earlier, which can be loaded with the code below. The first 10 rows of this \nDataFrame\n \nis also included for inspection, and in this exercise we are going to be interested in the \nEngineSize\n and \nHorsepower\n columns. \n\n\n\n\n\nstatic DataFrame<Integer,String> loadCarDataset() {\n    return DataFrame.read().csv(options -> {\n        options.setResource(\"http://zavtech.com/data/samples/cars93.csv\");\n        options.setExcludeColumnIndexes(0);\n    });\n}\n\n\n\n\n\n Index  |  Manufacturer  |    Model     |   Type    |  Min.Price  |   Price   |  Max.Price  |  MPG.city  |  MPG.highway  |       AirBags        |  DriveTrain  |  Cylinders  |  EngineSize  |  Horsepower  |  RPM   |  Rev.per.mile  |  Man.trans.avail  |  Fuel.tank.capacity  |  Passengers  |  Length  |  Wheelbase  |  Width  |  Turn.circle  |  Rear.seat.room  |  Luggage.room  |  Weight  |  Origin   |        Make        |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     0  |         Acura  |     Integra  |    Small  |    12.9000  |  15.9000  |    18.8000  |        25  |           31  |                None  |       Front  |          4  |      1.8000  |         140  |  6300  |          2890  |              Yes  |             13.2000  |           5  |     177  |        102  |     68  |           37  |            26.5  |            11  |    2705  |  non-USA  |     Acura Integra  |\n     1  |         Acura  |      Legend  |  Midsize  |    29.2000  |  33.9000  |    38.7000  |        18  |           25  |  Driver & Passenger  |       Front  |          6  |      3.2000  |         200  |  5500  |          2335  |              Yes  |             18.0000  |           5  |     195  |        115  |     71  |           38  |              30  |            15  |    3560  |  non-USA  |      Acura Legend  |\n     2  |          Audi  |          90  |  Compact  |    25.9000  |  29.1000  |    32.3000  |        20  |           26  |         Driver only  |       Front  |          6  |      2.8000  |         172  |  5500  |          2280  |              Yes  |             16.9000  |           5  |     180  |        102  |     67  |           37  |              28  |            14  |    3375  |  non-USA  |           Audi 90  |\n     3  |          Audi  |         100  |  Midsize  |    30.8000  |  37.7000  |    44.6000  |        19  |           26  |  Driver & Passenger  |       Front  |          6  |      2.8000  |         172  |  5500  |          2535  |              Yes  |             21.1000  |           6  |     193  |        106  |     70  |           37  |              31  |            17  |    3405  |  non-USA  |          Audi 100  |\n     4  |           BMW  |        535i  |  Midsize  |    23.7000  |  30.0000  |    36.2000  |        22  |           30  |         Driver only  |        Rear  |          4  |      3.5000  |         208  |  5700  |          2545  |              Yes  |             21.1000  |           4  |     186  |        109  |     69  |           39  |              27  |            13  |    3640  |  non-USA  |          BMW 535i  |\n     5  |         Buick  |     Century  |  Midsize  |    14.2000  |  15.7000  |    17.3000  |        22  |           31  |         Driver only  |       Front  |          4  |      2.2000  |         110  |  5200  |          2565  |               No  |             16.4000  |           6  |     189  |        105  |     69  |           41  |              28  |            16  |    2880  |      USA  |     Buick Century  |\n     6  |         Buick  |     LeSabre  |    Large  |    19.9000  |  20.8000  |    21.7000  |        19  |           28  |         Driver only  |       Front  |          6  |      3.8000  |         170  |  4800  |          1570  |               No  |             18.0000  |           6  |     200  |        111  |     74  |           42  |            30.5  |            17  |    3470  |      USA  |     Buick LeSabre  |\n     7  |         Buick  |  Roadmaster  |    Large  |    22.6000  |  23.7000  |    24.9000  |        16  |           25  |         Driver only  |        Rear  |          6  |      5.7000  |         180  |  4000  |          1320  |               No  |             23.0000  |           6  |     216  |        116  |     78  |           45  |            30.5  |            21  |    4105  |      USA  |  Buick Roadmaster  |\n     8  |         Buick  |     Riviera  |  Midsize  |    26.3000  |  26.3000  |    26.3000  |        19  |           27  |         Driver only  |       Front  |          6  |      3.8000  |         170  |  4800  |          1690  |               No  |             18.8000  |           5  |     198  |        108  |     73  |           41  |            26.5  |            14  |    3495  |      USA  |     Buick Riviera  |\n\n\n\n\nBased on our understanding of the factors that influence the power of an internal combustion engine, one might hypothesize that there is a positive and linear \nrelationship between the size of an engine and how much horsepower it produces. . We can use the car dataset to test this hypothesis. First we can use the Morpheus \nlibrary to generate a scatter plot of the data, with \nEngineSize\n on the x-axis and \nHorsepower\n on the y-axis as follows:\n\n\n\n\n\nfinal String y = \"Horsepower\";\nfinal String x = \"EngineSize\";\nfinal DataFrame<Integer,String> frame = loadCarDataset();\nfinal DataFrame<Integer,String> xy = frame.cols().select(y, x);\nChart.create().withScatterPlot(xy, false, x, chart -> {\n    chart.title().withText(y + \" vs \" + x);\n    chart.plot().style(y).withColor(Color.RED);\n    chart.plot().style(y).withPointsVisible(true).withPointShape(ChartShape.DIAMOND);\n    chart.plot().axes().domain().label().withText(x);\n    chart.plot().axes().domain().format().withPattern(\"0.00;-0.00\");\n    chart.plot().axes().range(0).label().withText(y);\n    chart.plot().axes().range(0).format().withPattern(\"0;-0\");\n    chart.show(845, 450);\n});\n\n\n\n\n\n    \n\n\n\n\n\nThe scatter plot certainly appears to suggest that there is a positive relationship between \nEngineSize\n and \nHorsepower\n. In addition, it seems somewhat \nlinear, however the dispersion appears to get more significant for larger engine sizes, which would be a violation of one of the \nGauss Markov\n \nassumptions, namely of \nhomoscedastic\n errors. Nevertheless, let us proceed to regress \nHorsepower\n on \n\nEngineSize\n and see what the results look like. The code below runs a single variable regression and simply prints the model results to standard-out \nfor inspection.\n\n\n\n\n\nDataFrame<Integer,String> frame = loadCarDataset();\nString regressand = \"Horsepower\";\nString regressor = \"EngineSize\";\nframe.regress().ols(regressand, regressor, true, model -> {\n    System.out.println(model);\n    return Optional.empty();\n});\n\n\n\n\n\n==============================================================================================\n                                   Linear Regression Results                                                            \n==============================================================================================\nModel:                                   OLS    R-Squared:                            0.5360\nObservations:                             93    R-Squared(adjusted):                  0.5309\nDF Model:                                  1    F-Statistic:                        105.1204\nDF Residuals:                             91    F-Statistic(Prob):                  1.11E-16\nStandard Error:                      35.8717    Runtime(millis)                           48\nDurbin-Watson:                        1.9591                                                \n==============================================================================================\n   Index     |  PARAMETER  |  STD_ERROR  |  T_STAT   |   P_VALUE   |  CI_LOWER  |  CI_UPPER  |\n----------------------------------------------------------------------------------------------\n  Intercept  |    45.2195  |    10.3119  |   4.3852  |   3.107E-5  |    24.736  |   65.7029  |\n EngineSize  |    36.9633  |     3.6052  |  10.2528  |  7.573E-17  |    29.802  |   44.1245  |\n==============================================================================================\n\n\n\n\nThe regression results yield a slope coefficient of \n36.96\n, suggesting that for every additional litre of engine capacity, we can expect to add another\n36.96 horsepower. While the \np-value\n associated with the slope coefficient suggests that it is statistically \nsignificantly different from zero, it does not tell us about how relevant the parameter is in the regression. In this case we can reasonably surmise \nthat engine size is relevant given our understanding of how an internal combustion engine works and what factors affect output power. Having said that, \nthe coefficient of determination is perhaps lower than one might expect, and the heteroscedasticity of the residuals provides some hint that the model \nmay be incomplete. In particular, \nomitted-variable bias\n may be at play here, in the sense that \nthere are other important factors that influence an engine's horsepower that we have not included.\n\n\nWhile the code example above simply prints the model results to standard out, the illustration below demonstrates how to access all the relevant model \noutputs via the API. The \nols()\n method takes a lambda parameter that consumes the regression model, which is an instance of the \nDataFrameLeastSquares\n \ninterface, and provides all the relevant hooks to access the model inputs and outputs.\n\n\n\n\n\nfinal String regressand = \"Horsepower\";\nfinal String regressor = \"EngineSize\";\nfinal DataFrame<Integer,String> frame = loadCarDataset();\nframe.regress().ols(regressand, regressor, true, model -> {\n    assert (model.getRegressand().equals(regressand));\n    assert (model.getRegressors().size() == 1);\n    assertEquals(model.getRSquared(), 0.5359992996664269, 0.00001);\n    assertEquals(model.getRSquaredAdj(), 0.5309003908715525, 0.000001);\n    assertEquals(model.getStdError(), 35.87167658782274, 0.00001);\n    assertEquals(model.getFValue(), 105.120393642, 0.00001);\n    assertEquals(model.getFValueProbability(), 0, 0.00001);\n    assertEquals(model.getBetaValue(\"EngineSize\", Field.PARAMETER), 36.96327914, 0.0000001);\n    assertEquals(model.getBetaValue(\"EngineSize\", Field.STD_ERROR), 3.60518041, 0.0000001);\n    assertEquals(model.getBetaValue(\"EngineSize\", Field.T_STAT), 10.25282369, 0.0000001);\n    assertEquals(model.getBetaValue(\"EngineSize\", Field.P_VALUE), 0.0000, 0.0000001);\n    assertEquals(model.getBetaValue(\"EngineSize\", Field.CI_LOWER), 29.80203113, 0.0000001);\n    assertEquals(model.getBetaValue(\"EngineSize\", Field.CI_UPPER), 44.12452714, 0.0000001);\n    assertEquals(model.getInterceptValue(Field.PARAMETER), 45.21946716, 0.0000001);\n    assertEquals(model.getInterceptValue(Field.STD_ERROR), 10.31194906, 0.0000001);\n    assertEquals(model.getInterceptValue(Field.T_STAT), 4.3851523, 0.0000001);\n    assertEquals(model.getInterceptValue(Field.P_VALUE), 0.00003107, 0.0000001);\n    assertEquals(model.getInterceptValue(Field.CI_LOWER), 24.73604714, 0.0000001);\n    assertEquals(model.getInterceptValue(Field.CI_UPPER), 65.70288719, 0.0000001);\n    System.out.println(model);\n    return Optional.of(model);\n});\n\n\n\n\nFinally, the chart below adds the OLS trendline to the initial scatter plot to get a better sense of how the solution fits the data. \n\n\n\n    \n\n\n\n\n\nThe code to generate this chart is as follows:\n\n\n\n\n\nfinal String regressand = \"Horsepower\";\nfinal String regressor = \"EngineSize\";\nfinal DataFrame<Integer,String> frame = loadCarDataset();\nfinal DataFrame<Integer,String> xy = frame.cols().select(regressand, regressor);\nChart.create().withScatterPlot(xy, false, regressor, chart -> {\n    chart.title().withFont(new Font(\"Verdana\", Font.BOLD, 16));\n    chart.title().withText(regressand + \" regressed on \" + regressor);\n    chart.subtitle().withText(\"Single Variable Linear Regression\");\n    chart.plot().style(regressand).withColor(Color.RED);\n    chart.plot().trend(regressand).withColor(Color.BLACK);\n    chart.plot().axes().domain().label().withText(regressor);\n    chart.plot().axes().domain().format().withPattern(\"0.00;-0.00\");\n    chart.plot().axes().range(0).label().withText(regressand);\n    chart.plot().axes().range(0).format().withPattern(\"0;-0\");\n    chart.show();\n});\n\n\n\n\nUnbiasedness\n\n\nAn Ordinary Least Squares estimator is said to be \nunbiased\n in the sense that \nif you run regressions on many samples of data generated from the same population process, the coefficient estimates from all these samples \nwould be centered on the true population values. To demonstrate this empirically, we can define a population process in 2D space with a known \nslope and intercept coefficient, and then proceed to generate many samples from this process while adding Gaussian noise to the dependent variable \nin order to simulate the error term. The code below defines a data generating function that returns a \nDataFrame\n of X and Y values initialized \nfrom the population coefficients, while adding white noise scaled according to the standard deviation specified by the \nsigma\n parameter.\n\n\n\n\n\n/**\n * Returns a 2D sample dataset based on a population process using the coefficients provided\n * @param alpha     the intercept term for population process\n * @param beta      the slope term for population process\n * @param startX    the start value for independent variable\n * @param stepX     the step size for independent variable\n * @param sigma     the variance to add noise to dependent variable\n * @param n         the size of the sample to generate\n * @return          the frame of XY values\n */\nDataFrame<Integer,String> sample(double alpha, double beta, double startX, double stepX, double sigma, int n) {\n    final Array<Double> xValues = Array.of(Double.class, n).applyDoubles(v -> startX + v.index() * stepX);\n    final Array<Double> yValues = Array.of(Double.class, n).applyDoubles(v -> {\n        final double yfit = alpha + beta * xValues.getDouble(v.index());\n        return new NormalDistribution(yfit, sigma).sample();\n    });\n    final Array<Integer> rowKeys = Range.of(0, n).toArray();\n    return DataFrame.of(rowKeys, String.class, columns -> {\n        columns.add(\"X\", xValues);\n        columns.add(\"Y\", yValues);\n    });\n}\n\n\n\n\nTo get a sense of the nature of the dataset generated by this function for some chosen set of parameters, we can plot a number \nof samples. The code below plots 4 random samples of this population process with beta = 1.45, alpha = 4.15 and a sigma value of 20.\n\n\n\n\n\nfinal double beta = 1.45d;\nfinal double alpha = 4.15d;\nfinal double sigma = 20d;\nChart.show(2, IntStream.range(0, 4).mapToObj(i -> {\n    DataFrame<Integer,String> frame = sample(alpha, beta, 0, 1, sigma, 100);\n    String title = \"Sample %s Dataset, Beta: %.2f Alpha: %.2f\";\n    String subtitle = \"Parameter estimates, Beta^: %.3f, Alpha^: %.3f\";\n    DataFrameLeastSquares<Integer,String> ols = frame.regress().ols(\"Y\", \"X\", true, Optional::of).get();\n    double betaHat = ols.getBetaValue(\"X\", DataFrameLeastSquares.Field.PARAMETER);\n    double alphaHat = ols.getInterceptValue(DataFrameLeastSquares.Field.PARAMETER);\n    return Chart.create().withScatterPlot(frame, false, \"X\", chart -> {\n        chart.title().withText(String.format(title, i, beta, alpha));\n        chart.title().withFont(new Font(\"Arial\", Font.BOLD, 14));\n        chart.subtitle().withText(String.format(subtitle, betaHat, alphaHat));\n        chart.plot().style(\"Y\").withColor(Color.RED).withPointsVisible(true);\n        chart.plot().trend(\"Y\");\n    });\n}));\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\nGiven this data generating function, we can produce many samples from a known population process and then proceed to run OLS regressions on \nthese samples. For each run we capture the coefficient estimates, and then plot a histogram of all the recorded estimates to confirm that the \ncoefficients are indeed centered on the known population values. The following code performs this procedure for 100,000 regressions, and is\nfollowed by the resulting plots.\n\n\n\n\n\nfinal int n = 100;\nfinal double actAlpha = 4.15d;\nfinal double actBeta = 1.45d;\nfinal double sigma = 20d;\nfinal int regressionCount = 100000;\nfinal Range<Integer> rows = Range.of(0, regressionCount);\nfinal Array<String> columns = Array.of(\"Beta\", \"Alpha\");\nfinal DataFrame<Integer,String> results = DataFrame.ofDoubles(rows, columns);\n\n//Run 100K regressions in parallel\nresults.rows().parallel().forEach(row -> {\n    final DataFrame<Integer,String> frame = sample(actAlpha, actBeta, 0, 1, sigma, n);\n    frame.regress().ols(\"Y\", \"X\", true, model -> {\n        final double alpha = model.getInterceptValue(DataFrameLeastSquares.Field.PARAMETER);\n        final double beta = model.getBetaValue(\"X\", DataFrameLeastSquares.Field.PARAMETER);\n        row.setDouble(\"Alpha\", alpha);\n        row.setDouble(\"Beta\", beta);\n        return Optional.empty();\n    });\n});\n\nArray.of(\"Beta\", \"Alpha\").forEach(coefficient -> {\n    Chart.create().withHistPlot(results, 250, coefficient, chart -> {\n        final double mean = results.colAt(coefficient).stats().mean();\n        final double stdDev = results.colAt(coefficient).stats().stdDev();\n        final double actual = coefficient.equals(\"Beta\") ? actBeta : actAlpha;\n        final String title = \"%s Histogram from %s Regressions (n=%s)\";\n        final String subtitle = \"Actual: %.4f, Mean: %.4f, StdDev: %.4f\";\n        chart.title().withText(String.format(title, coefficient, regressionCount, n));\n        chart.subtitle().withText(String.format(subtitle, actual, mean, stdDev));\n        chart.show(700, 400);\n    });\n});\n\n\n\n\n\n    \n\n    \n\n\n\n\n\nThe alpha and beta histogram plots above clearly show that the distribution of the 100000 estimates of each coefficient are centered on the \nknown population values. In the case of the slope coefficient, the known population value is 1.45 and the mean value over the 100000 estimates\nis a good match. Similarly, the intercept estimate mean 4.1266 is very close to the known population value of 4.15.\n\n\nConsistency\n\n\nAn OLS estimator is said to be \nconsistent\n in the sense that as the sample size increases, \nthe variance in the coefficient estimates should decrease. This can be demonstrated empirically once again using the data generation function introduced \nearlier. In this experiment we run a certain number of regressions based on samples generated from a known population process, but we would need to do \nthis multiple times with increasing sample sizes. The code below implements this by running 100,000 regressions for sample sizes ranging from 100 to 500 \nin steps of 100, and captures the coefficient estimates for each run. It then plots histograms for the beta and intercept estimates to illustrate the \nnarrowing variance as sample size increases.\n\n\n\n\n\nfinal double actAlpha = 4.15d;\nfinal double actBeta = 1.45d;\nfinal double sigma = 20d;\nfinal int regressionCount = 100000;\nfinal Range<Integer> sampleSizes = Range.of(100, 600, 100);\nfinal Range<Integer> rows = Range.of(0, regressionCount);\nfinal DataFrame<Integer,String> results = DataFrame.of(rows, String.class, columns -> {\n    sampleSizes.forEach(n -> {\n        columns.add(String.format(\"Beta(n=%s)\", n), Double.class);\n        columns.add(String.format(\"Alpha(n=%s)\", n), Double.class);\n    });\n});\n\nsampleSizes.forEach(n -> {\n    System.out.println(\"Running \" + regressionCount + \" regressions for n=\" + n);\n    final String betaKey = String.format(\"Beta(n=%s)\", n);\n    final String alphaKey = String.format(\"Alpha(n=%s)\", n);\n    results.rows().parallel().forEach(row -> {\n        final DataFrame<Integer,String> frame = sample(actAlpha, actBeta, 0, 1, sigma, n);\n        frame.regress().ols(\"Y\", \"X\", true, model -> {\n            final double alpha = model.getInterceptValue(DataFrameLeastSquares.Field.PARAMETER);\n            final double beta = model.getBetaValue(\"X\", DataFrameLeastSquares.Field.PARAMETER);\n            row.setDouble(alphaKey, alpha);\n            row.setDouble(betaKey, beta);\n            return Optional.empty();\n        });\n    });\n});\n\nArray.of(\"Beta\", \"Alpha\").forEach(coeff -> {\n    final DataFrame<Integer,String> coeffResults = results.cols().select(col -> col.key().startsWith(coeff));\n    Chart.create().withHistPlot(coeffResults, 250, true, chart -> {\n        chart.plot().axes().domain().label().withText(\"Coefficient Estimate\");\n        chart.title().withText(coeff + \" Histograms of \" + regressionCount + \" Regressions\");\n        chart.subtitle().withText(coeff + \" Variance decreases as sample size increases\");\n        chart.legend().on().bottom();\n        chart.show(700, 400);\n    });\n});\n\n\n\n\n\n    \n\n    \n\n\n\n\n\nIt is clear from the above plots that as sample size increases, the variance in the estimates decreases, which is what we expect if the\nestimator is consistent. The bar charts below summarize the change in variance for each of the coefficients, and is follow by the code\nthat generates these plots.\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\nArray<DataFrame<String,StatType>> variances = Array.of(\"Beta\", \"Alpha\").map(value -> {\n    final String coefficient = value.getValue();\n    final Matcher matcher = Pattern.compile(coefficient + \"\\\\(n=(\\\\d+)\\\\)\").matcher(\"\");\n    return results.cols().select(column -> {\n        final String name = column.key();\n        return matcher.reset(name).matches();\n    }).cols().mapKeys(column -> {\n        final String name = column.key();\n        if (matcher.reset(name).matches()) return matcher.group(1);\n        throw new IllegalArgumentException(\"Unexpected column name: \" + column.key());\n    }).cols().stats().variance();\n});\n\nChart.show(2, Collect.asList(\n    Chart.create().withBarPlot(variances.getValue(0), false, chart -> {\n        chart.title().withText(\"Beta variance with sample size\");\n        chart.plot().style(StatType.VARIANCE).withColor(new Color(255, 100, 100));\n        chart.plot().axes().range(0).label().withText(\"Beta Variance\");\n        chart.plot().axes().domain().label().withText(\"Sample Size\");\n    }),\n    Chart.create().withBarPlot(variances.getValue(1), false, chart -> {\n        chart.title().withText(\"Alpha variance with sample size\");\n        chart.plot().style(StatType.VARIANCE).withColor(new Color(102, 204, 255));\n        chart.plot().axes().range(0).label().withText(\"Alpha Variance\");\n        chart.plot().axes().domain().label().withText(\"Sample Size\");\n    })\n));",
            "title": "Ordinary Least Squares"
        },
        {
            "location": "/regression/ols/#ordinary-least-squares-ols",
            "text": "",
            "title": "Ordinary Least Squares (OLS)"
        },
        {
            "location": "/regression/ols/#introduction",
            "text": "Regression analysis  is a statistical technique used to fit a model expressed in \nterms of one or more variables to some data. In particular, it allows one to analyze the relationship of a dependent variable (also \nreferred to as the regressand) on one or more independent or predictor variables (also referred to as regressors), and assess how \ninfluential each of these are.    There are many types of regression analysis techniques, however one of the most commonly used is based on fitting data to a linear model, \nand in particular using an approach called  Least Squares . The Morpheus API currently supports \n3 variations of Linear Least Squares regression, namely  Ordinary Least Squares  (OLS),  Weighted Least Squares  (WLS) and  Generalized Least Squares \n(GLS). The following section reviews some OLS regression theory, and provides an example of how to use the Morhpeus API to apply this technique.",
            "title": "Introduction"
        },
        {
            "location": "/regression/ols/#theory",
            "text": "A linear regression model in matrix form can be expressed as:  $$ Y = X \\beta + \\epsilon \\ \\ \\ \\ where \\ E[\\epsilon]=0 \\ \\ \\ \\ Var[\\epsilon] = \\sigma^2 I  $$  Y represents an  nx1  vector of regressands, and X represents an  nxp  design matrix, where  n  is the number of observations in the data, \nand  p  represents the number of parameters to estimate. If the model includes an intercept term, the first column in the design matrix is \npopulated with 1's and therefore the first entry in the  px1  \\(\\beta\\) vector would represent the intercept value. The  nx1  \\(\\epsilon\\) \nvector represents the error or disturbance term, which is assumed to have a conditional mean of 0 and also to be free of any serial correlation \n(see section below on the  Gauss Markov  assumptions).  A least squares regression model estimates \\(\\beta\\) so as to minimize the sum of the squared error, or \\(\\epsilon^T\\epsilon\\). We \ncan use the model equation above to express \\(\\epsilon^T\\epsilon\\) in terms of Y and X\\(\\beta\\), differentiate this by \\(\\beta\\), \nset the result to zero since we wish to minimize the squared error, and solve for \\(\\beta\\) as follows:    $$ \\begin{align}\n\\epsilon^T\\epsilon &= (Y - X \\beta)^T(Y - X \\beta) \\\\\n &= Y^TY - \\beta^TX^TY - Y^TX\\beta + \\beta^T X^T X \\beta \\\\\n &= Y^TY - 2\\beta^T X^T Y + \\beta^T X^T X \\beta\n\\end{align} $$  We now differentiate this expression with respect to \\(\\beta\\) and set it to zero which yields the following:  $$ \\frac{d\\epsilon^T\\epsilon}{d\\beta} = -2X^T Y + 2X^T X \\beta = 0 $$  This expression can be re-arranged to solve for \\(\\beta\\) and yields the following equation:   $$ \\beta = (X^T X)^{-1} X^T Y $$   The value of \\(\\beta\\) can only be  estimated  given some sample data drawn from a population or data generating process, the  true  value is \nunknown. We usually refer to the estimate as \\(\\hat{\\beta}\\) (beta \"hat\") in order to distinguish it from the true population value. In addition, \nwhen expressing the model in terms of \\(\\hat{\\beta}\\), the stochastic term is referred to as  residuals  rather than  errors , and while they \nare conceptually related, they are  not the same thing . Errors represent the deviation of \nobservations from the unknown population line, while residuals represent the deviations from the estimation line, a subtle difference and easily \nconfused. In terms of \\(\\hat{\\beta}\\) and residuals  u , the model is expressed in the usual way.   $$ Y = X \\hat{\\beta} + u \\ \\ \\ \\ where \\ E[u]=0 \\ \\ \\ \\ Var[u] = \\sigma^2 I  $$  The residuals are of course still assumed to be consistent with the  Gauss Markov  \nassumptions, and so the estimator for \\(\\hat{\\beta}\\) is:   $$ \\hat{\\beta} = (X^T X)^{-1} X^T Y $$",
            "title": "Theory"
        },
        {
            "location": "/regression/ols/#solving",
            "text": "We can solve for \\(\\hat{\\beta}\\) directly by calculating the right hand side of the above equation, which is what the Morpheus library does\nby default. There are situations in which this can present numerical stability issues, in which case it may be preferable to solve for \\(\\hat{\\beta}\\) \nby factorizing the design matrix X using a  QR decomposition  where Q represents an orthogonal matrix \nsuch that \\(Q^T Q = I\\) and thus \\(Q^T = Q^{-1}\\), and where R is an upper triangular matrix. The QR decomposition approach is based on the \nfollowing reasoning.  $$ \\begin{align}\nX^T Y & = X^T X \\hat{\\beta}  \\\\\n\\big( QR \\big)^T Y & = \\big( QR \\big)^T \\big( QR \\big) \\hat{\\beta} \\\\\nR^T Q^T Y & = R^T Q^T Q R \\hat{\\beta} \\\\\nR^T Q^T Y & = R^T R \\hat{\\beta} \\\\\n(R^T)^{-1} R^T Q^T Y & = \\big( R^T \\big)^{-1} R^T R \\hat{\\beta} \\\\ \nQ^T Y & = R \\hat{\\beta}\n\\end{align} $$  This can be solved efficiently by  backward substitution  because the R \nmatrix is upper  triangular , and therefore no inversion of the design matrix is required. The Morpheus \nlibrary also supports estimating \\(\\hat{\\beta}\\) using this technique, which can be configured on a case by case basis via the  withSolver()  method \non the  DataFrameLeastSquares  interface.",
            "title": "Solving"
        },
        {
            "location": "/regression/ols/#diagnostics",
            "text": "Just like any other statistical technique, regression analysis is susceptible to  sampling error , and \nit is therefore common to compute the variance of the parameter estimates, as well as their  standard error , \nwhich can then be used for  inferential  purposes. In the context of a linear regression, the  null hypotheses  \\(H_{0}\\), is generally that the model parameters are zero. The parameter standard \nerrors can be used to calculate a  t-statistic  and corresponding  p-value  \nin order to decide if \\(H_{0}\\) can be rejected in favour of the alternative hypothesis, and thus assert that the estimates are statistically \nsignificantly different from zero.  The variance of \\(\\hat{\\beta}\\) with respect to the true population value can be expressed as follows:  $$ Var(\\hat{\\beta}) = E[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)^T] $$  We substitute our population model \\( Y = X \\beta + \\epsilon \\) in our sample estimator \\( \\hat{\\beta} = (X^T X)^{-1} X^T Y \\) as follows:    $$ \\begin{align}\n\\hat{\\beta} &= (X^T X)^{-1} X^T ( X \\beta + \\epsilon ) \\\\\n\\hat{\\beta} &= (X^T X)^{-1} X^T X \\beta + (X^T X)^{-1} X^T \\epsilon  \\\\\n\\hat{\\beta} &= \\beta + (X^T X)^{-1} X^T \\epsilon  \\\\\n\\hat{\\beta} - \\beta &= (X^T X)^{-1} X^T \\epsilon  \\\\\n\\end{align} $$  With the above expression for \\( \\hat{\\beta} - \\beta \\) we can now solve for the variance of the OLS estimator as follows:   $$ \\begin{align}\nVar(\\hat{\\beta}) &= E[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)^T] \\\\\n & = E[((X^T X)^{-1} X^T \\epsilon) ((X^T X)^{-1} X^T \\epsilon)^T] \\\\\n & = E[(X^T X)^{-1} X^T \\epsilon \\epsilon^T X (X^T X)^{-1} ]\n\\end{align} $$  Given that the design matrix X is non-stochastic and the \\( E[\\epsilon \\epsilon^T] = \\sigma^2 I \\):  $$ \\begin{align}\nVar(\\hat{\\beta}) & = (X^T X)^{-1} X^T E[\\epsilon \\epsilon^T] X (X^T X)^{-1} \\\\\n & = (X^T X)^{-1} X^T (\\sigma^2 I) X (X^T X)^{-1} \\\\\n & = \\sigma^2 I (X^T X)^{-1} X^T X (X^T X)^{-1} \\\\\n & = \\sigma^2 I (X^T X)^{-1} \\\\\n & = \\sigma^2 (X^T X)^{-1}\n\\end{align} $$  Other regression diagnostics that are calculated include the  coefficient of determination  or\n\\(R^2\\) which is a number that indicates the proportion of variance in the dependent variable that is explained by the independent variables, and is\ndocumented in the table below. The parameter variance estimates as calculated above are used to compute the standard errors and a corresponding t-statistic  which can be used for statistical inference.     Quantity  Description      Residual Sum of Squares (RSS)  $$ RSS = \\sum_{i=1}^n \\big(y_{i} - \\hat{y_{i}} \\big)^2 = \\sum_{i=1}^n \\epsilon_{i}^2 = \\epsilon^T \\epsilon $$    Total Sum of Squares (TSS)  $$ TSS = \\sum_{i=1}^{n} \\big(y_{i} - \\overline{y}\\big)^2 \\, \\textrm{where} \\; \\overline{y} = \\frac{1}{n} \\sum_{i=1}^n y_{i} $$    Explained Sum of Squares (ESS)  $$ ESS = \\sum_{i=1}^{n} \\big(\\hat{y_{i}} - \\overline{y}\\big)^2 \\, \\textrm{where} \\; \\overline{y} = \\frac{1}{n} \\sum_{i=1}^n y_{i} $$    R-Squared  $$ R^2 = 1 - \\frac{RSS}{TSS}  $$    R-Squared (Adjusted)  $$ R^2_{adj} = 1 - \\frac{ RSS * \\big( n - 1 \\big) }{ TSS * \\big( n - p \\big)}  $$    Regression Standard Error  $$ SE = \\sqrt{ \\frac{RSS}{ n - p }}  $$    Parameter Variance  $$ Var(\\hat{\\beta}) = SE^2( X^T X )^{-1}  $$    Parameter Std Errors  $$ SE(\\hat{\\beta_{i}}) = \\sqrt{ Var(\\hat{\\beta_{i}})} $$    Parameter T-statistics  $$ T_{\\beta_{i}} = \\frac{\\hat{ \\beta_{i}}}{ SE( \\hat{ \\beta_{i} } ) }    $$",
            "title": "Diagnostics"
        },
        {
            "location": "/regression/ols/#gauss-markov-assumptions",
            "text": "The  Gauss Markov Theorem  states that Ordinary Least Squares is the Best Linear Unbiased \nand Efficient (BLUE) estimator of \\(\\beta\\), conditional on a certain set of assumptions being met. In this context, \"best\" means that there are\nno other unbiased estimators with a smaller sampling variance than OLS. Unbiased means that that the expectation of \\( \\hat{\\beta}\\) is equal to the\npopulation \\(\\beta\\), or otherwise stated \\( E[ \\hat{\\beta} ] = \\beta \\). The assumptions that must hold for OLS to be BLUE are as follows:     Assumptions  Description      Assumption 1  The regression model is linear in parameters, and therefore well specified    Assumption 2  The regressors are linearly independent, and therefore do not exhibit perfect  multicollinearity    Assumption 3  The errors in the regression have a conditional mean of zero    Assumption 4  The errors are  homoscedastic , which means they exhibit constant variance    Assumption 5  The errors are uncorrelated between observations    Assumption 6  The errors are normally distributed, and independent and identically distributed (iid)",
            "title": "Gauss Markov Assumptions"
        },
        {
            "location": "/regression/ols/#linear-in-parameters",
            "text": "The first assumption regarding linearity suggests that the dependent variable is a linear function of the independent variables. This does not imply that there\nis a linear relationship between the independent and dependent variables, it only states the the model is linear in parameters. For example, a model of the form\n\\(y = \\alpha + \\beta x^2 \\) qualifies as being linear in parameters, while \\(y = \\alpha + \\beta^2 x \\) does not. If the functional form of a  model under\ninvestigation is not linear in parameters, it can often be transformed so as to render it linear.",
            "title": "Linear in Parameters"
        },
        {
            "location": "/regression/ols/#linearly-independent",
            "text": "The second assumption that there is no perfect  multicollinearity  between the regressors is important, as if\nit exists, the OLS estimator cannot be calculated. Another way of expressing this condition is that one of the independent variables cannot be a function of any \nof the other independent variables, and therefore the design matrix X must be non-singular, and therefore have full rank.",
            "title": "Linearly Independent"
        },
        {
            "location": "/regression/ols/#strict-exogeneity",
            "text": "The third assumption above states that the disturbance term averages out to zero for any given instance of X, which implies that no observations of the independent \nvariables convey any information about the error. Mathematically this is stated as \\( E[ \\epsilon | X ] = 0 \\). This assumption is violated if the independent\nvariables are  stochastic  in nature, which can arise as a result of  measurement error , or if there is  endogeneity  in the model.",
            "title": "Strict exogeneity"
        },
        {
            "location": "/regression/ols/#spherical-errors",
            "text": "The fourth and fifth assumptions relate to the  covariance matrix  of the error term, and specifically states \nthat \\(E[ \\epsilon \\epsilon^T | X] = \\sigma^2 I \\). There are two key concepts embedded in this statement, the first is that the disturbance term has uniform \nvariance of \\(\\sigma^2\\) regardless of the values of the independent variables, and is referred to as homoscedasticity. In addition, the off diagonal terms of \nthe covariance matrix are assumed to be zero, which suggests there is no serial correlation between errors. Either or both of these assumptions may not hold in \nthe real world, in which case a WLS or GLS estimator may prove to be a better, unbiased linear estimator.",
            "title": "Spherical Errors"
        },
        {
            "location": "/regression/ols/#example",
            "text": "The Morpheus API defines an interface called  DataFrameRegression  which exposes a number of methods that support different linear regression techniques, namely \nOLS, WLS and GLS. There are overloaded methods that take one or more regressors in order to conveniently support simple and multiple linear regression.   The regression interface, which operates on the column data in a  DataFrame , can be accessed by calling the  regress()  method on the frame. If  a regression in \nthe row dimension is required, simply call  transpose()  on the frame before calling  regress() .  To illustrate an example, consider the same motor vehicle dataset introduced earlier, which can be loaded with the code below. The first 10 rows of this  DataFrame  \nis also included for inspection, and in this exercise we are going to be interested in the  EngineSize  and  Horsepower  columns.    static DataFrame<Integer,String> loadCarDataset() {\n    return DataFrame.read().csv(options -> {\n        options.setResource(\"http://zavtech.com/data/samples/cars93.csv\");\n        options.setExcludeColumnIndexes(0);\n    });\n}  \n Index  |  Manufacturer  |    Model     |   Type    |  Min.Price  |   Price   |  Max.Price  |  MPG.city  |  MPG.highway  |       AirBags        |  DriveTrain  |  Cylinders  |  EngineSize  |  Horsepower  |  RPM   |  Rev.per.mile  |  Man.trans.avail  |  Fuel.tank.capacity  |  Passengers  |  Length  |  Wheelbase  |  Width  |  Turn.circle  |  Rear.seat.room  |  Luggage.room  |  Weight  |  Origin   |        Make        |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     0  |         Acura  |     Integra  |    Small  |    12.9000  |  15.9000  |    18.8000  |        25  |           31  |                None  |       Front  |          4  |      1.8000  |         140  |  6300  |          2890  |              Yes  |             13.2000  |           5  |     177  |        102  |     68  |           37  |            26.5  |            11  |    2705  |  non-USA  |     Acura Integra  |\n     1  |         Acura  |      Legend  |  Midsize  |    29.2000  |  33.9000  |    38.7000  |        18  |           25  |  Driver & Passenger  |       Front  |          6  |      3.2000  |         200  |  5500  |          2335  |              Yes  |             18.0000  |           5  |     195  |        115  |     71  |           38  |              30  |            15  |    3560  |  non-USA  |      Acura Legend  |\n     2  |          Audi  |          90  |  Compact  |    25.9000  |  29.1000  |    32.3000  |        20  |           26  |         Driver only  |       Front  |          6  |      2.8000  |         172  |  5500  |          2280  |              Yes  |             16.9000  |           5  |     180  |        102  |     67  |           37  |              28  |            14  |    3375  |  non-USA  |           Audi 90  |\n     3  |          Audi  |         100  |  Midsize  |    30.8000  |  37.7000  |    44.6000  |        19  |           26  |  Driver & Passenger  |       Front  |          6  |      2.8000  |         172  |  5500  |          2535  |              Yes  |             21.1000  |           6  |     193  |        106  |     70  |           37  |              31  |            17  |    3405  |  non-USA  |          Audi 100  |\n     4  |           BMW  |        535i  |  Midsize  |    23.7000  |  30.0000  |    36.2000  |        22  |           30  |         Driver only  |        Rear  |          4  |      3.5000  |         208  |  5700  |          2545  |              Yes  |             21.1000  |           4  |     186  |        109  |     69  |           39  |              27  |            13  |    3640  |  non-USA  |          BMW 535i  |\n     5  |         Buick  |     Century  |  Midsize  |    14.2000  |  15.7000  |    17.3000  |        22  |           31  |         Driver only  |       Front  |          4  |      2.2000  |         110  |  5200  |          2565  |               No  |             16.4000  |           6  |     189  |        105  |     69  |           41  |              28  |            16  |    2880  |      USA  |     Buick Century  |\n     6  |         Buick  |     LeSabre  |    Large  |    19.9000  |  20.8000  |    21.7000  |        19  |           28  |         Driver only  |       Front  |          6  |      3.8000  |         170  |  4800  |          1570  |               No  |             18.0000  |           6  |     200  |        111  |     74  |           42  |            30.5  |            17  |    3470  |      USA  |     Buick LeSabre  |\n     7  |         Buick  |  Roadmaster  |    Large  |    22.6000  |  23.7000  |    24.9000  |        16  |           25  |         Driver only  |        Rear  |          6  |      5.7000  |         180  |  4000  |          1320  |               No  |             23.0000  |           6  |     216  |        116  |     78  |           45  |            30.5  |            21  |    4105  |      USA  |  Buick Roadmaster  |\n     8  |         Buick  |     Riviera  |  Midsize  |    26.3000  |  26.3000  |    26.3000  |        19  |           27  |         Driver only  |       Front  |          6  |      3.8000  |         170  |  4800  |          1690  |               No  |             18.8000  |           5  |     198  |        108  |     73  |           41  |            26.5  |            14  |    3495  |      USA  |     Buick Riviera  |  Based on our understanding of the factors that influence the power of an internal combustion engine, one might hypothesize that there is a positive and linear \nrelationship between the size of an engine and how much horsepower it produces. . We can use the car dataset to test this hypothesis. First we can use the Morpheus \nlibrary to generate a scatter plot of the data, with  EngineSize  on the x-axis and  Horsepower  on the y-axis as follows:   final String y = \"Horsepower\";\nfinal String x = \"EngineSize\";\nfinal DataFrame<Integer,String> frame = loadCarDataset();\nfinal DataFrame<Integer,String> xy = frame.cols().select(y, x);\nChart.create().withScatterPlot(xy, false, x, chart -> {\n    chart.title().withText(y + \" vs \" + x);\n    chart.plot().style(y).withColor(Color.RED);\n    chart.plot().style(y).withPointsVisible(true).withPointShape(ChartShape.DIAMOND);\n    chart.plot().axes().domain().label().withText(x);\n    chart.plot().axes().domain().format().withPattern(\"0.00;-0.00\");\n    chart.plot().axes().range(0).label().withText(y);\n    chart.plot().axes().range(0).format().withPattern(\"0;-0\");\n    chart.show(845, 450);\n});  \n       The scatter plot certainly appears to suggest that there is a positive relationship between  EngineSize  and  Horsepower . In addition, it seems somewhat \nlinear, however the dispersion appears to get more significant for larger engine sizes, which would be a violation of one of the  Gauss Markov  \nassumptions, namely of  homoscedastic  errors. Nevertheless, let us proceed to regress  Horsepower  on  EngineSize  and see what the results look like. The code below runs a single variable regression and simply prints the model results to standard-out \nfor inspection.   DataFrame<Integer,String> frame = loadCarDataset();\nString regressand = \"Horsepower\";\nString regressor = \"EngineSize\";\nframe.regress().ols(regressand, regressor, true, model -> {\n    System.out.println(model);\n    return Optional.empty();\n});  \n==============================================================================================\n                                   Linear Regression Results                                                            \n==============================================================================================\nModel:                                   OLS    R-Squared:                            0.5360\nObservations:                             93    R-Squared(adjusted):                  0.5309\nDF Model:                                  1    F-Statistic:                        105.1204\nDF Residuals:                             91    F-Statistic(Prob):                  1.11E-16\nStandard Error:                      35.8717    Runtime(millis)                           48\nDurbin-Watson:                        1.9591                                                \n==============================================================================================\n   Index     |  PARAMETER  |  STD_ERROR  |  T_STAT   |   P_VALUE   |  CI_LOWER  |  CI_UPPER  |\n----------------------------------------------------------------------------------------------\n  Intercept  |    45.2195  |    10.3119  |   4.3852  |   3.107E-5  |    24.736  |   65.7029  |\n EngineSize  |    36.9633  |     3.6052  |  10.2528  |  7.573E-17  |    29.802  |   44.1245  |\n==============================================================================================  The regression results yield a slope coefficient of  36.96 , suggesting that for every additional litre of engine capacity, we can expect to add another\n36.96 horsepower. While the  p-value  associated with the slope coefficient suggests that it is statistically \nsignificantly different from zero, it does not tell us about how relevant the parameter is in the regression. In this case we can reasonably surmise \nthat engine size is relevant given our understanding of how an internal combustion engine works and what factors affect output power. Having said that, \nthe coefficient of determination is perhaps lower than one might expect, and the heteroscedasticity of the residuals provides some hint that the model \nmay be incomplete. In particular,  omitted-variable bias  may be at play here, in the sense that \nthere are other important factors that influence an engine's horsepower that we have not included.  While the code example above simply prints the model results to standard out, the illustration below demonstrates how to access all the relevant model \noutputs via the API. The  ols()  method takes a lambda parameter that consumes the regression model, which is an instance of the  DataFrameLeastSquares  \ninterface, and provides all the relevant hooks to access the model inputs and outputs.   final String regressand = \"Horsepower\";\nfinal String regressor = \"EngineSize\";\nfinal DataFrame<Integer,String> frame = loadCarDataset();\nframe.regress().ols(regressand, regressor, true, model -> {\n    assert (model.getRegressand().equals(regressand));\n    assert (model.getRegressors().size() == 1);\n    assertEquals(model.getRSquared(), 0.5359992996664269, 0.00001);\n    assertEquals(model.getRSquaredAdj(), 0.5309003908715525, 0.000001);\n    assertEquals(model.getStdError(), 35.87167658782274, 0.00001);\n    assertEquals(model.getFValue(), 105.120393642, 0.00001);\n    assertEquals(model.getFValueProbability(), 0, 0.00001);\n    assertEquals(model.getBetaValue(\"EngineSize\", Field.PARAMETER), 36.96327914, 0.0000001);\n    assertEquals(model.getBetaValue(\"EngineSize\", Field.STD_ERROR), 3.60518041, 0.0000001);\n    assertEquals(model.getBetaValue(\"EngineSize\", Field.T_STAT), 10.25282369, 0.0000001);\n    assertEquals(model.getBetaValue(\"EngineSize\", Field.P_VALUE), 0.0000, 0.0000001);\n    assertEquals(model.getBetaValue(\"EngineSize\", Field.CI_LOWER), 29.80203113, 0.0000001);\n    assertEquals(model.getBetaValue(\"EngineSize\", Field.CI_UPPER), 44.12452714, 0.0000001);\n    assertEquals(model.getInterceptValue(Field.PARAMETER), 45.21946716, 0.0000001);\n    assertEquals(model.getInterceptValue(Field.STD_ERROR), 10.31194906, 0.0000001);\n    assertEquals(model.getInterceptValue(Field.T_STAT), 4.3851523, 0.0000001);\n    assertEquals(model.getInterceptValue(Field.P_VALUE), 0.00003107, 0.0000001);\n    assertEquals(model.getInterceptValue(Field.CI_LOWER), 24.73604714, 0.0000001);\n    assertEquals(model.getInterceptValue(Field.CI_UPPER), 65.70288719, 0.0000001);\n    System.out.println(model);\n    return Optional.of(model);\n});  Finally, the chart below adds the OLS trendline to the initial scatter plot to get a better sense of how the solution fits the data.   \n       The code to generate this chart is as follows:   final String regressand = \"Horsepower\";\nfinal String regressor = \"EngineSize\";\nfinal DataFrame<Integer,String> frame = loadCarDataset();\nfinal DataFrame<Integer,String> xy = frame.cols().select(regressand, regressor);\nChart.create().withScatterPlot(xy, false, regressor, chart -> {\n    chart.title().withFont(new Font(\"Verdana\", Font.BOLD, 16));\n    chart.title().withText(regressand + \" regressed on \" + regressor);\n    chart.subtitle().withText(\"Single Variable Linear Regression\");\n    chart.plot().style(regressand).withColor(Color.RED);\n    chart.plot().trend(regressand).withColor(Color.BLACK);\n    chart.plot().axes().domain().label().withText(regressor);\n    chart.plot().axes().domain().format().withPattern(\"0.00;-0.00\");\n    chart.plot().axes().range(0).label().withText(regressand);\n    chart.plot().axes().range(0).format().withPattern(\"0;-0\");\n    chart.show();\n});",
            "title": "Example"
        },
        {
            "location": "/regression/ols/#unbiasedness",
            "text": "An Ordinary Least Squares estimator is said to be  unbiased  in the sense that \nif you run regressions on many samples of data generated from the same population process, the coefficient estimates from all these samples \nwould be centered on the true population values. To demonstrate this empirically, we can define a population process in 2D space with a known \nslope and intercept coefficient, and then proceed to generate many samples from this process while adding Gaussian noise to the dependent variable \nin order to simulate the error term. The code below defines a data generating function that returns a  DataFrame  of X and Y values initialized \nfrom the population coefficients, while adding white noise scaled according to the standard deviation specified by the  sigma  parameter.   /**\n * Returns a 2D sample dataset based on a population process using the coefficients provided\n * @param alpha     the intercept term for population process\n * @param beta      the slope term for population process\n * @param startX    the start value for independent variable\n * @param stepX     the step size for independent variable\n * @param sigma     the variance to add noise to dependent variable\n * @param n         the size of the sample to generate\n * @return          the frame of XY values\n */\nDataFrame<Integer,String> sample(double alpha, double beta, double startX, double stepX, double sigma, int n) {\n    final Array<Double> xValues = Array.of(Double.class, n).applyDoubles(v -> startX + v.index() * stepX);\n    final Array<Double> yValues = Array.of(Double.class, n).applyDoubles(v -> {\n        final double yfit = alpha + beta * xValues.getDouble(v.index());\n        return new NormalDistribution(yfit, sigma).sample();\n    });\n    final Array<Integer> rowKeys = Range.of(0, n).toArray();\n    return DataFrame.of(rowKeys, String.class, columns -> {\n        columns.add(\"X\", xValues);\n        columns.add(\"Y\", yValues);\n    });\n}  To get a sense of the nature of the dataset generated by this function for some chosen set of parameters, we can plot a number \nof samples. The code below plots 4 random samples of this population process with beta = 1.45, alpha = 4.15 and a sigma value of 20.   final double beta = 1.45d;\nfinal double alpha = 4.15d;\nfinal double sigma = 20d;\nChart.show(2, IntStream.range(0, 4).mapToObj(i -> {\n    DataFrame<Integer,String> frame = sample(alpha, beta, 0, 1, sigma, 100);\n    String title = \"Sample %s Dataset, Beta: %.2f Alpha: %.2f\";\n    String subtitle = \"Parameter estimates, Beta^: %.3f, Alpha^: %.3f\";\n    DataFrameLeastSquares<Integer,String> ols = frame.regress().ols(\"Y\", \"X\", true, Optional::of).get();\n    double betaHat = ols.getBetaValue(\"X\", DataFrameLeastSquares.Field.PARAMETER);\n    double alphaHat = ols.getInterceptValue(DataFrameLeastSquares.Field.PARAMETER);\n    return Chart.create().withScatterPlot(frame, false, \"X\", chart -> {\n        chart.title().withText(String.format(title, i, beta, alpha));\n        chart.title().withFont(new Font(\"Arial\", Font.BOLD, 14));\n        chart.subtitle().withText(String.format(subtitle, betaHat, alphaHat));\n        chart.plot().style(\"Y\").withColor(Color.RED).withPointsVisible(true);\n        chart.plot().trend(\"Y\");\n    });\n}));  \n       \n       \n       \n       Given this data generating function, we can produce many samples from a known population process and then proceed to run OLS regressions on \nthese samples. For each run we capture the coefficient estimates, and then plot a histogram of all the recorded estimates to confirm that the \ncoefficients are indeed centered on the known population values. The following code performs this procedure for 100,000 regressions, and is\nfollowed by the resulting plots.   final int n = 100;\nfinal double actAlpha = 4.15d;\nfinal double actBeta = 1.45d;\nfinal double sigma = 20d;\nfinal int regressionCount = 100000;\nfinal Range<Integer> rows = Range.of(0, regressionCount);\nfinal Array<String> columns = Array.of(\"Beta\", \"Alpha\");\nfinal DataFrame<Integer,String> results = DataFrame.ofDoubles(rows, columns);\n\n//Run 100K regressions in parallel\nresults.rows().parallel().forEach(row -> {\n    final DataFrame<Integer,String> frame = sample(actAlpha, actBeta, 0, 1, sigma, n);\n    frame.regress().ols(\"Y\", \"X\", true, model -> {\n        final double alpha = model.getInterceptValue(DataFrameLeastSquares.Field.PARAMETER);\n        final double beta = model.getBetaValue(\"X\", DataFrameLeastSquares.Field.PARAMETER);\n        row.setDouble(\"Alpha\", alpha);\n        row.setDouble(\"Beta\", beta);\n        return Optional.empty();\n    });\n});\n\nArray.of(\"Beta\", \"Alpha\").forEach(coefficient -> {\n    Chart.create().withHistPlot(results, 250, coefficient, chart -> {\n        final double mean = results.colAt(coefficient).stats().mean();\n        final double stdDev = results.colAt(coefficient).stats().stdDev();\n        final double actual = coefficient.equals(\"Beta\") ? actBeta : actAlpha;\n        final String title = \"%s Histogram from %s Regressions (n=%s)\";\n        final String subtitle = \"Actual: %.4f, Mean: %.4f, StdDev: %.4f\";\n        chart.title().withText(String.format(title, coefficient, regressionCount, n));\n        chart.subtitle().withText(String.format(subtitle, actual, mean, stdDev));\n        chart.show(700, 400);\n    });\n});  \n     \n       The alpha and beta histogram plots above clearly show that the distribution of the 100000 estimates of each coefficient are centered on the \nknown population values. In the case of the slope coefficient, the known population value is 1.45 and the mean value over the 100000 estimates\nis a good match. Similarly, the intercept estimate mean 4.1266 is very close to the known population value of 4.15.",
            "title": "Unbiasedness"
        },
        {
            "location": "/regression/ols/#consistency",
            "text": "An OLS estimator is said to be  consistent  in the sense that as the sample size increases, \nthe variance in the coefficient estimates should decrease. This can be demonstrated empirically once again using the data generation function introduced \nearlier. In this experiment we run a certain number of regressions based on samples generated from a known population process, but we would need to do \nthis multiple times with increasing sample sizes. The code below implements this by running 100,000 regressions for sample sizes ranging from 100 to 500 \nin steps of 100, and captures the coefficient estimates for each run. It then plots histograms for the beta and intercept estimates to illustrate the \nnarrowing variance as sample size increases.   final double actAlpha = 4.15d;\nfinal double actBeta = 1.45d;\nfinal double sigma = 20d;\nfinal int regressionCount = 100000;\nfinal Range<Integer> sampleSizes = Range.of(100, 600, 100);\nfinal Range<Integer> rows = Range.of(0, regressionCount);\nfinal DataFrame<Integer,String> results = DataFrame.of(rows, String.class, columns -> {\n    sampleSizes.forEach(n -> {\n        columns.add(String.format(\"Beta(n=%s)\", n), Double.class);\n        columns.add(String.format(\"Alpha(n=%s)\", n), Double.class);\n    });\n});\n\nsampleSizes.forEach(n -> {\n    System.out.println(\"Running \" + regressionCount + \" regressions for n=\" + n);\n    final String betaKey = String.format(\"Beta(n=%s)\", n);\n    final String alphaKey = String.format(\"Alpha(n=%s)\", n);\n    results.rows().parallel().forEach(row -> {\n        final DataFrame<Integer,String> frame = sample(actAlpha, actBeta, 0, 1, sigma, n);\n        frame.regress().ols(\"Y\", \"X\", true, model -> {\n            final double alpha = model.getInterceptValue(DataFrameLeastSquares.Field.PARAMETER);\n            final double beta = model.getBetaValue(\"X\", DataFrameLeastSquares.Field.PARAMETER);\n            row.setDouble(alphaKey, alpha);\n            row.setDouble(betaKey, beta);\n            return Optional.empty();\n        });\n    });\n});\n\nArray.of(\"Beta\", \"Alpha\").forEach(coeff -> {\n    final DataFrame<Integer,String> coeffResults = results.cols().select(col -> col.key().startsWith(coeff));\n    Chart.create().withHistPlot(coeffResults, 250, true, chart -> {\n        chart.plot().axes().domain().label().withText(\"Coefficient Estimate\");\n        chart.title().withText(coeff + \" Histograms of \" + regressionCount + \" Regressions\");\n        chart.subtitle().withText(coeff + \" Variance decreases as sample size increases\");\n        chart.legend().on().bottom();\n        chart.show(700, 400);\n    });\n});  \n     \n       It is clear from the above plots that as sample size increases, the variance in the estimates decreases, which is what we expect if the\nestimator is consistent. The bar charts below summarize the change in variance for each of the coefficients, and is follow by the code\nthat generates these plots.  \n       \n        Array<DataFrame<String,StatType>> variances = Array.of(\"Beta\", \"Alpha\").map(value -> {\n    final String coefficient = value.getValue();\n    final Matcher matcher = Pattern.compile(coefficient + \"\\\\(n=(\\\\d+)\\\\)\").matcher(\"\");\n    return results.cols().select(column -> {\n        final String name = column.key();\n        return matcher.reset(name).matches();\n    }).cols().mapKeys(column -> {\n        final String name = column.key();\n        if (matcher.reset(name).matches()) return matcher.group(1);\n        throw new IllegalArgumentException(\"Unexpected column name: \" + column.key());\n    }).cols().stats().variance();\n});\n\nChart.show(2, Collect.asList(\n    Chart.create().withBarPlot(variances.getValue(0), false, chart -> {\n        chart.title().withText(\"Beta variance with sample size\");\n        chart.plot().style(StatType.VARIANCE).withColor(new Color(255, 100, 100));\n        chart.plot().axes().range(0).label().withText(\"Beta Variance\");\n        chart.plot().axes().domain().label().withText(\"Sample Size\");\n    }),\n    Chart.create().withBarPlot(variances.getValue(1), false, chart -> {\n        chart.title().withText(\"Alpha variance with sample size\");\n        chart.plot().style(StatType.VARIANCE).withColor(new Color(102, 204, 255));\n        chart.plot().axes().range(0).label().withText(\"Alpha Variance\");\n        chart.plot().axes().domain().label().withText(\"Sample Size\");\n    })\n));",
            "title": "Consistency"
        },
        {
            "location": "/regression/wls/",
            "text": "Weighted Least Squares (WLS)\n\n\nIntroduction\n\n\nWhen fitting an OLS regression model, it may become apparent that there is an inconsistent variance in the residuals, which is known as \n\nheteroscedasticity\n. As discussed earlier, this is a violation of one of the \n\nGauss Markov\n assumptions, and therefore OLS is no longer the Best Linear Unbiased \nEstimator (BLUE). An OLS model in such circumstances is still expected to be \nunbiased\n \nand \nconsistent\n, however it will not be the most \nefficient\n estimator which suggests there are other estimators \nthat exhibit lower variance in parameter estimates. More concerning is that OLS is likely to yield biased estimates of the standard errors of the \ncoefficients, making statistical interference unreliable.\n\n\nTheory\n\n\nTo address the issue of heteroscedasticity, a Weighted Least Squares (WLS) regression model is used in favour of OLS, which in effect applies \na transformation to the original model so that the transformed model does in fact exhibit \nhomoscedastic\n \nerrors. To demonstrate, consider the usual form of the regression model, but in this case we assume the variance in the error term follows a \nform of \\( Var[\\epsilon] = \\sigma^2 D \\) where D is a diagonal matrix of factors that scales the variance appropriately.\n\n\n$$ Y = X \\beta + \\epsilon \\ \\ \\ \\ where \\ E[\\epsilon]=0 \\ \\ \\ \\ Var[\\epsilon] = \\sigma^2 D  $$\n\n\nNow let us assume there exists a matrix P that can transform the above model such that the resulting error term becomes homoscedastic, \nand therefore satisfies the \nGauss Markov\n assumption of constant variance.\n\n\n$$ P Y = P X \\beta + P \\epsilon \\ \\ \\ \\ where \\ E[P\\epsilon]=0 \\ \\ \\ \\ Var(P \\epsilon) = \\sigma^2 I $$\n\n\nSince the \nGauss Markov\n assumptions are satisfied for the transformed system, \nwe can apply OLS in the usual fashion. The next logical question therefore is, how do we come up with P? It turns out that we can solve for P \nin terms of D, the latter of which we can estimate reasonably well from the sample data. Consider the following derivation, where the key\nassumption is that P is a symmetric matrix and therefore \\( P^T = P \\).\n\n\n$$ \\begin{align}\nVar(P \\epsilon) & = \\sigma^2 I \\\\\nP Var(\\epsilon) P^T & = \\sigma^2 I \\\\\n\\sigma^2 P D P^T & = \\sigma^2 I \\\\\nP D P^ T & = I \\\\\nD P^T & = P^{-1} \\\\\nD & = (P^2)^{-1} \\\\\nP & = D^{-\\frac{1}{2}}\n\\end{align} $$\n\n\nNow let us re-write the transformed model in terms of Z, B and E as follows:\n\n\n$$ Z = B \\beta + E \\ \\ \\ \\  where \\ Z = P Y, \\ \\ B = PX, \\ \\ and \\ E = P \\epsilon $$\n\n\nWe can use the standard OLS estimator for this since we can assume it satisifies the \nGauss Markov\n assumptions.\n\n\n$$ \\hat{\\beta}_{ols} = (B^T B)^{-1} B^T Z $$\n\n\nKnowing that \\(B=PX\\) and that \\( P = D^{-\\frac{1}{2}} \\)we can write the WLS estimator as follows:\n\n\n$$ \\hat{\\beta}_{wls} = (X^T P^T P X)^{-1} X^T P^T P Y  $$\n\n\n$$ \\hat{\\beta}_{wls} = (X^T D^{-1} X)^{-1} X^T D^{-1} Y $$\n\n\nWe know that D is a diagonal matrix, so the inverse is also a diagonal matrix where the elements are simply the reciprocal of the non-zero \nelements of D, which we can think of as weights. This makes intuitive sense as the higher the variance, the lower the weight applied to those \nobservations. We can therefore declare that the diagonal weight matrix \\( W = D^{-1} \\) and can proceed to write the estimator as follows: \n\n\n$$ \\hat{\\beta}_{wls} = (X^T W X)^{-1} X^T W Y $$\n\n\nExample\n\n\nIn \nRegression Analysis By Example\n \nby Chatterjee, Hadi & Price, one of the demonstrations involves a study of 27 industrial companies of various sizes, where the number of workers and \nnumber of supervisors was recorded. The study attempts to fit a linear model to this data to assess the nature of the relationship between these \ntwo types of employee. As it turns out, when you plot the data using a scatter plot, it becomes apparent that the variance in the dependent variable \n(number of supervisors) gets larger as the independent variable (number of workers) gets larger. The Morpheus code below can be used to load and plot \nthis data as follows:\n\n\n\n\n\nDataFrame<Integer,String> frame = DataFrame.read().csv(\"/supervisor.csv\");\nChart.create().withScatterPlot(frame, false, \"x\", chart -> {\n    chart.plot().style(\"y\").withColor(Color.BLUE);\n    chart.title().withText(\"Supervisors (y) vs Workers (x)\");\n    chart.subtitle().withText(\"Source: Regression by Example, Chatterjee & Price (1977)\");\n    chart.plot().axes().domain().label().withText(\"Worker Count\");\n    chart.plot().axes().range(0).label().withText(\"Supervisor Count\");\n    chart.plot().trend(\"y\");\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nIf the increasing variance in the dependent variable is not obvious from the scatter plot, it can be useful to plot the residuals from an OLS\nregression against the fitted values. The code below shows how to do this, and is followed by the resulting plot.\n\n\n\n\n\nDataFrame<Integer,String> frame = DataFrame.read().csv(\"/supervisor.csv\");\nframe.regress().ols(\"y\", \"x\", true, model -> {\n    Chart.create().withResidualsVsFitted(model, chart -> {\n        chart.title().withText(\"OLS Residuals vs Fitted Y Values\");\n        chart.plot().axes().domain().label().withText(\"Y(fitted)\");\n        chart.plot().axes().range(0).label().withText(\"OLS Residual\");\n        chart.legend().off();\n        chart.show();\n    });\n    return Optional.empty();\n});\n\n\n\n\n\n    \n\n\n\n\n\nWhile it is pretty clear from the above plots that heteroscedasticity is present in the data, which is a violation of one of the\nGauss Markov assumptions, let us first proceed by running an OLS regression in the usual fashion. The OLS estimates will be useful for \ncomparison with those generated by a WLS regression, but more importantly, we can use the residuals of the former to estimate the weight \nmatrix required by the latter. The code below runs the OLS regression and the prints the results to standard out.\n\n\n\n\n\nDataFrame<Integer,String> frame = DataFrame.read().csv(\"/supervisor.csv\");\nframe.regress().ols(\"y\", \"x\", model -> {\n    System.out.println(model);\n    return Optional.empty();\n});\n\n\n\n\n\n===========================================================================================\n                                 Linear Regression Results                                                           \n===========================================================================================\nModel:                                 OLS    R-Squared:                          0.7759\nObservations:                           27    R-Squared(adjusted):                0.7669\nDF Model:                                1    F-Statistic:                       86.5435\nDF Residuals:                           25    F-Statistic(Prob):                 1.35E-9\nStandard Error:                    21.7293    Runtime(millis)                         81\nDurbin-Watson:                      2.6325                                              \n===========================================================================================\n   Index    |  PARAMETER  |  STD_ERROR  |  T_STAT  |  P_VALUE   |  CI_LOWER  |  CI_UPPER  |\n-------------------------------------------------------------------------------------------\n Intercept  |    14.4481  |      9.562  |   1.511  |  1.433E-1  |   -5.2453  |   34.1414  |\n         x  |     0.1054  |     0.0113  |  9.3029  |   1.35E-9  |     0.082  |    0.1287  |\n===========================================================================================\n\n\n\n\nThe next step is to use the residuals from the OLS model in order to estimate the weight matrix for WLS. There is no hard rule on how to do \nthis, and each case will require the researcher to come up with an approach for estimating W that makes sense given the sample dataset under\ninvestigation. In this particular case however, it is clear that the variance in the dependent variable is proportional to the level of the \nindependent variable, so we could fit a linear model to the OLS residuals against the independent variable and use that as a proxy for the\nchange in variance. Obviously the variance cannot be negative, so we could regress the absolute value of the residuals or the residuals squared.\nThe code below generates a plot of the absolute value of the residuals against the predictor, and fits and OLS model to the dataset.\n\n\n\n\n\nDataFrame<Integer,String> frame = DataFrame.read().csv(\"/supervisor.csv\");\nframe.regress().ols(\"y\", \"x\", true, model -> {\n    final DataFrame<Integer,String> residuals = model.getResiduals();\n    final DataFrame<Integer,String> residualsAbs = residuals.mapToDoubles(v -> Math.abs(v.getDouble()));\n    final DataFrame<Integer,String> xValues = frame.cols().select(\"x\");\n    final DataFrame<Integer,String> newData = DataFrame.concatColumns(residualsAbs, xValues);\n    Chart.create().withScatterPlot(newData, false, \"x\", chart -> {\n        chart.plot().style(\"Residuals\").withPointsVisible(true).withColor(Color.BLUE).withPointShape(ChartShape.DIAMOND);\n        chart.title().withText(\"ABS(Residuals) vs Predictor of Original Regression\");\n        chart.subtitle().withText(\"Regression line is a proxy for change in variance of dependent variable\");\n        chart.plot().axes().domain().label().withText(\"Worker Count\");\n        chart.plot().axes().range(0).label().withText(\"|Residual|\");\n        chart.plot().trend(\"Residuals\");\n        chart.show();\n    });\n    return Optional.empty();\n});\n\n\n\n\n\n    \n\n\n\n\n\nWe can now use the reciprocal of the fitted values of this regression as the elements of the diagonal weight matrix, W. The code\nbelow generates this array of weights which can then be passed to the \nDataFrame.regress().wls()\n method.\n\n\n\n\n\n/**\n * Returns the vector of weights for the WLS regression by regressing |residuals| on the predictor\n * @param frame     the frame of original data\n * @return          the weight vector for diagonal matrix in WLS\n */\nprivate Array<Double> computeWeights(DataFrame<Integer,String> frame) {\n    return frame.regress().ols(\"y\", \"x\", model -> {\n        final DataFrame<Integer,String> residuals = model.getResiduals();\n        final DataFrame<Integer,String> residualsAbs = residuals.mapToDoubles(v -> Math.abs(v.getDouble()));\n        final DataFrame<Integer,String> xValues = frame.cols().select(\"x\");\n        final DataFrame<Integer,String> newData = DataFrame.concatColumns(residualsAbs, xValues);\n        return newData.cols().ols(\"Residuals\", \"x\", ols -> {\n            ols.withIntercept(false);\n            final DataFrame<Integer,String> yHat = ols.getFittedValues();\n            final double[] weights = yHat.colAt(0).toDoubleStream().map(v -> 1d / Math.pow(v, 2d)).toArray();\n            return Optional.of(Array.of(weights));\n        });\n    }).orElse(null);\n}\n\n\n\n\nNow that we have the diagonal elements of the W matrix that describes how much weight we wish to apply to each observation of our \noriginal dataset, we can run a Weighted Least Squares regression. The code below does exactly this, and prints the model summary\nresults to standard out.\n\n\n\n\n\nDataFrame<Integer,String> frame = DataFrame.read().csv(\"/supervisor.csv\");\nArray<Double> weights = computeWeights(frame);\nframe.regress().wls(\"y\", \"x\", weights, model -> {\n    System.out.println(model);\n    return Optional.empty();\n});\n\n\n\n\n\n=============================================================================================\n                                  Linear Regression Results                                                            \n=============================================================================================\nModel:                                   WLS    R-Squared:                            0.8785\nObservations:                             27    R-Squared(adjusted):                  0.8737\nDF Model:                                  1    F-Statistic:                        180.7789\nDF Residuals:                             25    F-Statistic(Prob):                 6.044E-13\nStandard Error:                       0.0227    Runtime(millis)                           99\nDurbin-Watson:                        2.4675                                                \n=============================================================================================\n   Index    |  PARAMETER  |  STD_ERROR  |  T_STAT   |   P_VALUE   |  CI_LOWER  |  CI_UPPER  |\n---------------------------------------------------------------------------------------------\n Intercept  |     3.8033  |     4.5697  |   0.8323  |   4.131E-1  |   -5.6083  |   13.2149  |\n         x  |      0.121  |      0.009  |  13.4454  |  6.044E-13  |    0.1025  |    0.1395  |\n=============================================================================================\n\n\n\n\nThere are a few notable differences in the output between the OLS and WLS models as follows:\n\n\n\n\nThe WLS model yields lower parameter standard errors than OLS. \n\n\nThe WLS model has narrower confidence intervals for the parameters than the OLS.\n\n\nThe WLS model has a higher \\(R^2\\) than OLS\n\n\n\n\nFinally, the code below generates the scatter plot of the data while adding a regression line for both the OLS (red line) and WLS (green line) \nestimates to see how they compare. It is clear that the WLS regression line is being pulled closer to observations associated with lower values \nof the independent variable compared to the OLS line, which equally weights all observations.\n\n\n\n\n\n/**\n * Generate a scatter plot with both OLS and WLS regression lines\n */\npublic void plotCompare() throws Exception {\n\n    DataFrame<Integer,String> frame = DataFrame.read().csv(\"/supervisor.csv\");\n    double[] x = frame.colAt(\"x\").toDoubleStream().toArray();\n\n    DataFrameLeastSquares<Integer,String> ols = frame.regress().ols(\"y\", \"x\", true, Optional::of).orElse(null);\n    double olsAlpha = ols.getInterceptValue(DataFrameLeastSquares.Field.PARAMETER);\n    double olsBeta = ols.getBetaValue(\"x\", DataFrameLeastSquares.Field.PARAMETER);\n    DataFrame<Integer,String> olsFit = createFitted(olsAlpha, olsBeta, x, \"OLS\");\n\n    Array<Double> weights = computeWeights(frame);\n    DataFrameLeastSquares<Integer,String> wls = frame.regress().wls(\"y\", \"x\", weights, true, Optional::of).orElse(null);\n    double wlsAlpha = wls.getInterceptValue(DataFrameLeastSquares.Field.PARAMETER);\n    double wlsBeta = wls.getBetaValue(\"x\", DataFrameLeastSquares.Field.PARAMETER);\n    DataFrame<Integer,String> wlsFit = createFitted(wlsAlpha, wlsBeta, x, \"WLS\");\n\n    Chart.create().withScatterPlot(frame, false, \"x\", chart -> {\n        chart.plot().style(\"y\").withColor(Color.BLUE);\n        chart.plot().<String>data().add(olsFit, \"x\");\n        chart.plot().<String>data().add(wlsFit, \"x\");\n        chart.plot().render(1).withLines(false, false);\n        chart.plot().render(2).withLines(false, false);\n        chart.plot().axes().domain().label().withText(\"X-value\");\n        chart.plot().axes().range(0).label().withText(\"Y-value\");\n        chart.plot().style(\"OLS\").withColor(Color.RED).withLineWidth(2f);\n        chart.plot().style(\"WLS\").withColor(Color.GREEN).withLineWidth(2f);\n        chart.title().withText(\"OLS vs WLS Regression Comparison\");\n        chart.subtitle().withText(String.format(\"Beta OLS: %.3f, Beta WLS: %.3f\", olsBeta, wlsBeta));\n        chart.legend().on().right();\n        chart.show();\n    });\n}\n\n\n/**\n * Returns a DataFrame of fitted values given alpha, beta and the x-values\n * @param alpha     the alpha or intercept parameter\n * @param beta      the beta or slope parameter\n * @param x         the x-values or independent variable values\n * @param yName     the name for the fitted values\n * @return          the newly created DataFrame\n */\nprivate DataFrame<Integer,String> createFitted(double alpha, double beta, double[] x, String yName) {\n    return DataFrame.ofDoubles(Range.of(0, x.length), Array.of(\"x\", yName), v -> {\n        switch (v.colOrdinal()) {\n            case 0:  return x[v.rowOrdinal()];\n            default: return alpha + beta * x[v.rowOrdinal()];\n        }\n    });\n}\n\n\n\n\n\n    \n\n\n\n\n\nUnbiasedness\n\n\nConditional on the \nGauss Markov\n assumptions being met for the transformed linear system as described earlier, \nparameter estimates are expected to be unbiased. Similar to the \nOLS example\n, we can demonstrate this empirically by creating \nmany randomly generated 2-D data samples with the required characteristics given known population parameters for the slope and intercept. \n\n\nThe function below returns a Morpheus \nDataFrame\n with two columns representing X and Y values, where the Y values are generated by a linear \nprocess with given parameters \\( \\alpha \\) and \\( \\beta \\). White noise is added to the Y values with increasing variance as X increases, \nwhich will yield a dataset with the desired characteristics (i.e. heteroscedasticity).\n\n\n\n\n\n/**\n * Returns a sample dataset based on a known population process using the linear coefficients provided\n * The sample dataset exhibits increasing variance proprtional to the independent variable.\n * @param alpha     the intercept term for population process\n * @param beta      the slope term for population process\n * @param startX    the start value for independent variable\n * @param stepX     the step size for independent variable\n * @return          the frame of XY values\n */\nprivate DataFrame<Integer,String> sample(double alpha, double beta, double startX, double stepX, int n) {\n    Array<Double> xValues = Array.of(Double.class, n).applyDoubles(v -> startX + v.index() * stepX);\n    Array<Double> yValues = Array.of(Double.class, n).applyDoubles(v -> {\n        final double xValue = xValues.getDouble(v.index());\n        final double yFitted = alpha + beta * xValue;\n        final double stdDev = xValue * 2d;\n        return new NormalDistribution(yFitted, stdDev).sample();\n    });\n    Range<Integer> rowKeys = Range.of(0, n);\n    return DataFrame.of(rowKeys, String.class, columns -> {\n        columns.add(\"X\", xValues);\n        columns.add(\"Y\", yValues);\n    });\n}\n\n\n\n\nTo get a sense of what this data looks like, the code below generates 4 random samples of the dataset with 100 observations and plots them\non a scatter chart with a fitted OLS regression line. It should be clear from these charts that the dispersion in the dependent variable is \nincreasing for larger values of the independent variable.\n\n\n\n\n\nfinal double beta = 4d;\nfinal double alpha = 20d;\nChart.show(2, IntStream.range(0, 4).mapToObj(i -> {\n    DataFrame<Integer,String> frame = sample(alpha, beta, 1, 1, 100);\n    String title = \"Sample %s Dataset, Beta: %.2f Alpha: %.2f\";\n    String subtitle = \"Parameter estimates, Beta^: %.3f, Alpha^: %.3f\";\n    DataFrameLeastSquares<Integer,String> ols = frame.regress().ols(\"Y\", \"X\", true, Optional::of).get();\n    double betaHat = ols.getBetaValue(\"X\", DataFrameLeastSquares.Field.PARAMETER);\n    double alphaHat = ols.getInterceptValue(DataFrameLeastSquares.Field.PARAMETER);\n    return Chart.create().withScatterPlot(frame, false, \"X\", chart -> {\n        chart.title().withText(String.format(title, i, beta, alpha));\n        chart.title().withFont(new Font(\"Arial\", Font.BOLD, 14));\n        chart.subtitle().withText(String.format(subtitle, betaHat, alphaHat));\n        chart.plot().style(\"Y\").withColor(Color.RED).withPointsVisible(true);\n        chart.plot().trend(\"Y\").withLineWidth(2f);\n    });\n}));\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\nNow that we have a function that can generate data samples given some population parameters and with the appropriate variance characteristics,\nwe can proceed to run regressions on these samples and record the estimates and assess whether they are indeed centered on the known \npopulation values. The code below executes 100,000 regressions, collects the sample estimates for each run in a \nDataFrame\n, and then\nplots a histogram of the results.\n\n\n\n\n\nint n = 100;\ndouble beta = 4d;\ndouble alpha = 20d;\nint regressionCount = 100000;\nRange<Integer> rows = Range.of(0, regressionCount);\nArray<String> columns = Array.of(\"Beta\", \"Alpha\");\nDataFrame<Integer,String> results = DataFrame.ofDoubles(rows, columns);\n\nresults.rows().parallel().forEach(row -> {\n    final DataFrame<Integer,String> frame = sample(alpha, beta, 1, 1, n);\n    final Array<Double> weights = computeWeights(frame);\n    frame.regress().wls(\"Y\", \"X\", weights, true, model -> {\n        final double alphaHat = model.getInterceptValue(Field.PARAMETER);\n        final double betaHat = model.getBetaValue(\"X\", Field.PARAMETER);\n        row.setDouble(\"Alpha\", alphaHat);\n        row.setDouble(\"Beta\", betaHat);\n        return Optional.empty();\n    });\n});\n\nArray.of(\"Beta\", \"Alpha\").forEach(coeff -> {\n    final DataFrame<Integer,String> coeffResults = results.cols().select(col -> col.key().startsWith(coeff));\n    Chart.create().withHistPlot(coeffResults, 250, chart -> {\n        String title = \"%s Histogram of %s WLS regressions\";\n        String subtitle = \"%s estimate unbiasedness, Actual: %.2f, Mean: %.2f, Variance: %.2f\";\n        double actual = coeff.equals(\"Beta\") ? beta : alpha;\n        double estimate = coeffResults.colAt(coeff).stats().mean();\n        double variance = coeffResults.colAt(coeff).stats().variance();\n        Color color = coeff.equals(\"Beta\") ? new Color(255, 100, 100) : new Color(102, 204, 255);\n        chart.plot().style(coeff).withColor(color);\n        chart.plot().axes().domain().label().withText(coeff + \" Estimate\");\n        chart.title().withText(String.format(title, coeff, regressionCount));\n        chart.subtitle().withText(String.format(subtitle, coeff, actual, estimate, variance));\n        chart.show(700, 400);\n    });\n});\n\n\n\n\nObviously the exact estimates for this will vary each time given the stochastic term in our model, however the plots below over a very large\nnumber of regressions clearly demonstrate that the distribution of our estimates is centered on the known population value. In fact, for the\nresults printed below, the \nmean\n slope and intercept coefficient was an exact match to the known population value, at least to two \ndecimal places.\n\n\n\n    \n\n    \n\n\n\n\n\nEfficiency\n\n\nEarlier it was suggested that in the presence of \nheteroscedasticity\n, Ordinary Least Squares \n(OLS) was still expected to be \nunbiased\n and \nconsistent\n but less \nefficient\n than \nWeighted Least Squares (WLS) in that the variance of parameter estimates are likely to be higher for OLS. This section attempts to show this \nempirically using the Morpheus library.\n\n\nTo demonstrate the relative efficiency of the two estimators, we can use the same data generating function introduced earlier, and simply run \nboth OLS and WLS regressions on the datasets to see how they compare. In each case, we can capture the parameter estimates from each regression, \nand then plot the resulting frequency distributions.\n\n\nThe code below runs OLS and WLS regressions on 100,000 samples created by our data generation function, and plots the frequency \ndistributions of the resulting \\(\\beta\\) and \\(\\alpha\\) estimates. These results should ideally allow us to conclude that WLS estimates \nhave lower estimation variance than OLS, and we should also be able to confirm that OLS is still \nunbiased\n in the presence of \nheteroscedasticity, just not BLUE. The following code executes these regressions, and then generates the plots.\n\n\n\n\n\nfinal int n = 100;\nfinal double beta = 4d;\nfinal double alpha = 20d;\nfinal int regressionCount = 100000;\nRange<Integer> rows = Range.of(0, regressionCount);\nArray<String> columns = Array.of(\"Beta(OLS)\", \"Alpha(OLS)\", \"Beta(WLS)\", \"Alpha(WLS)\");\nDataFrame<Integer,String> results = DataFrame.ofDoubles(rows, columns);\n\nresults.rows().parallel().forEach(row -> {\n    final DataFrame<Integer,String> frame = sample(alpha, beta, 1, 1, n);\n    frame.regress().ols(\"Y\", \"X\", true, model -> {\n        double alphaHat = model.getInterceptValue(Field.PARAMETER);\n        double betaHat = model.getBetaValue(\"X\", Field.PARAMETER);\n        row.setDouble(\"Alpha(OLS)\", alphaHat);\n        row.setDouble(\"Beta(OLS)\", betaHat);\n        return Optional.empty();\n    });\n\n    final Array<Double> weights = computeWeights(frame);\n    frame.regress().wls(\"Y\", \"X\", weights, true, model -> {\n        double alphaHat = model.getInterceptValue(Field.PARAMETER);\n        double betaHat = model.getBetaValue(\"X\", Field.PARAMETER);\n        row.setDouble(\"Alpha(WLS)\", alphaHat);\n        row.setDouble(\"Beta(WLS)\", betaHat);\n        return Optional.empty();\n    });\n});\n\nArray.of(\"Alpha\", \"Beta\").forEach(coeff -> {\n    final String olsKey = coeff + \"(OLS)\";\n    final String wlsKey = coeff + \"(WLS)\";\n    final DataFrame<Integer,String> data = results.cols().select(olsKey, wlsKey);\n    Chart.create().withHistPlot(data, 200, true, chart -> {\n        double meanOls = results.colAt(olsKey).stats().mean();\n        double stdOls = results.colAt(olsKey).stats().stdDev();\n        double meanWls = results.colAt(wlsKey).stats().mean();\n        double stdWls = results.colAt(wlsKey).stats().stdDev();\n        double coeffAct = coeff.equals(\"Alpha\") ? alpha : beta;\n        String title = \"%s Histogram from %s OLS & WLS Regressions (n=%s)\";\n        String subtitle = \"Actual: %.4f, Mean(OLS): %.4f, Std(OLS): %.4f, Mean(WLS): %.4f, Std(WLS): %.4f\";\n        chart.title().withText(String.format(title, coeff, regressionCount, n));\n        chart.title().withFont(new Font(\"Arial\", Font.BOLD, 15));\n        chart.subtitle().withText(String.format(subtitle, coeffAct, meanOls, stdOls, meanWls, stdWls));\n        chart.plot().axes().domain().label().withText(coeff + \" Estimates\");\n        chart.legend().on().bottom();\n        chart.writerPng(new File(String.format(\"./docs/images/wls/wls-%s-efficiency.png\", coeff)), 700, 400, true);\n        chart.show(700, 400);\n    });\n});\n\n\n\n\nInspection of the histograms below of the \\(\\alpha \\) and \\(\\beta \\) estimates over the 100,000 regressions in the simulation\nclearly demonstrate that both OLS and WLS are \nunbiased\n in that they are centered on the known population values for each\ncoefficient. The efficiency of each model however is clearly different, and as expected, the WLS regression model yields much lower \nvariance in both the intercept and slope estimates compared to OLS.\n\n\n\n    \n\n    \n\n\n\n\n\nConsistency\n\n\nHaving empirically demonstrated the \nunbiasedness\n and \nefficiency\n \nof the WLS estimator, this section attempts to assess the \nconsistency\n using the same data \ngenerating function introduced earlier. A consistent estimator is one that has a property such that as the sample size of the data being fitted \nincreases, the variance of the estimates around the \ntrue\n value decreases. In other words, the estimated coefficients \nconverge in probability\n \non the true population values.\n\n\nThe code below runs 100,000 regressions with a \ntrue\n beta of 4 and an intercept or alpha of 20, except in this case the process is repeated 5 times \nwith sample sizes of 100, 200, 300, 400 and 500 (so 500,000 regressions in all). The slope and intercept coefficients for all these runs are captured \nin a \nDataFrame\n, which is then used to generate a histogram in order to be able to visualize the variance in the estimates for each scenario.\n\n\n\n\n\nfinal double beta = 4d;\nfinal double alpha = 20d;\nfinal int regressionCount = 100000;\nfinal Range<Integer> sampleSizes = Range.of(100, 600, 100);\nfinal Range<Integer> rows = Range.of(0, regressionCount);\nfinal DataFrame<Integer,String> results = DataFrame.of(rows, String.class, columns -> {\n    sampleSizes.forEach(n -> {\n        columns.add(String.format(\"Beta(n=%s)\", n), Double.class);\n        columns.add(String.format(\"Alpha(n=%s)\", n), Double.class);\n    });\n});\n\nsampleSizes.forEach(n -> {\n    System.out.println(\"Running \" + regressionCount + \" regressions for n=\" + n);\n    final String betaKey = String.format(\"Beta(n=%s)\", n);\n    final String alphaKey = String.format(\"Alpha(n=%s)\", n);\n    results.rows().parallel().forEach(row -> {\n        final DataFrame<Integer,String> frame = sample(alpha, beta, 1, 1, n);\n        final Array<Double> weights = computeWeights(frame);\n        frame.regress().wls(\"Y\", \"X\", weights, true, model -> {\n            final double alphaHat = model.getInterceptValue(Field.PARAMETER);\n            final double betaHat = model.getBetaValue(\"X\", Field.PARAMETER);\n            row.setDouble(alphaKey, alphaHat);\n            row.setDouble(betaKey, betaHat);\n            return Optional.empty();\n        });\n    });\n});\n\nArray.of(\"Beta\", \"Alpha\").forEach(coeff -> {\n    final DataFrame<Integer,String> coeffResults = results.cols().select(col -> col.key().startsWith(coeff));\n    Chart.create().withHistPlot(coeffResults, 250, true, chart -> {\n        chart.plot().axes().domain().label().withText(\"Coefficient Estimate\");\n        chart.title().withText(coeff + \" Histograms of \" + regressionCount + \" Regressions\");\n        chart.subtitle().withText(coeff + \" Estimate distribution as sample size increases\");\n        chart.legend().on().bottom();\n        chart.show(700, 400);\n    });\n});\n\n\n\n\nThe plots below demonstrate a clear pattern of decreasing variance in the slope estimate as the sample size increases. With\nregard to the intercept estimate, the evidence is less obvious, and the chart suggests only a maringal reduction in variance.\n\n\n\n    \n\n    \n\n\n\n\n\nTo get a better sense of the trend in estimation variance as sample size increases, we can compute the variance for all estimates\nin a given run, and generate a bar plot of the results. The visualization below suggests the variance in the intercept is decreasing,\njust not as aggressively as the slope coefficient in this case.\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\nThe code to generate this plot is as follows:\n\n\n\n\n\nArray<DataFrame<String,StatType>> variances = Array.of(\"Beta\", \"Alpha\").map(value -> {\n    final String coefficient = value.getValue();\n    final Matcher matcher = Pattern.compile(coefficient + \"\\\\(n=(\\\\d+)\\\\)\").matcher(\"\");\n    return results.cols().select(column -> {\n        final String name = column.key();\n        return matcher.reset(name).matches();\n    }).cols().mapKeys(column -> {\n        final String name = column.key();\n        if (matcher.reset(name).matches()) return matcher.group(1);\n        throw new IllegalArgumentException(\"Unexpected column name: \" + column.key());\n    }).cols().stats().variance().transpose();\n});\n\nChart.show(2, Collect.asList(\n    Chart.create().withBarPlot(variances.getValue(0), false, chart -> {\n        chart.title().withText(\"Beta variance with sample size\");\n        chart.plot().style(StatType.VARIANCE).withColor(new Color(255, 100, 100));\n        chart.plot().axes().range(0).label().withText(\"Beta Variance\");\n        chart.plot().axes().domain().label().withText(\"Sample Size\");\n    }),\n    Chart.create().withBarPlot(variances.getValue(1), false, chart -> {\n        chart.title().withText(\"Alpha variance with sample size\");\n        chart.plot().style(StatType.VARIANCE).withColor(new Color(102, 204, 255));\n        chart.plot().axes().range(0).label().withText(\"Alpha Variance\");\n        chart.plot().axes().domain().label().withText(\"Sample Size\");\n    }))\n);",
            "title": "Weighted Least Squares"
        },
        {
            "location": "/regression/wls/#weighted-least-squares-wls",
            "text": "",
            "title": "Weighted Least Squares (WLS)"
        },
        {
            "location": "/regression/wls/#introduction",
            "text": "When fitting an OLS regression model, it may become apparent that there is an inconsistent variance in the residuals, which is known as  heteroscedasticity . As discussed earlier, this is a violation of one of the  Gauss Markov  assumptions, and therefore OLS is no longer the Best Linear Unbiased \nEstimator (BLUE). An OLS model in such circumstances is still expected to be  unbiased  \nand  consistent , however it will not be the most  efficient  estimator which suggests there are other estimators \nthat exhibit lower variance in parameter estimates. More concerning is that OLS is likely to yield biased estimates of the standard errors of the \ncoefficients, making statistical interference unreliable.",
            "title": "Introduction"
        },
        {
            "location": "/regression/wls/#theory",
            "text": "To address the issue of heteroscedasticity, a Weighted Least Squares (WLS) regression model is used in favour of OLS, which in effect applies \na transformation to the original model so that the transformed model does in fact exhibit  homoscedastic  \nerrors. To demonstrate, consider the usual form of the regression model, but in this case we assume the variance in the error term follows a \nform of \\( Var[\\epsilon] = \\sigma^2 D \\) where D is a diagonal matrix of factors that scales the variance appropriately.  $$ Y = X \\beta + \\epsilon \\ \\ \\ \\ where \\ E[\\epsilon]=0 \\ \\ \\ \\ Var[\\epsilon] = \\sigma^2 D  $$  Now let us assume there exists a matrix P that can transform the above model such that the resulting error term becomes homoscedastic, \nand therefore satisfies the  Gauss Markov  assumption of constant variance.  $$ P Y = P X \\beta + P \\epsilon \\ \\ \\ \\ where \\ E[P\\epsilon]=0 \\ \\ \\ \\ Var(P \\epsilon) = \\sigma^2 I $$  Since the  Gauss Markov  assumptions are satisfied for the transformed system, \nwe can apply OLS in the usual fashion. The next logical question therefore is, how do we come up with P? It turns out that we can solve for P \nin terms of D, the latter of which we can estimate reasonably well from the sample data. Consider the following derivation, where the key\nassumption is that P is a symmetric matrix and therefore \\( P^T = P \\).  $$ \\begin{align}\nVar(P \\epsilon) & = \\sigma^2 I \\\\\nP Var(\\epsilon) P^T & = \\sigma^2 I \\\\\n\\sigma^2 P D P^T & = \\sigma^2 I \\\\\nP D P^ T & = I \\\\\nD P^T & = P^{-1} \\\\\nD & = (P^2)^{-1} \\\\\nP & = D^{-\\frac{1}{2}}\n\\end{align} $$  Now let us re-write the transformed model in terms of Z, B and E as follows:  $$ Z = B \\beta + E \\ \\ \\ \\  where \\ Z = P Y, \\ \\ B = PX, \\ \\ and \\ E = P \\epsilon $$  We can use the standard OLS estimator for this since we can assume it satisifies the  Gauss Markov  assumptions.  $$ \\hat{\\beta}_{ols} = (B^T B)^{-1} B^T Z $$  Knowing that \\(B=PX\\) and that \\( P = D^{-\\frac{1}{2}} \\)we can write the WLS estimator as follows:  $$ \\hat{\\beta}_{wls} = (X^T P^T P X)^{-1} X^T P^T P Y  $$  $$ \\hat{\\beta}_{wls} = (X^T D^{-1} X)^{-1} X^T D^{-1} Y $$  We know that D is a diagonal matrix, so the inverse is also a diagonal matrix where the elements are simply the reciprocal of the non-zero \nelements of D, which we can think of as weights. This makes intuitive sense as the higher the variance, the lower the weight applied to those \nobservations. We can therefore declare that the diagonal weight matrix \\( W = D^{-1} \\) and can proceed to write the estimator as follows:   $$ \\hat{\\beta}_{wls} = (X^T W X)^{-1} X^T W Y $$",
            "title": "Theory"
        },
        {
            "location": "/regression/wls/#example",
            "text": "In  Regression Analysis By Example  \nby Chatterjee, Hadi & Price, one of the demonstrations involves a study of 27 industrial companies of various sizes, where the number of workers and \nnumber of supervisors was recorded. The study attempts to fit a linear model to this data to assess the nature of the relationship between these \ntwo types of employee. As it turns out, when you plot the data using a scatter plot, it becomes apparent that the variance in the dependent variable \n(number of supervisors) gets larger as the independent variable (number of workers) gets larger. The Morpheus code below can be used to load and plot \nthis data as follows:   DataFrame<Integer,String> frame = DataFrame.read().csv(\"/supervisor.csv\");\nChart.create().withScatterPlot(frame, false, \"x\", chart -> {\n    chart.plot().style(\"y\").withColor(Color.BLUE);\n    chart.title().withText(\"Supervisors (y) vs Workers (x)\");\n    chart.subtitle().withText(\"Source: Regression by Example, Chatterjee & Price (1977)\");\n    chart.plot().axes().domain().label().withText(\"Worker Count\");\n    chart.plot().axes().range(0).label().withText(\"Supervisor Count\");\n    chart.plot().trend(\"y\");\n    chart.show();\n});  \n       If the increasing variance in the dependent variable is not obvious from the scatter plot, it can be useful to plot the residuals from an OLS\nregression against the fitted values. The code below shows how to do this, and is followed by the resulting plot.   DataFrame<Integer,String> frame = DataFrame.read().csv(\"/supervisor.csv\");\nframe.regress().ols(\"y\", \"x\", true, model -> {\n    Chart.create().withResidualsVsFitted(model, chart -> {\n        chart.title().withText(\"OLS Residuals vs Fitted Y Values\");\n        chart.plot().axes().domain().label().withText(\"Y(fitted)\");\n        chart.plot().axes().range(0).label().withText(\"OLS Residual\");\n        chart.legend().off();\n        chart.show();\n    });\n    return Optional.empty();\n});  \n       While it is pretty clear from the above plots that heteroscedasticity is present in the data, which is a violation of one of the\nGauss Markov assumptions, let us first proceed by running an OLS regression in the usual fashion. The OLS estimates will be useful for \ncomparison with those generated by a WLS regression, but more importantly, we can use the residuals of the former to estimate the weight \nmatrix required by the latter. The code below runs the OLS regression and the prints the results to standard out.   DataFrame<Integer,String> frame = DataFrame.read().csv(\"/supervisor.csv\");\nframe.regress().ols(\"y\", \"x\", model -> {\n    System.out.println(model);\n    return Optional.empty();\n});  \n===========================================================================================\n                                 Linear Regression Results                                                           \n===========================================================================================\nModel:                                 OLS    R-Squared:                          0.7759\nObservations:                           27    R-Squared(adjusted):                0.7669\nDF Model:                                1    F-Statistic:                       86.5435\nDF Residuals:                           25    F-Statistic(Prob):                 1.35E-9\nStandard Error:                    21.7293    Runtime(millis)                         81\nDurbin-Watson:                      2.6325                                              \n===========================================================================================\n   Index    |  PARAMETER  |  STD_ERROR  |  T_STAT  |  P_VALUE   |  CI_LOWER  |  CI_UPPER  |\n-------------------------------------------------------------------------------------------\n Intercept  |    14.4481  |      9.562  |   1.511  |  1.433E-1  |   -5.2453  |   34.1414  |\n         x  |     0.1054  |     0.0113  |  9.3029  |   1.35E-9  |     0.082  |    0.1287  |\n===========================================================================================  The next step is to use the residuals from the OLS model in order to estimate the weight matrix for WLS. There is no hard rule on how to do \nthis, and each case will require the researcher to come up with an approach for estimating W that makes sense given the sample dataset under\ninvestigation. In this particular case however, it is clear that the variance in the dependent variable is proportional to the level of the \nindependent variable, so we could fit a linear model to the OLS residuals against the independent variable and use that as a proxy for the\nchange in variance. Obviously the variance cannot be negative, so we could regress the absolute value of the residuals or the residuals squared.\nThe code below generates a plot of the absolute value of the residuals against the predictor, and fits and OLS model to the dataset.   DataFrame<Integer,String> frame = DataFrame.read().csv(\"/supervisor.csv\");\nframe.regress().ols(\"y\", \"x\", true, model -> {\n    final DataFrame<Integer,String> residuals = model.getResiduals();\n    final DataFrame<Integer,String> residualsAbs = residuals.mapToDoubles(v -> Math.abs(v.getDouble()));\n    final DataFrame<Integer,String> xValues = frame.cols().select(\"x\");\n    final DataFrame<Integer,String> newData = DataFrame.concatColumns(residualsAbs, xValues);\n    Chart.create().withScatterPlot(newData, false, \"x\", chart -> {\n        chart.plot().style(\"Residuals\").withPointsVisible(true).withColor(Color.BLUE).withPointShape(ChartShape.DIAMOND);\n        chart.title().withText(\"ABS(Residuals) vs Predictor of Original Regression\");\n        chart.subtitle().withText(\"Regression line is a proxy for change in variance of dependent variable\");\n        chart.plot().axes().domain().label().withText(\"Worker Count\");\n        chart.plot().axes().range(0).label().withText(\"|Residual|\");\n        chart.plot().trend(\"Residuals\");\n        chart.show();\n    });\n    return Optional.empty();\n});  \n       We can now use the reciprocal of the fitted values of this regression as the elements of the diagonal weight matrix, W. The code\nbelow generates this array of weights which can then be passed to the  DataFrame.regress().wls()  method.   /**\n * Returns the vector of weights for the WLS regression by regressing |residuals| on the predictor\n * @param frame     the frame of original data\n * @return          the weight vector for diagonal matrix in WLS\n */\nprivate Array<Double> computeWeights(DataFrame<Integer,String> frame) {\n    return frame.regress().ols(\"y\", \"x\", model -> {\n        final DataFrame<Integer,String> residuals = model.getResiduals();\n        final DataFrame<Integer,String> residualsAbs = residuals.mapToDoubles(v -> Math.abs(v.getDouble()));\n        final DataFrame<Integer,String> xValues = frame.cols().select(\"x\");\n        final DataFrame<Integer,String> newData = DataFrame.concatColumns(residualsAbs, xValues);\n        return newData.cols().ols(\"Residuals\", \"x\", ols -> {\n            ols.withIntercept(false);\n            final DataFrame<Integer,String> yHat = ols.getFittedValues();\n            final double[] weights = yHat.colAt(0).toDoubleStream().map(v -> 1d / Math.pow(v, 2d)).toArray();\n            return Optional.of(Array.of(weights));\n        });\n    }).orElse(null);\n}  Now that we have the diagonal elements of the W matrix that describes how much weight we wish to apply to each observation of our \noriginal dataset, we can run a Weighted Least Squares regression. The code below does exactly this, and prints the model summary\nresults to standard out.   DataFrame<Integer,String> frame = DataFrame.read().csv(\"/supervisor.csv\");\nArray<Double> weights = computeWeights(frame);\nframe.regress().wls(\"y\", \"x\", weights, model -> {\n    System.out.println(model);\n    return Optional.empty();\n});  \n=============================================================================================\n                                  Linear Regression Results                                                            \n=============================================================================================\nModel:                                   WLS    R-Squared:                            0.8785\nObservations:                             27    R-Squared(adjusted):                  0.8737\nDF Model:                                  1    F-Statistic:                        180.7789\nDF Residuals:                             25    F-Statistic(Prob):                 6.044E-13\nStandard Error:                       0.0227    Runtime(millis)                           99\nDurbin-Watson:                        2.4675                                                \n=============================================================================================\n   Index    |  PARAMETER  |  STD_ERROR  |  T_STAT   |   P_VALUE   |  CI_LOWER  |  CI_UPPER  |\n---------------------------------------------------------------------------------------------\n Intercept  |     3.8033  |     4.5697  |   0.8323  |   4.131E-1  |   -5.6083  |   13.2149  |\n         x  |      0.121  |      0.009  |  13.4454  |  6.044E-13  |    0.1025  |    0.1395  |\n=============================================================================================  There are a few notable differences in the output between the OLS and WLS models as follows:   The WLS model yields lower parameter standard errors than OLS.   The WLS model has narrower confidence intervals for the parameters than the OLS.  The WLS model has a higher \\(R^2\\) than OLS   Finally, the code below generates the scatter plot of the data while adding a regression line for both the OLS (red line) and WLS (green line) \nestimates to see how they compare. It is clear that the WLS regression line is being pulled closer to observations associated with lower values \nof the independent variable compared to the OLS line, which equally weights all observations.   /**\n * Generate a scatter plot with both OLS and WLS regression lines\n */\npublic void plotCompare() throws Exception {\n\n    DataFrame<Integer,String> frame = DataFrame.read().csv(\"/supervisor.csv\");\n    double[] x = frame.colAt(\"x\").toDoubleStream().toArray();\n\n    DataFrameLeastSquares<Integer,String> ols = frame.regress().ols(\"y\", \"x\", true, Optional::of).orElse(null);\n    double olsAlpha = ols.getInterceptValue(DataFrameLeastSquares.Field.PARAMETER);\n    double olsBeta = ols.getBetaValue(\"x\", DataFrameLeastSquares.Field.PARAMETER);\n    DataFrame<Integer,String> olsFit = createFitted(olsAlpha, olsBeta, x, \"OLS\");\n\n    Array<Double> weights = computeWeights(frame);\n    DataFrameLeastSquares<Integer,String> wls = frame.regress().wls(\"y\", \"x\", weights, true, Optional::of).orElse(null);\n    double wlsAlpha = wls.getInterceptValue(DataFrameLeastSquares.Field.PARAMETER);\n    double wlsBeta = wls.getBetaValue(\"x\", DataFrameLeastSquares.Field.PARAMETER);\n    DataFrame<Integer,String> wlsFit = createFitted(wlsAlpha, wlsBeta, x, \"WLS\");\n\n    Chart.create().withScatterPlot(frame, false, \"x\", chart -> {\n        chart.plot().style(\"y\").withColor(Color.BLUE);\n        chart.plot().<String>data().add(olsFit, \"x\");\n        chart.plot().<String>data().add(wlsFit, \"x\");\n        chart.plot().render(1).withLines(false, false);\n        chart.plot().render(2).withLines(false, false);\n        chart.plot().axes().domain().label().withText(\"X-value\");\n        chart.plot().axes().range(0).label().withText(\"Y-value\");\n        chart.plot().style(\"OLS\").withColor(Color.RED).withLineWidth(2f);\n        chart.plot().style(\"WLS\").withColor(Color.GREEN).withLineWidth(2f);\n        chart.title().withText(\"OLS vs WLS Regression Comparison\");\n        chart.subtitle().withText(String.format(\"Beta OLS: %.3f, Beta WLS: %.3f\", olsBeta, wlsBeta));\n        chart.legend().on().right();\n        chart.show();\n    });\n}\n\n\n/**\n * Returns a DataFrame of fitted values given alpha, beta and the x-values\n * @param alpha     the alpha or intercept parameter\n * @param beta      the beta or slope parameter\n * @param x         the x-values or independent variable values\n * @param yName     the name for the fitted values\n * @return          the newly created DataFrame\n */\nprivate DataFrame<Integer,String> createFitted(double alpha, double beta, double[] x, String yName) {\n    return DataFrame.ofDoubles(Range.of(0, x.length), Array.of(\"x\", yName), v -> {\n        switch (v.colOrdinal()) {\n            case 0:  return x[v.rowOrdinal()];\n            default: return alpha + beta * x[v.rowOrdinal()];\n        }\n    });\n}",
            "title": "Example"
        },
        {
            "location": "/regression/wls/#unbiasedness",
            "text": "Conditional on the  Gauss Markov  assumptions being met for the transformed linear system as described earlier, \nparameter estimates are expected to be unbiased. Similar to the  OLS example , we can demonstrate this empirically by creating \nmany randomly generated 2-D data samples with the required characteristics given known population parameters for the slope and intercept.   The function below returns a Morpheus  DataFrame  with two columns representing X and Y values, where the Y values are generated by a linear \nprocess with given parameters \\( \\alpha \\) and \\( \\beta \\). White noise is added to the Y values with increasing variance as X increases, \nwhich will yield a dataset with the desired characteristics (i.e. heteroscedasticity).   /**\n * Returns a sample dataset based on a known population process using the linear coefficients provided\n * The sample dataset exhibits increasing variance proprtional to the independent variable.\n * @param alpha     the intercept term for population process\n * @param beta      the slope term for population process\n * @param startX    the start value for independent variable\n * @param stepX     the step size for independent variable\n * @return          the frame of XY values\n */\nprivate DataFrame<Integer,String> sample(double alpha, double beta, double startX, double stepX, int n) {\n    Array<Double> xValues = Array.of(Double.class, n).applyDoubles(v -> startX + v.index() * stepX);\n    Array<Double> yValues = Array.of(Double.class, n).applyDoubles(v -> {\n        final double xValue = xValues.getDouble(v.index());\n        final double yFitted = alpha + beta * xValue;\n        final double stdDev = xValue * 2d;\n        return new NormalDistribution(yFitted, stdDev).sample();\n    });\n    Range<Integer> rowKeys = Range.of(0, n);\n    return DataFrame.of(rowKeys, String.class, columns -> {\n        columns.add(\"X\", xValues);\n        columns.add(\"Y\", yValues);\n    });\n}  To get a sense of what this data looks like, the code below generates 4 random samples of the dataset with 100 observations and plots them\non a scatter chart with a fitted OLS regression line. It should be clear from these charts that the dispersion in the dependent variable is \nincreasing for larger values of the independent variable.   final double beta = 4d;\nfinal double alpha = 20d;\nChart.show(2, IntStream.range(0, 4).mapToObj(i -> {\n    DataFrame<Integer,String> frame = sample(alpha, beta, 1, 1, 100);\n    String title = \"Sample %s Dataset, Beta: %.2f Alpha: %.2f\";\n    String subtitle = \"Parameter estimates, Beta^: %.3f, Alpha^: %.3f\";\n    DataFrameLeastSquares<Integer,String> ols = frame.regress().ols(\"Y\", \"X\", true, Optional::of).get();\n    double betaHat = ols.getBetaValue(\"X\", DataFrameLeastSquares.Field.PARAMETER);\n    double alphaHat = ols.getInterceptValue(DataFrameLeastSquares.Field.PARAMETER);\n    return Chart.create().withScatterPlot(frame, false, \"X\", chart -> {\n        chart.title().withText(String.format(title, i, beta, alpha));\n        chart.title().withFont(new Font(\"Arial\", Font.BOLD, 14));\n        chart.subtitle().withText(String.format(subtitle, betaHat, alphaHat));\n        chart.plot().style(\"Y\").withColor(Color.RED).withPointsVisible(true);\n        chart.plot().trend(\"Y\").withLineWidth(2f);\n    });\n}));  \n       \n       \n       \n       Now that we have a function that can generate data samples given some population parameters and with the appropriate variance characteristics,\nwe can proceed to run regressions on these samples and record the estimates and assess whether they are indeed centered on the known \npopulation values. The code below executes 100,000 regressions, collects the sample estimates for each run in a  DataFrame , and then\nplots a histogram of the results.   int n = 100;\ndouble beta = 4d;\ndouble alpha = 20d;\nint regressionCount = 100000;\nRange<Integer> rows = Range.of(0, regressionCount);\nArray<String> columns = Array.of(\"Beta\", \"Alpha\");\nDataFrame<Integer,String> results = DataFrame.ofDoubles(rows, columns);\n\nresults.rows().parallel().forEach(row -> {\n    final DataFrame<Integer,String> frame = sample(alpha, beta, 1, 1, n);\n    final Array<Double> weights = computeWeights(frame);\n    frame.regress().wls(\"Y\", \"X\", weights, true, model -> {\n        final double alphaHat = model.getInterceptValue(Field.PARAMETER);\n        final double betaHat = model.getBetaValue(\"X\", Field.PARAMETER);\n        row.setDouble(\"Alpha\", alphaHat);\n        row.setDouble(\"Beta\", betaHat);\n        return Optional.empty();\n    });\n});\n\nArray.of(\"Beta\", \"Alpha\").forEach(coeff -> {\n    final DataFrame<Integer,String> coeffResults = results.cols().select(col -> col.key().startsWith(coeff));\n    Chart.create().withHistPlot(coeffResults, 250, chart -> {\n        String title = \"%s Histogram of %s WLS regressions\";\n        String subtitle = \"%s estimate unbiasedness, Actual: %.2f, Mean: %.2f, Variance: %.2f\";\n        double actual = coeff.equals(\"Beta\") ? beta : alpha;\n        double estimate = coeffResults.colAt(coeff).stats().mean();\n        double variance = coeffResults.colAt(coeff).stats().variance();\n        Color color = coeff.equals(\"Beta\") ? new Color(255, 100, 100) : new Color(102, 204, 255);\n        chart.plot().style(coeff).withColor(color);\n        chart.plot().axes().domain().label().withText(coeff + \" Estimate\");\n        chart.title().withText(String.format(title, coeff, regressionCount));\n        chart.subtitle().withText(String.format(subtitle, coeff, actual, estimate, variance));\n        chart.show(700, 400);\n    });\n});  Obviously the exact estimates for this will vary each time given the stochastic term in our model, however the plots below over a very large\nnumber of regressions clearly demonstrate that the distribution of our estimates is centered on the known population value. In fact, for the\nresults printed below, the  mean  slope and intercept coefficient was an exact match to the known population value, at least to two \ndecimal places.",
            "title": "Unbiasedness"
        },
        {
            "location": "/regression/wls/#efficiency",
            "text": "Earlier it was suggested that in the presence of  heteroscedasticity , Ordinary Least Squares \n(OLS) was still expected to be  unbiased  and  consistent  but less  efficient  than \nWeighted Least Squares (WLS) in that the variance of parameter estimates are likely to be higher for OLS. This section attempts to show this \nempirically using the Morpheus library.  To demonstrate the relative efficiency of the two estimators, we can use the same data generating function introduced earlier, and simply run \nboth OLS and WLS regressions on the datasets to see how they compare. In each case, we can capture the parameter estimates from each regression, \nand then plot the resulting frequency distributions.  The code below runs OLS and WLS regressions on 100,000 samples created by our data generation function, and plots the frequency \ndistributions of the resulting \\(\\beta\\) and \\(\\alpha\\) estimates. These results should ideally allow us to conclude that WLS estimates \nhave lower estimation variance than OLS, and we should also be able to confirm that OLS is still  unbiased  in the presence of \nheteroscedasticity, just not BLUE. The following code executes these regressions, and then generates the plots.   final int n = 100;\nfinal double beta = 4d;\nfinal double alpha = 20d;\nfinal int regressionCount = 100000;\nRange<Integer> rows = Range.of(0, regressionCount);\nArray<String> columns = Array.of(\"Beta(OLS)\", \"Alpha(OLS)\", \"Beta(WLS)\", \"Alpha(WLS)\");\nDataFrame<Integer,String> results = DataFrame.ofDoubles(rows, columns);\n\nresults.rows().parallel().forEach(row -> {\n    final DataFrame<Integer,String> frame = sample(alpha, beta, 1, 1, n);\n    frame.regress().ols(\"Y\", \"X\", true, model -> {\n        double alphaHat = model.getInterceptValue(Field.PARAMETER);\n        double betaHat = model.getBetaValue(\"X\", Field.PARAMETER);\n        row.setDouble(\"Alpha(OLS)\", alphaHat);\n        row.setDouble(\"Beta(OLS)\", betaHat);\n        return Optional.empty();\n    });\n\n    final Array<Double> weights = computeWeights(frame);\n    frame.regress().wls(\"Y\", \"X\", weights, true, model -> {\n        double alphaHat = model.getInterceptValue(Field.PARAMETER);\n        double betaHat = model.getBetaValue(\"X\", Field.PARAMETER);\n        row.setDouble(\"Alpha(WLS)\", alphaHat);\n        row.setDouble(\"Beta(WLS)\", betaHat);\n        return Optional.empty();\n    });\n});\n\nArray.of(\"Alpha\", \"Beta\").forEach(coeff -> {\n    final String olsKey = coeff + \"(OLS)\";\n    final String wlsKey = coeff + \"(WLS)\";\n    final DataFrame<Integer,String> data = results.cols().select(olsKey, wlsKey);\n    Chart.create().withHistPlot(data, 200, true, chart -> {\n        double meanOls = results.colAt(olsKey).stats().mean();\n        double stdOls = results.colAt(olsKey).stats().stdDev();\n        double meanWls = results.colAt(wlsKey).stats().mean();\n        double stdWls = results.colAt(wlsKey).stats().stdDev();\n        double coeffAct = coeff.equals(\"Alpha\") ? alpha : beta;\n        String title = \"%s Histogram from %s OLS & WLS Regressions (n=%s)\";\n        String subtitle = \"Actual: %.4f, Mean(OLS): %.4f, Std(OLS): %.4f, Mean(WLS): %.4f, Std(WLS): %.4f\";\n        chart.title().withText(String.format(title, coeff, regressionCount, n));\n        chart.title().withFont(new Font(\"Arial\", Font.BOLD, 15));\n        chart.subtitle().withText(String.format(subtitle, coeffAct, meanOls, stdOls, meanWls, stdWls));\n        chart.plot().axes().domain().label().withText(coeff + \" Estimates\");\n        chart.legend().on().bottom();\n        chart.writerPng(new File(String.format(\"./docs/images/wls/wls-%s-efficiency.png\", coeff)), 700, 400, true);\n        chart.show(700, 400);\n    });\n});  Inspection of the histograms below of the \\(\\alpha \\) and \\(\\beta \\) estimates over the 100,000 regressions in the simulation\nclearly demonstrate that both OLS and WLS are  unbiased  in that they are centered on the known population values for each\ncoefficient. The efficiency of each model however is clearly different, and as expected, the WLS regression model yields much lower \nvariance in both the intercept and slope estimates compared to OLS.",
            "title": "Efficiency"
        },
        {
            "location": "/regression/wls/#consistency",
            "text": "Having empirically demonstrated the  unbiasedness  and  efficiency  \nof the WLS estimator, this section attempts to assess the  consistency  using the same data \ngenerating function introduced earlier. A consistent estimator is one that has a property such that as the sample size of the data being fitted \nincreases, the variance of the estimates around the  true  value decreases. In other words, the estimated coefficients  converge in probability  \non the true population values.  The code below runs 100,000 regressions with a  true  beta of 4 and an intercept or alpha of 20, except in this case the process is repeated 5 times \nwith sample sizes of 100, 200, 300, 400 and 500 (so 500,000 regressions in all). The slope and intercept coefficients for all these runs are captured \nin a  DataFrame , which is then used to generate a histogram in order to be able to visualize the variance in the estimates for each scenario.   final double beta = 4d;\nfinal double alpha = 20d;\nfinal int regressionCount = 100000;\nfinal Range<Integer> sampleSizes = Range.of(100, 600, 100);\nfinal Range<Integer> rows = Range.of(0, regressionCount);\nfinal DataFrame<Integer,String> results = DataFrame.of(rows, String.class, columns -> {\n    sampleSizes.forEach(n -> {\n        columns.add(String.format(\"Beta(n=%s)\", n), Double.class);\n        columns.add(String.format(\"Alpha(n=%s)\", n), Double.class);\n    });\n});\n\nsampleSizes.forEach(n -> {\n    System.out.println(\"Running \" + regressionCount + \" regressions for n=\" + n);\n    final String betaKey = String.format(\"Beta(n=%s)\", n);\n    final String alphaKey = String.format(\"Alpha(n=%s)\", n);\n    results.rows().parallel().forEach(row -> {\n        final DataFrame<Integer,String> frame = sample(alpha, beta, 1, 1, n);\n        final Array<Double> weights = computeWeights(frame);\n        frame.regress().wls(\"Y\", \"X\", weights, true, model -> {\n            final double alphaHat = model.getInterceptValue(Field.PARAMETER);\n            final double betaHat = model.getBetaValue(\"X\", Field.PARAMETER);\n            row.setDouble(alphaKey, alphaHat);\n            row.setDouble(betaKey, betaHat);\n            return Optional.empty();\n        });\n    });\n});\n\nArray.of(\"Beta\", \"Alpha\").forEach(coeff -> {\n    final DataFrame<Integer,String> coeffResults = results.cols().select(col -> col.key().startsWith(coeff));\n    Chart.create().withHistPlot(coeffResults, 250, true, chart -> {\n        chart.plot().axes().domain().label().withText(\"Coefficient Estimate\");\n        chart.title().withText(coeff + \" Histograms of \" + regressionCount + \" Regressions\");\n        chart.subtitle().withText(coeff + \" Estimate distribution as sample size increases\");\n        chart.legend().on().bottom();\n        chart.show(700, 400);\n    });\n});  The plots below demonstrate a clear pattern of decreasing variance in the slope estimate as the sample size increases. With\nregard to the intercept estimate, the evidence is less obvious, and the chart suggests only a maringal reduction in variance.  \n     \n       To get a better sense of the trend in estimation variance as sample size increases, we can compute the variance for all estimates\nin a given run, and generate a bar plot of the results. The visualization below suggests the variance in the intercept is decreasing,\njust not as aggressively as the slope coefficient in this case.  \n       \n       The code to generate this plot is as follows:   Array<DataFrame<String,StatType>> variances = Array.of(\"Beta\", \"Alpha\").map(value -> {\n    final String coefficient = value.getValue();\n    final Matcher matcher = Pattern.compile(coefficient + \"\\\\(n=(\\\\d+)\\\\)\").matcher(\"\");\n    return results.cols().select(column -> {\n        final String name = column.key();\n        return matcher.reset(name).matches();\n    }).cols().mapKeys(column -> {\n        final String name = column.key();\n        if (matcher.reset(name).matches()) return matcher.group(1);\n        throw new IllegalArgumentException(\"Unexpected column name: \" + column.key());\n    }).cols().stats().variance().transpose();\n});\n\nChart.show(2, Collect.asList(\n    Chart.create().withBarPlot(variances.getValue(0), false, chart -> {\n        chart.title().withText(\"Beta variance with sample size\");\n        chart.plot().style(StatType.VARIANCE).withColor(new Color(255, 100, 100));\n        chart.plot().axes().range(0).label().withText(\"Beta Variance\");\n        chart.plot().axes().domain().label().withText(\"Sample Size\");\n    }),\n    Chart.create().withBarPlot(variances.getValue(1), false, chart -> {\n        chart.title().withText(\"Alpha variance with sample size\");\n        chart.plot().style(StatType.VARIANCE).withColor(new Color(102, 204, 255));\n        chart.plot().axes().range(0).label().withText(\"Alpha Variance\");\n        chart.plot().axes().domain().label().withText(\"Sample Size\");\n    }))\n);",
            "title": "Consistency"
        },
        {
            "location": "/regression/gls/",
            "text": "Generalized Least Squares (GLS)\n\n\nIntroduction\n\n\nOne of the \nGauss Markov\n assumptions that has to be met in order for \nOLS\n to be the Best, \nLinear, Unbiased Estimator (BLUE) is that of \nspherical errors\n. More specifically, the assumption states that the error or disturbance \nterm in the model must exhibit uniform variance (i.e. \nhomoscedasticity\n), and also that \nthere is no \nautocorrelation\n between errors.\n\n\nIn the real world, this assumption will not always hold, and errors may exhibit \nheteroscedasticity\n \nor autocorrelation or both. When the errors in a regression show no serial correlation but are heteroscedastic, then a \nWeighted Least Squares\n \n(WLS) model should be used, whereas if correlation is present a \nGeneralized Least Squares\n (GLS)\nmodel is appropriate. WLS is in fact a special case of GLS where the the lack of autocorrelation implies that the off diagonal terms of the \n\ncovariance matrix\n of the errors are zero, thus simplifying the analysis.\n\n\nThe following section covers some of the theory behind GLS, provides an example of how to apply it with the Morpheus API, and then goes\nonto demonstrate empirically the \nunbiased\n, \nconsistent\n \nand \nefficient\n nature of the estimator.\n\n\nTheory\n\n\nA Generalized Least Squares model is based on the usual linear equation, however we now assume that the error term has a variance structure \nthat is described by a covariance matrix \\(\\Omega\\), which differs from the OLS model which assumes white noise in the form of \\(\\sigma^2 I\\).\n\n\n$$ Y = X \\beta + \\epsilon \\ \\ \\ \\ where \\ E[\\epsilon]=0 \\ \\ \\ \\ Var(\\epsilon) = \\sigma^2 \\Omega  $$ \n\n\nGiven that the presence of serial correlation and possibly heteroscedasticity is a violation of one of the \nGauss Markov\n\nassumptions, OLS is no longer BLUE. To address this, we follow the same strategy as in \nWeighted Least Squares\n, where we consider the \nexistence of an unknown matrix P that when applied to both sides of the model equation, transforms it in such a way that the transformed system \nexhibits \nspherical errors\n. If such a matrix P exists, we can apply OLS to the trasnsformed system given that it satisfies the Gauss Markov \nassumptions. Consider the transformed system below:\n\n\n$$ P Y = P X \\beta + P \\epsilon \\ \\ \\ \\ where \\ E[P\\epsilon]=0 \\ \\ \\ \\ Var(P\\epsilon) = \\sigma^2 I $$\n\n\nThe next step is to solve for P in terms of something we can observe or estimate. As it turns out, it is possible to solve for P in terms of \\(\\Omega\\) \nwhich itself can be estimated from the sample data of whatever problem happens to be under investigation.  Based on an assumption that P is a symmetric \nmatrix and therefore \\(P^T = P\\), we can express \\(P\\) in terms of \\(\\Omega\\) as follows:\n\n\n$$ \\begin{align}\nVar(P \\epsilon) & = \\sigma^2 I \\\\\nP Var(\\epsilon) P^T & = \\sigma^2 I \\\\\n\\sigma^2 P \\Omega P^T & = \\sigma^2 I \\\\\nP \\Omega P^ T & = I \\\\\n\\Omega P^T & = P^{-1} \\\\\n\\Omega & = (P^2)^{-1} \\\\\nP & = \\Omega^{-\\frac{1}{2}}\n\\end{align} $$\n\n\nThis analysis suggests that the transform matrix P is in fact equal to \\(\\Omega^{-\\frac{1}{2}}\\), and when applied to our original model,\nshould yield a new model with spherical errors against which we can apply a standard OLS estimator. Let us re-write the transformed model \nin terms of Z, B and E as follows:\n\n\n$$ Z = B \\beta + E \\ \\ \\ \\  where \\ Z = P Y, \\ \\ B = PX, \\ \\ and \\ E = P \\epsilon $$\n\n\nWe can use the standard OLS estimator for this since we know the residuals constitute white noise:\n\n\n$$ \\hat{\\beta}_{ols} = (B^T B)^{-1} B^T Z $$\n\n\nKnowing that \\(B=PX\\) we can write the GLS estimator as follows:\n\n\n$$ \\hat{\\beta}_{gls} = (X^T P^T P X)^{-1} X^T P^T P Y  $$\n\n\nSubstituting \\(\\Omega\\) for P:\n\n\n$$ \\hat{\\beta}_{gls} = (X^T \\Omega^{-1} X)^{-1} X^T \\Omega^{-1} Y $$\n\n\nGiven the \\(\\hat{\\beta}_{gls}\\) expression above, it is clear that some estimate of \\(\\Omega\\) is required. Note that the absolute scale\nof \\(\\Omega\\) is not required to be known up front since the scale comes from \\(\\sigma^2\\) in the expression \\(Var(\\epsilon)=\\sigma^2\\Omega\\).\nIf the errors are uncorrelated but exhibit non-uniform variance, \\(\\Omega\\) is a diagonal matrix such that all off diagonal terms are zero, and\nthe problem becomes WLS. If the errors are correlated, which is often the case in time series data, the off diagonal terms will be non-zero.\n\n\nExample\n\n\nTo demonstrate a GLS regression using the Morpheus API, we will define a data generating function which we can use to create a sample dataset\nwith the appropriate characteristics (i.e. serially correlated errors in a linear regression). For convenience, we will limit the dataset\nto two dimensions so that the results are easy to visualize. Furthermore, to make our life easier in the context of this example, we will \nenforce a correlation structure in the error term, namely that of a \nautoregressive\n\nprocess of order 1, or AR(1) for short. Knowing this upfront makes the estimation of \\(\\Omega\\) easy. Let us define our data generating\nprocess as follows:\n\n\n$$ y_{t} = \\alpha + \\beta x_{t} + u_{t} $$\n\n\n$$ u_{t} = \\rho u_{t-1} + \\epsilon_{t} $$\n\n\nHere we impose a structure on the error term of the form dictated by the second equation, which suggests that an error at time \\(t\\) is equal \nto the prior error multiplied by some correlation coefficient \\(\\rho\\) where \\(|\\rho|<1\\) plus white noise in the form of \\(\\epsilon\\) \nwhich is assumed to be \\(N(0,\\sigma^2)\\). The code below defines a function that is parameterized as per the above equations, and returns a \n\nDataFrame\n with two columns containing the X and Y values output by this model.\n\n\n\n\n\n/**\n * Returns a DataFrame of X & Y values based on the coefficients provided.\n * Noise is added to the dependent variable based on an AR(1) process\n * @param alpha     the intercept term for population process\n * @param beta      the slope term for population process\n * @param rho       the AR(1) coefficient used to generate serially correlated errors\n * @param sigma     the std deviation of the normal distribution for noise\n * @param seed      if true, initialize with a fixed seed for reproducible results\n * @return          the frame of XY values with serially correlated residuals\n */\nDataFrame<Integer,String> sample(double alpha, double beta, double rho, double sigma, int n, boolean seed) {\n    final double startX = 1d;\n    final double stepX = 0.5d;\n    final RandomGenerator rand = seed ? new Well19937c(1234565) : new Well19937c();\n    final RealDistribution noise = new NormalDistribution(rand, 0, sigma);\n    final Array<Double> xValues = Array.of(Double.class, n).applyDoubles(v -> startX + v.index() * stepX);\n    final Array<Integer> rowKeys = Range.of(0, n).toArray();\n    return DataFrame.of(rowKeys, String.class, columns -> {\n        columns.add(\"X\", xValues);\n        columns.add(\"Y\", Array.of(Double.class, n).applyDoubles(v -> {\n            final double xValue = xValues.getDouble(v.index());\n            final double yFitted = alpha + beta * xValue;\n            if (v.index() == 0) return yFitted + noise.sample();\n            else {\n                final double priorX = xValues.getDouble(v.index()-1);\n                final double priorY = v.array().getDouble(v.index()-1);\n                final double priorError = priorY - (alpha + beta * priorX);\n                final double error = rho * priorError + noise.sample();\n                return yFitted + error;\n            }\n        }));\n    });\n}\n\n\n\n\nTo get a sense of what this data looks like, we can generate 4 random samples and plot them as below, while fitting an OLS regression model \nto the data. The serial correlation in the residuals should be fairly obvious given that we can see a sequence of errors above the regression \nline followed by a sequence below the line, which clearly is not typical of white noise.\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\nThe code to generate these plots is as follows:\n\n\n\n\n\nfinal int n = 100;\nfinal double rho = 0.5d;\nfinal double beta = 4d;\nfinal double alpha = 20d;\nfinal double sigma = 10d;\nChart.show(2, IntStream.range(0, 4).mapToObj(i -> {\n    DataFrame<Integer,String> frame = sample(alpha, beta, rho, sigma, n, false);\n    String title = \"Sample %s Dataset, Beta: %.2f Alpha: %.2f\";\n    String subtitle = \"Parameter estimates, Beta^: %.3f, Alpha^: %.3f\";\n    DataFrameLeastSquares<Integer,String> ols = frame.regress().ols(\"Y\", \"X\", true, Optional::of).get();\n    double betaHat = ols.getBetaValue(\"X\", DataFrameLeastSquares.Field.PARAMETER);\n    double alphaHat = ols.getInterceptValue(DataFrameLeastSquares.Field.PARAMETER);\n    return Chart.create().withScatterPlot(frame, false, \"X\", chart -> {\n        chart.title().withText(String.format(title, i, beta, alpha));\n        chart.title().withFont(new Font(\"Arial\", Font.BOLD, 14));\n        chart.subtitle().withText(String.format(subtitle, betaHat, alphaHat));\n        chart.plot().style(\"Y\").withColor(Color.RED).withPointsVisible(true);\n        chart.plot().trend(\"Y\");\n    });\n}));\n\n\n\n\nIn the real world we would obviously not know if there was serial correlation in the residuals, and if there was, what type of model might\nbe appropriate for modeling it. Other than looking at a scatter plot, there are various ways of checking for the presence of serial correlation.\nOne commonly used technique is to compute the \nDurbin Watson\n test statistic, \nwhich by default is included in the model output generated by the Morpheus library, and is defined as:\n\n\n$$ d = \\frac{\\sum_{t=2}^T (u_{t} - u_{t-1}^2)}{\\sum_{t=1}^T u_{t}^2} $$\n\n\nThe value of this statistic lies between 0 and 4, and if it is substantially less than 2, there is evidence for positive serial correlation. \nFor more details on this statistic, consult the above link. The code below runs an OLS regression given a sample of our data and prints the \nmodel results to standard out. The Durbin Watson statistic comes out to be \n1.0603\n which is highly suggestive that there is positive serial \ncorrelation in the residuals.\n\n\n\n\n\nint n = 100;\ndouble rho = 0.5d;\ndouble beta = 4d;\ndouble alpha = 20d;\ndouble sigma = 10d;\nDataFrame<Integer,String> frame = sample(alpha, beta, rho, sigma, n, true);\nframe.regress().ols(\"Y\", \"X\", true, model -> {\n    System.out.println(model);\n    return Optional.empty();\n});\n\n\n\n\n\n=============================================================================================\n                                  Linear Regression Results                                                            \n=============================================================================================\nModel:                                   OLS    R-Squared:                            0.9583\nObservations:                            100    R-Squared(adjusted):                  0.9579\nDF Model:                                  1    F-Statistic:                       2253.2056\nDF Residuals:                             98    F-Statistic(Prob):                       0E0\nStandard Error:                      11.5149    Runtime(millis)                           51\nDurbin-Watson:                        1.0603                                                \n=============================================================================================\n   Index    |  PARAMETER  |  STD_ERROR  |  T_STAT   |   P_VALUE   |  CI_LOWER  |  CI_UPPER  |\n---------------------------------------------------------------------------------------------\n Intercept  |    26.3017  |     2.3551  |   11.168  |  3.607E-19  |   21.6281  |   30.9753  |\n         X  |     3.7871  |     0.0798  |  47.4679  |  1.955E-69  |    3.6288  |    3.9454  |\n=============================================================================================\n\n\n\n\nOnce we have established a belief that the residuals are serially correlated, and therefore OLS is no longer BLUE, we then need to understand \nthe nature of the serial correlation. In this example we know the structure comes from an AR(1) process, but in reality some analysis of the residuals \nwill be required in order to come up with a reasonable model, and hence some estimation of the covariance matrix \\(\\Omega\\). A common first task would \nbe to plot the autocorrelation function of the residuals, which is a visualization usually referred to as a \ncorrelogram\n,\nand is illustrated below for this example. The peak at lag 1 clearly breaches the upper bound of the 95% confidence interval represented by the dashed blue \nline (the bar at lag 0 is by definition equal to 1).\n\n\n\n    \n\n\n\n\n\nThe code to generate this plot is shown below, which effectively runs an OLS regression and then passes the resulting model object to the chart \n\nacf()\n function, which is an abbreviation for Autocorrelation Function. In addition, the third argument to the \nacf()\n method is the significance \nlevel to use in order to generate the confidence intervals for the ACF. In this case, we chose a 5% level of significance, resulting in the dashed \nblue lines representing the 95% confidence intervals.\n\n\n\n\n\nfinal int n = 200;\nfinal double rho = 0.5d;\nfinal double beta = 4d;\nfinal double alpha = 20d;\nfinal double sigma = 10d;\nfinal DataFrame<Integer,String> frame = sample(alpha, beta, rho, sigma, n, true);\nframe.regress().ols(\"Y\", \"X\", true, model -> {\n    Chart.create().withAcf(model, frame.rowCount()/2, 0.05d, chart -> {\n        final double rhoHat = model.getResiduals().colAt(0).stats().autocorr(1);\n        chart.title().withText(\"Residual Autocorrelation Plot\");\n        chart.subtitle().withText(String.format(\"Autocorrelation Lag 1 = %.3f\", rhoHat));\n        chart.show(800, 300);\n    });\n    return Optional.empty();\n});\n\n\n\n\nWe have now established that there is serial correlation in the residuals based on the Durbin Watson test statistic, and we can reasonably\nassume that an AR(1) process is an appropriate model for the residuals based on our observations in the ACF plot. Given this information\nwe now have to estimate \\(\\Omega\\) which is the covariance matrix of the residuals across all lags in the sample. As it turns out,\nan AR(1) process has a well defined correlation structure which is described in more detail \nhere\n, \nand suggests that the correlation between two observations k periods apart is equal to \\(\\rho^k\\). \n\n\nThe elements of the \\(\\Omega\\) matrix can therefore be easily computed since the entry in the \\(i^{th}\\) row and \\(j^{th}\\) column will \nsimply be \\(\\rho^{|i-j|}\\). The code below returns a \nDataFrame\n to represent \\(\\Omega\\) given the value of \\(\\rho\\), which should be the \nautocorrelation coefficient at lag 1 calculated from the residuals, and the sample size of the data since \\(\\Omega\\) has \nnxn\n dimensions.\n\n\n\n\n\n/**\n * Returns the correlation matrix omega for an AR(1) process\n * @param size      the size for the correlation matrix\n * @param autocorr  the auto correlation coefficient\n * @return          the newly created correlation matrix\n */\nprivate DataFrame<Integer,Integer> createOmega(int size, double autocorr) {\n    final Range<Integer> keys = Range.of(0, size);\n    return DataFrame.ofDoubles(keys, keys, v -> {\n        return Math.pow(autocorr,  Math.abs(v.rowOrdinal() - v.colOrdinal()));\n    });\n}\n\n\n\n\nNow that we have an estimate for \\(\\Omega\\) we can run a GLS regression which is illustrated by the code and results below. While we specify\nour value of \\(\\rho\\) upfront in order to generate our data sample, we first run an OLS regression so that we can calculate an estimate\nof \\(\\hat{\\rho}\\) based on the autocorrelation coefficient of the residuals with lag 1. This \\(\\hat{\\rho}\\) estimate is then used to construct \\(\\Omega\\)\nwhich we pass to the \ngls()\n method on the \nDataFrameRegression\n interface.\n\n\n\n\n\nint n = 100;\ndouble rho = 0.5d;\ndouble beta = 4d;\ndouble alpha = 20d;\ndouble sigma = 10d;\nDataFrame<Integer,String> frame = sample(alpha, beta, rho, sigma, n, true);\nframe.regress().ols(\"Y\", \"X\", true, ols -> {\n    final double rhoHat = ols.getResiduals().colAt(0).stats().autocorr(1);\n    final DataFrame<Integer,Integer> omega = createOmega(n, rhoHat);\n    frame.regress().gls(\"Y\", \"X\", omega, true, gls -> {\n        System.out.println(gls);\n        return Optional.empty();\n    });\n    return Optional.empty();\n});\n\n\n\n\n\n=============================================================================================\n                                  Linear Regression Results                                                            \n=============================================================================================\nModel:                                   GLS    R-Squared:                            0.9080\nObservations:                            100    R-Squared(adjusted):                  0.9071\nDF Model:                                  1    F-Statistic:                        967.2651\nDF Residuals:                             98    F-Statistic(Prob):                       0E0\nStandard Error:                      11.5135    Runtime(millis)                           49\nDurbin-Watson:                        1.8814                                                \n=============================================================================================\n   Index    |  PARAMETER  |  STD_ERROR  |  T_STAT   |   P_VALUE   |  CI_LOWER  |  CI_UPPER  |\n---------------------------------------------------------------------------------------------\n Intercept  |    26.3744  |     3.8188  |   6.9065  |   5.01E-10  |   18.7961  |   33.9527  |\n         X  |     3.7934  |     0.1288  |  29.4463  |  1.796E-50  |    3.5378  |    4.0491  |\n=============================================================================================\n\n\n\n\nThe coefficient estimates from the GLS regression are very close to that produced by the OLS model, however the standard errors are higher for\nGLS, and the 95% confidence intervals substantially wider. One of the known problems of OLS in the presence of serially correlated errors is\nthat it yields downwardly biased estimates of the standard errors, and therefore statistical inference from these is unreliable. In this\nparticular problem, the OLS parameter confidence interval includes the known population values, but the magnitude of the noise that drives\nthe residuals is relatively small, so this will not always be the case.\n\n\nUnbiasedness\n\n\nAs per the \nOLS\n and \nWLS\n examples, here we attempt to demonstrate empirically the unbiased nature of \nthe GLS estimator. We have already introduced our data generating process earlier (see \nsample()\n function above), which can be used to manufacture \nmany data samples on which we can fit a GLS model, while collecting all the parameter estimates for each run. The code below generates 100,000\nsamples of data with \\(n=100\\), runs a GLS regression on each, and then plots the frequency distribution of the slope and intercept estimates. \nAs expected, we get a bell shaped normal distribution centered on the known population values for both the slope and intercept parameters.\n\n\n\n\n\nfinal int n = 100;\nfinal double rho = 0.5d;\nfinal double beta = 4d;\nfinal double alpha = 20d;\nfinal double sigma = 20d;\nfinal int regressionCount = 100000;\nRange<Integer> rows = Range.of(0, regressionCount);\nArray<String> columns = Array.of(\"Beta\", \"Alpha\");\nDataFrame<Integer,String> results = DataFrame.ofDoubles(rows, columns);\n\n//Run regressions in parallel to leverage all 4 cores...\nresults.rows().parallel().forEach(row -> {\n    final DataFrame<Integer,String> frame = sample(alpha, beta, rho, sigma, n, false);\n    frame.regress().ols(\"Y\", \"X\", true, ols -> {\n        final double rhoHat = ols.getResiduals().colAt(0).stats().autocorr(1);\n        final DataFrame<Integer,Integer> omega = createOmega(frame.rowCount(), rhoHat);\n        frame.regress().gls(\"Y\", \"X\", omega, true, model -> {\n            final double alphaHat = model.getInterceptValue(Field.PARAMETER);\n            final double betaHat = model.getBetaValue(\"X\", Field.PARAMETER);\n            row.setDouble(\"Alpha\", alphaHat);\n            row.setDouble(\"Beta\", betaHat);\n            return Optional.empty();\n        });\n        return Optional.empty();\n    });\n});\n\nArray.of(\"Beta\", \"Alpha\").forEach(coeff -> {\n    final DataFrame<Integer,String> coeffResults = results.cols().select(col -> col.key().startsWith(coeff));\n    Chart.create().withHistPlot(coeffResults, 250, chart -> {\n        String title = \"%s Histogram of %s GLS regressions, Rho = %.3f\";\n        String subtitle = \"%s estimate unbiasedness, Actual: %.2f, Mean: %.2f, Variance: %.2f\";\n        double actual = coeff.equals(\"Beta\") ? beta : alpha;\n        double estimate = coeffResults.colAt(coeff).stats().mean();\n        double variance = coeffResults.colAt(coeff).stats().variance();\n        Color color = coeff.equals(\"Beta\") ? new Color(255, 100, 100) : new Color(102, 204, 255);\n        chart.plot().style(coeff).withColor(color);\n        chart.plot().axes().domain().label().withText(coeff + \" Estimate\");\n        chart.title().withText(String.format(title, coeff, regressionCount, rho));\n        chart.subtitle().withText(String.format(subtitle, coeff, actual, estimate, variance));\n        chart.show(700, 400);\n    });\n});\n\n\n\n\n\n    \n\n    \n\n\n\n\n\nEfficiency\n\n\nA more efficient estimator implies one with lower \nsampling error\n in the parameter estimates \nthan other less efficient estimators. In the case of the \nWLS\n example, the relative efficiency versus OLS in the presence \nof \nheteroscedastic\n errors was fairly stark. As it turns out, for the GLS data generating \nprocess introduced earlier (see \nsample()\n function above), the comparative efficiency with OLS is only marginally better. In this example, \nI have therefore increased the value of \\(\\sigma\\) to 20 and the value of \\(\\rho\\) to 0.8 in order to get a noticeable difference in the \nresulting frequency distributions.\n\n\nThe code below again generates 100,000 data samples and runs both OLS and GLS regressions on these. In each case the OLS residuals\nare used to compute the autocorrelation coefficient with lag 1 which serves as the \\(\\rho\\) value that is then used to create the\n\\(\\Omega\\) matrix for the GLS regression. Both the OLS and GLS parameters are captured in a \nDataFrame\n and the results are used\nto generate the frequency distributions below. The story is slightly less compelling than for the WLS example, however the superior \nefficiency of GLS in this case is still apparent.\n\n\n\n\n\nfinal int n = 100;\nfinal double rho = 0.8d;\nfinal double beta = 4d;\nfinal double alpha = 20d;\nfinal double sigma = 20d;\nfinal int regressionCount = 100000;\nRange<Integer> rows = Range.of(0, regressionCount);\nArray<String> columns = Array.of(\"Beta(OLS)\", \"Alpha(OLS)\", \"Beta(GLS)\", \"Alpha(GLS)\");\nDataFrame<Integer,String> results = DataFrame.ofDoubles(rows, columns);\n\n//Run regressions in parallel to leverage all 4 cores...\nresults.rows().parallel().forEach(row -> {\n    DataFrame<Integer,String> data = sample(alpha, beta, rho, sigma, n, false);\n    data.regress().ols(\"Y\", \"X\", true, ols -> {\n        final double alphaHatOls = ols.getInterceptValue(Field.PARAMETER);\n        final double betaHatOls = ols.getBetaValue(\"X\", Field.PARAMETER);\n        row.setDouble(\"Alpha(OLS)\", alphaHatOls);\n        row.setDouble(\"Beta(OLS)\", betaHatOls);\n        final double rhoHat = ols.getResiduals().colAt(0).stats().autocorr(1);\n        final DataFrame<Integer,Integer> omega = createOmega(n, rhoHat);\n        data.regress().gls(\"Y\", \"X\", omega, true, gls -> {\n            double alphaHat = gls.getInterceptValue(Field.PARAMETER);\n            double betaHat = gls.getBetaValue(\"X\", Field.PARAMETER);\n            row.setDouble(\"Alpha(GLS)\", alphaHat);\n            row.setDouble(\"Beta(GLS)\", betaHat);\n            return Optional.empty();\n        });\n        return Optional.empty();\n    });\n});\n\nArray.of(\"Alpha\", \"Beta\").forEach(coeff -> {\n    final String olsKey = coeff + \"(OLS)\";\n    final String glsKey = coeff + \"(GLS)\";\n    final DataFrame<Integer,String> data = results.cols().select(olsKey, glsKey);\n    Chart.create().withHistPlot(data, 350, chart -> {\n        double meanOls = results.colAt(olsKey).stats().mean();\n        double stdOls = results.colAt(olsKey).stats().stdDev();\n        double meanWls = results.colAt(glsKey).stats().mean();\n        double stdWls = results.colAt(glsKey).stats().stdDev();\n        double coeffAct = coeff.equals(\"Alpha\") ? alpha : beta;\n        String title = \"%s Histogram from %s OLS & GLS Regressions (n=%s)\";\n        String subtitle = \"Actual: %.4f, Mean(OLS): %.4f, Std(OLS): %.4f, Mean(GLS): %.4f, Std(GLS): %.4f\";\n        chart.title().withText(String.format(title, coeff, regressionCount, n));\n        chart.title().withFont(new Font(\"Arial\", Font.BOLD, 15));\n        chart.subtitle().withText(String.format(subtitle, coeffAct, meanOls, stdOls, meanWls, stdWls));\n        chart.plot().axes().domain().label().withText(coeff + \" Estimates\");\n        chart.legend().on().bottom();\n        chart.show(700, 400);\n    });\n});\n\n\n\n\n\n    \n\n    \n\n\n\n\n\nConsistency\n\n\nFinally we consider the consistency of the GLS estimator. A consistent estimator is one where the variance in the estimates decreases\nas the sample size increases, which was demonstrated empirically to be true for both \nOLS\n and \nWLS\n.\nIn this case we follow the same procedure whereby we manufacture 100,000 random samples from our data generating process and proceed to\nfit both OLS and GLS models to each sample. In this case we repeat the process 5 times for sample sizes ranging from 20 to 100 inclusive\nin steps of 20, and capture all estimates in a \nDataFrame\n before plotting the resulting frequency distributions of the parameter estimates.\nThe plots below clearly demonstrate the reduction in estimation variance as the sample size increases, which confirms the consistency of\nthe estimator, at least empirically.\n\n\n\n\n\nfinal double beta = 4d;\nfinal double rho = 0.5d;\nfinal double alpha = 20d;\nfinal double sigma = 10d;\nfinal int regressionCount = 100000;\nfinal Range<Integer> sampleSizes = Range.of(20, 120, 20);\nfinal Range<Integer> rows = Range.of(0, regressionCount);\nfinal DataFrame<Integer,String> results = DataFrame.of(rows, String.class, columns -> {\n    sampleSizes.forEach(n -> {\n        columns.add(String.format(\"Beta(n=%s)\", n), Double.class);\n        columns.add(String.format(\"Alpha(n=%s)\", n), Double.class);\n    });\n});\n\nsampleSizes.forEach(n -> {\n    System.out.println(\"Running \" + regressionCount + \" regressions for n=\" + n);\n    final String betaKey = String.format(\"Beta(n=%s)\", n);\n    final String alphaKey = String.format(\"Alpha(n=%s)\", n);\n    results.rows().parallel().forEach(row -> {\n        DataFrame<Integer,String> data = sample(alpha, beta, rho, sigma, n, false);\n        DataFrame<Integer,Integer> omega = createOmega(n, rho);\n        data.regress().gls(\"Y\", \"X\", omega, true, model -> {\n            final double alphaHat = model.getInterceptValue(Field.PARAMETER);\n            final double betaHat = model.getBetaValue(\"X\", Field.PARAMETER);\n            row.setDouble(alphaKey, alphaHat);\n            row.setDouble(betaKey, betaHat);\n            return Optional.empty();\n        });\n    });\n});\n\nArray.of(\"Beta\", \"Alpha\").forEach(coeff -> {\n    final DataFrame<Integer,String> coeffResults = results.cols().select(col -> col.key().startsWith(coeff));\n    Chart.create().withHistPlot(coeffResults, 250, true, chart -> {\n        chart.plot().axes().domain().label().withText(\"Coefficient Estimate\");\n        chart.title().withText(coeff + \" Histograms of \" + regressionCount + \" Regressions\");\n        chart.subtitle().withText(coeff + \" Variance decreases as sample size increases\");\n        chart.legend().on().bottom();\n        chart.show(700, 400);\n    });\n});\n\n\n\n\n\n    \n\n    \n\n\n\n\n\nThe bar charts below illustrate the monotonically decreasing variance in both the \\(\\beta\\) and \\(\\alpha\\) coefficients.\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\nThe code to generate this plot is as follows:\n\n\n\n\n\nArray<DataFrame<String,StatType>> variances = Array.of(\"Beta\", \"Alpha\").map(value -> {\n    final String coefficient = value.getValue();\n    final Matcher matcher = Pattern.compile(coefficient + \"\\\\(n=(\\\\d+)\\\\)\").matcher(\"\");\n    return results.cols().select(column -> {\n        final String name = column.key();\n        return matcher.reset(name).matches();\n    }).cols().mapKeys(column -> {\n        final String name = column.key();\n        if (matcher.reset(name).matches()) return matcher.group(1);\n        throw new IllegalArgumentException(\"Unexpected column name: \" + column.key());\n    }).cols().stats().variance().transpose();\n});\n\nChart.show(2, Array.of(\n    Chart.create().withBarPlot(variances.getValue(0), false, chart -> {\n        chart.title().withText(\"Beta variance with sample size\");\n        chart.plot().style(StatType.VARIANCE).withColor(new Color(255, 100, 100));\n        chart.plot().axes().range(0).label().withText(\"Beta Variance\");\n        chart.plot().axes().domain().label().withText(\"Sample Size\");\n    }),\n    Chart.create().withBarPlot(variances.getValue(1), false, chart -> {\n        chart.title().withText(\"Beta variance with sample size\");\n        chart.plot().style(StatType.VARIANCE).withColor(new Color(102, 204, 255));\n        chart.plot().axes().range(0).label().withText(\"Alpha Variance\");\n        chart.plot().axes().domain().label().withText(\"Sample Size\");\n    }))\n);",
            "title": "Generalized Least Squares"
        },
        {
            "location": "/regression/gls/#generalized-least-squares-gls",
            "text": "",
            "title": "Generalized Least Squares (GLS)"
        },
        {
            "location": "/regression/gls/#introduction",
            "text": "One of the  Gauss Markov  assumptions that has to be met in order for  OLS  to be the Best, \nLinear, Unbiased Estimator (BLUE) is that of  spherical errors . More specifically, the assumption states that the error or disturbance \nterm in the model must exhibit uniform variance (i.e.  homoscedasticity ), and also that \nthere is no  autocorrelation  between errors.  In the real world, this assumption will not always hold, and errors may exhibit  heteroscedasticity  \nor autocorrelation or both. When the errors in a regression show no serial correlation but are heteroscedastic, then a  Weighted Least Squares  \n(WLS) model should be used, whereas if correlation is present a  Generalized Least Squares  (GLS)\nmodel is appropriate. WLS is in fact a special case of GLS where the the lack of autocorrelation implies that the off diagonal terms of the  covariance matrix  of the errors are zero, thus simplifying the analysis.  The following section covers some of the theory behind GLS, provides an example of how to apply it with the Morpheus API, and then goes\nonto demonstrate empirically the  unbiased ,  consistent  \nand  efficient  nature of the estimator.",
            "title": "Introduction"
        },
        {
            "location": "/regression/gls/#theory",
            "text": "A Generalized Least Squares model is based on the usual linear equation, however we now assume that the error term has a variance structure \nthat is described by a covariance matrix \\(\\Omega\\), which differs from the OLS model which assumes white noise in the form of \\(\\sigma^2 I\\).  $$ Y = X \\beta + \\epsilon \\ \\ \\ \\ where \\ E[\\epsilon]=0 \\ \\ \\ \\ Var(\\epsilon) = \\sigma^2 \\Omega  $$   Given that the presence of serial correlation and possibly heteroscedasticity is a violation of one of the  Gauss Markov \nassumptions, OLS is no longer BLUE. To address this, we follow the same strategy as in  Weighted Least Squares , where we consider the \nexistence of an unknown matrix P that when applied to both sides of the model equation, transforms it in such a way that the transformed system \nexhibits  spherical errors . If such a matrix P exists, we can apply OLS to the trasnsformed system given that it satisfies the Gauss Markov \nassumptions. Consider the transformed system below:  $$ P Y = P X \\beta + P \\epsilon \\ \\ \\ \\ where \\ E[P\\epsilon]=0 \\ \\ \\ \\ Var(P\\epsilon) = \\sigma^2 I $$  The next step is to solve for P in terms of something we can observe or estimate. As it turns out, it is possible to solve for P in terms of \\(\\Omega\\) \nwhich itself can be estimated from the sample data of whatever problem happens to be under investigation.  Based on an assumption that P is a symmetric \nmatrix and therefore \\(P^T = P\\), we can express \\(P\\) in terms of \\(\\Omega\\) as follows:  $$ \\begin{align}\nVar(P \\epsilon) & = \\sigma^2 I \\\\\nP Var(\\epsilon) P^T & = \\sigma^2 I \\\\\n\\sigma^2 P \\Omega P^T & = \\sigma^2 I \\\\\nP \\Omega P^ T & = I \\\\\n\\Omega P^T & = P^{-1} \\\\\n\\Omega & = (P^2)^{-1} \\\\\nP & = \\Omega^{-\\frac{1}{2}}\n\\end{align} $$  This analysis suggests that the transform matrix P is in fact equal to \\(\\Omega^{-\\frac{1}{2}}\\), and when applied to our original model,\nshould yield a new model with spherical errors against which we can apply a standard OLS estimator. Let us re-write the transformed model \nin terms of Z, B and E as follows:  $$ Z = B \\beta + E \\ \\ \\ \\  where \\ Z = P Y, \\ \\ B = PX, \\ \\ and \\ E = P \\epsilon $$  We can use the standard OLS estimator for this since we know the residuals constitute white noise:  $$ \\hat{\\beta}_{ols} = (B^T B)^{-1} B^T Z $$  Knowing that \\(B=PX\\) we can write the GLS estimator as follows:  $$ \\hat{\\beta}_{gls} = (X^T P^T P X)^{-1} X^T P^T P Y  $$  Substituting \\(\\Omega\\) for P:  $$ \\hat{\\beta}_{gls} = (X^T \\Omega^{-1} X)^{-1} X^T \\Omega^{-1} Y $$  Given the \\(\\hat{\\beta}_{gls}\\) expression above, it is clear that some estimate of \\(\\Omega\\) is required. Note that the absolute scale\nof \\(\\Omega\\) is not required to be known up front since the scale comes from \\(\\sigma^2\\) in the expression \\(Var(\\epsilon)=\\sigma^2\\Omega\\).\nIf the errors are uncorrelated but exhibit non-uniform variance, \\(\\Omega\\) is a diagonal matrix such that all off diagonal terms are zero, and\nthe problem becomes WLS. If the errors are correlated, which is often the case in time series data, the off diagonal terms will be non-zero.",
            "title": "Theory"
        },
        {
            "location": "/regression/gls/#example",
            "text": "To demonstrate a GLS regression using the Morpheus API, we will define a data generating function which we can use to create a sample dataset\nwith the appropriate characteristics (i.e. serially correlated errors in a linear regression). For convenience, we will limit the dataset\nto two dimensions so that the results are easy to visualize. Furthermore, to make our life easier in the context of this example, we will \nenforce a correlation structure in the error term, namely that of a  autoregressive \nprocess of order 1, or AR(1) for short. Knowing this upfront makes the estimation of \\(\\Omega\\) easy. Let us define our data generating\nprocess as follows:  $$ y_{t} = \\alpha + \\beta x_{t} + u_{t} $$  $$ u_{t} = \\rho u_{t-1} + \\epsilon_{t} $$  Here we impose a structure on the error term of the form dictated by the second equation, which suggests that an error at time \\(t\\) is equal \nto the prior error multiplied by some correlation coefficient \\(\\rho\\) where \\(|\\rho|<1\\) plus white noise in the form of \\(\\epsilon\\) \nwhich is assumed to be \\(N(0,\\sigma^2)\\). The code below defines a function that is parameterized as per the above equations, and returns a  DataFrame  with two columns containing the X and Y values output by this model.   /**\n * Returns a DataFrame of X & Y values based on the coefficients provided.\n * Noise is added to the dependent variable based on an AR(1) process\n * @param alpha     the intercept term for population process\n * @param beta      the slope term for population process\n * @param rho       the AR(1) coefficient used to generate serially correlated errors\n * @param sigma     the std deviation of the normal distribution for noise\n * @param seed      if true, initialize with a fixed seed for reproducible results\n * @return          the frame of XY values with serially correlated residuals\n */\nDataFrame<Integer,String> sample(double alpha, double beta, double rho, double sigma, int n, boolean seed) {\n    final double startX = 1d;\n    final double stepX = 0.5d;\n    final RandomGenerator rand = seed ? new Well19937c(1234565) : new Well19937c();\n    final RealDistribution noise = new NormalDistribution(rand, 0, sigma);\n    final Array<Double> xValues = Array.of(Double.class, n).applyDoubles(v -> startX + v.index() * stepX);\n    final Array<Integer> rowKeys = Range.of(0, n).toArray();\n    return DataFrame.of(rowKeys, String.class, columns -> {\n        columns.add(\"X\", xValues);\n        columns.add(\"Y\", Array.of(Double.class, n).applyDoubles(v -> {\n            final double xValue = xValues.getDouble(v.index());\n            final double yFitted = alpha + beta * xValue;\n            if (v.index() == 0) return yFitted + noise.sample();\n            else {\n                final double priorX = xValues.getDouble(v.index()-1);\n                final double priorY = v.array().getDouble(v.index()-1);\n                final double priorError = priorY - (alpha + beta * priorX);\n                final double error = rho * priorError + noise.sample();\n                return yFitted + error;\n            }\n        }));\n    });\n}  To get a sense of what this data looks like, we can generate 4 random samples and plot them as below, while fitting an OLS regression model \nto the data. The serial correlation in the residuals should be fairly obvious given that we can see a sequence of errors above the regression \nline followed by a sequence below the line, which clearly is not typical of white noise.  \n       \n       \n       \n       The code to generate these plots is as follows:   final int n = 100;\nfinal double rho = 0.5d;\nfinal double beta = 4d;\nfinal double alpha = 20d;\nfinal double sigma = 10d;\nChart.show(2, IntStream.range(0, 4).mapToObj(i -> {\n    DataFrame<Integer,String> frame = sample(alpha, beta, rho, sigma, n, false);\n    String title = \"Sample %s Dataset, Beta: %.2f Alpha: %.2f\";\n    String subtitle = \"Parameter estimates, Beta^: %.3f, Alpha^: %.3f\";\n    DataFrameLeastSquares<Integer,String> ols = frame.regress().ols(\"Y\", \"X\", true, Optional::of).get();\n    double betaHat = ols.getBetaValue(\"X\", DataFrameLeastSquares.Field.PARAMETER);\n    double alphaHat = ols.getInterceptValue(DataFrameLeastSquares.Field.PARAMETER);\n    return Chart.create().withScatterPlot(frame, false, \"X\", chart -> {\n        chart.title().withText(String.format(title, i, beta, alpha));\n        chart.title().withFont(new Font(\"Arial\", Font.BOLD, 14));\n        chart.subtitle().withText(String.format(subtitle, betaHat, alphaHat));\n        chart.plot().style(\"Y\").withColor(Color.RED).withPointsVisible(true);\n        chart.plot().trend(\"Y\");\n    });\n}));  In the real world we would obviously not know if there was serial correlation in the residuals, and if there was, what type of model might\nbe appropriate for modeling it. Other than looking at a scatter plot, there are various ways of checking for the presence of serial correlation.\nOne commonly used technique is to compute the  Durbin Watson  test statistic, \nwhich by default is included in the model output generated by the Morpheus library, and is defined as:  $$ d = \\frac{\\sum_{t=2}^T (u_{t} - u_{t-1}^2)}{\\sum_{t=1}^T u_{t}^2} $$  The value of this statistic lies between 0 and 4, and if it is substantially less than 2, there is evidence for positive serial correlation. \nFor more details on this statistic, consult the above link. The code below runs an OLS regression given a sample of our data and prints the \nmodel results to standard out. The Durbin Watson statistic comes out to be  1.0603  which is highly suggestive that there is positive serial \ncorrelation in the residuals.   int n = 100;\ndouble rho = 0.5d;\ndouble beta = 4d;\ndouble alpha = 20d;\ndouble sigma = 10d;\nDataFrame<Integer,String> frame = sample(alpha, beta, rho, sigma, n, true);\nframe.regress().ols(\"Y\", \"X\", true, model -> {\n    System.out.println(model);\n    return Optional.empty();\n});  \n=============================================================================================\n                                  Linear Regression Results                                                            \n=============================================================================================\nModel:                                   OLS    R-Squared:                            0.9583\nObservations:                            100    R-Squared(adjusted):                  0.9579\nDF Model:                                  1    F-Statistic:                       2253.2056\nDF Residuals:                             98    F-Statistic(Prob):                       0E0\nStandard Error:                      11.5149    Runtime(millis)                           51\nDurbin-Watson:                        1.0603                                                \n=============================================================================================\n   Index    |  PARAMETER  |  STD_ERROR  |  T_STAT   |   P_VALUE   |  CI_LOWER  |  CI_UPPER  |\n---------------------------------------------------------------------------------------------\n Intercept  |    26.3017  |     2.3551  |   11.168  |  3.607E-19  |   21.6281  |   30.9753  |\n         X  |     3.7871  |     0.0798  |  47.4679  |  1.955E-69  |    3.6288  |    3.9454  |\n=============================================================================================  Once we have established a belief that the residuals are serially correlated, and therefore OLS is no longer BLUE, we then need to understand \nthe nature of the serial correlation. In this example we know the structure comes from an AR(1) process, but in reality some analysis of the residuals \nwill be required in order to come up with a reasonable model, and hence some estimation of the covariance matrix \\(\\Omega\\). A common first task would \nbe to plot the autocorrelation function of the residuals, which is a visualization usually referred to as a  correlogram ,\nand is illustrated below for this example. The peak at lag 1 clearly breaches the upper bound of the 95% confidence interval represented by the dashed blue \nline (the bar at lag 0 is by definition equal to 1).  \n       The code to generate this plot is shown below, which effectively runs an OLS regression and then passes the resulting model object to the chart  acf()  function, which is an abbreviation for Autocorrelation Function. In addition, the third argument to the  acf()  method is the significance \nlevel to use in order to generate the confidence intervals for the ACF. In this case, we chose a 5% level of significance, resulting in the dashed \nblue lines representing the 95% confidence intervals.   final int n = 200;\nfinal double rho = 0.5d;\nfinal double beta = 4d;\nfinal double alpha = 20d;\nfinal double sigma = 10d;\nfinal DataFrame<Integer,String> frame = sample(alpha, beta, rho, sigma, n, true);\nframe.regress().ols(\"Y\", \"X\", true, model -> {\n    Chart.create().withAcf(model, frame.rowCount()/2, 0.05d, chart -> {\n        final double rhoHat = model.getResiduals().colAt(0).stats().autocorr(1);\n        chart.title().withText(\"Residual Autocorrelation Plot\");\n        chart.subtitle().withText(String.format(\"Autocorrelation Lag 1 = %.3f\", rhoHat));\n        chart.show(800, 300);\n    });\n    return Optional.empty();\n});  We have now established that there is serial correlation in the residuals based on the Durbin Watson test statistic, and we can reasonably\nassume that an AR(1) process is an appropriate model for the residuals based on our observations in the ACF plot. Given this information\nwe now have to estimate \\(\\Omega\\) which is the covariance matrix of the residuals across all lags in the sample. As it turns out,\nan AR(1) process has a well defined correlation structure which is described in more detail  here , \nand suggests that the correlation between two observations k periods apart is equal to \\(\\rho^k\\).   The elements of the \\(\\Omega\\) matrix can therefore be easily computed since the entry in the \\(i^{th}\\) row and \\(j^{th}\\) column will \nsimply be \\(\\rho^{|i-j|}\\). The code below returns a  DataFrame  to represent \\(\\Omega\\) given the value of \\(\\rho\\), which should be the \nautocorrelation coefficient at lag 1 calculated from the residuals, and the sample size of the data since \\(\\Omega\\) has  nxn  dimensions.   /**\n * Returns the correlation matrix omega for an AR(1) process\n * @param size      the size for the correlation matrix\n * @param autocorr  the auto correlation coefficient\n * @return          the newly created correlation matrix\n */\nprivate DataFrame<Integer,Integer> createOmega(int size, double autocorr) {\n    final Range<Integer> keys = Range.of(0, size);\n    return DataFrame.ofDoubles(keys, keys, v -> {\n        return Math.pow(autocorr,  Math.abs(v.rowOrdinal() - v.colOrdinal()));\n    });\n}  Now that we have an estimate for \\(\\Omega\\) we can run a GLS regression which is illustrated by the code and results below. While we specify\nour value of \\(\\rho\\) upfront in order to generate our data sample, we first run an OLS regression so that we can calculate an estimate\nof \\(\\hat{\\rho}\\) based on the autocorrelation coefficient of the residuals with lag 1. This \\(\\hat{\\rho}\\) estimate is then used to construct \\(\\Omega\\)\nwhich we pass to the  gls()  method on the  DataFrameRegression  interface.   int n = 100;\ndouble rho = 0.5d;\ndouble beta = 4d;\ndouble alpha = 20d;\ndouble sigma = 10d;\nDataFrame<Integer,String> frame = sample(alpha, beta, rho, sigma, n, true);\nframe.regress().ols(\"Y\", \"X\", true, ols -> {\n    final double rhoHat = ols.getResiduals().colAt(0).stats().autocorr(1);\n    final DataFrame<Integer,Integer> omega = createOmega(n, rhoHat);\n    frame.regress().gls(\"Y\", \"X\", omega, true, gls -> {\n        System.out.println(gls);\n        return Optional.empty();\n    });\n    return Optional.empty();\n});  \n=============================================================================================\n                                  Linear Regression Results                                                            \n=============================================================================================\nModel:                                   GLS    R-Squared:                            0.9080\nObservations:                            100    R-Squared(adjusted):                  0.9071\nDF Model:                                  1    F-Statistic:                        967.2651\nDF Residuals:                             98    F-Statistic(Prob):                       0E0\nStandard Error:                      11.5135    Runtime(millis)                           49\nDurbin-Watson:                        1.8814                                                \n=============================================================================================\n   Index    |  PARAMETER  |  STD_ERROR  |  T_STAT   |   P_VALUE   |  CI_LOWER  |  CI_UPPER  |\n---------------------------------------------------------------------------------------------\n Intercept  |    26.3744  |     3.8188  |   6.9065  |   5.01E-10  |   18.7961  |   33.9527  |\n         X  |     3.7934  |     0.1288  |  29.4463  |  1.796E-50  |    3.5378  |    4.0491  |\n=============================================================================================  The coefficient estimates from the GLS regression are very close to that produced by the OLS model, however the standard errors are higher for\nGLS, and the 95% confidence intervals substantially wider. One of the known problems of OLS in the presence of serially correlated errors is\nthat it yields downwardly biased estimates of the standard errors, and therefore statistical inference from these is unreliable. In this\nparticular problem, the OLS parameter confidence interval includes the known population values, but the magnitude of the noise that drives\nthe residuals is relatively small, so this will not always be the case.",
            "title": "Example"
        },
        {
            "location": "/regression/gls/#unbiasedness",
            "text": "As per the  OLS  and  WLS  examples, here we attempt to demonstrate empirically the unbiased nature of \nthe GLS estimator. We have already introduced our data generating process earlier (see  sample()  function above), which can be used to manufacture \nmany data samples on which we can fit a GLS model, while collecting all the parameter estimates for each run. The code below generates 100,000\nsamples of data with \\(n=100\\), runs a GLS regression on each, and then plots the frequency distribution of the slope and intercept estimates. \nAs expected, we get a bell shaped normal distribution centered on the known population values for both the slope and intercept parameters.   final int n = 100;\nfinal double rho = 0.5d;\nfinal double beta = 4d;\nfinal double alpha = 20d;\nfinal double sigma = 20d;\nfinal int regressionCount = 100000;\nRange<Integer> rows = Range.of(0, regressionCount);\nArray<String> columns = Array.of(\"Beta\", \"Alpha\");\nDataFrame<Integer,String> results = DataFrame.ofDoubles(rows, columns);\n\n//Run regressions in parallel to leverage all 4 cores...\nresults.rows().parallel().forEach(row -> {\n    final DataFrame<Integer,String> frame = sample(alpha, beta, rho, sigma, n, false);\n    frame.regress().ols(\"Y\", \"X\", true, ols -> {\n        final double rhoHat = ols.getResiduals().colAt(0).stats().autocorr(1);\n        final DataFrame<Integer,Integer> omega = createOmega(frame.rowCount(), rhoHat);\n        frame.regress().gls(\"Y\", \"X\", omega, true, model -> {\n            final double alphaHat = model.getInterceptValue(Field.PARAMETER);\n            final double betaHat = model.getBetaValue(\"X\", Field.PARAMETER);\n            row.setDouble(\"Alpha\", alphaHat);\n            row.setDouble(\"Beta\", betaHat);\n            return Optional.empty();\n        });\n        return Optional.empty();\n    });\n});\n\nArray.of(\"Beta\", \"Alpha\").forEach(coeff -> {\n    final DataFrame<Integer,String> coeffResults = results.cols().select(col -> col.key().startsWith(coeff));\n    Chart.create().withHistPlot(coeffResults, 250, chart -> {\n        String title = \"%s Histogram of %s GLS regressions, Rho = %.3f\";\n        String subtitle = \"%s estimate unbiasedness, Actual: %.2f, Mean: %.2f, Variance: %.2f\";\n        double actual = coeff.equals(\"Beta\") ? beta : alpha;\n        double estimate = coeffResults.colAt(coeff).stats().mean();\n        double variance = coeffResults.colAt(coeff).stats().variance();\n        Color color = coeff.equals(\"Beta\") ? new Color(255, 100, 100) : new Color(102, 204, 255);\n        chart.plot().style(coeff).withColor(color);\n        chart.plot().axes().domain().label().withText(coeff + \" Estimate\");\n        chart.title().withText(String.format(title, coeff, regressionCount, rho));\n        chart.subtitle().withText(String.format(subtitle, coeff, actual, estimate, variance));\n        chart.show(700, 400);\n    });\n});",
            "title": "Unbiasedness"
        },
        {
            "location": "/regression/gls/#efficiency",
            "text": "A more efficient estimator implies one with lower  sampling error  in the parameter estimates \nthan other less efficient estimators. In the case of the  WLS  example, the relative efficiency versus OLS in the presence \nof  heteroscedastic  errors was fairly stark. As it turns out, for the GLS data generating \nprocess introduced earlier (see  sample()  function above), the comparative efficiency with OLS is only marginally better. In this example, \nI have therefore increased the value of \\(\\sigma\\) to 20 and the value of \\(\\rho\\) to 0.8 in order to get a noticeable difference in the \nresulting frequency distributions.  The code below again generates 100,000 data samples and runs both OLS and GLS regressions on these. In each case the OLS residuals\nare used to compute the autocorrelation coefficient with lag 1 which serves as the \\(\\rho\\) value that is then used to create the\n\\(\\Omega\\) matrix for the GLS regression. Both the OLS and GLS parameters are captured in a  DataFrame  and the results are used\nto generate the frequency distributions below. The story is slightly less compelling than for the WLS example, however the superior \nefficiency of GLS in this case is still apparent.   final int n = 100;\nfinal double rho = 0.8d;\nfinal double beta = 4d;\nfinal double alpha = 20d;\nfinal double sigma = 20d;\nfinal int regressionCount = 100000;\nRange<Integer> rows = Range.of(0, regressionCount);\nArray<String> columns = Array.of(\"Beta(OLS)\", \"Alpha(OLS)\", \"Beta(GLS)\", \"Alpha(GLS)\");\nDataFrame<Integer,String> results = DataFrame.ofDoubles(rows, columns);\n\n//Run regressions in parallel to leverage all 4 cores...\nresults.rows().parallel().forEach(row -> {\n    DataFrame<Integer,String> data = sample(alpha, beta, rho, sigma, n, false);\n    data.regress().ols(\"Y\", \"X\", true, ols -> {\n        final double alphaHatOls = ols.getInterceptValue(Field.PARAMETER);\n        final double betaHatOls = ols.getBetaValue(\"X\", Field.PARAMETER);\n        row.setDouble(\"Alpha(OLS)\", alphaHatOls);\n        row.setDouble(\"Beta(OLS)\", betaHatOls);\n        final double rhoHat = ols.getResiduals().colAt(0).stats().autocorr(1);\n        final DataFrame<Integer,Integer> omega = createOmega(n, rhoHat);\n        data.regress().gls(\"Y\", \"X\", omega, true, gls -> {\n            double alphaHat = gls.getInterceptValue(Field.PARAMETER);\n            double betaHat = gls.getBetaValue(\"X\", Field.PARAMETER);\n            row.setDouble(\"Alpha(GLS)\", alphaHat);\n            row.setDouble(\"Beta(GLS)\", betaHat);\n            return Optional.empty();\n        });\n        return Optional.empty();\n    });\n});\n\nArray.of(\"Alpha\", \"Beta\").forEach(coeff -> {\n    final String olsKey = coeff + \"(OLS)\";\n    final String glsKey = coeff + \"(GLS)\";\n    final DataFrame<Integer,String> data = results.cols().select(olsKey, glsKey);\n    Chart.create().withHistPlot(data, 350, chart -> {\n        double meanOls = results.colAt(olsKey).stats().mean();\n        double stdOls = results.colAt(olsKey).stats().stdDev();\n        double meanWls = results.colAt(glsKey).stats().mean();\n        double stdWls = results.colAt(glsKey).stats().stdDev();\n        double coeffAct = coeff.equals(\"Alpha\") ? alpha : beta;\n        String title = \"%s Histogram from %s OLS & GLS Regressions (n=%s)\";\n        String subtitle = \"Actual: %.4f, Mean(OLS): %.4f, Std(OLS): %.4f, Mean(GLS): %.4f, Std(GLS): %.4f\";\n        chart.title().withText(String.format(title, coeff, regressionCount, n));\n        chart.title().withFont(new Font(\"Arial\", Font.BOLD, 15));\n        chart.subtitle().withText(String.format(subtitle, coeffAct, meanOls, stdOls, meanWls, stdWls));\n        chart.plot().axes().domain().label().withText(coeff + \" Estimates\");\n        chart.legend().on().bottom();\n        chart.show(700, 400);\n    });\n});",
            "title": "Efficiency"
        },
        {
            "location": "/regression/gls/#consistency",
            "text": "Finally we consider the consistency of the GLS estimator. A consistent estimator is one where the variance in the estimates decreases\nas the sample size increases, which was demonstrated empirically to be true for both  OLS  and  WLS .\nIn this case we follow the same procedure whereby we manufacture 100,000 random samples from our data generating process and proceed to\nfit both OLS and GLS models to each sample. In this case we repeat the process 5 times for sample sizes ranging from 20 to 100 inclusive\nin steps of 20, and capture all estimates in a  DataFrame  before plotting the resulting frequency distributions of the parameter estimates.\nThe plots below clearly demonstrate the reduction in estimation variance as the sample size increases, which confirms the consistency of\nthe estimator, at least empirically.   final double beta = 4d;\nfinal double rho = 0.5d;\nfinal double alpha = 20d;\nfinal double sigma = 10d;\nfinal int regressionCount = 100000;\nfinal Range<Integer> sampleSizes = Range.of(20, 120, 20);\nfinal Range<Integer> rows = Range.of(0, regressionCount);\nfinal DataFrame<Integer,String> results = DataFrame.of(rows, String.class, columns -> {\n    sampleSizes.forEach(n -> {\n        columns.add(String.format(\"Beta(n=%s)\", n), Double.class);\n        columns.add(String.format(\"Alpha(n=%s)\", n), Double.class);\n    });\n});\n\nsampleSizes.forEach(n -> {\n    System.out.println(\"Running \" + regressionCount + \" regressions for n=\" + n);\n    final String betaKey = String.format(\"Beta(n=%s)\", n);\n    final String alphaKey = String.format(\"Alpha(n=%s)\", n);\n    results.rows().parallel().forEach(row -> {\n        DataFrame<Integer,String> data = sample(alpha, beta, rho, sigma, n, false);\n        DataFrame<Integer,Integer> omega = createOmega(n, rho);\n        data.regress().gls(\"Y\", \"X\", omega, true, model -> {\n            final double alphaHat = model.getInterceptValue(Field.PARAMETER);\n            final double betaHat = model.getBetaValue(\"X\", Field.PARAMETER);\n            row.setDouble(alphaKey, alphaHat);\n            row.setDouble(betaKey, betaHat);\n            return Optional.empty();\n        });\n    });\n});\n\nArray.of(\"Beta\", \"Alpha\").forEach(coeff -> {\n    final DataFrame<Integer,String> coeffResults = results.cols().select(col -> col.key().startsWith(coeff));\n    Chart.create().withHistPlot(coeffResults, 250, true, chart -> {\n        chart.plot().axes().domain().label().withText(\"Coefficient Estimate\");\n        chart.title().withText(coeff + \" Histograms of \" + regressionCount + \" Regressions\");\n        chart.subtitle().withText(coeff + \" Variance decreases as sample size increases\");\n        chart.legend().on().bottom();\n        chart.show(700, 400);\n    });\n});  \n     \n       The bar charts below illustrate the monotonically decreasing variance in both the \\(\\beta\\) and \\(\\alpha\\) coefficients.  \n       \n       The code to generate this plot is as follows:   Array<DataFrame<String,StatType>> variances = Array.of(\"Beta\", \"Alpha\").map(value -> {\n    final String coefficient = value.getValue();\n    final Matcher matcher = Pattern.compile(coefficient + \"\\\\(n=(\\\\d+)\\\\)\").matcher(\"\");\n    return results.cols().select(column -> {\n        final String name = column.key();\n        return matcher.reset(name).matches();\n    }).cols().mapKeys(column -> {\n        final String name = column.key();\n        if (matcher.reset(name).matches()) return matcher.group(1);\n        throw new IllegalArgumentException(\"Unexpected column name: \" + column.key());\n    }).cols().stats().variance().transpose();\n});\n\nChart.show(2, Array.of(\n    Chart.create().withBarPlot(variances.getValue(0), false, chart -> {\n        chart.title().withText(\"Beta variance with sample size\");\n        chart.plot().style(StatType.VARIANCE).withColor(new Color(255, 100, 100));\n        chart.plot().axes().range(0).label().withText(\"Beta Variance\");\n        chart.plot().axes().domain().label().withText(\"Sample Size\");\n    }),\n    Chart.create().withBarPlot(variances.getValue(1), false, chart -> {\n        chart.title().withText(\"Beta variance with sample size\");\n        chart.plot().style(StatType.VARIANCE).withColor(new Color(102, 204, 255));\n        chart.plot().axes().range(0).label().withText(\"Alpha Variance\");\n        chart.plot().axes().domain().label().withText(\"Sample Size\");\n    }))\n);",
            "title": "Consistency"
        },
        {
            "location": "/analysis/pca/",
            "text": "Principal Component Analysis\n\n\nPrincipal Component Analysis\n (PCA) is a statistical tool \nused in data analysis and also for building predictive models. The technique involves transforming a dataset into a new \n\nbasis\n whereby the transformed data is uncorrelated. The transformed \nbasis, which can be represented by an \northogonal matrix\n, defines the \nPrincipal Components of the original dataset. These basis vectors are usually ordered so that the first principal component \nis the one that accounts for the largest variance in the data, and the last component accounts for the least variance.\n\n\nPCA is also often referred to as a \ndimensional reduction\n technique\nin the sense that by dropping components that account for a negligible amount of variance in the original data, we can linearly\nmap the data into a lower dimensional space without loosing material information. The assumption here is that the variability\nin the data represents the essential dynamics we are trying to understand, so dropping dimensions with negligible variance\nresults in minimal loss of information.\n\n\nThe following sections introduces some PCA theory, and then proceeds to illustrate an example of how to use the Morpheus API\nto perform PCA on a dataset. Only a superficial overview of the theory is covered below, so for a more detailed treatment of \nthe topic I would suggest a Google search, or perhaps \nthis\n\ntutorial as a primer.\n\n\nTheory\n\n\nAs a data analysis technique, PCA begins with the definition of our data which in general can be described in two dimensions,\nnamely the number of observations and the number of measurements. Such data can be represented by an \nnxp\n matrix where \nn\n \nrepresents the number of observations for each measurement, and \np\n represents the number of measurements being recorded. \n\n\nThe dimensions in which we record the observations that constitute our data are assumed to be a naive basis, since we do not \nactually understand the true dynamics of the system we are investigating (hence the analysis). Having said that, our hope is \nthat while our measurements may be recorded in a naive basis, they are informative enough so that we can compute a new basis \nthat maximises the signal to noise ratio and removes any redundancy in the data, enabling us to better understand its true \ndynamics. \n\n\nWith this in mind, let us assume that there exists an \northogonal matrix\n \nV of dimensions \npxp\n that can transform our data X into Y such that the covariance matrix of Y (denoted by \\(\\Sigma_{y}\\)) \nis diagonal.\n\n\n$$ X V = Y $$\n\n\nThe question then is how to find V? We can tackle this by working backwards based on our desire that the transformed data Y \nhas a diagonal covariance matrix, given our motivation to remove noise and redundancy from our dataset. Assuming that Y is \n\ncentered\n or \ndemeaned\n, we can define the covariance of Y as follows:\n\n\n$$ \\Sigma_{Y} = \\frac{1}{n-1} Y^T Y $$\n\n\nGiven our transform \\(X V = Y \\) we can express the covariance of Y in terms V and X as follows:\n\n\n$$ \\begin{align}\n\\Sigma_{Y} &= \\frac{1}{n-1} Y^T Y  \\\\\n &= \\frac{1}{n-1}(XV)^T(XV) \\\\\n &= \\frac{1}{n-1}V^T X^T X V \\\\\n\\end{align} $$\n\n\nLet us define a new matrix A, which by definition is a \npxp\n \nsymmetric matrix\n as:\n\n\n$$ A = \\frac{1}{n-1} X^T X $$\n\n\nWe can therefore re-write our earlier expression for \\(\\Sigma_{Y}\\) in terms of A as:\n\n\n$$ \\Sigma_{Y} = V^T A V $$\n\n\nIn the next section, we illustrate how we can choose \\(V\\) such that we diagonalize \\(\\Sigma_{Y}\\). \n\n\nEigen Decomposition\n\n\nBased on our earlier discussion, we know that we need to choose a transform matrix V such that \\(V^TAV = D\\) where \\(D\\) \nis a diagonal matrix. Given that \\(A\\) is symmetric, we know that we can factorize it using an \nEigendecomposition\n \ninto an orthogonal matrix of its \neigenvectors\n and a diagonal \nmatrix of its \neigenvalues\n:\n\n\n$$ A = Q \\Lambda Q^{-1} $$\n\n\nIn this factorization the column vectors of \\(Q\\) are the eigenvectors of \\(A\\) and the diagonal elements of \\(\\Lambda\\) \nare the eigenvalues of \\(A\\). Recall that an eigenvector of a matrix is one which is only scaled and not rotated when operated \non by that matrix. This is otherwise stated as \\(Av = \\lambda v \\) where \\(v\\) is an \npx1\n eigenvector of A and \\(\\lambda\\) is \nthe corresponding eigenvalue, which is a scalar. We can therefore expand this to say that \\(A Q = Q \\Lambda \\) which then \nimplies that \\(A = Q \\Lambda Q^{-1}\\).\n\n\nIf we choose our matrix \\(V = Q\\) and noting that Q is an orthogonal matrix such that \\(Q^{-1} = Q^T \\) we can plug these back \ninto the expression for the covariance matrix of Y to yield the following:\n\n\n$$ \\begin{align}\n\\Sigma_{Y} &= V^T A V \\\\\n &= V^T (V \\Lambda V^{-1}) V \\\\\n &= (V^TV)\\Lambda(V^{-1}V) \\\\\n &= (V^{-1}V)\\Lambda(V^{-1}V) \\\\\n &= \\Lambda \\\\\n\\end{align} $$\n\n\nBy choosing the transform matrix V to be the eigenvectors of \\(A \\) (which in essence is the covariance matrix of our data\non the assumption that \\(X\\) is centered), we end up diagonalizing the covariance matrix of the transformed dataset, which is \nthe ultimate objective of our PCA. These eigenvectors essentially define the new basis along which variance in the data is maximised, \nand the corresponding eigenvalues define the magnitude of this variance. As noted earlier, the eigenvectors or principal components \nare usually ordered so that the first component accounts for the largest variance (ie the largest eigenvalue), and the last component \naccounts for the smallest variance (ie the smallest eigenvalue).\n\n\nSingular Value Decomposition\n\n\nThe previous section demonstrated that an eigen decomposition of the covariance matrix of \\(X\\) yields the set of eigenvectors \\(V\\) \nthat define the principal axes of our data. When we transform our data using \\(V\\) the resulting dataset has a diagonal covariance\nmatrix which reflects that our new basis maximises the signal to noise ratio and/or removes any redundancy.\n\n\nThe Morpheus library supports solving PCA in this way, but by default, it performs a \nSingular Value Decomposition\n\n(SVD) of the \ncentered\n or \ndemeaned\n data in \\(X\\), as this generally offers better numerical stability and also tends to be faster\nthen an eigendecomposition of the covariance matrix of \\(X\\). An SVD of \\(X\\) on the assumption that \\(X\\) is centered yields\n\n\n$$ X = U S V^T $$\n\n\nwhere S is a diagonal matrix of singular values, and the columns of \\(U\\) and \\(V\\) are called the left-singular vectors and right-singular \nvectors of \\(X\\) , respectively. If we substitute the SVD decomposition into the expression for the covariance of \\(X\\) as below, we can \nsee that the solution is essentially equivalent to above, however in this case the eigenvalues are equivalent to the square of the singular \nvalues divided by \\(n-1\\).\n\n\n$$ \\begin{align}\n\\Sigma_{X} &= \\frac{1}{n-1} X^T X \\\\\n &= \\frac{1}{n-1} (U S V^T)^T(U S V^T) \\\\\n &= \\frac{1}{n-1} VSU^TUSV^T \\\\ \n &= \\frac{1}{n-1} VS^2V^T \\\\\n\\end{align} $$\n\n\nExample\n\n\nData Model\n\n\nIn this example, we demonstrate the use of Principal Component Analysis as a dimensional reduction technique, and in particular \nwe apply it to the problem of image compression. Consider the photo below of my dog who is called \nPoppet\n, \nwhich is 504 pixels wide and 360 pixels high. The pixels that make up this image can be thought of as a \n360x504\n matrix, where the elements \nrepresent the color of each pixel. It is most common in computer graphics to represent such an image using the \nRGBA Color Space\n\nwhere each pixel is defined by a \n32-bit\n integer which has encoded within it four 8-bit \nvalues representing its red, green, blue and alpha intensity.  Since each component within the RGBA value is represented by an \n8-bit sequence, they have a range between 0 and 255 in base-10.\n\n\n\n    \n\n\n\n\n\nWe can load the target image into a Morpheus \nDataFrame\n of RGBA values using the code below. Here we initialize a frame of\ninteger values with the row and column count based on the image dimensions.\n\n\n\n\n\nimport java.net.URL;\nimport javax.imageio.ImageIO;\nimport java.awt.image.BufferedImage;\n\nURL url = getClass().getResource(\"/poppet.jpg\");\nBufferedImage image = ImageIO.read(url);\nint rowCount = image.getHeight()\nint colCount = image.getWidth();\nDataFrame<Integer,Integer> rgbFrame = DataFrame.ofInts(rowCount, colCount, v -> {\n    return image.getRGB(v.colOrdinal(), v.rowOrdinal());\n});\n\n\n\n\nGiven that each pixel is represented by a 32-bit RGBA value, we can decompose the \n360x504\n matrix into a \n360x504x4\n cube of data, \nor a \n360x504x3\n cube if we ignore the alpha channel on the assumption that each pixel is 100% opaque (which is a reasonable assumption\nin this case). In order to decompose the matrix into the red, green and blue components we need to perform some \nbitwise operations\n. \nIn this example, we load the target image using \njava.awt.image.BufferedImage\n which exposes the 32-bit RGB values in the form \nillustrated below, where the first 8 most significant bits represent the alpha channel, the next 8 represent the red value, followed \nby green and then blue.\n\n\n\n    \n\n\n\n\n\nTo extract the 8-bit value representing the red intensity, we first need to shift our string of bits 16 places to the right so that \nthe 8-bits representing the value of red appear in bit positions 0-7 (which before the shift represented the blue intensity). With our \nbit string now in this form, we can bitwise AND it with a base-10 value of 255 or \n0xFF\n in hexadecimal so that all bits in positions \n8-31 become zero, leaving only the value of our red intensity. Similarly, to extract the value of green, we right shift our RGBA bit \nstring by 8, and then bitwise AND with \n0xFF\n. In the case of extracting blue, no bit shifting is required and we simply bitwise AND \nwith \n0xFF\n. The code below generates 3 separate frames to capture the red, green and blue intensities by performing the bitwise \noperations just described.\n\n\n\n\n\nDataFrame<Integer,Integer> red = rgbFrame.mapToDoubles(v -> (v.getInt() >> 16) & 0xFF);\nDataFrame<Integer,Integer> green = rgbFrame.mapToDoubles(v -> (v.getInt() >> 8) & 0xFF);\nDataFrame<Integer,Integer> blue = rgbFrame.mapToDoubles(v -> v.getInt() & 0xFF);\n\n\n\n\nExplained Variance\n\n\nNow that we know how to decompose our image into three \n360x504\n frames representing the red, green and blue intensity of our image \npixels, we can perform Principal Component Analysis on each \nDataFrame\n, and assess the results. Since PCA is all about transforming data \ninto a new basis in which the variance is maximised, it is often useful to get a sense of how much of the variance in the data is explained \nby the first N components. The code below uses the \nrgbFrame\n initialized above, extracts the red, green and blue components, performs \nPCA on each of these frames, and then collects the percent of variance explained by the respective principal components. This data is \nthen trimmed to only include the first 10 components, which is then plotted using a bar chart. Note that in this example we \ntranspose\n \nthe \nDataFrame\n before calling the \npca()\n method as the Morpheus library assumes that the data is an \nnxp\n matrix where \nn >= p\n.\n\n\n\n\n\nURL url = getClass().getResource(\"/poppet.jpg\");\nDataFrame<Integer,Integer> rgbFrame = DataFrame.ofImage(url);\nRange<Integer> rowKeys = Range.of(0, rgbFrame.rowCount());\n\nDataFrame<Integer,String> result = DataFrame.ofDoubles(rowKeys, Array.of(\"Red\", \"Green\", \"Blue\"));\nCollect.<String,DataFrame<Integer,Integer>>asMap(mapping -> {\n    mapping.put(\"Red\", rgbFrame.mapToDoubles(v -> (v.getInt() >> 16) & 0xFF));\n    mapping.put(\"Green\", rgbFrame.mapToDoubles(v -> (v.getInt() >> 8) & 0xFF));\n    mapping.put(\"Blue\", rgbFrame.mapToDoubles(v -> v.getInt() & 0xFF));\n}).forEach((name, color) -> {\n    color.transpose().pca().apply(true, model -> {\n        DataFrame<Integer,Field> eigenFrame = model.getEigenValues();\n        DataFrame<Integer,Field> varPercent = eigenFrame.cols().select(Field.VAR_PERCENT);\n        result.update(varPercent.cols().mapKeys(k -> name), false, false);\n        return Optional.empty();\n    });\n});\n\nDataFrame<Integer,String> chartData = result.rows().select(c -> c.ordinal() < 10).copy();\nChart.create().withBarPlot(chartData.rows().mapKeys(r -> String.valueOf(r.ordinal())), false, chart -> {\n    chart.plot().style(\"Red\").withColor(Color.RED);\n    chart.plot().style(\"Green\").withColor(Color.GREEN);\n    chart.plot().style(\"Blue\").withColor(Color.BLUE);\n    chart.plot().axes().range(0).label().withText(\"Percent of Variance\");\n    chart.plot().axes().domain().label().withText(\"Principal Component\");\n    chart.title().withText(\"Eigen Spectrum (Percent of Explained Variance)\");\n    chart.legend().on().bottom();\n    chart.show();\n});\n\n\n\n\nWe can see from this chart that in the case of the red frame, the first principal component explains around 45% of the variance, for \ngreen it is just under 35% and for blue it is just over 25%. The percentage of the variance explained by subsequent components drops \noff fairly monotonically, and by the time we get to the fifth component, only about 5% of the variance is captured for each of the\ncolors.\n\n\n\n    \n\n\n\n\n\nDimensional Reduction\n\n\nAs per the earlier discussion on PCA theory, we established that the eigenvectors of the covariance matrix of our data are the principal\naxes, and when combined as the columns of a matrix, serve as a transformation into the new basis. If we let \\(X_i\\) represent our \nnxp\n \ninput dataset of either red, green or blue intensities, and \\(V_i\\) our matrix of \npxp\n eigenvectors of the covariance matrix of \\(X_i\\), \nwe can write the transform as follows (where \\(i\\) is either red, green or blue).\n\n\n$$ X_i V_i = Y_i $$\n\n\nThis projection of the original data onto the new basis are called the \nprincipal component scores\n, and are directly accessible from\nthe Morpheus interface named \nDataFramePCA.Model\n via the \ngetScores()\n method. The following code demonstrates how to access these scores,\nand here we assert our expectation of the dimensions of these scores being \nnxp\n or \n504x360\n in this case (since we take the transpose\nof the image).\n\n\n\n\n\nURL url = getClass().getResource(\"/poppet.jpg\");\nDataFrame<Integer,Integer> image = DataFrame.ofImage(url).transpose();\nDataFrame<Integer,Integer> red = image.mapToDoubles(v -> (v.getInt() >> 16) & 0xFF);\nred.pca().apply(true, model -> {\n    DataFrame<Integer,Integer> scores = model.getScores();\n    Assert.assertEquals(scores.rowCount(), 504);\n    Assert.assertEquals(scores.colCount(), 360);\n    return Optional.empty();\n});\n\n\n\n\nSince we know that \\(V_i\\) is an orthogonal matrix by design, once we have transformed to the new basis of \\(Y_i\\) we can transform\nback to the original basis by taking the dot product of \\(Y_i\\) with \\(V_i^T\\) as follows:\n\n\n$$ X_i = Y_i V_i^T $$\n\n\nThe eigenvectors that constitute the columns of \\(V_i\\) are arranged so that the first column is associated with the highest eigenvalue, \nand the last column with the lowest eigenvalue (recall that the eigenvalue represents the variance in the direction of the corresponding \neigenvector). Given that much of the variance in the data will be explained by the leading eigenvectors, we consider truncating \\(V_i\\) \nby only retaining the first N columns. In this case, we can re-write the above expression using a tilde over \\(V_i\\) and \\(Y_i\\) to \nindicate that some information has been lost in this new transform due to the truncation of \\(V_i\\).\n\n\n$$ X_i \\tilde{V_i} = \\tilde{Y_i} $$ \n\n\nThe Morpheus API provides an over-loaded \ngetScores()\n method on \nDataFramePCA.Model\n where we can generate \\(\\tilde{Y_i}\\) by selecting\nonly the first \nj\n columns of \\(V_i\\) as shown below. In this case we assert that the expected dimensions of the scores is \nnxk\n rather\nthan \nnxp\n, where \nk\n is the number of components to include (below we use \nk=10\n).\n\n\n\n\n\nURL url = getClass().getResource(\"/poppet.jpg\");\nDataFrame<Integer,Integer> image = DataFrame.ofImage(url).transpose();\nDataFrame<Integer,Integer> red = image.mapToDoubles(v -> (v.getInt() >> 16) & 0xFF);\nred.pca().apply(true, model -> {\n    DataFrame<Integer,Integer> scores = model.getScores(10);\n    Assert.assertEquals(scores.rowCount(), 504);\n    Assert.assertEquals(scores.colCount(), 10);\n    return Optional.empty();\n});\n\n\n\n\nGiven the truncated scores in the form of \\(\\tilde{Y_i}\\), it turns out that we can project these scores back onto the original basis \nusing \\(\\tilde{V_i}^T\\) much in the same way as described earlier. If we right multiply \\(\\tilde{Y_i}\\) by \\(\\tilde{V_i}^T\\)\nwe get back \nnxp\n data since \\(\\tilde{Y_i}\\) is \nnxk\n and \\(\\tilde{V_i}^T\\) is \nkxp\n, and yields an estimate of our original data\nwe now call \\(\\tilde{X_i}\\) \n\n\n$$ \\tilde{X_i} = \\tilde{Y_i} \\tilde{V_i}^T = X_i \\tilde{V_i} \\tilde{V_i}^T $$ \n\n\nThe Morpheus API provides a convenient API to generate \\(\\tilde{X_i}\\) based on a specified number of components, \nk\n. The following\ncode shows how to access the projection of our original data using only the first \nk=10\n components, and we assert that the dimensions\nof this data matches our original image, namely \n504x360\n (since we transpose the image for reasons discussed earlier).\n\n\n\n\n\nURL url = getClass().getResource(\"/poppet.jpg\");\nDataFrame<Integer,Integer> image = DataFrame.ofImage(url).transpose();\nDataFrame<Integer,Integer> red = image.mapToDoubles(v -> (v.getInt() >> 16) & 0xFF);\nred.pca().apply(true, model -> {\n    DataFrame<Integer,Integer> projection = model.getProjection(10);\n    Assert.assertEquals(projection.rowCount(), 504);\n    Assert.assertEquals(projection.colCount(), 360);\n    return Optional.empty();\n});\n\n\n\n\nWe have established that it is possible to reconstitute an estimate of our original data \\(X_i\\), which we called \\(\\tilde{X_i}\\), \nfrom the principal component scores and a subset of the principal axes associated with the highest variance. The next question is how \nmany columns of \\(V_i\\) need to be retained to ensure \\(\\tilde{X_i}\\) is a reasonable representation of the original data? There \nis no hard rule in this regard, however a common rule of thumb is to select enough components to ensure that 90% of the variance is \ncaptured. Having said that, each problem will be unique, and it will often be useful to generate an eigen spectrum plot as shown above \nto draw some conclusion as to a reasonable initial estimate.\n\n\nIn the case of our image of Poppet, we will demonstrate the effect of retaining an increasing number of principal axes in \\(V_i\\) \nto compute principal component scores, and then to project this back onto the original basis. The images below show a range of scenarios \nwhere we project the image using only 5 components all the way through to 70 components. Note that this is still a small subset of the \ntotal number of components, namely 360 in this case, but it is clear that once we include up to 50 components, the transformed image \nis almost indistinguishable from the original, at least to the human eye.\n\n\n\n    \n\n        \n\n            \n\n            \n5 Principal Components\n\n        \n\n        \n\n            \n\n            \n10 Principal Components\n\n        \n\n        \n\n            \n\n            \n15 Principal Components\n\n        \n\n    \n\n    \n\n        \n\n            \n\n            \n20 Principal Components\n\n        \n\n        \n\n            \n\n            \n25 Principal Components\n\n        \n\n        \n\n            \n\n            \n30 Principal Components\n\n        \n\n    \n\n    \n\n        \n\n            \n\n            \n35 Principal Components\n\n        \n\n        \n\n            \n\n            \n40 Principal Components\n\n        \n\n        \n\n            \n\n            \n45 Principal Components\n\n        \n\n    \n\n    \n\n        \n\n            \n\n            \n50 Principal Components\n\n        \n\n        \n\n            \n\n            \n55 Principal Components\n\n        \n\n        \n\n            \n\n            \n60 Principal Components\n\n        \n\n    \n\n    \n\n        \n\n            \n\n            \n65 Principal Components\n\n        \n\n        \n\n            \n\n            \n70 Principal Components\n\n        \n\n        \n\n            \n\n            \n360 Principal Components\n\n        \n\n    \n\n\n\n\n\nThe final image in the table above is essentially the same as the original since we retain all components and so \\(V_i {V_i}^T = I \\)\ngiven that we know \\(V_i\\) is an orthogonal matrix by design. The code to generate this array of images is shown below. Here we load the \noriginal image into a Morpheus \nDataFrame\n, and then proceed to decompose it into red, green and blue components, perform PCA on each color, \nproject the image as described above using only a subset of the principal axes associated with highest variance, and then record the resulting \nprojection back out as an image file. \n\n\n\n\n\n//Load image from classpath\nURL url = getClass().getResource(\"/poppet.jpg\");\n\n//Re-create PCA reduced image while retaining different number of principal components\nArray.of(5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 360).forEach(nComp -> {\n\n    //Initialize the **transpose** of image as we need nxp frame where n >= p\n    DataFrame<Integer,Integer> rgbFrame = DataFrame.ofImage(url).transpose();\n\n    //Create 3 frames from RGB data, one for red, green and blue\n    DataFrame<Integer,Integer> red = rgbFrame.mapToDoubles(v -> (v.getInt() >> 16) & 0xFF);\n    DataFrame<Integer,Integer> green = rgbFrame.mapToDoubles(v -> (v.getInt() >> 8) & 0xFF);\n    DataFrame<Integer,Integer> blue = rgbFrame.mapToDoubles(v -> v.getInt() & 0xFF);\n\n    //Perform PCA on each color frame, and project using only first N principal components\n    Stream.of(red, green, blue).parallel().forEach(color -> {\n        color.pca().apply(true, model -> {\n            DataFrame<Integer,Integer> projection = model.getProjection(nComp);\n            projection.cap(true).doubles(0, 255);  //cap values between 0 and 255\n            color.update(projection, false, false);\n            return null;\n        });\n    });\n\n    //Apply reduced RBG values onto the original frame so we don't need to allocate memory\n    rgbFrame.applyInts(v -> {\n        int i = v.rowOrdinal();\n        int j = v.colOrdinal();\n        int r = (int)red.data().getDouble(i,j);\n        int g = (int)green.data().getDouble(i,j);\n        int b = (int)blue.data().getDouble(i,j);\n        return ((0xFF) << 24) | ((r & 0xFF) << 16) | ((g & 0xFF) << 8) | ((b & 0xFF));\n    });\n\n    //Create reduced image from **transpose** of the DataFrame to get back original orientation\n    int width = rgbFrame.rowCount();\n    int height = rgbFrame.colCount();\n    BufferedImage transformed = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB);\n    rgbFrame.forEachValue(v -> {\n        int i = v.colOrdinal();\n        int j = v.rowOrdinal();\n        int rgb = v.getInt();\n        transformed.setRGB(j, i, rgb);\n    });\n\n    try {\n        File outputfile = new File(\"/Users/witdxav/temp/poppet-\" + nComp + \".jpg\");\n        outputfile.getParentFile().mkdirs();\n        ImageIO.write(transformed, \"jpg\", outputfile);\n    } catch (Exception ex) {\n        throw new RuntimeException(\"Failed to record image result\", ex);\n    }\n});\n\n\n\n\nCompression Story\n\n\nThe dimensions of our original input data \\(X_i\\) is \n504x360\n which generates a covariance matrix with dimensions \n360x360\n. \nPerforming PCA on this data yields a set of \n360\n eigenvectors each of the same length, implying that the non-truncated version of \n\\(V_i\\) is also \n360x360\n. If we decide to only keep the first \nk\n columns of \\(V_i\\) to create \\(\\tilde{V_i}\\), then the resulting \ndimensions of \\(\\tilde{Y_i}\\) will be \n504xk\n. If \nk\n can be significantly smaller than \n360\n (the height of the original image) then \n\\(\\tilde{Y_i}\\) would require much less storage space. We obviously also need to store \\(\\tilde{V_i}\\) so that we can reconstitute \nour estimate of the data \\(\\tilde{X_i}\\), but the expectation is that \nk\n can be small enough so that the storage required for two \nsmaller matrices is less than that required for the original image. \n\n\nIn our example, the original image requires \n181,440\n 32-bit RGBA values given it has dimensions of \n504x360\n pixels. The table below\nsummarizes the total number of elements required to store \\(\\tilde{Y_i}\\) and \\(\\tilde{V_i}\\) for various values of k ranging\nfrom 5 through 60. We need to multiply this by 3 since we need to store a red, green and blue version of these matrices. The final\ncolumn indicates the percent reduction on the original number of elements we achieve, and since the image reconstituted by retaining\nonly the first 45 components is almost indistinguishable from the original, we can achieve 35% compression in that case. \n\n\n\n\n\n\n\n\nk\n\n\n\\(\\tilde{Y_i}\\)\n\n\n\\(\\tilde{V_i}\\)\n\n\nTotal\n\n\nTotal x 3\n\n\nCompression\n\n\n\n\n\n\n\n\n\n\n5\n\n\n504x5  =  2,520\n\n\n360x5  =  1,800\n\n\n4,320\n\n\n12,960\n\n\n92.86%\n\n\n\n\n\n\n10\n\n\n504x10 =  5,040\n\n\n360x10 =  3,600\n\n\n8,640\n\n\n25,920\n\n\n85.71%\n\n\n\n\n\n\n15\n\n\n504x15 =  7,560\n\n\n360x15 =  5,400\n\n\n12,960\n\n\n38,880\n\n\n78.57%\n\n\n\n\n\n\n20\n\n\n504x20 = 10,080\n\n\n360x20 =  7,200\n\n\n17,280\n\n\n51,840\n\n\n71.43%\n\n\n\n\n\n\n25\n\n\n504x25 = 12,600\n\n\n360x25 =  9,000\n\n\n21,600\n\n\n64,800\n\n\n64.29%\n\n\n\n\n\n\n30\n\n\n504x30 = 15,120\n\n\n360x30 = 10,800\n\n\n25,920\n\n\n77,760\n\n\n57.14%\n\n\n\n\n\n\n35\n\n\n504x35 = 17,640\n\n\n360x35 = 12,600\n\n\n30,240\n\n\n90,720\n\n\n50.00%\n\n\n\n\n\n\n40\n\n\n504x40 = 20,160\n\n\n360x40 = 14,400\n\n\n34,560\n\n\n103,680\n\n\n42.86%\n\n\n\n\n\n\n45\n\n\n504x45 = 22,680\n\n\n360x45 = 16,200\n\n\n38,880\n\n\n116,640\n\n\n35.71%\n\n\n\n\n\n\n50\n\n\n504x50 = 25,200\n\n\n360x50 = 18,000\n\n\n43,200\n\n\n129,600\n\n\n28.57%\n\n\n\n\n\n\n55\n\n\n504x55 = 27,720\n\n\n360x55 = 19,800\n\n\n47,520\n\n\n142,560\n\n\n21.43%\n\n\n\n\n\n\n60\n\n\n504x60 = 30,240\n\n\n360x60 = 21,600\n\n\n51,840\n\n\n155,520\n\n\n7.14%",
            "title": "Principal Component Analysis"
        },
        {
            "location": "/analysis/pca/#principal-component-analysis",
            "text": "Principal Component Analysis  (PCA) is a statistical tool \nused in data analysis and also for building predictive models. The technique involves transforming a dataset into a new  basis  whereby the transformed data is uncorrelated. The transformed \nbasis, which can be represented by an  orthogonal matrix , defines the \nPrincipal Components of the original dataset. These basis vectors are usually ordered so that the first principal component \nis the one that accounts for the largest variance in the data, and the last component accounts for the least variance.  PCA is also often referred to as a  dimensional reduction  technique\nin the sense that by dropping components that account for a negligible amount of variance in the original data, we can linearly\nmap the data into a lower dimensional space without loosing material information. The assumption here is that the variability\nin the data represents the essential dynamics we are trying to understand, so dropping dimensions with negligible variance\nresults in minimal loss of information.  The following sections introduces some PCA theory, and then proceeds to illustrate an example of how to use the Morpheus API\nto perform PCA on a dataset. Only a superficial overview of the theory is covered below, so for a more detailed treatment of \nthe topic I would suggest a Google search, or perhaps  this \ntutorial as a primer.",
            "title": "Principal Component Analysis"
        },
        {
            "location": "/analysis/pca/#theory",
            "text": "As a data analysis technique, PCA begins with the definition of our data which in general can be described in two dimensions,\nnamely the number of observations and the number of measurements. Such data can be represented by an  nxp  matrix where  n  \nrepresents the number of observations for each measurement, and  p  represents the number of measurements being recorded.   The dimensions in which we record the observations that constitute our data are assumed to be a naive basis, since we do not \nactually understand the true dynamics of the system we are investigating (hence the analysis). Having said that, our hope is \nthat while our measurements may be recorded in a naive basis, they are informative enough so that we can compute a new basis \nthat maximises the signal to noise ratio and removes any redundancy in the data, enabling us to better understand its true \ndynamics.   With this in mind, let us assume that there exists an  orthogonal matrix  \nV of dimensions  pxp  that can transform our data X into Y such that the covariance matrix of Y (denoted by \\(\\Sigma_{y}\\)) \nis diagonal.  $$ X V = Y $$  The question then is how to find V? We can tackle this by working backwards based on our desire that the transformed data Y \nhas a diagonal covariance matrix, given our motivation to remove noise and redundancy from our dataset. Assuming that Y is  centered  or  demeaned , we can define the covariance of Y as follows:  $$ \\Sigma_{Y} = \\frac{1}{n-1} Y^T Y $$  Given our transform \\(X V = Y \\) we can express the covariance of Y in terms V and X as follows:  $$ \\begin{align}\n\\Sigma_{Y} &= \\frac{1}{n-1} Y^T Y  \\\\\n &= \\frac{1}{n-1}(XV)^T(XV) \\\\\n &= \\frac{1}{n-1}V^T X^T X V \\\\\n\\end{align} $$  Let us define a new matrix A, which by definition is a  pxp   symmetric matrix  as:  $$ A = \\frac{1}{n-1} X^T X $$  We can therefore re-write our earlier expression for \\(\\Sigma_{Y}\\) in terms of A as:  $$ \\Sigma_{Y} = V^T A V $$  In the next section, we illustrate how we can choose \\(V\\) such that we diagonalize \\(\\Sigma_{Y}\\).",
            "title": "Theory"
        },
        {
            "location": "/analysis/pca/#eigen-decomposition",
            "text": "Based on our earlier discussion, we know that we need to choose a transform matrix V such that \\(V^TAV = D\\) where \\(D\\) \nis a diagonal matrix. Given that \\(A\\) is symmetric, we know that we can factorize it using an  Eigendecomposition  \ninto an orthogonal matrix of its  eigenvectors  and a diagonal \nmatrix of its  eigenvalues :  $$ A = Q \\Lambda Q^{-1} $$  In this factorization the column vectors of \\(Q\\) are the eigenvectors of \\(A\\) and the diagonal elements of \\(\\Lambda\\) \nare the eigenvalues of \\(A\\). Recall that an eigenvector of a matrix is one which is only scaled and not rotated when operated \non by that matrix. This is otherwise stated as \\(Av = \\lambda v \\) where \\(v\\) is an  px1  eigenvector of A and \\(\\lambda\\) is \nthe corresponding eigenvalue, which is a scalar. We can therefore expand this to say that \\(A Q = Q \\Lambda \\) which then \nimplies that \\(A = Q \\Lambda Q^{-1}\\).  If we choose our matrix \\(V = Q\\) and noting that Q is an orthogonal matrix such that \\(Q^{-1} = Q^T \\) we can plug these back \ninto the expression for the covariance matrix of Y to yield the following:  $$ \\begin{align}\n\\Sigma_{Y} &= V^T A V \\\\\n &= V^T (V \\Lambda V^{-1}) V \\\\\n &= (V^TV)\\Lambda(V^{-1}V) \\\\\n &= (V^{-1}V)\\Lambda(V^{-1}V) \\\\\n &= \\Lambda \\\\\n\\end{align} $$  By choosing the transform matrix V to be the eigenvectors of \\(A \\) (which in essence is the covariance matrix of our data\non the assumption that \\(X\\) is centered), we end up diagonalizing the covariance matrix of the transformed dataset, which is \nthe ultimate objective of our PCA. These eigenvectors essentially define the new basis along which variance in the data is maximised, \nand the corresponding eigenvalues define the magnitude of this variance. As noted earlier, the eigenvectors or principal components \nare usually ordered so that the first component accounts for the largest variance (ie the largest eigenvalue), and the last component \naccounts for the smallest variance (ie the smallest eigenvalue).",
            "title": "Eigen Decomposition"
        },
        {
            "location": "/analysis/pca/#singular-value-decomposition",
            "text": "The previous section demonstrated that an eigen decomposition of the covariance matrix of \\(X\\) yields the set of eigenvectors \\(V\\) \nthat define the principal axes of our data. When we transform our data using \\(V\\) the resulting dataset has a diagonal covariance\nmatrix which reflects that our new basis maximises the signal to noise ratio and/or removes any redundancy.  The Morpheus library supports solving PCA in this way, but by default, it performs a  Singular Value Decomposition \n(SVD) of the  centered  or  demeaned  data in \\(X\\), as this generally offers better numerical stability and also tends to be faster\nthen an eigendecomposition of the covariance matrix of \\(X\\). An SVD of \\(X\\) on the assumption that \\(X\\) is centered yields  $$ X = U S V^T $$  where S is a diagonal matrix of singular values, and the columns of \\(U\\) and \\(V\\) are called the left-singular vectors and right-singular \nvectors of \\(X\\) , respectively. If we substitute the SVD decomposition into the expression for the covariance of \\(X\\) as below, we can \nsee that the solution is essentially equivalent to above, however in this case the eigenvalues are equivalent to the square of the singular \nvalues divided by \\(n-1\\).  $$ \\begin{align}\n\\Sigma_{X} &= \\frac{1}{n-1} X^T X \\\\\n &= \\frac{1}{n-1} (U S V^T)^T(U S V^T) \\\\\n &= \\frac{1}{n-1} VSU^TUSV^T \\\\ \n &= \\frac{1}{n-1} VS^2V^T \\\\\n\\end{align} $$",
            "title": "Singular Value Decomposition"
        },
        {
            "location": "/analysis/pca/#example",
            "text": "",
            "title": "Example"
        },
        {
            "location": "/analysis/pca/#data-model",
            "text": "In this example, we demonstrate the use of Principal Component Analysis as a dimensional reduction technique, and in particular \nwe apply it to the problem of image compression. Consider the photo below of my dog who is called  Poppet , \nwhich is 504 pixels wide and 360 pixels high. The pixels that make up this image can be thought of as a  360x504  matrix, where the elements \nrepresent the color of each pixel. It is most common in computer graphics to represent such an image using the  RGBA Color Space \nwhere each pixel is defined by a  32-bit  integer which has encoded within it four 8-bit \nvalues representing its red, green, blue and alpha intensity.  Since each component within the RGBA value is represented by an \n8-bit sequence, they have a range between 0 and 255 in base-10.  \n       We can load the target image into a Morpheus  DataFrame  of RGBA values using the code below. Here we initialize a frame of\ninteger values with the row and column count based on the image dimensions.   import java.net.URL;\nimport javax.imageio.ImageIO;\nimport java.awt.image.BufferedImage;\n\nURL url = getClass().getResource(\"/poppet.jpg\");\nBufferedImage image = ImageIO.read(url);\nint rowCount = image.getHeight()\nint colCount = image.getWidth();\nDataFrame<Integer,Integer> rgbFrame = DataFrame.ofInts(rowCount, colCount, v -> {\n    return image.getRGB(v.colOrdinal(), v.rowOrdinal());\n});  Given that each pixel is represented by a 32-bit RGBA value, we can decompose the  360x504  matrix into a  360x504x4  cube of data, \nor a  360x504x3  cube if we ignore the alpha channel on the assumption that each pixel is 100% opaque (which is a reasonable assumption\nin this case). In order to decompose the matrix into the red, green and blue components we need to perform some  bitwise operations . \nIn this example, we load the target image using  java.awt.image.BufferedImage  which exposes the 32-bit RGB values in the form \nillustrated below, where the first 8 most significant bits represent the alpha channel, the next 8 represent the red value, followed \nby green and then blue.  \n       To extract the 8-bit value representing the red intensity, we first need to shift our string of bits 16 places to the right so that \nthe 8-bits representing the value of red appear in bit positions 0-7 (which before the shift represented the blue intensity). With our \nbit string now in this form, we can bitwise AND it with a base-10 value of 255 or  0xFF  in hexadecimal so that all bits in positions \n8-31 become zero, leaving only the value of our red intensity. Similarly, to extract the value of green, we right shift our RGBA bit \nstring by 8, and then bitwise AND with  0xFF . In the case of extracting blue, no bit shifting is required and we simply bitwise AND \nwith  0xFF . The code below generates 3 separate frames to capture the red, green and blue intensities by performing the bitwise \noperations just described.   DataFrame<Integer,Integer> red = rgbFrame.mapToDoubles(v -> (v.getInt() >> 16) & 0xFF);\nDataFrame<Integer,Integer> green = rgbFrame.mapToDoubles(v -> (v.getInt() >> 8) & 0xFF);\nDataFrame<Integer,Integer> blue = rgbFrame.mapToDoubles(v -> v.getInt() & 0xFF);",
            "title": "Data Model"
        },
        {
            "location": "/analysis/pca/#explained-variance",
            "text": "Now that we know how to decompose our image into three  360x504  frames representing the red, green and blue intensity of our image \npixels, we can perform Principal Component Analysis on each  DataFrame , and assess the results. Since PCA is all about transforming data \ninto a new basis in which the variance is maximised, it is often useful to get a sense of how much of the variance in the data is explained \nby the first N components. The code below uses the  rgbFrame  initialized above, extracts the red, green and blue components, performs \nPCA on each of these frames, and then collects the percent of variance explained by the respective principal components. This data is \nthen trimmed to only include the first 10 components, which is then plotted using a bar chart. Note that in this example we  transpose  \nthe  DataFrame  before calling the  pca()  method as the Morpheus library assumes that the data is an  nxp  matrix where  n >= p .   URL url = getClass().getResource(\"/poppet.jpg\");\nDataFrame<Integer,Integer> rgbFrame = DataFrame.ofImage(url);\nRange<Integer> rowKeys = Range.of(0, rgbFrame.rowCount());\n\nDataFrame<Integer,String> result = DataFrame.ofDoubles(rowKeys, Array.of(\"Red\", \"Green\", \"Blue\"));\nCollect.<String,DataFrame<Integer,Integer>>asMap(mapping -> {\n    mapping.put(\"Red\", rgbFrame.mapToDoubles(v -> (v.getInt() >> 16) & 0xFF));\n    mapping.put(\"Green\", rgbFrame.mapToDoubles(v -> (v.getInt() >> 8) & 0xFF));\n    mapping.put(\"Blue\", rgbFrame.mapToDoubles(v -> v.getInt() & 0xFF));\n}).forEach((name, color) -> {\n    color.transpose().pca().apply(true, model -> {\n        DataFrame<Integer,Field> eigenFrame = model.getEigenValues();\n        DataFrame<Integer,Field> varPercent = eigenFrame.cols().select(Field.VAR_PERCENT);\n        result.update(varPercent.cols().mapKeys(k -> name), false, false);\n        return Optional.empty();\n    });\n});\n\nDataFrame<Integer,String> chartData = result.rows().select(c -> c.ordinal() < 10).copy();\nChart.create().withBarPlot(chartData.rows().mapKeys(r -> String.valueOf(r.ordinal())), false, chart -> {\n    chart.plot().style(\"Red\").withColor(Color.RED);\n    chart.plot().style(\"Green\").withColor(Color.GREEN);\n    chart.plot().style(\"Blue\").withColor(Color.BLUE);\n    chart.plot().axes().range(0).label().withText(\"Percent of Variance\");\n    chart.plot().axes().domain().label().withText(\"Principal Component\");\n    chart.title().withText(\"Eigen Spectrum (Percent of Explained Variance)\");\n    chart.legend().on().bottom();\n    chart.show();\n});  We can see from this chart that in the case of the red frame, the first principal component explains around 45% of the variance, for \ngreen it is just under 35% and for blue it is just over 25%. The percentage of the variance explained by subsequent components drops \noff fairly monotonically, and by the time we get to the fifth component, only about 5% of the variance is captured for each of the\ncolors.",
            "title": "Explained Variance"
        },
        {
            "location": "/analysis/pca/#dimensional-reduction",
            "text": "As per the earlier discussion on PCA theory, we established that the eigenvectors of the covariance matrix of our data are the principal\naxes, and when combined as the columns of a matrix, serve as a transformation into the new basis. If we let \\(X_i\\) represent our  nxp  \ninput dataset of either red, green or blue intensities, and \\(V_i\\) our matrix of  pxp  eigenvectors of the covariance matrix of \\(X_i\\), \nwe can write the transform as follows (where \\(i\\) is either red, green or blue).  $$ X_i V_i = Y_i $$  This projection of the original data onto the new basis are called the  principal component scores , and are directly accessible from\nthe Morpheus interface named  DataFramePCA.Model  via the  getScores()  method. The following code demonstrates how to access these scores,\nand here we assert our expectation of the dimensions of these scores being  nxp  or  504x360  in this case (since we take the transpose\nof the image).   URL url = getClass().getResource(\"/poppet.jpg\");\nDataFrame<Integer,Integer> image = DataFrame.ofImage(url).transpose();\nDataFrame<Integer,Integer> red = image.mapToDoubles(v -> (v.getInt() >> 16) & 0xFF);\nred.pca().apply(true, model -> {\n    DataFrame<Integer,Integer> scores = model.getScores();\n    Assert.assertEquals(scores.rowCount(), 504);\n    Assert.assertEquals(scores.colCount(), 360);\n    return Optional.empty();\n});  Since we know that \\(V_i\\) is an orthogonal matrix by design, once we have transformed to the new basis of \\(Y_i\\) we can transform\nback to the original basis by taking the dot product of \\(Y_i\\) with \\(V_i^T\\) as follows:  $$ X_i = Y_i V_i^T $$  The eigenvectors that constitute the columns of \\(V_i\\) are arranged so that the first column is associated with the highest eigenvalue, \nand the last column with the lowest eigenvalue (recall that the eigenvalue represents the variance in the direction of the corresponding \neigenvector). Given that much of the variance in the data will be explained by the leading eigenvectors, we consider truncating \\(V_i\\) \nby only retaining the first N columns. In this case, we can re-write the above expression using a tilde over \\(V_i\\) and \\(Y_i\\) to \nindicate that some information has been lost in this new transform due to the truncation of \\(V_i\\).  $$ X_i \\tilde{V_i} = \\tilde{Y_i} $$   The Morpheus API provides an over-loaded  getScores()  method on  DataFramePCA.Model  where we can generate \\(\\tilde{Y_i}\\) by selecting\nonly the first  j  columns of \\(V_i\\) as shown below. In this case we assert that the expected dimensions of the scores is  nxk  rather\nthan  nxp , where  k  is the number of components to include (below we use  k=10 ).   URL url = getClass().getResource(\"/poppet.jpg\");\nDataFrame<Integer,Integer> image = DataFrame.ofImage(url).transpose();\nDataFrame<Integer,Integer> red = image.mapToDoubles(v -> (v.getInt() >> 16) & 0xFF);\nred.pca().apply(true, model -> {\n    DataFrame<Integer,Integer> scores = model.getScores(10);\n    Assert.assertEquals(scores.rowCount(), 504);\n    Assert.assertEquals(scores.colCount(), 10);\n    return Optional.empty();\n});  Given the truncated scores in the form of \\(\\tilde{Y_i}\\), it turns out that we can project these scores back onto the original basis \nusing \\(\\tilde{V_i}^T\\) much in the same way as described earlier. If we right multiply \\(\\tilde{Y_i}\\) by \\(\\tilde{V_i}^T\\)\nwe get back  nxp  data since \\(\\tilde{Y_i}\\) is  nxk  and \\(\\tilde{V_i}^T\\) is  kxp , and yields an estimate of our original data\nwe now call \\(\\tilde{X_i}\\)   $$ \\tilde{X_i} = \\tilde{Y_i} \\tilde{V_i}^T = X_i \\tilde{V_i} \\tilde{V_i}^T $$   The Morpheus API provides a convenient API to generate \\(\\tilde{X_i}\\) based on a specified number of components,  k . The following\ncode shows how to access the projection of our original data using only the first  k=10  components, and we assert that the dimensions\nof this data matches our original image, namely  504x360  (since we transpose the image for reasons discussed earlier).   URL url = getClass().getResource(\"/poppet.jpg\");\nDataFrame<Integer,Integer> image = DataFrame.ofImage(url).transpose();\nDataFrame<Integer,Integer> red = image.mapToDoubles(v -> (v.getInt() >> 16) & 0xFF);\nred.pca().apply(true, model -> {\n    DataFrame<Integer,Integer> projection = model.getProjection(10);\n    Assert.assertEquals(projection.rowCount(), 504);\n    Assert.assertEquals(projection.colCount(), 360);\n    return Optional.empty();\n});  We have established that it is possible to reconstitute an estimate of our original data \\(X_i\\), which we called \\(\\tilde{X_i}\\), \nfrom the principal component scores and a subset of the principal axes associated with the highest variance. The next question is how \nmany columns of \\(V_i\\) need to be retained to ensure \\(\\tilde{X_i}\\) is a reasonable representation of the original data? There \nis no hard rule in this regard, however a common rule of thumb is to select enough components to ensure that 90% of the variance is \ncaptured. Having said that, each problem will be unique, and it will often be useful to generate an eigen spectrum plot as shown above \nto draw some conclusion as to a reasonable initial estimate.  In the case of our image of Poppet, we will demonstrate the effect of retaining an increasing number of principal axes in \\(V_i\\) \nto compute principal component scores, and then to project this back onto the original basis. The images below show a range of scenarios \nwhere we project the image using only 5 components all the way through to 70 components. Note that this is still a small subset of the \ntotal number of components, namely 360 in this case, but it is clear that once we include up to 50 components, the transformed image \nis almost indistinguishable from the original, at least to the human eye.  \n     \n         \n             \n             5 Principal Components \n         \n         \n             \n             10 Principal Components \n         \n         \n             \n             15 Principal Components \n         \n     \n     \n         \n             \n             20 Principal Components \n         \n         \n             \n             25 Principal Components \n         \n         \n             \n             30 Principal Components \n         \n     \n     \n         \n             \n             35 Principal Components \n         \n         \n             \n             40 Principal Components \n         \n         \n             \n             45 Principal Components \n         \n     \n     \n         \n             \n             50 Principal Components \n         \n         \n             \n             55 Principal Components \n         \n         \n             \n             60 Principal Components \n         \n     \n     \n         \n             \n             65 Principal Components \n         \n         \n             \n             70 Principal Components \n         \n         \n             \n             360 Principal Components \n         \n       The final image in the table above is essentially the same as the original since we retain all components and so \\(V_i {V_i}^T = I \\)\ngiven that we know \\(V_i\\) is an orthogonal matrix by design. The code to generate this array of images is shown below. Here we load the \noriginal image into a Morpheus  DataFrame , and then proceed to decompose it into red, green and blue components, perform PCA on each color, \nproject the image as described above using only a subset of the principal axes associated with highest variance, and then record the resulting \nprojection back out as an image file.    //Load image from classpath\nURL url = getClass().getResource(\"/poppet.jpg\");\n\n//Re-create PCA reduced image while retaining different number of principal components\nArray.of(5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 360).forEach(nComp -> {\n\n    //Initialize the **transpose** of image as we need nxp frame where n >= p\n    DataFrame<Integer,Integer> rgbFrame = DataFrame.ofImage(url).transpose();\n\n    //Create 3 frames from RGB data, one for red, green and blue\n    DataFrame<Integer,Integer> red = rgbFrame.mapToDoubles(v -> (v.getInt() >> 16) & 0xFF);\n    DataFrame<Integer,Integer> green = rgbFrame.mapToDoubles(v -> (v.getInt() >> 8) & 0xFF);\n    DataFrame<Integer,Integer> blue = rgbFrame.mapToDoubles(v -> v.getInt() & 0xFF);\n\n    //Perform PCA on each color frame, and project using only first N principal components\n    Stream.of(red, green, blue).parallel().forEach(color -> {\n        color.pca().apply(true, model -> {\n            DataFrame<Integer,Integer> projection = model.getProjection(nComp);\n            projection.cap(true).doubles(0, 255);  //cap values between 0 and 255\n            color.update(projection, false, false);\n            return null;\n        });\n    });\n\n    //Apply reduced RBG values onto the original frame so we don't need to allocate memory\n    rgbFrame.applyInts(v -> {\n        int i = v.rowOrdinal();\n        int j = v.colOrdinal();\n        int r = (int)red.data().getDouble(i,j);\n        int g = (int)green.data().getDouble(i,j);\n        int b = (int)blue.data().getDouble(i,j);\n        return ((0xFF) << 24) | ((r & 0xFF) << 16) | ((g & 0xFF) << 8) | ((b & 0xFF));\n    });\n\n    //Create reduced image from **transpose** of the DataFrame to get back original orientation\n    int width = rgbFrame.rowCount();\n    int height = rgbFrame.colCount();\n    BufferedImage transformed = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB);\n    rgbFrame.forEachValue(v -> {\n        int i = v.colOrdinal();\n        int j = v.rowOrdinal();\n        int rgb = v.getInt();\n        transformed.setRGB(j, i, rgb);\n    });\n\n    try {\n        File outputfile = new File(\"/Users/witdxav/temp/poppet-\" + nComp + \".jpg\");\n        outputfile.getParentFile().mkdirs();\n        ImageIO.write(transformed, \"jpg\", outputfile);\n    } catch (Exception ex) {\n        throw new RuntimeException(\"Failed to record image result\", ex);\n    }\n});",
            "title": "Dimensional Reduction"
        },
        {
            "location": "/analysis/pca/#compression-story",
            "text": "The dimensions of our original input data \\(X_i\\) is  504x360  which generates a covariance matrix with dimensions  360x360 . \nPerforming PCA on this data yields a set of  360  eigenvectors each of the same length, implying that the non-truncated version of \n\\(V_i\\) is also  360x360 . If we decide to only keep the first  k  columns of \\(V_i\\) to create \\(\\tilde{V_i}\\), then the resulting \ndimensions of \\(\\tilde{Y_i}\\) will be  504xk . If  k  can be significantly smaller than  360  (the height of the original image) then \n\\(\\tilde{Y_i}\\) would require much less storage space. We obviously also need to store \\(\\tilde{V_i}\\) so that we can reconstitute \nour estimate of the data \\(\\tilde{X_i}\\), but the expectation is that  k  can be small enough so that the storage required for two \nsmaller matrices is less than that required for the original image.   In our example, the original image requires  181,440  32-bit RGBA values given it has dimensions of  504x360  pixels. The table below\nsummarizes the total number of elements required to store \\(\\tilde{Y_i}\\) and \\(\\tilde{V_i}\\) for various values of k ranging\nfrom 5 through 60. We need to multiply this by 3 since we need to store a red, green and blue version of these matrices. The final\ncolumn indicates the percent reduction on the original number of elements we achieve, and since the image reconstituted by retaining\nonly the first 45 components is almost indistinguishable from the original, we can achieve 35% compression in that case.      k  \\(\\tilde{Y_i}\\)  \\(\\tilde{V_i}\\)  Total  Total x 3  Compression      5  504x5  =  2,520  360x5  =  1,800  4,320  12,960  92.86%    10  504x10 =  5,040  360x10 =  3,600  8,640  25,920  85.71%    15  504x15 =  7,560  360x15 =  5,400  12,960  38,880  78.57%    20  504x20 = 10,080  360x20 =  7,200  17,280  51,840  71.43%    25  504x25 = 12,600  360x25 =  9,000  21,600  64,800  64.29%    30  504x30 = 15,120  360x30 = 10,800  25,920  77,760  57.14%    35  504x35 = 17,640  360x35 = 12,600  30,240  90,720  50.00%    40  504x40 = 20,160  360x40 = 14,400  34,560  103,680  42.86%    45  504x45 = 22,680  360x45 = 16,200  38,880  116,640  35.71%    50  504x50 = 25,200  360x50 = 18,000  43,200  129,600  28.57%    55  504x55 = 27,720  360x55 = 19,800  47,520  142,560  21.43%    60  504x60 = 30,240  360x60 = 21,600  51,840  155,520  7.14%",
            "title": "Compression Story"
        },
        {
            "location": "/analysis/linearalgebra/",
            "text": "Linear Algebra\n\n\nTo be completed...\n\n\nIntroduction\n\n\nTo be completed...\n\n\nBasic Operations\n\n\nTo be completed...\n\n\nMatrix Decompositions\n\n\nTo be completed...\n\n\nLower/Upper Decomposition\n\n\nTo be completed...\n\n\nEigen Value Decomposition\n\n\nTo be completed...\n\n\nSingular Value Decomposition\n\n\nTo be completed...\n\n\nCholesky Decomposition\n\n\nTo be completed...",
            "title": "Linear Algebra"
        },
        {
            "location": "/analysis/linearalgebra/#linear-algebra",
            "text": "To be completed...",
            "title": "Linear Algebra"
        },
        {
            "location": "/analysis/linearalgebra/#introduction",
            "text": "To be completed...",
            "title": "Introduction"
        },
        {
            "location": "/analysis/linearalgebra/#basic-operations",
            "text": "To be completed...",
            "title": "Basic Operations"
        },
        {
            "location": "/analysis/linearalgebra/#matrix-decompositions",
            "text": "To be completed...",
            "title": "Matrix Decompositions"
        },
        {
            "location": "/analysis/linearalgebra/#lowerupper-decomposition",
            "text": "To be completed...",
            "title": "Lower/Upper Decomposition"
        },
        {
            "location": "/analysis/linearalgebra/#eigen-value-decomposition",
            "text": "To be completed...",
            "title": "Eigen Value Decomposition"
        },
        {
            "location": "/analysis/linearalgebra/#singular-value-decomposition",
            "text": "To be completed...",
            "title": "Singular Value Decomposition"
        },
        {
            "location": "/analysis/linearalgebra/#cholesky-decomposition",
            "text": "To be completed...",
            "title": "Cholesky Decomposition"
        },
        {
            "location": "/analysis/timeseries/",
            "text": "Time Series Analysis\n\n\nTo be completed...\n\n\nIntroduction\n\n\nTo be completed...\n\n\nAutoregressive Model (AR)\n\n\nTo be completed...\n\n\nAutoregressive Moving Average (ARMA)\n\n\nTo be completed...\n\n\nAutoregressive Integrated Moving Average (ARIMA)\n\n\nTo be completed...\n\n\nGeneralized Autoregressive Conditional Heteroskedasticity (GARCH)\n\n\nTo be completed...",
            "title": "Time Series Analysis"
        },
        {
            "location": "/analysis/timeseries/#time-series-analysis",
            "text": "To be completed...",
            "title": "Time Series Analysis"
        },
        {
            "location": "/analysis/timeseries/#introduction",
            "text": "To be completed...",
            "title": "Introduction"
        },
        {
            "location": "/analysis/timeseries/#autoregressive-model-ar",
            "text": "To be completed...",
            "title": "Autoregressive Model (AR)"
        },
        {
            "location": "/analysis/timeseries/#autoregressive-moving-average-arma",
            "text": "To be completed...",
            "title": "Autoregressive Moving Average (ARMA)"
        },
        {
            "location": "/analysis/timeseries/#autoregressive-integrated-moving-average-arima",
            "text": "To be completed...",
            "title": "Autoregressive Integrated Moving Average (ARIMA)"
        },
        {
            "location": "/analysis/timeseries/#generalized-autoregressive-conditional-heteroskedasticity-garch",
            "text": "To be completed...",
            "title": "Generalized Autoregressive Conditional Heteroskedasticity (GARCH)"
        },
        {
            "location": "/examples/mpt/",
            "text": "Modern Portfolio Theory\n\n\nIntroduction\n\n\nIn 1952, \nHarry Markowitz\n published a seminal paper in the Journal of Finance \ntitled \nPortfolio Selection\n, where he first introduced \nModern Portfolio Theory\n \n(MPT), which quantifies the \nrisk\n and \nreturn\n trade-off when constructing portfolios of risky assets. The theory formalizes \nthe concept of \ndiversification\n in mathematical terms, and suggests that rational investors seek to make investment decisions \nthat \nmaximize portfolio return\n for a given level of \nrisk\n. It frames the investment problem not only as an asset selection \nchallenge, but also of \nsizing the positions\n of each chosen risky asset in the portfolio. MPT also introduces the concept of \n\nsystematic\n versus \nnon-systematic\n \nrisk, the latter of which can be diversified away in a well proportioned portfolio. Systematic risk refers to general market \nwide risks such as interest rate risk, business recessions or wars, while non-systematic risk (also known as specific risk) \nrelates to the \nidiosyncratic risks\n associated with an individual security.\n\n\nModern Portfolio Theory is still in widespread use today in professional investment management circles, and remains one of\nthe foundational frameworks used to build efficient risk adjusted portfolios. Markowitz was awarded the \nNobel Memorial\nPrize in Economic Sciences\n in 1990 for his work on MPT.\n\n\nRisk & Return\n\n\nBefore we look at an example, we first need to discuss the concepts of \nrisk\n and \nreturn\n in the context of a portfolio \nof risky assets. It seems eminently reasonable to expect that a rational investor makes decisions so as to \nmaximize investment \nreturn\n while \nminimizing the risk\n or uncertainty of that return. While return is an unambiguous concept, risk or uncertainty \nis not necessarily so easily quantified. In the context of MPT however, risk is defined as the \nvariance of the portfolio returns\n, \nwhich is a function of the \nvariance and covarinace\n of the individual asset returns in the portfolio. More on this \nlater\n,\nbut first let us discuss portfolio return.\n\n\nPortfolio Return\n\n\nConsider investment returns generated by a portfolio of risky assets. \nPortfolio return\n is simply the weighted sum of the \nreturns on the individual assets in the portfolio, which can be expressed as per the equation below, where \\(w_{i}\\) \nrepresents the \nweight\n (percentage of capital) of asset i, \\(r_{i}\\) the return on asset i, and \\(R_{p}\\) the overall \nportfolio return.\n\n\n$$ R_{p} = \\sum_{i=1}^{n} w_{i} * r_{i} $$\n\n\nIn order to illustrate the validity of this expression, consider a two asset hypothetical portfolio. Let us assume we have $1000 \nof capital, and we have decided to invest in just two stocks (n=2), namely Amazon and Apple. For lack of a strong opinion, we \nsimply split our investment 50/50 between the two names, and then over a year we see Apple return 15% and Amazon returns 8%. \nHow much does our portfolio return? \n\n\nIgnoring the above formula for the moment, we can easily calculate the profit from each $500 dollar investment, sum these profits \nand then calculate the resulting return on our $1000 initial investment, which comes out to be 11.5%.  In general terms, the \n\nfuture value\n \nF\n of some monetary amount can be related to its \npresent value\n \nP\n and a \nrate of return\n \nR\n as follows:\n\n\n$$ F = P (1 + R) $$\n\n\nSince we are ultimately trying to calculate the portfolio return, we need to write this expression in terms of the \npresent \nvalue and the return on the individual assets\n in the portfolio. The expression below does just this, where \\(p_{i}\\) and \n\\(r_{i}\\) represent the present value and return of individual assets respectively, and subscript \np\n denotes the portfolio \nvalue: \n\n\n$$ F_{p} = \\sum_{i=1}^{n} p_{i} (1 + r_{i}) \\ where \\ P_{p} = \\sum_{i=1}^{n} p_{i} $$\n\n\nConsidering our \ntwo asset example\n, we can expand the equation to yield:\n\n\n$$ \\begin{align}\nF_{p} &= p_{1} (1 + r_{1}) + p_{2} ( 1 + r_{2}) \\\\\nF_{p} &= p_{1} + p_{2} + p_{1} r_{1} + p_{2} r_{2} \\\\\nF_{p} &= P_{p} + p_{1} r_{1} + p_{2} r_{2} \\\\\n\\end{align} $$\n\n\nWe know that from our original future value equation we can express the return of the portfolio as:\n\n\n$$ R_{p} = \\frac{F_{p}}{P_{p}} - 1 $$\n\n\nTherefore if we divide the prior equation for our two asset scenario by the present value of the portfolio we get an expression\nfor the portfolio return in terms of the individual asset returns, which is essentially the same as the weighted sum of the asset \nreturns as defined earlier.\n\n\n$$ R_{p} = \\frac{F_{p}}{P_{p}} - 1 = \\frac{p_{1} r_{1} + p_{2} r_{2}}{P_{p}} = w_{1} r_{1} + w_{2} r_{2} $$\n\n\nPortfolio Risk\n\n\nWhile \nportfolio return\n is \nconceptually unambiguous\n, defining portfolio risk is less clear cut. In the case of Modern\nPortfolio Theory, risk is defined as the \nvariance or volatility of the returns\n, which is certainly consistent with intuition, \nsince high variance implies a high degree of uncertainty. Portfolio returns that are extremely volatile are not only emotionally \nhard to stomach, but may force an investor to crystallize losses at a very inopportune time due to a requirement to gain access \nto capital. Long term investors often think of risk in different terms, and in fact \nWarren Buffet\n, \nwho is arguably the greatest investor of all time, would probably not consider volatility as an appropriate metric. Instead, \nhe would be more likely to think in terms of the potential for \npermanent loss of capital\n.\n\n\nFew of us have the investment acumen or long horizon of Warren Buffet, so for mere mortals, let us stick with return volatility \nas our best measure of portfolio risk. While individual asset return volatility is simple to compute, calculating portfolio \nlevel risk is less trivial as it involves understanding the \ninteraction of individual asset returns\n. That is to say, no \ntwo assets in a portfolio are likely to be 100% correlated, and therefore combining uncorrelated assets is bound to affect \nrisk in ways that need to be quantified. Let us begin with our definition of portfolio return variance which using the \n\nexpectation operator\n is just the usual expression for \nvariance\n. \n\n\n$$ \\sigma^2 = E[ ( R_{p} - \\bar{R}_{p} )^2 ] $$\n\n\nIn this equation \\(r_{p}\\) represents the return generated by an instance of the portfolio while \\(\\bar{r}_{p}\\) represents \nthe \naverage expected return\n from this portfolio. To illustrate, consider a two asset portfolio where we expand the above \nexpression to be in terms of the individual assets that make up the portfolio.\n\n\n$$ \\begin{align}\n\\sigma^2 &= E[ ( R_{p} - \\bar{R_{p}})^2 ] \\\\\n\\sigma^2 &= E[ ( w_{1} r_{1} + w_{2} r_{2} - (w_{1} \\bar{r_{1}} + w_{2}\\bar{r_{2}}) )^2 ] \\\\\n\\sigma^2 &= E[ ( w_{1} ( r_{1} - \\bar{r_{1}} ) + w_{2} ( r_{2} - \\bar{r_{2}}))^2 ] \\\\\n\\sigma^2 &= E[ w_{1}^2 ( r_{1} - \\bar{r_{1}} )^2 + w_{2}^2 ( r_{2} - \\bar{r_{2}})^2 + 2 w_{1} w_{2} E[(r_{1} - \\bar{r_{1}})(r_{2} - \\bar{r_{2}})]] \\\\\n\\end{align} $$\n\n\nSince the \nasset weights are not stochastic in nature\n, we can take them outside of the expectation operator to yield the following:\n\n\n$$ \\sigma^2 = w_{1}^2 E[( r_{1} - \\bar{r_{1}})^2] + w_{2}^2 E[( r_{2} - \\bar{r_{2}})^2] + 2 w_{1} w_{2} E[(r_{1} - \\bar{r_{1}})(r_{2} - \\bar{r_{2}})]] $$\n\n\nIt now becomes clear that the portfolio variance is a function of the \nindividual asset variances\n as well as their \n\ncovariance\n, so we can write the same expression in a more concise manner as shown below. Notice that this expression \nsuggests that if we combine risky assets in a portfolio that have a \nnegative covariance\n, the overall portfolio \nrisk will \nbe reduced\n. This is essentially diversification quantified, and it is a central principle of MPT.\n\n\n$$ \\sigma^2 = w_{1}^2 \\sigma_{1}^2 + w_{2}^2 \\sigma_{2}^2 + 2 w_{1} w_{2} \\sigma_{12}  $$\n\n\nWhile the above derivation is for a 2 asset portfolio, this expression can be generalized to an N asset portfolio and \nwritten in \nmatrix form\n as below. The \nw\n term represents an \nnx1\n vector of asset weights, and capital sigma represents \nthe \nnxn\n covariance matrix of asset returns, the diagonal elements of which are the individual asset return variances.\n\n\n$$ \\sigma^2 = w^T \\Sigma w $$\n\n\nSharpe Ratio\n\n\nThe \nSharpe Ratio\n is a widely used performance metric in finance and is named \nafter Nobel Laureate \nWilliam F. Sharpe\n who first developed it. The ratio \nis basically the average return earned in \nexcess\n of the \nrisk free rate\n per unit of volatility, and is therefore a \nstandardized way of expressing \nrisk adjusted return\n. Mathematically, it is defined as follows (where \\(R_{p}\\) \nrepresents portfolio return, \\(\\bar{R_{f}}\\) is the risk free return and \\(\\sigma_{p}\\) is portfolio risk):\n\n\n$$ S_{p} = \\frac{E[ R_{p} - \\bar{R_{f}}]}{\\sigma_{p}}  $$\n\n\nA Sharpe Ratio can be computed \nex-ante\n based on \nforecasts\n of expected risk and return, or otherwise as an \nex-post\n \nmeasure based on realized returns. Given that the return measure (the stuff we like) is in the numerator and the risk measure\n(the stuff we don't like) is in the denominator, the higher the Sharpe Ratio the better.\n\n\nWhile it is very widely used, the Sharpe Ratio is not without its detractors. One of the often quoted grievances with the\nmeasure is that it treats \nupside volatility\n equally with \ndownside volatility\n. This may be reasonable in a long / short\nportfolio, but perhaps less so in a long-only portfolio (i.e. no shorting constraint). In order to address this, a \nvariation of the Sharpe Ratio exists called the \nSortino Ratio\n which\nonly includes returns below a certain threshold when computing volatility. \n\n\nAnother potential concern is that the Sharpe Ratio does not distinguish between \nsystematic\n and \nnon-systematic\n risk, \nthe latter of which can be diversified away in a well balanced portfolio. The \nTreynor Ratio\n\nattempts to address this by estimating the systematic risk only, and using that as the denominator in the above expression.\n\n\nFinally, one of the trickiest issues with the Sharpe Ratio is scaling it to different time horizons. For example, how\ndo you compute an annualized Sharpe from daily returns? Most techniques scale risk and return on the assumption that\nreturns are \nnormally distributed\n, however it is well known that asset returns are not normal and exhibit excess\n\nkurtosis\n and often \nskewness\n. For more details on some of the challenges in computing Sharpe Ratios, I high recommend a \npaper by Andrew W. Lo titled \nThe Statistics of Sharpe Ratios\n.\n\n\nExamples\n\n\nNow that we know how to calculate \nportfolio return and risk\n, let us look at how we can apply this knowledge, and \nmore importantly, demonstrate how we can use Modern Portfolio Theory to construct an efficient investment portfolio of \nrisky assets.  The examples in the following sections leverage the Morpheus data source adapter for \nYahoo Finance\n,\nmore details of which can be found \nhere\n. The library is available on \nMaven Central\n and can\ntherefore be added to your build tool of choice:\n\n\n<dependency>\n    <groupId>com.zavtech</groupId>\n    <artifactId>morpheus-yahoo</artifactId>\n    <version>${VERSION}</version>\n</dependency>\n\n\n\n\nTwo Assets\n\n\nConsider a \ntwo asset\n portfolio where we have already convinced ourselves that we want to buy Apple and Amazon, but we are not sure how much of our capital to invest in each. We obviously do not know what the future may bring, \nbut let us look back over the past year to see what happened on the assumption that it may help influence our decision.\nThe plot below shows the cumulative returns of both securities over the past year, which clearly demonstrates what a great \nrun they have had. With the benefit of hindsight of course, you would have invested all your capital in Apple as it \noutperformed Amazon by some margin. Looking forward however, the performance of these stocks could be reversed, so we \nshould probably spread our bets across the two.\n\n\n\n    \n\n\n\n\n\nThe code to generate this plot is as follows:\n\n\n\n\n\nLocalDate end = LocalDate.now();\nLocalDate start = end.minusYears(1);\nArray<String> tickers = Array.of(\"AAPL\", \"AMZN\");\n\nYahooFinance yahoo = new YahooFinance();\nDataFrame<LocalDate,String> cumReturns = yahoo.getCumReturns(start, end, tickers);\ncumReturns.applyDoubles(v -> v.getDouble() * 100d);\n\nChart.create().withLinePlot(cumReturns, chart -> {\n    chart.title().withText(\"Cumulative Asset Returns\");\n    chart.subtitle().withText(\"Range: (\" + start + \" to\" + end + \")\");\n    chart.plot().axes().domain().label().withText(\"Date\");\n    chart.plot().axes().range(0).label().withText(\"Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.show();\n});\n\n\n\n\nTo get a sense of the risk / return characteristics of our two asset portfolio, we are going to \ngenerate 10,000 random \nportfolios\n where we invest different amounts in Apple and Amazon ranging from 0% to 100% of our capital in one asset and \nthe remainder in the other asset. The function below returns a \nDataFrame\n of random portfolio weights in N assets that \nsum to 1 in all cases, and therefore each portfolio represents a \nfully invested\n scenario.\n\n\n\n\n\n/**\n * A function that generates N long only random portfolios with weights that sum to 1\n * @param count     the number of portfolios / rows in the DataFrame\n * @param tickers   the security tickers to include\n * @return          the frame of N random portfolios, 1 per row\n */\nDataFrame<Integer,String> randomPortfolios(int count, Iterable<String> tickers) {\n    DataFrame<Integer,String> weights = DataFrame.ofDoubles(Range.of(0, count), tickers);\n    weights.applyDoubles(v -> Math.random());\n    weights.rows().forEach(row -> {\n        final double sum = row.stats().sum();\n        row.applyDoubles(v -> {\n            double weight = v.getDouble();\n            return weight / sum;\n        });\n    });\n    return weights;\n}\n\n\n\n\nIn order to calculate the risk and return characteristics of these 10,000 portfolios, we first need to compute the \n\ncovariance matrix of the returns\n between Apple and Amazon, and then also compute the \ncumulative asset returns\n \nover the historical horizon in question. With these quantities, it is fairly simple to compute the risk & return of \neach portfolio, which can then be plotted on a scatter chart to see how they compare. The resulting plot is shown \nbelow, and is followed by the code to generate it. Note that since we are using daily returns to compute the covariance \nmatrix, we need to \nannualize\n it, which we do by multiplying by 252 (on the assumption there are 252 trading days \nin the year). Since the cumulative returns are based on a 1-year look back window, there is no need to annualize the \nreturns.\n\n\n\n    \n\n\n\n\n\n\n\n\n//Define portfolio count, investment horizon and universe\nint count = 10000;\nLocalDate end = LocalDate.now();\nLocalDate start = end.minusYears(1);\nArray<String> tickers = Array.of(\"AAPL\", \"AMZN\");\n\n//Grab daily returns and cumulative returns from Yahoo Finance\nYahooFinance yahoo = new YahooFinance();\nDataFrame<LocalDate,String> dayReturns = yahoo.getDailyReturns(start, end, tickers);\nDataFrame<LocalDate,String> cumReturns = yahoo.getCumReturns(start, end, tickers);\n\n//Compute asset covariance matrix from daily returns and annualize\nDataFrame<String,String> sigma = dayReturns.cols().stats().covariance().applyDoubles(v -> v.getDouble() * 252);\nDataFrame<LocalDate,String> assetReturns = cumReturns.rows().last().map(DataFrameRow::toDataFrame).get();\n\n//Generate random portfolios and compute risk & return for each\nDataFrame<Integer,String> portfolios = randomPortfolios(count, tickers);\nDataFrame<Integer,String>  results = DataFrame.ofDoubles(Range.of(0, count), Array.of(\"Risk\", \"Return\"));\nportfolios.rows().forEach(row -> {\n    DataFrame<Integer,String> weights = row.toDataFrame();\n    double portReturn = weights.dot(assetReturns.transpose()).data().getDouble(0, 0);\n    double portVariance = weights.dot(sigma).dot(weights.transpose()).data().getDouble(0, 0);\n    results.data().setDouble(row.key(), \"Return\", portReturn * 100d);\n    results.data().setDouble(row.key(), \"Risk\", Math.sqrt(portVariance) * 100d);\n});\n\n//Plot the results using a scatter plot\nChart.create().withScatterPlot(results, false, \"Risk\", chart -> {\n    chart.title().withText(\"Risk / Return Profiles For AAPL+AMZN Portfolios\");\n    chart.subtitle().withText(count + \" Portfolio Combinations Simulated\");\n    chart.plot().axes().domain().label().withText(\"Portfolio Risk\");\n    chart.plot().axes().domain().format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.plot().axes().range(0).label().withText(\"Portfolio Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.show();\n});\n\n\n\n\nThere are a few notable observations about the plot as follows:\n\n\n\n\nIt appears to be impossible to create a portfolio with a risk lower than about 15.7% volatility.\n\n\nIf you were willing to accept 16% risk, there are two portfolios, one of which has a much better return than the other.\n\n\nMuch beyond 17% risk, it appears returns suffer tremendously due to overweight in one of the assets.\n\n\nThe best possible return appears to be about 37% at a risk level just north of 17%.\n\n\n\n\nA rational investor clearly wants to be on the upper part of this curve for a \ngiven level of risk\n. For example, \nat 17% volatility, you could either have achieved 23.5% return with a portfolio on the lower segment of the curve \nor close to 37% on the upper part. Which one do you prefer? The upper part of the curve is referred to as the \n\nEfficient Frontier\n, and moreover, there exists a special case \non this curve often referred to as the \nMarkowitz Portfolio\n, which has the highest risk adjusted return of all \nthe candidate portfolios. It is essentially the \nmean-variance optimal portfolio\n and can be calculated by \nmaximizing the utility defined by the objective function below, subject to whatever constraints you may impose \n(in this case we assume \nlong only\n and \nfully invested\n):\n\n\n$$ U_{p} = w^T r - w^T \\Sigma w $$\n\n\nThe \\(w\\) and \\(r\\) terms represent \nnx1\n vectors of asset \nweights\n and \nreturns\n respectively, while capital \n\nsigma\n represents the \nnxn\n asset covariance matrix. This objective function is essentially saying we like return \nand do not like risk, and our goal is to select an \nnx1\n vector of asset weights that maximizes this expression given \nasset returns and a covariance matrix. In this example, we are looking back at a historical scenario so we can calculate \nreturns and the covariance based on realized market prices. In reality, we need to make \njudgements about the future\n, \nand therefore are required to \nestimate future returns and covariance\n, which is where things can get tricky. Calculating \nthe \nMarkowitz Portfolio\n given constraints (as in this case where we impose a long only fully invested constraint such \nthat the elements of W are all positive and sum to 1) is a \nquadratic optimization\n\nproblem that requires appropriate software and is beyond the scope of this article. A good commercial package \nto consider is \nMOSEK\n, or for an Open Source \nsolution you may consider \nOptaPlanner\n.\n\n\nAsset Selection\n\n\nModern Portfolio Theory is fundamentally about \nsizing positions\n of risky assets in a portfolio, it is not \nabout asset selection. Having said that, it can be useful to compare the risk / return profiles of portfolios \nconstructed from different risky assets. In the prior example we had already decided that we wanted to invest \nin Apple and Amazon, but were there better two asset portfolio combinations we should have considered? The plot \nbelow, followed by the code that generated it, is essentially an extension of the prior example where in this \ncase we generate multiple 10,000 portfolio combinations with different asset constituents to see how they compare.\n\n\n\n    \n\n\n\n\n\nIt is clear from the chart that the 5 two asset portfolios in this example have very different risk / return\nprofiles, with the Apple / Amazom combination being the most risky, but certainly having some of the highest\nreturns. With that being said, if your investment objective was to achieve around a 12.0% return (very\naspirational in today's world of zero interest rates), your best bet would be to choose the VTI / BND combination\nas it appears to have the \npotentia\nl to generate this return for less than 5% risk. Compare this to some of the other \nportfolio combinations for which you need to accept a much \nhigher level of risk\n to achieve the \nsame return\n.\n\n\n\n\n\n//Define portfolio count, investment horizon\nint count = 10000;\nLocalDate end = LocalDate.now();\nLocalDate start = end.minusYears(1);\nYahooFinance yahoo = new YahooFinance();\n\nArray<DataFrame<Integer,String>> results = Array.of(\n    Array.of(\"VTI\", \"BND\"),\n    Array.of(\"AAPL\", \"AMZN\"),\n    Array.of(\"GOOGL\", \"BND\"),\n    Array.of(\"ORCL\", \"KO\"),\n    Array.of(\"VWO\", \"VNQ\")\n).map(v -> {\n    //Access tickers\n    Array<String> tickers = v.getValue();\n    //Grab daily returns and cumulative returns from Yahoo Finance\n    DataFrame<LocalDate,String> dayReturns = yahoo.getDailyReturns(start, end, tickers);\n    DataFrame<LocalDate,String> cumReturns = yahoo.getCumReturns(start, end, tickers);\n    //Compute asset covariance matrix from daily returns and annualize\n    DataFrame<String,String> sigma = dayReturns.cols().stats().covariance().applyDoubles(x -> x.getDouble() * 252);\n    DataFrame<LocalDate,String> assetReturns = cumReturns.rows().last().map(DataFrameRow::toDataFrame).get();\n    //Generate random portfolios and compute risk & return for each\n    String label = String.format(\"%s+%s\", tickers.getValue(0), tickers.getValue(1));\n    DataFrame<Integer,String> portfolios = randomPortfolios(count, tickers);\n    DataFrame<Integer,String>  riskReturn = DataFrame.ofDoubles(Range.of(0, count), Array.of(\"Risk\", label));\n    portfolios.rows().forEach(row -> {\n        DataFrame<Integer,String> weights = row.toDataFrame();\n        double portReturn = weights.dot(assetReturns.transpose()).data().getDouble(0, 0);\n        double portVariance = weights.dot(sigma).dot(weights.transpose()).data().getDouble(0, 0);\n        riskReturn.data().setDouble(row.key(), label, portReturn * 100d);\n        riskReturn.data().setDouble(row.key(), \"Risk\", Math.sqrt(portVariance) * 100d);\n    });\n    return riskReturn;\n});\n\nDataFrame<Integer,String> first = results.getValue(0);\nChart.create().<Integer,String>withScatterPlot(first, false, \"Risk\", chart -> {\n    for (int i=1; i<results.length(); ++i) {\n        chart.plot().<String>data().add(results.getValue(i), \"Risk\");\n        chart.plot().render(i).withDots();\n    }\n    chart.plot().axes().domain().label().withText(\"Portfolio Risk\");\n    chart.plot().axes().domain().format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.plot().axes().range(0).label().withText(\"Portfolio Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.title().withText(\"Risk / Return Profiles of Various Two Asset Portfolios\");\n    chart.subtitle().withText(count + \" Portfolio Combinations Simulated\");\n    chart.legend().on().right();\n    chart.show();\n});\n\n\n\n\nMultiple Assets\n\n\nSo far we have limited our example portfolios to two assets in order to help develop the intuition behind Modern \nPortfolio Theory. In reality however, real world portfolios are likely to include more assets, although exactly \nhow many are required to achieve a reasonable level of diversification is open to debate. Consider an investable \nuniverse of 6 securities represented by broad based low cost ETFs that serve as reasonable proxies for major asset \nclasses. The table below summarizes these candidates.\n\n\n\n\n\n\n\n\nTicker\n\n\nName\n\n\nProvider\n\n\n\n\n\n\n\n\n\n\n\n\nVWO\n\n\nVanguard FTSE Emerging Markets ETF\n\n\nVanguard\n\n\nDetails\n\n\n\n\n\n\nVNQ\n\n\nVanguard REIT ETF\n\n\nVanguard\n\n\nDetails\n\n\n\n\n\n\nVEA\n\n\nVanguard FTSE Developed Markets ETF\n\n\nVanguard\n\n\nDetails\n\n\n\n\n\n\nDBC\n\n\nPowerShares DB Commodity Tracking ETF\n\n\nPowershares\n\n\nDetails\n\n\n\n\n\n\nVTI\n\n\nVanguard Total Stock Market ETF\n\n\nVanguard\n\n\nDetails\n\n\n\n\n\n\nBND\n\n\nVanguard Total Bond Market ETF\n\n\nVanguard\n\n\nDetails\n\n\n\n\n\n\n\n\nTo get a sense of how the risk / return profiles of portfolios evolve as we include more assets, we can generate 10,000 \nrandom portfolios first with 2 assets, then 3 and all the way to 6. In the case of two assets, we expect to see our \nrotated parabola, but what happens when you have more degrees of freedom in the portfolio? The plot below is the answer.\nAs we go beyond two assets, the scatter becomes more pronounced, and while risk is generally reduced due to the fact that \nthe assets are not perfectly correlated, return also suffers somewhat. Having said that, the \nEfficient Frontiers\n of\nthe more diversified portfolios appear to provide a better risk / return trade off than the two asset portfolio.  \n\n\n\n    \n\n\n\n\n\nThe code to generate the above plot of 5 lots of 10,000 portfolios with various assets is as follows: \n\n\n\n\n\n//Define portfolio count, investment horizon\nint count = 10000;\nLocalDate end = LocalDate.now();\nLocalDate start = end.minusYears(1);\nYahooFinance yahoo = new YahooFinance();\n\nArray<DataFrame<Integer,String>> results = Array.of(\n    Array.of(\"VWO\", \"VNQ\"),\n    Array.of(\"VWO\", \"VNQ\", \"VEA\"),\n    Array.of(\"VWO\", \"VNQ\", \"VEA\", \"DBC\"),\n    Array.of(\"VWO\", \"VNQ\", \"VEA\", \"DBC\", \"VTI\"),\n    Array.of(\"VWO\", \"VNQ\", \"VEA\", \"DBC\", \"VTI\", \"BND\")\n).map(v -> {\n    //Access tickers\n    Array<String> tickers = v.getValue();\n    //Grab daily returns and cumulative returns from Yahoo Finance\n    DataFrame<LocalDate,String> dayReturns = yahoo.getDailyReturns(start, end, tickers);\n    DataFrame<LocalDate,String> cumReturns = yahoo.getCumReturns(start, end, tickers);\n    //Compute asset covariance matrix from daily returns and annualize\n    DataFrame<String,String> sigma = dayReturns.cols().stats().covariance().applyDoubles(x -> x.getDouble() * 252);\n    DataFrame<LocalDate,String> assetReturns = cumReturns.rows().last().map(DataFrameRow::toDataFrame).get();\n    //Generate random portfolios and compute risk & return for each\n    String label = String.format(\"%s Assets\", tickers.length());\n    DataFrame<Integer,String> portfolios = randomPortfolios(count, tickers);\n    DataFrame<Integer,String>  riskReturn = DataFrame.ofDoubles(Range.of(0, count), Array.of(\"Risk\", label));\n    portfolios.rows().forEach(row -> {\n        DataFrame<Integer,String> weights = row.toDataFrame();\n        double portReturn = weights.dot(assetReturns.transpose()).data().getDouble(0, 0);\n        double portVariance = weights.dot(sigma).dot(weights.transpose()).data().getDouble(0, 0);\n        riskReturn.data().setDouble(row.key(), 1, portReturn * 100d);\n        riskReturn.data().setDouble(row.key(), 0, Math.sqrt(portVariance) * 100d);\n    });\n    return riskReturn;\n});\n\nDataFrame<Integer,String> first = results.getValue(0);\nChart.create().<Integer,String>withScatterPlot(first, false, \"Risk\", chart -> {\n    for (int i=1; i<results.length(); ++i) {\n        chart.plot().<String>data().add(results.getValue(i), \"Risk\");\n        chart.plot().render(i).withDots();\n    }\n    chart.plot().axes().domain().label().withText(\"Portfolio Risk\");\n    chart.plot().axes().domain().format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.plot().axes().range(0).label().withText(\"Portfolio Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.title().withText(\"Risk / Return Profiles of Portfolios With Increasing Assets\");\n    chart.subtitle().withText(count + \" Portfolio Combinations Simulated\");\n    chart.legend().on().bottom();\n    chart.show();\n});\n\n\n\n\nRobo-Advisor\n\n\nA fairly recent innovation in the investment management space relates to what are called \nRobo-Advisors\n,\nwhich are essentially online investment solutions aimed mostly at retail investors. They are called Robo-Advisors\nbecause they automate the construction of portfolios using software based on an investor's risk appetite and their \ninvestment objective, which they assess by posing a number of questions via their website. There are already many \nplayers in this space, and most of them construct well-balanced portfolios consisting of 5-7 assets, all of which \nare \nbroad based and low cost\n \nExchange Traded Funds\n.\n\n\nTo avoid any suggestion that I am endorsing or recommending these services, I am consciously avoiding naming names, but \nI visited the website of one of the larger advisors and proceeded to complete the questionnaire after which it proposed \nthe portfolio in the table below. The purpose of this discussion is not to make any judgement on how good this portfolio \nis versus other potential investments, but really to get a sense of how efficient the proposed portfolio is from a \nrisk / return  stand point.\n\n\n\n\n\n\n\n\nTicker\n\n\nName\n\n\nWeight\n\n\nProvider\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nVTI\n\n\nVanguard Total Stock Market ETF\n\n\n35%\n\n\nVanguard\n\n\nDetails\n\n\n\n\n\n\nVEA\n\n\nVanguard FTSE Developed Markets ETF\n\n\n21%\n\n\nVanguard\n\n\nDetails\n\n\n\n\n\n\nVWO\n\n\nVanguard FTSE Emerging Markets ETF\n\n\n16%\n\n\nVanguard\n\n\nDetails\n\n\n\n\n\n\nVTEB\n\n\nVanguard Tax-Exempt Bond ETF\n\n\n15%\n\n\nVanguard\n\n\nDetails\n\n\n\n\n\n\nVIG\n\n\nVanguard Dividend Appreciation ETF\n\n\n8%\n\n\nVanguard\n\n\nDetails\n\n\n\n\n\n\nXLE\n\n\nEnergy Select Sector SPDR ETF\n\n\n5%\n\n\nState Street\n\n\nDetails\n\n\n\n\n\n\n\n\nRamdom Portfolios\n\n\nIgnoring the proposed weightings for the moment, consider generating 10,000 \nlong-only fully invested random portfolios\n \ninvolving these assets, and then computing the resulting equity curves. This will give us a sense of the various scenarios we can \ngenerate with this asset universe, and in particular allow us to understand the \ndegree of dispersion\n in outcomes. The plot \nbelow illustrates 10,000 equity curves based on the \npast 1 year returns\n, suggesting a return spread ranging from approximately \n2.5% all the way to 18.15%, which is pretty enormous.\n\n\n\n    \n\n\n\n\n\nThe code to generate the above plot is as follows:\n\n\n\n\n\nint portfolioCount = 10000;\nRange<LocalDate> range = Range.of(LocalDate.now().minusYears(1), LocalDate.now());\nArray<String> tickers = Array.of(\"VTI\", \"BND\", \"VWO\", \"VTEB\", \"VIG\", \"XLE\");\nDataFrame<Integer,String> portfolios = randomPortfolios(portfolioCount, tickers);\nDataFrame<LocalDate,String> performance = getEquityCurves(range, portfolios);\nChart.create().withLinePlot(performance.applyDoubles(v -> v.getDouble() * 100d), chart -> {\n    chart.title().withText(portfolioCount + \" Equity Curves (Past 1 Year Returns)\");\n    chart.subtitle().withText(\"Robo-Advisor Universe: VTI, BND, VWO, VTEB, VIG, XLE\");\n    chart.plot().axes().domain().label().withText(\"Date\");\n    chart.plot().axes().range(0).label().withText(\"Portfolio Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.show();\n});\n\n\n\n\nThe example uses the following function in order to compute the equity curves for the \nDataFrame\n of 10,000 random portfolios. \nThis function expects an \nmxn\n frame of portfolio weighting configurations where \nm\n is the number of portfolios and \nn\n the number \nof assets. The resulting \nDataFrame\n has \ntxm\n dimensions where \nt\n is the number of dates, and the \nm\n columns are labelled \nP0\n, \n\nP1\n through to \nP(m)\n.\n\n\n\n\n\n/**\n * Calculates equity curves over a date range given a frame on initial portfolio weight configurations\n * @param range         the date range for historical returns\n * @param portfolios    MxN DataFrame of portfolio weights, M portfolios, N assets\n * @return              the cumulative returns for each portfolio, TxM, portfolios labelled P0, P1 etc...\n */\nDataFrame<LocalDate,String> getEquityCurves(Range<LocalDate> range, DataFrame<Integer,String> portfolios) {\n    final YahooFinance yahoo = new YahooFinance();\n    final Iterable<String> tickers = portfolios.cols().keyArray();\n    final DataFrame<LocalDate,String> cumReturns = yahoo.getCumReturns(range.start(), range.end(), tickers);\n    final Range<String> colKeys = Range.of(0, portfolios.rowCount()).map(i -> \"P\" + i);\n    return DataFrame.ofDoubles(cumReturns.rows().keyArray(), colKeys, v -> {\n        double totalReturn = 0d;\n        for (int i=0; i<portfolios.colCount(); ++i) {\n            final double weight = portfolios.data().getDouble(v.colOrdinal(), i);\n            final double assetReturn = cumReturns.data().getDouble(v.rowOrdinal(), i);\n            totalReturn += (weight * assetReturn);\n        }\n        return totalReturn;\n    });\n}\n\n\n\n\nProposed Portfolio\n\n\nGiven the \nproposed portfolio weights\n presented earlier, we can use the past 1 year of returns for the assets in question\nto assess what the risk and return of this portfolio would have been had we invested a year ago. The code below performs \nthis analysis and suggests that the portfolio returned a very respectable \n14.9%\n for \n7.0%\n risk, which is pretty phenomenal \n(a 2 \nSharpe\n portfolio over the past year). The code to generate these results is as follows:\n\n\n\n\n\n//Defines investment horizon & universe\nLocalDate end = LocalDate.now();\nLocalDate start = end.minusYears(1);\n//Define investment universe and weights\nArray<String> tickers = Array.of(\"VTI\", \"VEA\", \"VWO\", \"VTEB\", \"VIG\", \"XLE\");\n//Define DataFrame of position weights suggested by Wealthfront\nDataFrame<String,String> portfolio = DataFrame.of(tickers, String.class, columns -> {\n    columns.add(\"Weights\", Array.of(0.35d, 0.21d, 0.16d, 0.15d, 0.08d, 0.05d));\n});\n//Grab daily returns and cumulative returns from Yahoo Finance\nYahooFinance yahoo = new YahooFinance();\nDataFrame<LocalDate,String> dayReturns = yahoo.getDailyReturns(start, end, tickers);\nDataFrame<LocalDate,String> cumReturns = yahoo.getCumReturns(start, end, tickers);\n//Compute asset covariance matrix from daily returns and annualize\nDataFrame<String,String> sigma = dayReturns.cols().stats().covariance().applyDoubles(x -> x.getDouble() * 252);\nDataFrame<LocalDate,String> assetReturns = cumReturns.rows().last().map(DataFrameRow::toDataFrame).get();\n//Generate DataFrame of portfolio weights\ndouble portReturn = portfolio.transpose().dot(assetReturns.transpose()).data().getDouble(0, 0);\ndouble portVariance = portfolio.transpose().dot(sigma).dot(portfolio).data().getDouble(0, 0);\nIO.println(String.format(\"Portfolio Return: %s\", portReturn));\nIO.println(String.format(\"Portfolio Risk: %s\", Math.sqrt(portVariance)));\n\n\n\n\nNow that we know how the currently proposed portfolio performed over the past year, let us generate 100,000 random portfolios \nin these 6 assets to get a sense of the overall risk / return profile of this universe, and see how the proposed portfolio \ncompares. The plot below suggests that the proposed portfolio (green dot) is indeed pretty efficient, and may also suggest \nthat this particular Robo-Advisor's future expectations are not that different from the past year.\n\n\nOne may wonder why the \nproposed portfolio\n is not exactly on the \nEfficient Frontier\n, and there are several explanations \nfor this. The first is that the Robo-Advisor proposed weights are for a \nforward looking portfolio\n, and our example is using \npast 1 year returns to test its efficiency. Secondly, there are an infinite number of ways of estimating an \nex-ante\n covariance \nmatrix, which may involve \nexponentially smoothing\n returns (perhaps using different \nhalf-lives\n to estimate diagonal versus \noff-diagonal terms), and perhaps applying shrinkage to off-diagonal terms to improve stability. In our example, we have the \n\nbenefit of hindsight\n and simply compute an \nex-post\n covariance matrix, doing nothing fancy at all. Finally, the Robo-Advisor \nmay impose risk constraints (such as capping any one position to be no more than 35% of the portfolio perhaps), which may \npenalize the strategy versus a less constrained instance, but which may be a very sensible compromise.\n\n\n\n    \n\n\n\n\n\nThe code to generate this plot is as follows:\n\n\n\n\n\n//Define portfolio count, investment horizon and universe\nint count = 100000;\nLocalDate end = LocalDate.now();\nLocalDate start = end.minusYears(1);\nArray<String> tickers = Array.of(\"VTI\", \"VEA\", \"VWO\", \"VTEB\", \"VIG\", \"XLE\");\n\n//Grab daily returns and cumulative returns from Yahoo Finance\nYahooFinance yahoo = new YahooFinance();\nDataFrame<LocalDate,String> dayReturns = yahoo.getDailyReturns(start, end, tickers);\nDataFrame<LocalDate,String> cumReturns = yahoo.getCumReturns(start, end, tickers);\n\n//Compute asset covariance matrix from daily returns and annualize\nDataFrame<String,String> sigma = dayReturns.cols().stats().covariance().applyDoubles(v -> v.getDouble() * 252);\nDataFrame<LocalDate,String> assetReturns = cumReturns.rows().last().map(DataFrameRow::toDataFrame).get();\n\n//Generate random portfolios and compute risk & return for each\nDataFrame<Integer,String> portfolios = randomPortfolios(count, tickers);\nDataFrame<Integer,String>  results = DataFrame.ofDoubles(Range.of(0, count), Array.of(\"Risk\", \"Random\"));\nportfolios.rows().forEach(row -> {\n    DataFrame<Integer,String> weights = row.toDataFrame();\n    double portReturn = weights.dot(assetReturns.transpose()).data().getDouble(0, 0);\n    double portVariance = weights.dot(sigma).dot(weights.transpose()).data().getDouble(0, 0);\n    results.data().setDouble(row.key(), \"Random\", portReturn * 100d);\n    results.data().setDouble(row.key(), \"Risk\", Math.sqrt(portVariance) * 100d);\n});\n\n//Create DataFrame with risk / return of proposed Wealthfront portfolio\nDataFrame<Integer,String> proposed = DataFrame.of(Range.of(0, 1), String.class, cols -> {\n    cols.add(\"Risk\", Array.of(0.068950 * 100d));\n    cols.add(\"Chosen\", Array.of(0.1587613 * 100d));\n});\n\n//Plot the results using a scatter plot\nChart.create().withScatterPlot(results, false, \"Risk\", chart -> {\n    chart.title().withText(\"Risk / Return Profile For Wealthfront Portfolio\");\n    chart.subtitle().withText(count + \" Portfolio Combinations Simulated\");\n    chart.plot().<String>data().add(proposed, \"Risk\");\n    chart.plot().render(1).withDots();\n    chart.plot().axes().domain().label().withText(\"Portfolio Risk\");\n    chart.plot().axes().domain().format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.plot().axes().range(0).label().withText(\"Portfolio Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.legend().on().right();\n    chart.show();\n});\n\n\n\n\nEfficiency\n\n\nThe previous section established that the \nproposed portfolio\n was reasonably efficient in the context of the past 1 year of \nreturns (how efficient it will be over the next year is of course impossible to know, but all else being equal, it seems a decent \nconfiguration). In this section, we further consider its relative efficiency by looking at the equity curve of the \nbest\n and \n\nworst\n portfolios that we find in our 100,000 random candidates (based on their \nSharpe Ratios\n). In addition, \nwe overlay the equity curve of the \nS&P500\n by using the SPY Exchange Traded Fund as a proxy.\n\n\nThe plot below shows that the \nproposed portfolio\n actually outperformed what is labelled as the \nBest\n portfolio in pure \n\nreturn space\n, but not in \nrisk-adjusted terms\n. That is, it yields a lower return per unit of risk than the \nbest\n portfolio,\nand the fact that it outperformed in return space is just a fluke. The \nworst\n portfolio is a genuine shocker, and in fact\nspent much of the past year going nowhere before bouncing back starting in August.\n\n\n\n    \n\n\n\n\n\nThe code to generate this plot is as follows, and leverages the \ngetEquityCurves()\n function discussed earlier.\n\n\n\n\n\nint portfolioCount = 1000;\nRange<LocalDate> range = Range.of(LocalDate.now().minusYears(1), LocalDate.now());\nArray<String> tickers = Array.of(\"VTI\", \"VEA\", \"VWO\", \"VTEB\", \"VIG\", \"XLE\");\nArray<Double> proposed = Array.of(0.35d, 0.21d, 0.16d, 0.15d, 0.08d, 0.05d);\nDataFrame<Integer,String> portfolios = randomPortfolios(portfolioCount, tickers);\nportfolios.rowAt(0).applyDoubles(v -> proposed.getDouble(v.colOrdinal()));\n//Compute risk / return / sharpe for random portfolios\nDataFrame<Integer,String> riskReturn = calcRiskReturn(range, portfolios).rows().sort(false, \"Sharpe\");\n//Capture portfolio keys for chosen, best and worst portfolio based on sharpe\nDataFrame<Integer,String> candidates = portfolios.rows().select(0,\n    riskReturn.rows().first().get().key(),\n    riskReturn.rows().last().get().key()\n);\n//Compute equity curves for chosen, best and worst\nDataFrame<LocalDate,String> equityCurves = getEquityCurves(range, candidates).cols().mapKeys(col -> {\n    switch (col.ordinal()) {\n        case 0: return \"Chosen\";\n        case 1: return \"Best\";\n        case 2: return \"Worst\";\n        default: return col.key();\n    }\n});\n//Capture returns of S&P 500\nYahooFinance yahoo = new YahooFinance();\nDataFrame<LocalDate,String> spy = yahoo.getCumReturns(range.start(), range.end(), \"SPY\");\n\n//Plot the equity curves\nChart.create().withLinePlot(equityCurves.applyDoubles(v -> v.getDouble() * 100d), chart -> {\n    chart.title().withText(\"Best/Worst/Chosen Equity Curves (Past 1 Year Returns) + SPY\");\n    chart.subtitle().withText(\"Robo-Advisor Universe: VTI, VEA, VWO, VTEB, VIG, XLE\");\n    chart.plot().<String>data().add(spy.times(100d));\n    chart.plot().axes().domain().label().withText(\"Date\");\n    chart.plot().axes().range(0).label().withText(\"Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.plot().style(\"Chosen\").withColor(Color.BLACK).withLineWidth(1.5f);\n    chart.plot().style(\"Best\").withColor(Color.GREEN.darker().darker()).withLineWidth(1.5f);\n    chart.plot().style(\"Worst\").withColor(Color.RED).withLineWidth(1.5f);\n    chart.plot().style(\"SPY\").withColor(Color.BLUE);\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\nThe implementation of \ncalcRiskReturn()\n in this example is as follows:\n\n\n\n\n\n/**\n * Returns a DataFrame containing risk, return and sharpe ratio for portfolios over the date range\n * @param range         the date range for historical returns\n * @param portfolios    the DataFrame of portfolio weights, one row per portfolio configuration\n * @return              the DataFrame with risk, return and sharpe\n */\nDataFrame<Integer,String> calcRiskReturn(Range<LocalDate> range, DataFrame<Integer,String> portfolios) {\n    YahooFinance yahoo = new YahooFinance();\n    Array<String> tickers = portfolios.cols().keyArray();\n    DataFrame<LocalDate,String> dayReturns = yahoo.getDailyReturns(range.start(), range.end(), tickers);\n    DataFrame<LocalDate,String> cumReturns = yahoo.getCumReturns(range.start(), range.end(), tickers);\n    //Compute asset covariance matrix from daily returns and annualize\n    DataFrame<String,String> sigma = dayReturns.cols().stats().covariance().applyDoubles(x -> x.getDouble() * 252);\n    DataFrame<LocalDate,String> assetReturns = cumReturns.rows().last().map(DataFrameRow::toDataFrame).get();\n    //Prepare 3 column DataFrame to capture results\n    DataFrame<Integer,String>  riskReturn = DataFrame.ofDoubles(\n        portfolios.rows().keyArray(),\n        Array.of(\"Risk\", \"Return\", \"Sharpe\")\n    );\n    portfolios.rows().forEach(row -> {\n        DataFrame<Integer,String> weights = row.toDataFrame();\n        double portReturn = weights.dot(assetReturns.transpose()).data().getDouble(0, 0);\n        double portVariance = weights.dot(sigma).dot(weights.transpose()).data().getDouble(0, 0);\n        riskReturn.data().setDouble(row.key(), \"Return\", portReturn * 100d);\n        riskReturn.data().setDouble(row.key(), \"Risk\", Math.sqrt(portVariance) * 100d);\n        riskReturn.data().setDouble(row.key(), \"Sharpe\", portReturn / Math.sqrt(portVariance));\n    });\n    return riskReturn;\n}",
            "title": "Modern Portfolio Theory"
        },
        {
            "location": "/examples/mpt/#modern-portfolio-theory",
            "text": "",
            "title": "Modern Portfolio Theory"
        },
        {
            "location": "/examples/mpt/#introduction",
            "text": "In 1952,  Harry Markowitz  published a seminal paper in the Journal of Finance \ntitled  Portfolio Selection , where he first introduced  Modern Portfolio Theory  \n(MPT), which quantifies the  risk  and  return  trade-off when constructing portfolios of risky assets. The theory formalizes \nthe concept of  diversification  in mathematical terms, and suggests that rational investors seek to make investment decisions \nthat  maximize portfolio return  for a given level of  risk . It frames the investment problem not only as an asset selection \nchallenge, but also of  sizing the positions  of each chosen risky asset in the portfolio. MPT also introduces the concept of  systematic  versus  non-systematic  \nrisk, the latter of which can be diversified away in a well proportioned portfolio. Systematic risk refers to general market \nwide risks such as interest rate risk, business recessions or wars, while non-systematic risk (also known as specific risk) \nrelates to the  idiosyncratic risks  associated with an individual security.  Modern Portfolio Theory is still in widespread use today in professional investment management circles, and remains one of\nthe foundational frameworks used to build efficient risk adjusted portfolios. Markowitz was awarded the  Nobel Memorial\nPrize in Economic Sciences  in 1990 for his work on MPT.",
            "title": "Introduction"
        },
        {
            "location": "/examples/mpt/#risk-return",
            "text": "Before we look at an example, we first need to discuss the concepts of  risk  and  return  in the context of a portfolio \nof risky assets. It seems eminently reasonable to expect that a rational investor makes decisions so as to  maximize investment \nreturn  while  minimizing the risk  or uncertainty of that return. While return is an unambiguous concept, risk or uncertainty \nis not necessarily so easily quantified. In the context of MPT however, risk is defined as the  variance of the portfolio returns , \nwhich is a function of the  variance and covarinace  of the individual asset returns in the portfolio. More on this  later ,\nbut first let us discuss portfolio return.",
            "title": "Risk &amp; Return"
        },
        {
            "location": "/examples/mpt/#portfolio-return",
            "text": "Consider investment returns generated by a portfolio of risky assets.  Portfolio return  is simply the weighted sum of the \nreturns on the individual assets in the portfolio, which can be expressed as per the equation below, where \\(w_{i}\\) \nrepresents the  weight  (percentage of capital) of asset i, \\(r_{i}\\) the return on asset i, and \\(R_{p}\\) the overall \nportfolio return.  $$ R_{p} = \\sum_{i=1}^{n} w_{i} * r_{i} $$  In order to illustrate the validity of this expression, consider a two asset hypothetical portfolio. Let us assume we have $1000 \nof capital, and we have decided to invest in just two stocks (n=2), namely Amazon and Apple. For lack of a strong opinion, we \nsimply split our investment 50/50 between the two names, and then over a year we see Apple return 15% and Amazon returns 8%. \nHow much does our portfolio return?   Ignoring the above formula for the moment, we can easily calculate the profit from each $500 dollar investment, sum these profits \nand then calculate the resulting return on our $1000 initial investment, which comes out to be 11.5%.  In general terms, the  future value   F  of some monetary amount can be related to its  present value   P  and a  rate of return   R  as follows:  $$ F = P (1 + R) $$  Since we are ultimately trying to calculate the portfolio return, we need to write this expression in terms of the  present \nvalue and the return on the individual assets  in the portfolio. The expression below does just this, where \\(p_{i}\\) and \n\\(r_{i}\\) represent the present value and return of individual assets respectively, and subscript  p  denotes the portfolio \nvalue:   $$ F_{p} = \\sum_{i=1}^{n} p_{i} (1 + r_{i}) \\ where \\ P_{p} = \\sum_{i=1}^{n} p_{i} $$  Considering our  two asset example , we can expand the equation to yield:  $$ \\begin{align}\nF_{p} &= p_{1} (1 + r_{1}) + p_{2} ( 1 + r_{2}) \\\\\nF_{p} &= p_{1} + p_{2} + p_{1} r_{1} + p_{2} r_{2} \\\\\nF_{p} &= P_{p} + p_{1} r_{1} + p_{2} r_{2} \\\\\n\\end{align} $$  We know that from our original future value equation we can express the return of the portfolio as:  $$ R_{p} = \\frac{F_{p}}{P_{p}} - 1 $$  Therefore if we divide the prior equation for our two asset scenario by the present value of the portfolio we get an expression\nfor the portfolio return in terms of the individual asset returns, which is essentially the same as the weighted sum of the asset \nreturns as defined earlier.  $$ R_{p} = \\frac{F_{p}}{P_{p}} - 1 = \\frac{p_{1} r_{1} + p_{2} r_{2}}{P_{p}} = w_{1} r_{1} + w_{2} r_{2} $$",
            "title": "Portfolio Return"
        },
        {
            "location": "/examples/mpt/#portfolio-risk",
            "text": "While  portfolio return  is  conceptually unambiguous , defining portfolio risk is less clear cut. In the case of Modern\nPortfolio Theory, risk is defined as the  variance or volatility of the returns , which is certainly consistent with intuition, \nsince high variance implies a high degree of uncertainty. Portfolio returns that are extremely volatile are not only emotionally \nhard to stomach, but may force an investor to crystallize losses at a very inopportune time due to a requirement to gain access \nto capital. Long term investors often think of risk in different terms, and in fact  Warren Buffet , \nwho is arguably the greatest investor of all time, would probably not consider volatility as an appropriate metric. Instead, \nhe would be more likely to think in terms of the potential for  permanent loss of capital .  Few of us have the investment acumen or long horizon of Warren Buffet, so for mere mortals, let us stick with return volatility \nas our best measure of portfolio risk. While individual asset return volatility is simple to compute, calculating portfolio \nlevel risk is less trivial as it involves understanding the  interaction of individual asset returns . That is to say, no \ntwo assets in a portfolio are likely to be 100% correlated, and therefore combining uncorrelated assets is bound to affect \nrisk in ways that need to be quantified. Let us begin with our definition of portfolio return variance which using the  expectation operator  is just the usual expression for  variance .   $$ \\sigma^2 = E[ ( R_{p} - \\bar{R}_{p} )^2 ] $$  In this equation \\(r_{p}\\) represents the return generated by an instance of the portfolio while \\(\\bar{r}_{p}\\) represents \nthe  average expected return  from this portfolio. To illustrate, consider a two asset portfolio where we expand the above \nexpression to be in terms of the individual assets that make up the portfolio.  $$ \\begin{align}\n\\sigma^2 &= E[ ( R_{p} - \\bar{R_{p}})^2 ] \\\\\n\\sigma^2 &= E[ ( w_{1} r_{1} + w_{2} r_{2} - (w_{1} \\bar{r_{1}} + w_{2}\\bar{r_{2}}) )^2 ] \\\\\n\\sigma^2 &= E[ ( w_{1} ( r_{1} - \\bar{r_{1}} ) + w_{2} ( r_{2} - \\bar{r_{2}}))^2 ] \\\\\n\\sigma^2 &= E[ w_{1}^2 ( r_{1} - \\bar{r_{1}} )^2 + w_{2}^2 ( r_{2} - \\bar{r_{2}})^2 + 2 w_{1} w_{2} E[(r_{1} - \\bar{r_{1}})(r_{2} - \\bar{r_{2}})]] \\\\\n\\end{align} $$  Since the  asset weights are not stochastic in nature , we can take them outside of the expectation operator to yield the following:  $$ \\sigma^2 = w_{1}^2 E[( r_{1} - \\bar{r_{1}})^2] + w_{2}^2 E[( r_{2} - \\bar{r_{2}})^2] + 2 w_{1} w_{2} E[(r_{1} - \\bar{r_{1}})(r_{2} - \\bar{r_{2}})]] $$  It now becomes clear that the portfolio variance is a function of the  individual asset variances  as well as their  covariance , so we can write the same expression in a more concise manner as shown below. Notice that this expression \nsuggests that if we combine risky assets in a portfolio that have a  negative covariance , the overall portfolio  risk will \nbe reduced . This is essentially diversification quantified, and it is a central principle of MPT.  $$ \\sigma^2 = w_{1}^2 \\sigma_{1}^2 + w_{2}^2 \\sigma_{2}^2 + 2 w_{1} w_{2} \\sigma_{12}  $$  While the above derivation is for a 2 asset portfolio, this expression can be generalized to an N asset portfolio and \nwritten in  matrix form  as below. The  w  term represents an  nx1  vector of asset weights, and capital sigma represents \nthe  nxn  covariance matrix of asset returns, the diagonal elements of which are the individual asset return variances.  $$ \\sigma^2 = w^T \\Sigma w $$",
            "title": "Portfolio Risk"
        },
        {
            "location": "/examples/mpt/#sharpe-ratio",
            "text": "The  Sharpe Ratio  is a widely used performance metric in finance and is named \nafter Nobel Laureate  William F. Sharpe  who first developed it. The ratio \nis basically the average return earned in  excess  of the  risk free rate  per unit of volatility, and is therefore a \nstandardized way of expressing  risk adjusted return . Mathematically, it is defined as follows (where \\(R_{p}\\) \nrepresents portfolio return, \\(\\bar{R_{f}}\\) is the risk free return and \\(\\sigma_{p}\\) is portfolio risk):  $$ S_{p} = \\frac{E[ R_{p} - \\bar{R_{f}}]}{\\sigma_{p}}  $$  A Sharpe Ratio can be computed  ex-ante  based on  forecasts  of expected risk and return, or otherwise as an  ex-post  \nmeasure based on realized returns. Given that the return measure (the stuff we like) is in the numerator and the risk measure\n(the stuff we don't like) is in the denominator, the higher the Sharpe Ratio the better.  While it is very widely used, the Sharpe Ratio is not without its detractors. One of the often quoted grievances with the\nmeasure is that it treats  upside volatility  equally with  downside volatility . This may be reasonable in a long / short\nportfolio, but perhaps less so in a long-only portfolio (i.e. no shorting constraint). In order to address this, a \nvariation of the Sharpe Ratio exists called the  Sortino Ratio  which\nonly includes returns below a certain threshold when computing volatility.   Another potential concern is that the Sharpe Ratio does not distinguish between  systematic  and  non-systematic  risk, \nthe latter of which can be diversified away in a well balanced portfolio. The  Treynor Ratio \nattempts to address this by estimating the systematic risk only, and using that as the denominator in the above expression.  Finally, one of the trickiest issues with the Sharpe Ratio is scaling it to different time horizons. For example, how\ndo you compute an annualized Sharpe from daily returns? Most techniques scale risk and return on the assumption that\nreturns are  normally distributed , however it is well known that asset returns are not normal and exhibit excess kurtosis  and often  skewness . For more details on some of the challenges in computing Sharpe Ratios, I high recommend a \npaper by Andrew W. Lo titled  The Statistics of Sharpe Ratios .",
            "title": "Sharpe Ratio"
        },
        {
            "location": "/examples/mpt/#examples",
            "text": "Now that we know how to calculate  portfolio return and risk , let us look at how we can apply this knowledge, and \nmore importantly, demonstrate how we can use Modern Portfolio Theory to construct an efficient investment portfolio of \nrisky assets.  The examples in the following sections leverage the Morpheus data source adapter for  Yahoo Finance ,\nmore details of which can be found  here . The library is available on  Maven Central  and can\ntherefore be added to your build tool of choice:  <dependency>\n    <groupId>com.zavtech</groupId>\n    <artifactId>morpheus-yahoo</artifactId>\n    <version>${VERSION}</version>\n</dependency>",
            "title": "Examples"
        },
        {
            "location": "/examples/mpt/#two-assets",
            "text": "Consider a  two asset  portfolio where we have already convinced ourselves that we want to buy Apple and Amazon, but we are not sure how much of our capital to invest in each. We obviously do not know what the future may bring, \nbut let us look back over the past year to see what happened on the assumption that it may help influence our decision.\nThe plot below shows the cumulative returns of both securities over the past year, which clearly demonstrates what a great \nrun they have had. With the benefit of hindsight of course, you would have invested all your capital in Apple as it \noutperformed Amazon by some margin. Looking forward however, the performance of these stocks could be reversed, so we \nshould probably spread our bets across the two.  \n       The code to generate this plot is as follows:   LocalDate end = LocalDate.now();\nLocalDate start = end.minusYears(1);\nArray<String> tickers = Array.of(\"AAPL\", \"AMZN\");\n\nYahooFinance yahoo = new YahooFinance();\nDataFrame<LocalDate,String> cumReturns = yahoo.getCumReturns(start, end, tickers);\ncumReturns.applyDoubles(v -> v.getDouble() * 100d);\n\nChart.create().withLinePlot(cumReturns, chart -> {\n    chart.title().withText(\"Cumulative Asset Returns\");\n    chart.subtitle().withText(\"Range: (\" + start + \" to\" + end + \")\");\n    chart.plot().axes().domain().label().withText(\"Date\");\n    chart.plot().axes().range(0).label().withText(\"Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.show();\n});  To get a sense of the risk / return characteristics of our two asset portfolio, we are going to  generate 10,000 random \nportfolios  where we invest different amounts in Apple and Amazon ranging from 0% to 100% of our capital in one asset and \nthe remainder in the other asset. The function below returns a  DataFrame  of random portfolio weights in N assets that \nsum to 1 in all cases, and therefore each portfolio represents a  fully invested  scenario.   /**\n * A function that generates N long only random portfolios with weights that sum to 1\n * @param count     the number of portfolios / rows in the DataFrame\n * @param tickers   the security tickers to include\n * @return          the frame of N random portfolios, 1 per row\n */\nDataFrame<Integer,String> randomPortfolios(int count, Iterable<String> tickers) {\n    DataFrame<Integer,String> weights = DataFrame.ofDoubles(Range.of(0, count), tickers);\n    weights.applyDoubles(v -> Math.random());\n    weights.rows().forEach(row -> {\n        final double sum = row.stats().sum();\n        row.applyDoubles(v -> {\n            double weight = v.getDouble();\n            return weight / sum;\n        });\n    });\n    return weights;\n}  In order to calculate the risk and return characteristics of these 10,000 portfolios, we first need to compute the  covariance matrix of the returns  between Apple and Amazon, and then also compute the  cumulative asset returns  \nover the historical horizon in question. With these quantities, it is fairly simple to compute the risk & return of \neach portfolio, which can then be plotted on a scatter chart to see how they compare. The resulting plot is shown \nbelow, and is followed by the code to generate it. Note that since we are using daily returns to compute the covariance \nmatrix, we need to  annualize  it, which we do by multiplying by 252 (on the assumption there are 252 trading days \nin the year). Since the cumulative returns are based on a 1-year look back window, there is no need to annualize the \nreturns.  \n        //Define portfolio count, investment horizon and universe\nint count = 10000;\nLocalDate end = LocalDate.now();\nLocalDate start = end.minusYears(1);\nArray<String> tickers = Array.of(\"AAPL\", \"AMZN\");\n\n//Grab daily returns and cumulative returns from Yahoo Finance\nYahooFinance yahoo = new YahooFinance();\nDataFrame<LocalDate,String> dayReturns = yahoo.getDailyReturns(start, end, tickers);\nDataFrame<LocalDate,String> cumReturns = yahoo.getCumReturns(start, end, tickers);\n\n//Compute asset covariance matrix from daily returns and annualize\nDataFrame<String,String> sigma = dayReturns.cols().stats().covariance().applyDoubles(v -> v.getDouble() * 252);\nDataFrame<LocalDate,String> assetReturns = cumReturns.rows().last().map(DataFrameRow::toDataFrame).get();\n\n//Generate random portfolios and compute risk & return for each\nDataFrame<Integer,String> portfolios = randomPortfolios(count, tickers);\nDataFrame<Integer,String>  results = DataFrame.ofDoubles(Range.of(0, count), Array.of(\"Risk\", \"Return\"));\nportfolios.rows().forEach(row -> {\n    DataFrame<Integer,String> weights = row.toDataFrame();\n    double portReturn = weights.dot(assetReturns.transpose()).data().getDouble(0, 0);\n    double portVariance = weights.dot(sigma).dot(weights.transpose()).data().getDouble(0, 0);\n    results.data().setDouble(row.key(), \"Return\", portReturn * 100d);\n    results.data().setDouble(row.key(), \"Risk\", Math.sqrt(portVariance) * 100d);\n});\n\n//Plot the results using a scatter plot\nChart.create().withScatterPlot(results, false, \"Risk\", chart -> {\n    chart.title().withText(\"Risk / Return Profiles For AAPL+AMZN Portfolios\");\n    chart.subtitle().withText(count + \" Portfolio Combinations Simulated\");\n    chart.plot().axes().domain().label().withText(\"Portfolio Risk\");\n    chart.plot().axes().domain().format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.plot().axes().range(0).label().withText(\"Portfolio Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.show();\n});  There are a few notable observations about the plot as follows:   It appears to be impossible to create a portfolio with a risk lower than about 15.7% volatility.  If you were willing to accept 16% risk, there are two portfolios, one of which has a much better return than the other.  Much beyond 17% risk, it appears returns suffer tremendously due to overweight in one of the assets.  The best possible return appears to be about 37% at a risk level just north of 17%.   A rational investor clearly wants to be on the upper part of this curve for a  given level of risk . For example, \nat 17% volatility, you could either have achieved 23.5% return with a portfolio on the lower segment of the curve \nor close to 37% on the upper part. Which one do you prefer? The upper part of the curve is referred to as the  Efficient Frontier , and moreover, there exists a special case \non this curve often referred to as the  Markowitz Portfolio , which has the highest risk adjusted return of all \nthe candidate portfolios. It is essentially the  mean-variance optimal portfolio  and can be calculated by \nmaximizing the utility defined by the objective function below, subject to whatever constraints you may impose \n(in this case we assume  long only  and  fully invested ):  $$ U_{p} = w^T r - w^T \\Sigma w $$  The \\(w\\) and \\(r\\) terms represent  nx1  vectors of asset  weights  and  returns  respectively, while capital  sigma  represents the  nxn  asset covariance matrix. This objective function is essentially saying we like return \nand do not like risk, and our goal is to select an  nx1  vector of asset weights that maximizes this expression given \nasset returns and a covariance matrix. In this example, we are looking back at a historical scenario so we can calculate \nreturns and the covariance based on realized market prices. In reality, we need to make  judgements about the future , \nand therefore are required to  estimate future returns and covariance , which is where things can get tricky. Calculating \nthe  Markowitz Portfolio  given constraints (as in this case where we impose a long only fully invested constraint such \nthat the elements of W are all positive and sum to 1) is a  quadratic optimization \nproblem that requires appropriate software and is beyond the scope of this article. A good commercial package \nto consider is  MOSEK , or for an Open Source \nsolution you may consider  OptaPlanner .",
            "title": "Two Assets"
        },
        {
            "location": "/examples/mpt/#asset-selection",
            "text": "Modern Portfolio Theory is fundamentally about  sizing positions  of risky assets in a portfolio, it is not \nabout asset selection. Having said that, it can be useful to compare the risk / return profiles of portfolios \nconstructed from different risky assets. In the prior example we had already decided that we wanted to invest \nin Apple and Amazon, but were there better two asset portfolio combinations we should have considered? The plot \nbelow, followed by the code that generated it, is essentially an extension of the prior example where in this \ncase we generate multiple 10,000 portfolio combinations with different asset constituents to see how they compare.  \n       It is clear from the chart that the 5 two asset portfolios in this example have very different risk / return\nprofiles, with the Apple / Amazom combination being the most risky, but certainly having some of the highest\nreturns. With that being said, if your investment objective was to achieve around a 12.0% return (very\naspirational in today's world of zero interest rates), your best bet would be to choose the VTI / BND combination\nas it appears to have the  potentia l to generate this return for less than 5% risk. Compare this to some of the other \nportfolio combinations for which you need to accept a much  higher level of risk  to achieve the  same return .   //Define portfolio count, investment horizon\nint count = 10000;\nLocalDate end = LocalDate.now();\nLocalDate start = end.minusYears(1);\nYahooFinance yahoo = new YahooFinance();\n\nArray<DataFrame<Integer,String>> results = Array.of(\n    Array.of(\"VTI\", \"BND\"),\n    Array.of(\"AAPL\", \"AMZN\"),\n    Array.of(\"GOOGL\", \"BND\"),\n    Array.of(\"ORCL\", \"KO\"),\n    Array.of(\"VWO\", \"VNQ\")\n).map(v -> {\n    //Access tickers\n    Array<String> tickers = v.getValue();\n    //Grab daily returns and cumulative returns from Yahoo Finance\n    DataFrame<LocalDate,String> dayReturns = yahoo.getDailyReturns(start, end, tickers);\n    DataFrame<LocalDate,String> cumReturns = yahoo.getCumReturns(start, end, tickers);\n    //Compute asset covariance matrix from daily returns and annualize\n    DataFrame<String,String> sigma = dayReturns.cols().stats().covariance().applyDoubles(x -> x.getDouble() * 252);\n    DataFrame<LocalDate,String> assetReturns = cumReturns.rows().last().map(DataFrameRow::toDataFrame).get();\n    //Generate random portfolios and compute risk & return for each\n    String label = String.format(\"%s+%s\", tickers.getValue(0), tickers.getValue(1));\n    DataFrame<Integer,String> portfolios = randomPortfolios(count, tickers);\n    DataFrame<Integer,String>  riskReturn = DataFrame.ofDoubles(Range.of(0, count), Array.of(\"Risk\", label));\n    portfolios.rows().forEach(row -> {\n        DataFrame<Integer,String> weights = row.toDataFrame();\n        double portReturn = weights.dot(assetReturns.transpose()).data().getDouble(0, 0);\n        double portVariance = weights.dot(sigma).dot(weights.transpose()).data().getDouble(0, 0);\n        riskReturn.data().setDouble(row.key(), label, portReturn * 100d);\n        riskReturn.data().setDouble(row.key(), \"Risk\", Math.sqrt(portVariance) * 100d);\n    });\n    return riskReturn;\n});\n\nDataFrame<Integer,String> first = results.getValue(0);\nChart.create().<Integer,String>withScatterPlot(first, false, \"Risk\", chart -> {\n    for (int i=1; i<results.length(); ++i) {\n        chart.plot().<String>data().add(results.getValue(i), \"Risk\");\n        chart.plot().render(i).withDots();\n    }\n    chart.plot().axes().domain().label().withText(\"Portfolio Risk\");\n    chart.plot().axes().domain().format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.plot().axes().range(0).label().withText(\"Portfolio Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.title().withText(\"Risk / Return Profiles of Various Two Asset Portfolios\");\n    chart.subtitle().withText(count + \" Portfolio Combinations Simulated\");\n    chart.legend().on().right();\n    chart.show();\n});",
            "title": "Asset Selection"
        },
        {
            "location": "/examples/mpt/#multiple-assets",
            "text": "So far we have limited our example portfolios to two assets in order to help develop the intuition behind Modern \nPortfolio Theory. In reality however, real world portfolios are likely to include more assets, although exactly \nhow many are required to achieve a reasonable level of diversification is open to debate. Consider an investable \nuniverse of 6 securities represented by broad based low cost ETFs that serve as reasonable proxies for major asset \nclasses. The table below summarizes these candidates.     Ticker  Name  Provider       VWO  Vanguard FTSE Emerging Markets ETF  Vanguard  Details    VNQ  Vanguard REIT ETF  Vanguard  Details    VEA  Vanguard FTSE Developed Markets ETF  Vanguard  Details    DBC  PowerShares DB Commodity Tracking ETF  Powershares  Details    VTI  Vanguard Total Stock Market ETF  Vanguard  Details    BND  Vanguard Total Bond Market ETF  Vanguard  Details     To get a sense of how the risk / return profiles of portfolios evolve as we include more assets, we can generate 10,000 \nrandom portfolios first with 2 assets, then 3 and all the way to 6. In the case of two assets, we expect to see our \nrotated parabola, but what happens when you have more degrees of freedom in the portfolio? The plot below is the answer.\nAs we go beyond two assets, the scatter becomes more pronounced, and while risk is generally reduced due to the fact that \nthe assets are not perfectly correlated, return also suffers somewhat. Having said that, the  Efficient Frontiers  of\nthe more diversified portfolios appear to provide a better risk / return trade off than the two asset portfolio.    \n       The code to generate the above plot of 5 lots of 10,000 portfolios with various assets is as follows:    //Define portfolio count, investment horizon\nint count = 10000;\nLocalDate end = LocalDate.now();\nLocalDate start = end.minusYears(1);\nYahooFinance yahoo = new YahooFinance();\n\nArray<DataFrame<Integer,String>> results = Array.of(\n    Array.of(\"VWO\", \"VNQ\"),\n    Array.of(\"VWO\", \"VNQ\", \"VEA\"),\n    Array.of(\"VWO\", \"VNQ\", \"VEA\", \"DBC\"),\n    Array.of(\"VWO\", \"VNQ\", \"VEA\", \"DBC\", \"VTI\"),\n    Array.of(\"VWO\", \"VNQ\", \"VEA\", \"DBC\", \"VTI\", \"BND\")\n).map(v -> {\n    //Access tickers\n    Array<String> tickers = v.getValue();\n    //Grab daily returns and cumulative returns from Yahoo Finance\n    DataFrame<LocalDate,String> dayReturns = yahoo.getDailyReturns(start, end, tickers);\n    DataFrame<LocalDate,String> cumReturns = yahoo.getCumReturns(start, end, tickers);\n    //Compute asset covariance matrix from daily returns and annualize\n    DataFrame<String,String> sigma = dayReturns.cols().stats().covariance().applyDoubles(x -> x.getDouble() * 252);\n    DataFrame<LocalDate,String> assetReturns = cumReturns.rows().last().map(DataFrameRow::toDataFrame).get();\n    //Generate random portfolios and compute risk & return for each\n    String label = String.format(\"%s Assets\", tickers.length());\n    DataFrame<Integer,String> portfolios = randomPortfolios(count, tickers);\n    DataFrame<Integer,String>  riskReturn = DataFrame.ofDoubles(Range.of(0, count), Array.of(\"Risk\", label));\n    portfolios.rows().forEach(row -> {\n        DataFrame<Integer,String> weights = row.toDataFrame();\n        double portReturn = weights.dot(assetReturns.transpose()).data().getDouble(0, 0);\n        double portVariance = weights.dot(sigma).dot(weights.transpose()).data().getDouble(0, 0);\n        riskReturn.data().setDouble(row.key(), 1, portReturn * 100d);\n        riskReturn.data().setDouble(row.key(), 0, Math.sqrt(portVariance) * 100d);\n    });\n    return riskReturn;\n});\n\nDataFrame<Integer,String> first = results.getValue(0);\nChart.create().<Integer,String>withScatterPlot(first, false, \"Risk\", chart -> {\n    for (int i=1; i<results.length(); ++i) {\n        chart.plot().<String>data().add(results.getValue(i), \"Risk\");\n        chart.plot().render(i).withDots();\n    }\n    chart.plot().axes().domain().label().withText(\"Portfolio Risk\");\n    chart.plot().axes().domain().format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.plot().axes().range(0).label().withText(\"Portfolio Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.title().withText(\"Risk / Return Profiles of Portfolios With Increasing Assets\");\n    chart.subtitle().withText(count + \" Portfolio Combinations Simulated\");\n    chart.legend().on().bottom();\n    chart.show();\n});",
            "title": "Multiple Assets"
        },
        {
            "location": "/examples/mpt/#robo-advisor",
            "text": "A fairly recent innovation in the investment management space relates to what are called  Robo-Advisors ,\nwhich are essentially online investment solutions aimed mostly at retail investors. They are called Robo-Advisors\nbecause they automate the construction of portfolios using software based on an investor's risk appetite and their \ninvestment objective, which they assess by posing a number of questions via their website. There are already many \nplayers in this space, and most of them construct well-balanced portfolios consisting of 5-7 assets, all of which \nare  broad based and low cost   Exchange Traded Funds .  To avoid any suggestion that I am endorsing or recommending these services, I am consciously avoiding naming names, but \nI visited the website of one of the larger advisors and proceeded to complete the questionnaire after which it proposed \nthe portfolio in the table below. The purpose of this discussion is not to make any judgement on how good this portfolio \nis versus other potential investments, but really to get a sense of how efficient the proposed portfolio is from a \nrisk / return  stand point.     Ticker  Name  Weight  Provider  Details      VTI  Vanguard Total Stock Market ETF  35%  Vanguard  Details    VEA  Vanguard FTSE Developed Markets ETF  21%  Vanguard  Details    VWO  Vanguard FTSE Emerging Markets ETF  16%  Vanguard  Details    VTEB  Vanguard Tax-Exempt Bond ETF  15%  Vanguard  Details    VIG  Vanguard Dividend Appreciation ETF  8%  Vanguard  Details    XLE  Energy Select Sector SPDR ETF  5%  State Street  Details",
            "title": "Robo-Advisor"
        },
        {
            "location": "/examples/mpt/#ramdom-portfolios",
            "text": "Ignoring the proposed weightings for the moment, consider generating 10,000  long-only fully invested random portfolios  \ninvolving these assets, and then computing the resulting equity curves. This will give us a sense of the various scenarios we can \ngenerate with this asset universe, and in particular allow us to understand the  degree of dispersion  in outcomes. The plot \nbelow illustrates 10,000 equity curves based on the  past 1 year returns , suggesting a return spread ranging from approximately \n2.5% all the way to 18.15%, which is pretty enormous.  \n       The code to generate the above plot is as follows:   int portfolioCount = 10000;\nRange<LocalDate> range = Range.of(LocalDate.now().minusYears(1), LocalDate.now());\nArray<String> tickers = Array.of(\"VTI\", \"BND\", \"VWO\", \"VTEB\", \"VIG\", \"XLE\");\nDataFrame<Integer,String> portfolios = randomPortfolios(portfolioCount, tickers);\nDataFrame<LocalDate,String> performance = getEquityCurves(range, portfolios);\nChart.create().withLinePlot(performance.applyDoubles(v -> v.getDouble() * 100d), chart -> {\n    chart.title().withText(portfolioCount + \" Equity Curves (Past 1 Year Returns)\");\n    chart.subtitle().withText(\"Robo-Advisor Universe: VTI, BND, VWO, VTEB, VIG, XLE\");\n    chart.plot().axes().domain().label().withText(\"Date\");\n    chart.plot().axes().range(0).label().withText(\"Portfolio Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.show();\n});  The example uses the following function in order to compute the equity curves for the  DataFrame  of 10,000 random portfolios. \nThis function expects an  mxn  frame of portfolio weighting configurations where  m  is the number of portfolios and  n  the number \nof assets. The resulting  DataFrame  has  txm  dimensions where  t  is the number of dates, and the  m  columns are labelled  P0 ,  P1  through to  P(m) .   /**\n * Calculates equity curves over a date range given a frame on initial portfolio weight configurations\n * @param range         the date range for historical returns\n * @param portfolios    MxN DataFrame of portfolio weights, M portfolios, N assets\n * @return              the cumulative returns for each portfolio, TxM, portfolios labelled P0, P1 etc...\n */\nDataFrame<LocalDate,String> getEquityCurves(Range<LocalDate> range, DataFrame<Integer,String> portfolios) {\n    final YahooFinance yahoo = new YahooFinance();\n    final Iterable<String> tickers = portfolios.cols().keyArray();\n    final DataFrame<LocalDate,String> cumReturns = yahoo.getCumReturns(range.start(), range.end(), tickers);\n    final Range<String> colKeys = Range.of(0, portfolios.rowCount()).map(i -> \"P\" + i);\n    return DataFrame.ofDoubles(cumReturns.rows().keyArray(), colKeys, v -> {\n        double totalReturn = 0d;\n        for (int i=0; i<portfolios.colCount(); ++i) {\n            final double weight = portfolios.data().getDouble(v.colOrdinal(), i);\n            final double assetReturn = cumReturns.data().getDouble(v.rowOrdinal(), i);\n            totalReturn += (weight * assetReturn);\n        }\n        return totalReturn;\n    });\n}",
            "title": "Ramdom Portfolios"
        },
        {
            "location": "/examples/mpt/#proposed-portfolio",
            "text": "Given the  proposed portfolio weights  presented earlier, we can use the past 1 year of returns for the assets in question\nto assess what the risk and return of this portfolio would have been had we invested a year ago. The code below performs \nthis analysis and suggests that the portfolio returned a very respectable  14.9%  for  7.0%  risk, which is pretty phenomenal \n(a 2  Sharpe  portfolio over the past year). The code to generate these results is as follows:   //Defines investment horizon & universe\nLocalDate end = LocalDate.now();\nLocalDate start = end.minusYears(1);\n//Define investment universe and weights\nArray<String> tickers = Array.of(\"VTI\", \"VEA\", \"VWO\", \"VTEB\", \"VIG\", \"XLE\");\n//Define DataFrame of position weights suggested by Wealthfront\nDataFrame<String,String> portfolio = DataFrame.of(tickers, String.class, columns -> {\n    columns.add(\"Weights\", Array.of(0.35d, 0.21d, 0.16d, 0.15d, 0.08d, 0.05d));\n});\n//Grab daily returns and cumulative returns from Yahoo Finance\nYahooFinance yahoo = new YahooFinance();\nDataFrame<LocalDate,String> dayReturns = yahoo.getDailyReturns(start, end, tickers);\nDataFrame<LocalDate,String> cumReturns = yahoo.getCumReturns(start, end, tickers);\n//Compute asset covariance matrix from daily returns and annualize\nDataFrame<String,String> sigma = dayReturns.cols().stats().covariance().applyDoubles(x -> x.getDouble() * 252);\nDataFrame<LocalDate,String> assetReturns = cumReturns.rows().last().map(DataFrameRow::toDataFrame).get();\n//Generate DataFrame of portfolio weights\ndouble portReturn = portfolio.transpose().dot(assetReturns.transpose()).data().getDouble(0, 0);\ndouble portVariance = portfolio.transpose().dot(sigma).dot(portfolio).data().getDouble(0, 0);\nIO.println(String.format(\"Portfolio Return: %s\", portReturn));\nIO.println(String.format(\"Portfolio Risk: %s\", Math.sqrt(portVariance)));  Now that we know how the currently proposed portfolio performed over the past year, let us generate 100,000 random portfolios \nin these 6 assets to get a sense of the overall risk / return profile of this universe, and see how the proposed portfolio \ncompares. The plot below suggests that the proposed portfolio (green dot) is indeed pretty efficient, and may also suggest \nthat this particular Robo-Advisor's future expectations are not that different from the past year.  One may wonder why the  proposed portfolio  is not exactly on the  Efficient Frontier , and there are several explanations \nfor this. The first is that the Robo-Advisor proposed weights are for a  forward looking portfolio , and our example is using \npast 1 year returns to test its efficiency. Secondly, there are an infinite number of ways of estimating an  ex-ante  covariance \nmatrix, which may involve  exponentially smoothing  returns (perhaps using different  half-lives  to estimate diagonal versus \noff-diagonal terms), and perhaps applying shrinkage to off-diagonal terms to improve stability. In our example, we have the  benefit of hindsight  and simply compute an  ex-post  covariance matrix, doing nothing fancy at all. Finally, the Robo-Advisor \nmay impose risk constraints (such as capping any one position to be no more than 35% of the portfolio perhaps), which may \npenalize the strategy versus a less constrained instance, but which may be a very sensible compromise.  \n       The code to generate this plot is as follows:   //Define portfolio count, investment horizon and universe\nint count = 100000;\nLocalDate end = LocalDate.now();\nLocalDate start = end.minusYears(1);\nArray<String> tickers = Array.of(\"VTI\", \"VEA\", \"VWO\", \"VTEB\", \"VIG\", \"XLE\");\n\n//Grab daily returns and cumulative returns from Yahoo Finance\nYahooFinance yahoo = new YahooFinance();\nDataFrame<LocalDate,String> dayReturns = yahoo.getDailyReturns(start, end, tickers);\nDataFrame<LocalDate,String> cumReturns = yahoo.getCumReturns(start, end, tickers);\n\n//Compute asset covariance matrix from daily returns and annualize\nDataFrame<String,String> sigma = dayReturns.cols().stats().covariance().applyDoubles(v -> v.getDouble() * 252);\nDataFrame<LocalDate,String> assetReturns = cumReturns.rows().last().map(DataFrameRow::toDataFrame).get();\n\n//Generate random portfolios and compute risk & return for each\nDataFrame<Integer,String> portfolios = randomPortfolios(count, tickers);\nDataFrame<Integer,String>  results = DataFrame.ofDoubles(Range.of(0, count), Array.of(\"Risk\", \"Random\"));\nportfolios.rows().forEach(row -> {\n    DataFrame<Integer,String> weights = row.toDataFrame();\n    double portReturn = weights.dot(assetReturns.transpose()).data().getDouble(0, 0);\n    double portVariance = weights.dot(sigma).dot(weights.transpose()).data().getDouble(0, 0);\n    results.data().setDouble(row.key(), \"Random\", portReturn * 100d);\n    results.data().setDouble(row.key(), \"Risk\", Math.sqrt(portVariance) * 100d);\n});\n\n//Create DataFrame with risk / return of proposed Wealthfront portfolio\nDataFrame<Integer,String> proposed = DataFrame.of(Range.of(0, 1), String.class, cols -> {\n    cols.add(\"Risk\", Array.of(0.068950 * 100d));\n    cols.add(\"Chosen\", Array.of(0.1587613 * 100d));\n});\n\n//Plot the results using a scatter plot\nChart.create().withScatterPlot(results, false, \"Risk\", chart -> {\n    chart.title().withText(\"Risk / Return Profile For Wealthfront Portfolio\");\n    chart.subtitle().withText(count + \" Portfolio Combinations Simulated\");\n    chart.plot().<String>data().add(proposed, \"Risk\");\n    chart.plot().render(1).withDots();\n    chart.plot().axes().domain().label().withText(\"Portfolio Risk\");\n    chart.plot().axes().domain().format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.plot().axes().range(0).label().withText(\"Portfolio Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.legend().on().right();\n    chart.show();\n});",
            "title": "Proposed Portfolio"
        },
        {
            "location": "/examples/mpt/#efficiency",
            "text": "The previous section established that the  proposed portfolio  was reasonably efficient in the context of the past 1 year of \nreturns (how efficient it will be over the next year is of course impossible to know, but all else being equal, it seems a decent \nconfiguration). In this section, we further consider its relative efficiency by looking at the equity curve of the  best  and  worst  portfolios that we find in our 100,000 random candidates (based on their  Sharpe Ratios ). In addition, \nwe overlay the equity curve of the  S&P500  by using the SPY Exchange Traded Fund as a proxy.  The plot below shows that the  proposed portfolio  actually outperformed what is labelled as the  Best  portfolio in pure  return space , but not in  risk-adjusted terms . That is, it yields a lower return per unit of risk than the  best  portfolio,\nand the fact that it outperformed in return space is just a fluke. The  worst  portfolio is a genuine shocker, and in fact\nspent much of the past year going nowhere before bouncing back starting in August.  \n       The code to generate this plot is as follows, and leverages the  getEquityCurves()  function discussed earlier.   int portfolioCount = 1000;\nRange<LocalDate> range = Range.of(LocalDate.now().minusYears(1), LocalDate.now());\nArray<String> tickers = Array.of(\"VTI\", \"VEA\", \"VWO\", \"VTEB\", \"VIG\", \"XLE\");\nArray<Double> proposed = Array.of(0.35d, 0.21d, 0.16d, 0.15d, 0.08d, 0.05d);\nDataFrame<Integer,String> portfolios = randomPortfolios(portfolioCount, tickers);\nportfolios.rowAt(0).applyDoubles(v -> proposed.getDouble(v.colOrdinal()));\n//Compute risk / return / sharpe for random portfolios\nDataFrame<Integer,String> riskReturn = calcRiskReturn(range, portfolios).rows().sort(false, \"Sharpe\");\n//Capture portfolio keys for chosen, best and worst portfolio based on sharpe\nDataFrame<Integer,String> candidates = portfolios.rows().select(0,\n    riskReturn.rows().first().get().key(),\n    riskReturn.rows().last().get().key()\n);\n//Compute equity curves for chosen, best and worst\nDataFrame<LocalDate,String> equityCurves = getEquityCurves(range, candidates).cols().mapKeys(col -> {\n    switch (col.ordinal()) {\n        case 0: return \"Chosen\";\n        case 1: return \"Best\";\n        case 2: return \"Worst\";\n        default: return col.key();\n    }\n});\n//Capture returns of S&P 500\nYahooFinance yahoo = new YahooFinance();\nDataFrame<LocalDate,String> spy = yahoo.getCumReturns(range.start(), range.end(), \"SPY\");\n\n//Plot the equity curves\nChart.create().withLinePlot(equityCurves.applyDoubles(v -> v.getDouble() * 100d), chart -> {\n    chart.title().withText(\"Best/Worst/Chosen Equity Curves (Past 1 Year Returns) + SPY\");\n    chart.subtitle().withText(\"Robo-Advisor Universe: VTI, VEA, VWO, VTEB, VIG, XLE\");\n    chart.plot().<String>data().add(spy.times(100d));\n    chart.plot().axes().domain().label().withText(\"Date\");\n    chart.plot().axes().range(0).label().withText(\"Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.plot().style(\"Chosen\").withColor(Color.BLACK).withLineWidth(1.5f);\n    chart.plot().style(\"Best\").withColor(Color.GREEN.darker().darker()).withLineWidth(1.5f);\n    chart.plot().style(\"Worst\").withColor(Color.RED).withLineWidth(1.5f);\n    chart.plot().style(\"SPY\").withColor(Color.BLUE);\n    chart.legend().on();\n    chart.show();\n});  The implementation of  calcRiskReturn()  in this example is as follows:   /**\n * Returns a DataFrame containing risk, return and sharpe ratio for portfolios over the date range\n * @param range         the date range for historical returns\n * @param portfolios    the DataFrame of portfolio weights, one row per portfolio configuration\n * @return              the DataFrame with risk, return and sharpe\n */\nDataFrame<Integer,String> calcRiskReturn(Range<LocalDate> range, DataFrame<Integer,String> portfolios) {\n    YahooFinance yahoo = new YahooFinance();\n    Array<String> tickers = portfolios.cols().keyArray();\n    DataFrame<LocalDate,String> dayReturns = yahoo.getDailyReturns(range.start(), range.end(), tickers);\n    DataFrame<LocalDate,String> cumReturns = yahoo.getCumReturns(range.start(), range.end(), tickers);\n    //Compute asset covariance matrix from daily returns and annualize\n    DataFrame<String,String> sigma = dayReturns.cols().stats().covariance().applyDoubles(x -> x.getDouble() * 252);\n    DataFrame<LocalDate,String> assetReturns = cumReturns.rows().last().map(DataFrameRow::toDataFrame).get();\n    //Prepare 3 column DataFrame to capture results\n    DataFrame<Integer,String>  riskReturn = DataFrame.ofDoubles(\n        portfolios.rows().keyArray(),\n        Array.of(\"Risk\", \"Return\", \"Sharpe\")\n    );\n    portfolios.rows().forEach(row -> {\n        DataFrame<Integer,String> weights = row.toDataFrame();\n        double portReturn = weights.dot(assetReturns.transpose()).data().getDouble(0, 0);\n        double portVariance = weights.dot(sigma).dot(weights.transpose()).data().getDouble(0, 0);\n        riskReturn.data().setDouble(row.key(), \"Return\", portReturn * 100d);\n        riskReturn.data().setDouble(row.key(), \"Risk\", Math.sqrt(portVariance) * 100d);\n        riskReturn.data().setDouble(row.key(), \"Sharpe\", portReturn / Math.sqrt(portVariance));\n    });\n    return riskReturn;\n}",
            "title": "Efficiency"
        },
        {
            "location": "/viz/charts/overview/",
            "text": "Introduction\n\n\nThe Morpheus visualization library defines a \nsimple chart abstraction API\n with adapters supporting both \n\nJFreeChart\n as well as \nGoogle Charts\n (with others\nto follow by popular demand). This design makes it possible to generate interactive \nJava Swing\n \ncharts as well as HTML5 browser based charts via the same API. By default, the framework is configured to use the JFreeChart \nadapter, however this can be re-configured on a global basis by calling either \nhtmlMode()\n or \nswingMode()\n as shown \nbelow.\n\n\n\n\n\n//Switch chart adapter to HTML mode globally \nChart.create().htmlMode();\n\n//Switch chart adapter to SWING mode globally \nChart.create().swingMode();\n\n\n\n\nIt is also possible to operate in \nmixed mode\n from within the the same application rather than switching the adapter \nglobally. By explicitly calling \nasHtml()\n or \nasSwing()\n prior to invoking one of the plotting functions on the \nChart\n\ninterface, Html and Swing based charts can be generated from within the same application as shown below.  \n\n\n\n\n\n//Create chart using SWING adapter\nChart.create().asSwing().withLinePlot(frame, chart -> {\n    chart.title().withText(\"Chart Title Goes Here...\");\n    chart.legend().on().bottom();\n    chart.show();\n});\n\n//Create chart using HTML adapter\nChart.create().asHtml().withLinePlot(frame, chart -> {\n    chart.title().withText(\"Chart Title Goes Here...\");\n    chart.legend().on().bottom();\n    chart.show();\n});\n\n\n\n\nThe following sections demonstrate how to use the Morpheus charting API, and provide various examples of what kind of \ncharts are supported. The illustrations below are PNG files generated using the JFreeChart adapter, however a \ngallery\n \nof the same plots generated via the \nGoogle adapter\n show just how similar the plots from the two implementations are. \nWhile most of the functionality exposed by the Morpheus Charting API are supported by both adapters, there are some gaps \nin the Google adapter which are documented below.\n\n\nLine Charts\n\n\nSingle Series\n\n\nConsider the \nDataFrame\n below with dimensions \n1000x1\n which has a row axis of type \nLocalDate\n and 1 column of \ndouble precision values representing the cumulative sum of an \nArray\n of normally distributed random values. The first \n10 rows of this frame are printed below.\n\n\n\n\n\nimport com.zavtech.morpheus.array.Array;\nimport com.zavtech.morpheus.frame.DataFrame;\n\nint rowCount = 1000;\nLocalDate startDate = LocalDate.of(2013, 1, 1);\nRange<LocalDate> dates = Range.of(0, rowCount).map(startDate::plusDays);\nDataFrame<LocalDate,String> frame = DataFrame.of(dates, String.class, columns -> {\n    columns.add(\"A\", Array.randn(rowCount).cumSum());\n});\n\n\n\n\n\n   Index     |       A       |\n------------------------------\n 2013-01-01  |   0.58638321  |\n 2013-01-02  |  -0.44176283  |\n 2013-01-03  |  -0.07187819  |\n 2013-01-04  |  -1.31157143  |\n 2013-01-05  |  -1.69375864  |\n 2013-01-06  |  -2.23840733  |\n 2013-01-07  |  -2.42279587  |\n 2013-01-08  |  -2.95871372  |\n 2013-01-09  |  -3.63748847  |\n \n\n\n\nTo generate an line plot of this series, we can use the dates in the \nDataFrame\n row axis as the \nx-values\n, and \nthe numeric values in column \nA\n as the range or \ny-values\n. The \nwithLines()\n method on the \nChartFactory\n interface\nexpects the \nDataFrame\n containing the data to plot, and a \nConsumer\n which is used to configure various features of \nthe chart. In the example below, we simply display the chart with no further customization by calling \nshow()\n.\n\n\n\n\n\nChart.create().withLinePlot(frame, chart -> {\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nMultiple Series\n\n\nA common scenario is to generate an line plot where the domain or \nx-axis\n is based on data in a specific column of the frame \nrather than the \nrow axis\n as in the previous example. This can be done by passing the label of the column to use for \nthe domain axis to the \nwithLines()\n method. In the example below, we create a similar dataset to above but with dimensions \n\n1000x5\n, and in this case the row axis is simply a sequence of integers with the dates included as a column keyed as \nDataDate\n. \n\n\n\n\n\nint rowCount = 1000;\nLocalDate startDate = LocalDate.of(2013, 1, 1);\nRange<Integer> rowKeys = Range.of(0, rowCount);\nRange<LocalDate> dates = rowKeys.map(startDate::plusDays);\nDataFrame<Integer,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    columns.add(\"DataDate\", dates);\n    Stream.of(\"A\", \"B\", \"C\", \"D\").forEach(label -> {\n        columns.add(label, Array.randn(rowCount).cumSum());\n    });\n});\n\n\n\n\n\n Index  |   DataDate   |      A       |       B       |       C       |       D       |\n---------------------------------------------------------------------------------------\n     0  |  2013-01-01  |  1.29439793  |  -0.91248479  |  -0.51141634  |   0.45271667  |\n     1  |  2013-01-02  |  1.04502117  |   -1.5841936  |   1.04050209  |    0.3773374  |\n     2  |  2013-01-03  |   3.0924616  |  -2.35489228  |   3.35960923  |  -1.52130364  |\n     3  |  2013-01-04  |  2.58882443  |  -1.80476051  |   3.79982322  |  -2.03083571  |\n     4  |  2013-01-05  |  2.95265199  |  -3.22752153  |   4.92200533  |  -0.05268766  |\n     5  |  2013-01-06  |  2.54600084  |  -3.46228629  |   3.34627638  |  -0.73170641  |\n     6  |  2013-01-07  |  2.11506239  |  -3.34459359  |   3.68209019  |  -0.80961531  |\n     7  |  2013-01-08  |   3.1889199  |  -3.75034967  |    3.7681561  |    0.0668972  |\n     8  |  2013-01-09  |  4.83991797  |  -3.21969887  |   2.99041415  |   0.89681662  |\n     9  |  2013-01-10  |  4.33025037  |  -2.65846363  |   2.44799574  |   0.62896343  |\n\n\n\n\n\n\n\nGiven that we are plotting multiple series, we also turn on the chart legend, and place it at the bottom of the chart.\n\n\n\n\n\nChart.create().withLinePlot(frame, \"DataDate\", chart -> {\n    chart.legend().on().bottom();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nSeries Specific Style\n\n\nExtending the prior example, below we generate the same \n1000x5\n dataset but add an additional column post construction \nwhich represents the sum of values in each row. In addition, we add \ntext\n to the chart in the form of a \ntitle\n, \n\nsubtitle\n, \nx-axis\n and \ny-axis\n label. Finally, we explicitly configure the chart to render the \nTotal\n column \nin \nblack\n, and using a \nthicker\n point size to make it distinguishable from the other series.\n\n\n\n\n\nint rowCount = 1000;\nLocalDate startDate = LocalDate.of(2013, 1, 1);\nRange<Integer> rowKeys = Range.of(0, rowCount);\nRange<LocalDate> dates = rowKeys.map(startDate::plusDays);\nDataFrame<Integer,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    columns.add(\"DataDate\", dates);\n    Stream.of(\"A\", \"B\", \"C\", \"D\").forEach(label -> {\n        columns.add(label, Array.randn(rowCount).cumSum());\n    });\n});\n\n//Add a total column that sumns A+B+C+D\nframe.cols().add(\"Total\", Double.class, v -> v.row().stats().sum());\n\n\n\n\n\n Index  |   DataDate   |      A       |       B       |       C       |       D       |     Total     |\n-------------------------------------------------------------------------------------------------------\n     0  |  2013-01-01  |  1.25934218  |   0.69220501  |  -0.31238595  |  -0.56357062  |   1.07559062  |\n     1  |  2013-01-02  |  2.29908995  |   0.17146519  |  -1.60587813  |   0.05049886  |   0.91517587  |\n     2  |  2013-01-03  |  2.28895812  |   0.19584665  |  -1.81601295  |  -0.93573356  |  -0.26694175  |\n     3  |  2013-01-04  |   0.5587514  |   1.73915178  |   0.29149944  |  -0.29126603  |   2.29813659  |\n     4  |  2013-01-05  |  2.11189209  |   0.72883374  |   0.22025859  |  -0.15000083  |   2.91098359  |\n     5  |  2013-01-06  |  2.26632523  |  -0.65280247  |  -0.48525074  |   1.66929384  |   2.79756585  |\n     6  |  2013-01-07  |  3.32634531  |  -0.37416553  |  -0.71419742  |   0.73102638  |   2.96900874  |\n     7  |  2013-01-08  |  2.85345213  |   1.50372178  |   1.14531164  |   1.84564308  |   7.34812862  |\n     8  |  2013-01-09  |  1.70266008  |   1.20891251  |   0.87620806  |  -1.10380341  |   2.68397724  |\n     9  |  2013-01-10  |  1.90697991  |   2.20887263  |  -1.22202884  |  -0.30028665  |   2.59353704  |\n \n\n\n\n\n\n\n\n\n\nChart.create().withLinePlot(frame, \"DataDate\", chart -> {\n    chart.title().withText(\"Example Time Series Chart\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Normal Data\");\n    chart.plot().axes().domain().label().withText(\"Data Date\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.plot().style(\"Total\").withLineWidth(2f).withColor(Color.BLACK);\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nMultiple Axis\n\n\nIt is often useful to be able to plot multiple series on the same chart even when those series happen to have very\ndifferent scales. This is supported by the Morpheus charting API in that is is possible to add many \nDataFrames\n to\na single chart, and each frame can be found to its own range axis. \n\n\nBelow we create a frame similar to prior examples, however we impose a larger scale on series \nC\n and \nD\n compared \nwith \nA\n and \nB\n. If we plotted this as a single frame, the scale of \nC\n and \nD\n would dominate and it would be hard \nto see changes in \nA\n and \nB\n. In order to address this, we filter the frame into 2 sets of columns, and bind the \nsecond \nDataFrame\n to a secondary axis via the \nsetRangeAxis()\n method. The arguments to this method are the dataset \nindex, and the index of the range axis to bind it to. In principal you can bind as many frames to as many axis, however \nthis would rapidly become hard to read.\n\n\n\n\n\nint rowCount = 1000;\nRange<Integer> rowKeys = Range.of(0, rowCount);\nDataFrame<Integer,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    Stream.of(\"A\", \"B\").forEach(c -> columns.add(c, Array.randn(rowCount).cumSum()));\n    Stream.of(\"C\", \"D\").forEach(c -> {\n        columns.add(c, Array.randn(rowCount).mapToDoubles(v -> v.getDouble() * 100).cumSum());\n    });\n});\n\nChart.create().withLinePlot(frame.cols().select(\"A\", \"B\"), chart -> {\n    chart.plot().data().add(frame.cols().select(\"C\", \"D\"));\n    chart.plot().data().setRangeAxis(1, 1);\n    chart.title().withText(\"Time Series Chart - Multiple Axis\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Normal Data\");\n    chart.plot().axes().domain().label().withText(\"Data Date\");\n    chart.plot().axes().range(0).label().withText(\"Random Value-1\");\n    chart.plot().axes().range(1).label().withText(\"Random Value-2\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nMultiple Renderers\n\n\nIn the prior example, the idea that multiple \nDataFrames\n can be added to a chart was introduced in order to demonstrate \nhow to bind different data series to different axis. This same idea can be used to bind different rendering strategies\nto different series.\n\n\nIn the example below, we generate a \nDataFrame\n with dimensions \n20x6\n and then plot this data by filtering the frame\ninto 3 pairs of columns and use different rendering strategies to draw each pair of series. The first frame containing \ncolumns \nA\n and \nB\n is plotted with straight \nlines\n and \nshapes\n rendered at each datum. The second frame containing \ncolumns \nC\n and \nD\n is plotted with a \nspline\n renderer thereby yielding the smooth trajectory for these series. Finally, \nthe third frame is rendered with \ndashed lines\n and no shapes at the datum points.\n\n\n\n\n\nint rowCount = 20;\nRange<Integer> rowKeys = Range.of(0, rowCount);\nDataFrame<Integer,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    Stream.of(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\").forEach(label -> {\n        columns.add(label, Array.randn(rowCount).cumSum());\n    });\n});\n\nChart.create().withLinePlot(frame.cols().select(\"A\", \"B\"), chart -> {\n    chart.plot().data().add(frame.cols().select(\"C\", \"D\"));\n    chart.plot().data().add(frame.cols().select(\"E\", \"F\"));\n    chart.plot().render(0).withLines(true, false);\n    chart.plot().render(1).withSpline(false, false);\n    chart.plot().render(2).withLines(false, true);\n    chart.title().withText(\"Time Series Chart - Multiple Renderers\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Normal Data\");\n    chart.plot().axes().domain().label().withText(\"Data Date\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nSimple Bar Charts\n\n\nDiscrete Domain Axis\n\n\nTo create bar charts with Morpheus we use an identical API as in the prior examples, but in this case we simply make a \ncall to \nChart.ofBars()\n instead of \nChart.ofLines()\n. Plotting a \nDataFrame\n where the \ndomain axis\n is initialized \nfrom the \nrow axis\n keys or alternatively a column of data in the frame works the same way. In the example below we \ngenerate a bar chart based on a \ndiscrete\n or \ncategorical\n domain axis using the frame row axis of type \nYear\n.\nGenerating bar charts based on \ncontinuous\n data is also possible and is demonstrated in the next example.\n\n\n\n\n\nRange<Year> years = Range.of(2000, 2006).map(Year::of);\nDataFrame<Year,String> data = DataFrame.of(years, String.class, columns -> {\n    Stream.of(\"A\", \"B\", \"C\", \"D\").forEach(label -> {\n        columns.add(label, Array.of(Double.class, 6).applyDoubles(v -> Math.random()).cumSum());\n    });\n});\n\nChart.create().withBarPlot(data, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Year\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.title().withText(\"Bar Chart - Categorical Domain Axis\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Uniform Data\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nSwitching to a horizontal orientation can be done by simply calling the \norient().horizontal()\n method as shown below.\n\n\n\n\n\nChart.create().withBarPlot(data, false, chart -> {\n    chart.plot().orient().horizontal();\n    chart.plot().axes().domain().label().withText(\"Year\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.title().withText(\"Bar Chart - Categorical Domain Axis\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Uniform Data\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nContinuous Domain Axis\n\n\nBar charts are often used to display \ncategorical\n or \ndiscrete\n data, however there are scenarios where a \n\ncontinuous\n variable in the \ndomain axis\n is appropriate. In such cases, the question is how wide to make the bars? \nIn a discrete variable bar chart, the width of the bars is sized so that all content fits on the plot, so it is a \nfunction of how many discrete observations and series exist in the data. When creating a bar chart with a continuous \nvariable in the domain, we need to be explicit about how wide we want to make the bars, otherwise they will get \nrepresented as single vertical line.\n\n\nConsider the \nDataFrame\n below with dimensions of \n20x1\n and a row axis of type \nLocalDateTime\n where the row key \ninterval is 10 minutes wide. The data series \nA\n is simply initialized to the sum of a randomly generated array of \nvalues, and the first 10 rows of this frame are shown below.\n\n\n\n\n\nint rowCount = 20;\nLocalDateTime start = LocalDateTime.of(2014, 1, 1, 8, 30);\nRange<LocalDateTime> rowKeys = Range.of(0, rowCount).map(i -> start.plusMinutes(i * 10));\nDataFrame<LocalDateTime,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    columns.add(\"A\", Array.of(Double.class, rowCount).applyDoubles(v -> Math.random()).cumSum());\n});\n\n\n\n\n\n        Index         |      A       |\n--------------------------------------\n 2014-01-01T08:30:00  |  0.27653233  |\n 2014-01-01T08:40:00  |  0.63501106  |\n 2014-01-01T08:50:00  |  1.19265518  |\n 2014-01-01T09:00:00  |   1.9021335  |\n 2014-01-01T09:10:00  |  2.32102833  |\n 2014-01-01T09:20:00  |  3.02188896  |\n 2014-01-01T09:30:00  |  3.08679679  |\n 2014-01-01T09:40:00  |  3.43408424  |\n 2014-01-01T09:50:00  |  4.35521882  |\n 2014-01-01T10:00:00  |  5.13252283  |\n\n\n\n\nIf we simply plot this as a bar chart in the usual fashion as per the code below, we end up with a plot where each\nobservation is represented by a vertical bar at each point in time that is 1 pixel wide. This may indeed be the result\none desires, but given that this \nDataFrame\n most likely represents some cumulative observations for each 10 minute\ninterval, it would be nice to explicitly make the bars 10 minutes wide.\n\n\n\n\n\nChart.create().withBarPlot(frame, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Year\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.title().withText(\"Bar Chart - Continuous Domain Axis\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Uniform Data\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nThankfully this can be achieved very easily through the API, namely via two methods on the \nChartModel\n interface\ncalled \nwithLowerDomainInterval()\n and \nwithUpperDomainInterval()\n which each accept a lambda expression with an\nadjustment function. In this particular example, let us assume that the instantaneous times in the \nDataFrame\n row\naxis are the end of the measurement intervals, so the values in series \nA\n represent the aggregate values for the\nprior 10 minutes. We therefore bind an adjustment function that subtracts 10 minutes from row keys as shown in the\ncode example below, with a call to \nwithLowerDomainInterval()\n.\n\n\n\n\n\nChart.create().withBarPlot(frame, false, chart -> {\n    chart.plot().data().at(0).withLowerDomainInterval(t -> t.minusMinutes(10));\n    chart.plot().axes().domain().label().withText(\"Year\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.title().withText(\"Bar Chart - Continuous Domain Axis\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Uniform Data\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nThe flexibility of the \nwithLowerDomainInterval()\n and \nwithUpperDomainInterval()\n methods means lambdas with any\ndegree of complexity can be used to change the width of bars, and even make variable width bars on the same plot.\n\n\nStacked Bar Charts\n\n\nDiscrete Domain Axis\n\n\nStacked bar charts are useful for visualizing the decomposition of some quantity into various sub-categories. For \nexample, it might be useful to decompose the total revenue of a company into various product and service categories\nto visualize how well balanced revenue sources are. Creating a stacked bar plot with the Morpheus API simply boils\ndown to passing \ntrue\n for the second argument of the \nwithBarPlot()\n function. To illustrate, consider a \nDataFrame\n\nwith 10 rows and 5 columns of randomly initialized values which can be generated as follows.\n\n\n\n\n\nRange<Year> years = Range.of(2000, 2010).map(Year::of);\nDataFrame<Year,String> frame = DataFrame.of(years, String.class, columns -> {\n    Stream.of(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\").forEach(label -> {\n        columns.add(label, Array.randn(10).applyDoubles(v -> Math.abs(v.getDouble())).cumSum());\n    });\n});\n\n\n\n\n\n Index  |      A       |       B       |       C       |      D       |      E       |      F       |      G       |\n--------------------------------------------------------------------------------------------------------------------\n  2000  |  1.31995055  |   0.53881463  |    0.3279245  |  0.11259494  |  1.67395423  |  0.18586162  |  0.87989445  |\n  2001  |  1.72056405  |   1.68922095  |   0.49067667  |  0.23015735  |  2.13037774  |  0.44835517  |  1.36713607  |\n  2002  |  2.79809076  |   3.94390632  |   3.30168445  |  1.29757044  |  3.06911301  |   0.9306128  |  1.80211665  |\n  2003  |  3.59071864  |   5.88855153  |   4.37231856  |  2.52348568  |   4.7653336  |  0.94635161  |  3.99710948  |\n  2004  |  6.08102384  |    7.7523729  |    4.5286778  |  2.59847512  |  4.85460383  |  2.42353332  |  5.06462412  |\n  2005  |  6.55934094  |   9.17683641  |   6.23641685  |  3.34769981  |  4.96006758  |   3.0262002  |  5.14085585  |\n  2006  |  7.60618601  |   9.59875411  |   6.42311354  |  3.58823615  |  5.81808152  |  3.59782465  |  6.08934143  |\n  2007  |  8.50669158  |  10.79619646  |   8.76745963  |  4.71219626  |  6.06259072  |   4.7197505  |  6.80808702  |\n  2008  |   9.3043879  |  10.98733186  |   9.91013585  |  5.78904589  |  7.63390658  |  6.01700338  |  8.27038276  |\n  2009  |  9.40761316  |  11.53533956  |  10.32051078  |  5.85462161  |  8.54673512  |   6.1083706  |  9.53989261  |\n\n\n\n\nWe can plot this data using stacked bars with the following code: \n\n\n\n\n\nChart.create().withBarPlot(frame, true, chart -> {\n    chart.plot().axes().domain().label().withText(\"Year\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.title().withText(\"Stacked Bar Chart - Categorical Domain Axis\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Uniform Data\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nSwitching to a horizontal orientation can be done by simply calling the \norient().horizontal()\n method as shown below.\n\n\n\n\n\nChart.create().withBarPlot(frame, true, chart -> {\n    chart.plot().axes().domain().label().withText(\"Year\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.plot().orient().horizontal();\n    chart.title().withText(\"Stacked Bar Chart - Categorical Domain Axis\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Uniform Data\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nContinuous Domain Axis\n\n\nStacked bar charts involving a continuous domain axis are also supported, much the same way as in the simple bar plot example\ndiscussed earlier. In order to control the \nwidth of the bars\n, it is necessary to inject a function that expresses the \ndomain interval for each datum in the domain. The bar width can be tailored by applying either a lower internal, upper interval\nor both to each observation in the dataset. In the example below we create a \nDataFrame\n with datetimes to serve as the domain, \nand then proceed to make the bars 10 minutes wide by injecting a \nlower\n domain interval function:\n\n\n\n\n\nint rowCount = 40;\nLocalDateTime start = LocalDateTime.of(2014, 1, 1, 8, 30);\nRange<LocalDateTime> rowKeys = Range.of(0, rowCount).map(i -> start.plusMinutes(i * 10));\nDataFrame<LocalDateTime,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    Stream.of(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\").forEach(label -> {\n        columns.add(label, Array.randn(40, 1, 5).cumSum());\n    });\n});\n\nChart.create().withBarPlot(frame, true, chart -> {\n    chart.plot().data().at(0).withLowerDomainInterval(t -> t.minusMinutes(10));\n    chart.plot().axes().domain().label().withText(\"Year\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.title().withText(\"Stacked Bar Chart - Continuous Domain Axis\");\n    chart.subtitle().withText(\"Random Uniform Data\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nCompatibility Note\n: The Google Chart adapter \ndoes not\n support lower / upper domain interval functions as the underlying library\nonly supports the ability to specify inter-bar spacing. Bar plots created by the Google adapter that involve a continuous \ndomain axis, stacked or otherwise, will by default render bars with no spacing between them, which for the most part seems \nlike an appropriate strategy for most scenarios. Calling \nwithLowerDomainInterval()\n or \nwithUpperDomainInterval()\n with the\nGoogle adapter will not have any effect on the output. Future support for more customizable HTML charting libraries may \naddress this limitation.\n\n\nHistograms\n\n\nSingle Distribution\n\n\nThe Morpheus Charting API includes some convenience functions to plot frequency distributions of column data in a \n\nDataFrame\n. In the example below we generate a single column \n1000000x1\n frame of data randomly generated from a \nstandard normal distribution. Calling \nwithHistPlot()\n and indicating the number of bins, 50 in this case, yields\nthe plot below.\n\n\n\n\n\nint recordCount = 1000000;\nDataFrame<Integer,String> frame = DataFrame.of(Range.of(0, recordCount), String.class, columns -> {\n    columns.add(\"A\", Array.randn(recordCount));\n});\n\nChart.create().withHistPlot(frame, 50, chart -> {\n    chart.title().withText(\"Normal Distribution\");\n    chart.subtitle().withText(\"Single Distribution\");\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nMultiple Distributions\n\n\nA slight extension of the prior example is to consider a frequency distribution involving multiple series. In this case we \ncreate a \nDataFrame\n with 4 columns, where each column of 1 million observations is initialized from a normal distribution \nall with a mean of zero but different standard deviations. The different spreads of each series requires bins to be computed\non a per series basis, as it would not be optimal to create one set of bins across all series. We call the same \nwithHistPlot()\n\nfunction as before, and in this example we generate 100 bins per series to yield the plot below.\n\n\n\n\n\nint recordCount = 1000000;\nDataFrame<Integer,String> frame = DataFrame.of(Range.of(0, recordCount), String.class, columns -> {\n    columns.add(\"A\", Array.randn(recordCount, 0d, 1d));\n    columns.add(\"B\", Array.randn(recordCount, 0d, 0.8d));\n    columns.add(\"C\", Array.randn(recordCount, 0d, 0.6d));\n    columns.add(\"D\", Array.randn(recordCount, 0d, 0.4d));\n});\n\nChart.create().withHistPlot(frame, 100, chart -> {\n    chart.title().withText(\"Normal Distribution\");\n    chart.subtitle().withText(\"Multiple Distributions\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nFitted Distribution\n\n\n\n    \n\n\n\n\n\nScatter Charts\n\n\nTo demonstrate scatter plots, consider the random data generating function defined below, which yields an \nnx2\n \n\nDataFrame\n with a column of \nX\n and \nY\n values based on a linear model with noise. The method expects the intercept \n(\nalpha\n), slope (\nbeta\n), the standard deviation for the noise term (\nsigma\n), the step size for the domain interval\nand the number of data points to generate. The table below illustrates the basic structure of the data.\n\n\n\n\n\n/**\n * Returns a 2D sample dataset given a slope and intercept while adding white noise based on sigma.\n * @param alpha     the intercept term for data\n * @param beta      the slope term for for data\n * @param sigma     the standard deviation for noise\n * @param stepSize  the step size for domain variable\n * @param n         the size of the sample to generate\n * @return          the frame of XY values\n */\nprivate DataFrame<Integer,String> sample(double alpha, double beta, double sigma, double stepSize, int n) {\n    final Array<Double> xValues = Array.of(Double.class, n).applyDoubles(v -> 0d + v.index() * stepSize);\n    final Array<Double> yValues = Array.of(Double.class, n).applyDoubles(v -> {\n        final double yfit = alpha + beta * xValues.getDouble(v.index());\n        return new NormalDistribution(yfit, sigma).sample();\n    });\n    final Array<Integer> rowKeys = Range.of(0, n).toArray();\n    return DataFrame.of(rowKeys, String.class, columns -> {\n        columns.add(\"X\", xValues);\n        columns.add(\"Y\", yValues);\n    }).cols().demean(true);\n}\n\n\n\n\n\n Index  |      X      |        Y        |\n-----------------------------------------\n     0  |  -249.7500  |  -134.80224667  |\n     1  |  -249.2500  |  -120.77578676  |\n     2  |  -248.7500  |  -119.33607918  |\n     3  |  -248.2500  |  -126.59963476  |\n     4  |  -247.7500  |  -131.08470325  |\n     5  |  -247.2500  |  -120.08991786  |\n     6  |  -246.7500  |  -124.15182976  |\n     7  |  -246.2500  |   -115.3617302  |\n     8  |  -245.7500  |   -119.0303664  |\n     9  |  -245.2500  |  -141.49936933  |\n\n\n\n\nThe following examples leverage this data generating function to create various scatter plots.\n\n\nSingle Series\n\n\nThe example below generates a 1000 point sample dataset using our data generating function, and creates a scatter plot \nvia the \nwithScatterPlot()\n method using the column labelled \nX\n for the domain axis. \n\n\n\n\n\nDataFrame<Integer,String> frame = scatter(4d, 0.5d, 20d, 0.5, 1000);\n\nChart.create().withScatterPlot(frame, false, \"X\", chart -> {\n    chart.plot().axes().domain().label().withText(\"X-Value\");\n    chart.plot().axes().range(0).label().withText(\"Y-Value\");\n    chart.title().withText(\"Scatter Chart\");\n    chart.subtitle().withText(\"Single DataFrame, Single Series\");\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nMultiple Series\n\n\nBelow we combine 3 \nDataFrames\n using our data generating function and re-label the various \nY\n columns \nA\n, \n\nB\n and \nC\n respectively so that they are distinct in the combined frame. We plot the result using \nwithScatterPlot()\n\nin this case, and proceed to set specific colors for each series, and configure the \nC\n series to render with a \n\ndiamond\n shape. Note that when we combine the frames, the 3 versions of the \nX\n column collapses into a single\ncolumn in the resulting frame based on a combine first rule. Since all 3 frames have identical x-values, this is \nimmaterial in this case.\n\n\n\n\n\nDataFrame<Integer,String> frame = DataFrame.concatColumns(\n    scatter(4d, 1d, 80d, 0.5, 500).cols().replaceKey(\"Y\", \"A\"),\n    scatter(4d, 6d, 100d, 0.5, 500).cols().replaceKey(\"Y\", \"B\"),\n    scatter(4d, 12d, 180d, 0.5, 500).cols().replaceKey(\"Y\", \"C\")\n);\n\nChart.create().withScatterPlot(frame, false, \"X\", chart -> {\n    chart.plot().axes().domain().label().withText(\"X-Value\");\n    chart.plot().axes().range(0).label().withText(\"Y-Value\");\n    chart.plot().style(\"A\").withColor(new Color(255, 225, 25));\n    chart.plot().style(\"B\").withColor(new Color(0, 130, 200));\n    chart.plot().style(\"C\").withColor(new Color(245, 0, 48));\n    chart.title().withText(\"Scatter Chart\");\n    chart.subtitle().withText(\"Single DataFrame, Multiple Series, Custom Style\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nMultiple Frames\n\n\nConsider a scenario where you have results from multiple experiments which you would like to combine in a scatter\nchart, and while the measurements share the same domain (i.e. x-dimension), the actual observations of \nx\n are\nnot the same across the samples. It is possible to create a single sparse \nDataFrame\n with all the results combined, \nhowever it is most likely to be more convenient to keep them separate. \n\n\nIn the example below we generate two \nDataFrames\n, each containing two series, and the x-values between the two frames \nare \nnot\n coincident since we provide a different step size to our data generating function in each case. We can still \nplot both frames together with very little additional effort, namely by adding the second frame and instructing the chart \nto also render it with dots as follows.\n\n\n\n\n\nDataFrame<Integer,String> frame1 = DataFrame.concatColumns(\n    sample(4d, 1d, 80d, 0.5, 500).cols().replaceKey(\"Y\", \"A\"),\n    sample(4d, 3d, 100d, 0.5, 500).cols().replaceKey(\"Y\", \"B\")\n);\n\nDataFrame<Integer,String> frame2 = DataFrame.concatColumns(\n    sample(4d, 7d, 80d, 0.55, 600).cols().replaceKey(\"Y\", \"C\"),\n    sample(4d, -10d, 100d, 0.55, 600).cols().replaceKey(\"Y\", \"D\")\n);\n\nChart.create().withScatterPlot(frame1, false, \"X\", chart -> {\n    chart.plot().data().add(frame2, \"X\");\n    chart.plot().render(1).withDots();\n    chart.plot().axes().domain().label().withText(\"X-Value\");\n    chart.plot().axes().range(0).label().withText(\"Y-Value\");\n    chart.title().withText(\"Scatter Chart\");\n    chart.subtitle().withText(\"Multiple DataFrames, Multiple Series\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nRegression Charts\n\n\nSingle Frame\n\n\nThe Morpheus charting API supports fitting a linear trendline to a dataset based on an \nOrdinary Least Squares\n\nregression model. Both the JFreeChart and Google adapters expose the model equation as a tooltip when hovering over\nany of the data points that make up the trendline. By default, the trend line is rendered in black with a thicker\nline stroke, however this can easily be adjusted using the standard series \nstyle()\n controller.\n\n\nIn the example below we use the same data generating function introduced earlier, and the only additional call is to the \n\ntrend()\n function which accepts the key of the series to which a linear model should be fitted. The resulting plot is shown \nbelow with the standard style applied to the regression line.\n\n\n\n\n\nDataFrame<Integer,String> frame = scatter(4d, 1d, 80d, 0.5d, 1000);\nChart.create().withScatterPlot(frame, false, \"X\", chart -> {\n    chart.plot().axes().domain().label().withText(\"X-Value\");\n    chart.plot().axes().range(0).label().withText(\"Y-Value\");\n    chart.plot().trend(\"Y\");\n    chart.title().withText(\"Regression Chart\");\n    chart.subtitle().withText(\"Single DataFrame, Single Series\");\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nMultiple Frames\n\n\nFitting linear trend lines to multiple series or frames follows naturally through the same API as the single model\nexample. Below we generate two \nDataFrames\n each of which has a non-intersecting set of domain values. By calling the \n\ntrend()\n function on the plot and passing the relevant series keys across the two frames, trend lines are generated as \nexpected. Once again, the model equations are exposed via tooltips when hovering over data points that make up the \nfitted line.\n\n\n\n\n\nDataFrame<Integer,String> frame1 = DataFrame.concatColumns(\n    scatter(4d, 1d, 80d, 0.5, 500).cols().replaceKey(\"Y\", \"A\"),\n    scatter(4d, 4d, 100d, 0.5, 500).cols().replaceKey(\"Y\", \"B\")\n);\n\nDataFrame<Integer,String> frame2 = DataFrame.concatColumns(\n    scatter(4d, -3d, 80d, 0.55, 600).cols().replaceKey(\"Y\", \"C\"),\n    scatter(4d, -10d, 100d, 0.45, 600).cols().replaceKey(\"Y\", \"D\")\n);\n\nChart.create().withScatterPlot(frame1, false, \"X\", chart -> {\n    chart.plot().<String>data().add(frame2, \"X\");\n    chart.plot().render(1).withDots();\n    chart.plot().axes().domain().label().withText(\"X-Value\");\n    chart.plot().axes().range(0).label().withText(\"Y-Value\");\n    chart.plot().trend(\"A\");\n    chart.plot().trend(\"B\");\n    chart.plot().trend(\"C\");\n    chart.title().withText(\"Regression Chart\");\n    chart.subtitle().withText(\"Multiple DataFrame, Multiple Series\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nArea Charts\n\n\nAn area chart is basically the same as a line chart however the area between the domain axis and the line is filled\nwith a series specific color. There is one variation on this to note, which is that more often than not, area charts \nare generated as stacked areas, which is useful to visualize the decomposition of some aggregate quantity. The two\nexamples below show a stacked and non-stacked area chart for the identical dataset.\n\n\n\n\n\nint rowCount = 100;\nRange<Integer> rowKeys = Range.of(0, rowCount);\nDataFrame<Integer,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    Stream.of(\"A\", \"B\", \"C\", \"D\", \"E\").forEach(label -> {\n        columns.add(label, Array.randn(rowCount, 10d, 100d).cumSum());\n    });\n});\n\nChart.create().withAreaPlot(frame, true, chart -> {\n    chart.plot().axes().domain().label().withText(\"X-Value\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.title().withText(\"Stacked Area Chart\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Normal Data\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nThe following code plots the identical dataset but with overlapping areas rather than stacked (which is simply achieved by\npassing \nstacked=false\n to the \nwithAreaPlot()\n method) . Notice the different scale in the y-axis between the two plots, the \nformer being of a larger scale given the aggregate nature of that visualization.\n\n\n\n\n\nint rowCount = 100;\nRange<Integer> rowKeys = Range.of(0, rowCount);\nDataFrame<Integer,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    Stream.of(\"A\", \"B\", \"C\", \"D\", \"E\").forEach(label -> {\n        columns.add(label, Array.randn(rowCount, 10d, 100d).cumSum());\n    });\n});\n\nChart.create().withAreaPlot(frame, false, chart -> {\n    chart.plot().render(0).withArea(false);\n    chart.plot().axes().domain().label().withText(\"X-Value\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.title().withText(\"Overlapping Area Chart\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Normal Data\");\n    chart.legend().on();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nPie Charts\n\n\nPie charts are perhaps the simplest of all the plots, although there are a few customizations that are worth being\naware of. Generally speaking pie plots are used to illustrate the decomposition of some measurement into different\ncategories, and the Morpheus API allows a specific column of a \nDataFrame\n to be presented in this way. Pie section \nlabels can be customized to show the actual value of the quantity, its percentage of the total or simply the name of \nthe category (which can also be shown via an optional legend).\n\n\nConsider a single column \nDataFrame\n of random values that we will assume represent some measurement for different\ncountries which we have defined as the row keys in terms of their 3-character ISO codes.  \n\n\n\n\n\nDataFrame<String,String> frame = DataFrame.ofDoubles(\n    Array.of(\"AUS\", \"GBR\", \"USA\", \"DEU\", \"ITA\", \"ESP\", \"ZAF\"),\n    Array.of(\"Random\"),\n    value -> Math.random() * 10d\n);\n\n\n\n\n\n Index  |    Random    |\n------------------------\n   AUS  |  4.05807605  |\n   GBR  |  2.80847587  |\n   USA  |  2.92350137  |\n   DEU  |  3.39330077  |\n   ITA  |  7.52006927  |\n   ESP  |  1.86101073  |\n   ZAF  |  4.03358896  |\n\n\n\n\nDefault Style\n\n\nThe simplest pie chart can be generated via the code below, which by default labels the pie sections with the percentage \nof the total for the column being plotted. In this example we not indicate a specific column in the \nDataFrame\n to plot, \nin which case the \nfirst numeric column\n is chosen. Both the Swing version and Google version of this plot includes \ninformative tooltips with additional data as you mouse over each section.\n\n\n\n\n\nChart.create().withPiePlot(frame, false, chart -> {\n    chart.title().withText(\"Pie Chart of Random Data\");\n    chart.subtitle().withText(\"Labels with Section Percent\");\n    chart.legend().on().right();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nDonut Pie Plot\n\n\nA slight adjustment to the pie plot is to include a hole in order to create a donut chart. In this example we also\nexplicitly pass the name of the column to plot, although given that there is only one numeric column in the \nDataFrame\n\nthis is superfluous.\n\n\n\n\n\nChart.create().withPiePlot(frame, false, \"Random\", chart -> {\n    chart.title().withText(\"Donut Pie Chart of Random Data\");\n    chart.subtitle().withText(\"Labels with Section Value\");\n    chart.plot().withPieHole(0.4);\n    chart.plot().labels().on().withValue();\n    chart.legend().on().right();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\n3D Pie Plot\n\n\nTurning this into a 3D plot is as easy as passing in \ntrue\n as the second argument to the \nwithPiePlot()\n method. \n\n\n\n\n\nChart.create().withPiePlot(frame, true, chart -> {\n    chart.title().withText(\"3D Pie Chart of Random Data\");\n    chart.subtitle().withText(\"Labels with Section Name\");\n    chart.plot().labels().on().withName();\n    chart.legend().on().right();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nExploded Section\n\n\nCalling out one or more pie sections is possible by imposing an offset from the center as shown below. In this case\nwe explicitly access the section controller for \nAUS\n and apply an offset which must be a value between 0 and 1. In\naddition, we have rotated the plot clockwise by 90 degrees by calling the \nwithStartAngle()\n method as illustrated. \n\n\n\n\n\nChart.create().withPiePlot(frame, false, chart -> {\n    chart.title().withText(\"Pie Chart of Random Data\");\n    chart.subtitle().withText(\"Custom Label Style with Exploded Pie Section\");\n    chart.plot().labels().withBackgroundColor(Color.WHITE).withFont(new Font(\"Arial\", Font.BOLD, 11));\n    chart.plot().section(\"AUS\").withOffset(0.2);\n    chart.plot().withStartAngle(90);\n    chart.legend().on().right();\n    chart.show();\n});",
            "title": "Overview"
        },
        {
            "location": "/viz/charts/overview/#introduction",
            "text": "The Morpheus visualization library defines a  simple chart abstraction API  with adapters supporting both  JFreeChart  as well as  Google Charts  (with others\nto follow by popular demand). This design makes it possible to generate interactive  Java Swing  \ncharts as well as HTML5 browser based charts via the same API. By default, the framework is configured to use the JFreeChart \nadapter, however this can be re-configured on a global basis by calling either  htmlMode()  or  swingMode()  as shown \nbelow.   //Switch chart adapter to HTML mode globally \nChart.create().htmlMode();\n\n//Switch chart adapter to SWING mode globally \nChart.create().swingMode();  It is also possible to operate in  mixed mode  from within the the same application rather than switching the adapter \nglobally. By explicitly calling  asHtml()  or  asSwing()  prior to invoking one of the plotting functions on the  Chart \ninterface, Html and Swing based charts can be generated from within the same application as shown below.     //Create chart using SWING adapter\nChart.create().asSwing().withLinePlot(frame, chart -> {\n    chart.title().withText(\"Chart Title Goes Here...\");\n    chart.legend().on().bottom();\n    chart.show();\n});\n\n//Create chart using HTML adapter\nChart.create().asHtml().withLinePlot(frame, chart -> {\n    chart.title().withText(\"Chart Title Goes Here...\");\n    chart.legend().on().bottom();\n    chart.show();\n});  The following sections demonstrate how to use the Morpheus charting API, and provide various examples of what kind of \ncharts are supported. The illustrations below are PNG files generated using the JFreeChart adapter, however a  gallery  \nof the same plots generated via the  Google adapter  show just how similar the plots from the two implementations are. \nWhile most of the functionality exposed by the Morpheus Charting API are supported by both adapters, there are some gaps \nin the Google adapter which are documented below.",
            "title": "Introduction"
        },
        {
            "location": "/viz/charts/overview/#line-charts",
            "text": "",
            "title": "Line Charts"
        },
        {
            "location": "/viz/charts/overview/#single-series",
            "text": "Consider the  DataFrame  below with dimensions  1000x1  which has a row axis of type  LocalDate  and 1 column of \ndouble precision values representing the cumulative sum of an  Array  of normally distributed random values. The first \n10 rows of this frame are printed below.   import com.zavtech.morpheus.array.Array;\nimport com.zavtech.morpheus.frame.DataFrame;\n\nint rowCount = 1000;\nLocalDate startDate = LocalDate.of(2013, 1, 1);\nRange<LocalDate> dates = Range.of(0, rowCount).map(startDate::plusDays);\nDataFrame<LocalDate,String> frame = DataFrame.of(dates, String.class, columns -> {\n    columns.add(\"A\", Array.randn(rowCount).cumSum());\n});  \n   Index     |       A       |\n------------------------------\n 2013-01-01  |   0.58638321  |\n 2013-01-02  |  -0.44176283  |\n 2013-01-03  |  -0.07187819  |\n 2013-01-04  |  -1.31157143  |\n 2013-01-05  |  -1.69375864  |\n 2013-01-06  |  -2.23840733  |\n 2013-01-07  |  -2.42279587  |\n 2013-01-08  |  -2.95871372  |\n 2013-01-09  |  -3.63748847  |\n   To generate an line plot of this series, we can use the dates in the  DataFrame  row axis as the  x-values , and \nthe numeric values in column  A  as the range or  y-values . The  withLines()  method on the  ChartFactory  interface\nexpects the  DataFrame  containing the data to plot, and a  Consumer  which is used to configure various features of \nthe chart. In the example below, we simply display the chart with no further customization by calling  show() .   Chart.create().withLinePlot(frame, chart -> {\n    chart.show();\n});",
            "title": "Single Series"
        },
        {
            "location": "/viz/charts/overview/#multiple-series",
            "text": "A common scenario is to generate an line plot where the domain or  x-axis  is based on data in a specific column of the frame \nrather than the  row axis  as in the previous example. This can be done by passing the label of the column to use for \nthe domain axis to the  withLines()  method. In the example below, we create a similar dataset to above but with dimensions  1000x5 , and in this case the row axis is simply a sequence of integers with the dates included as a column keyed as  DataDate .    int rowCount = 1000;\nLocalDate startDate = LocalDate.of(2013, 1, 1);\nRange<Integer> rowKeys = Range.of(0, rowCount);\nRange<LocalDate> dates = rowKeys.map(startDate::plusDays);\nDataFrame<Integer,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    columns.add(\"DataDate\", dates);\n    Stream.of(\"A\", \"B\", \"C\", \"D\").forEach(label -> {\n        columns.add(label, Array.randn(rowCount).cumSum());\n    });\n});  \n Index  |   DataDate   |      A       |       B       |       C       |       D       |\n---------------------------------------------------------------------------------------\n     0  |  2013-01-01  |  1.29439793  |  -0.91248479  |  -0.51141634  |   0.45271667  |\n     1  |  2013-01-02  |  1.04502117  |   -1.5841936  |   1.04050209  |    0.3773374  |\n     2  |  2013-01-03  |   3.0924616  |  -2.35489228  |   3.35960923  |  -1.52130364  |\n     3  |  2013-01-04  |  2.58882443  |  -1.80476051  |   3.79982322  |  -2.03083571  |\n     4  |  2013-01-05  |  2.95265199  |  -3.22752153  |   4.92200533  |  -0.05268766  |\n     5  |  2013-01-06  |  2.54600084  |  -3.46228629  |   3.34627638  |  -0.73170641  |\n     6  |  2013-01-07  |  2.11506239  |  -3.34459359  |   3.68209019  |  -0.80961531  |\n     7  |  2013-01-08  |   3.1889199  |  -3.75034967  |    3.7681561  |    0.0668972  |\n     8  |  2013-01-09  |  4.83991797  |  -3.21969887  |   2.99041415  |   0.89681662  |\n     9  |  2013-01-10  |  4.33025037  |  -2.65846363  |   2.44799574  |   0.62896343  |   Given that we are plotting multiple series, we also turn on the chart legend, and place it at the bottom of the chart.   Chart.create().withLinePlot(frame, \"DataDate\", chart -> {\n    chart.legend().on().bottom();\n    chart.show();\n});",
            "title": "Multiple Series"
        },
        {
            "location": "/viz/charts/overview/#series-specific-style",
            "text": "Extending the prior example, below we generate the same  1000x5  dataset but add an additional column post construction \nwhich represents the sum of values in each row. In addition, we add  text  to the chart in the form of a  title ,  subtitle ,  x-axis  and  y-axis  label. Finally, we explicitly configure the chart to render the  Total  column \nin  black , and using a  thicker  point size to make it distinguishable from the other series.   int rowCount = 1000;\nLocalDate startDate = LocalDate.of(2013, 1, 1);\nRange<Integer> rowKeys = Range.of(0, rowCount);\nRange<LocalDate> dates = rowKeys.map(startDate::plusDays);\nDataFrame<Integer,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    columns.add(\"DataDate\", dates);\n    Stream.of(\"A\", \"B\", \"C\", \"D\").forEach(label -> {\n        columns.add(label, Array.randn(rowCount).cumSum());\n    });\n});\n\n//Add a total column that sumns A+B+C+D\nframe.cols().add(\"Total\", Double.class, v -> v.row().stats().sum());  \n Index  |   DataDate   |      A       |       B       |       C       |       D       |     Total     |\n-------------------------------------------------------------------------------------------------------\n     0  |  2013-01-01  |  1.25934218  |   0.69220501  |  -0.31238595  |  -0.56357062  |   1.07559062  |\n     1  |  2013-01-02  |  2.29908995  |   0.17146519  |  -1.60587813  |   0.05049886  |   0.91517587  |\n     2  |  2013-01-03  |  2.28895812  |   0.19584665  |  -1.81601295  |  -0.93573356  |  -0.26694175  |\n     3  |  2013-01-04  |   0.5587514  |   1.73915178  |   0.29149944  |  -0.29126603  |   2.29813659  |\n     4  |  2013-01-05  |  2.11189209  |   0.72883374  |   0.22025859  |  -0.15000083  |   2.91098359  |\n     5  |  2013-01-06  |  2.26632523  |  -0.65280247  |  -0.48525074  |   1.66929384  |   2.79756585  |\n     6  |  2013-01-07  |  3.32634531  |  -0.37416553  |  -0.71419742  |   0.73102638  |   2.96900874  |\n     7  |  2013-01-08  |  2.85345213  |   1.50372178  |   1.14531164  |   1.84564308  |   7.34812862  |\n     8  |  2013-01-09  |  1.70266008  |   1.20891251  |   0.87620806  |  -1.10380341  |   2.68397724  |\n     9  |  2013-01-10  |  1.90697991  |   2.20887263  |  -1.22202884  |  -0.30028665  |   2.59353704  |\n     Chart.create().withLinePlot(frame, \"DataDate\", chart -> {\n    chart.title().withText(\"Example Time Series Chart\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Normal Data\");\n    chart.plot().axes().domain().label().withText(\"Data Date\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.plot().style(\"Total\").withLineWidth(2f).withColor(Color.BLACK);\n    chart.legend().on();\n    chart.show();\n});",
            "title": "Series Specific Style"
        },
        {
            "location": "/viz/charts/overview/#multiple-axis",
            "text": "It is often useful to be able to plot multiple series on the same chart even when those series happen to have very\ndifferent scales. This is supported by the Morpheus charting API in that is is possible to add many  DataFrames  to\na single chart, and each frame can be found to its own range axis.   Below we create a frame similar to prior examples, however we impose a larger scale on series  C  and  D  compared \nwith  A  and  B . If we plotted this as a single frame, the scale of  C  and  D  would dominate and it would be hard \nto see changes in  A  and  B . In order to address this, we filter the frame into 2 sets of columns, and bind the \nsecond  DataFrame  to a secondary axis via the  setRangeAxis()  method. The arguments to this method are the dataset \nindex, and the index of the range axis to bind it to. In principal you can bind as many frames to as many axis, however \nthis would rapidly become hard to read.   int rowCount = 1000;\nRange<Integer> rowKeys = Range.of(0, rowCount);\nDataFrame<Integer,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    Stream.of(\"A\", \"B\").forEach(c -> columns.add(c, Array.randn(rowCount).cumSum()));\n    Stream.of(\"C\", \"D\").forEach(c -> {\n        columns.add(c, Array.randn(rowCount).mapToDoubles(v -> v.getDouble() * 100).cumSum());\n    });\n});\n\nChart.create().withLinePlot(frame.cols().select(\"A\", \"B\"), chart -> {\n    chart.plot().data().add(frame.cols().select(\"C\", \"D\"));\n    chart.plot().data().setRangeAxis(1, 1);\n    chart.title().withText(\"Time Series Chart - Multiple Axis\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Normal Data\");\n    chart.plot().axes().domain().label().withText(\"Data Date\");\n    chart.plot().axes().range(0).label().withText(\"Random Value-1\");\n    chart.plot().axes().range(1).label().withText(\"Random Value-2\");\n    chart.legend().on();\n    chart.show();\n});",
            "title": "Multiple Axis"
        },
        {
            "location": "/viz/charts/overview/#multiple-renderers",
            "text": "In the prior example, the idea that multiple  DataFrames  can be added to a chart was introduced in order to demonstrate \nhow to bind different data series to different axis. This same idea can be used to bind different rendering strategies\nto different series.  In the example below, we generate a  DataFrame  with dimensions  20x6  and then plot this data by filtering the frame\ninto 3 pairs of columns and use different rendering strategies to draw each pair of series. The first frame containing \ncolumns  A  and  B  is plotted with straight  lines  and  shapes  rendered at each datum. The second frame containing \ncolumns  C  and  D  is plotted with a  spline  renderer thereby yielding the smooth trajectory for these series. Finally, \nthe third frame is rendered with  dashed lines  and no shapes at the datum points.   int rowCount = 20;\nRange<Integer> rowKeys = Range.of(0, rowCount);\nDataFrame<Integer,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    Stream.of(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\").forEach(label -> {\n        columns.add(label, Array.randn(rowCount).cumSum());\n    });\n});\n\nChart.create().withLinePlot(frame.cols().select(\"A\", \"B\"), chart -> {\n    chart.plot().data().add(frame.cols().select(\"C\", \"D\"));\n    chart.plot().data().add(frame.cols().select(\"E\", \"F\"));\n    chart.plot().render(0).withLines(true, false);\n    chart.plot().render(1).withSpline(false, false);\n    chart.plot().render(2).withLines(false, true);\n    chart.title().withText(\"Time Series Chart - Multiple Renderers\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Normal Data\");\n    chart.plot().axes().domain().label().withText(\"Data Date\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.legend().on();\n    chart.show();\n});",
            "title": "Multiple Renderers"
        },
        {
            "location": "/viz/charts/overview/#simple-bar-charts",
            "text": "",
            "title": "Simple Bar Charts"
        },
        {
            "location": "/viz/charts/overview/#discrete-domain-axis",
            "text": "To create bar charts with Morpheus we use an identical API as in the prior examples, but in this case we simply make a \ncall to  Chart.ofBars()  instead of  Chart.ofLines() . Plotting a  DataFrame  where the  domain axis  is initialized \nfrom the  row axis  keys or alternatively a column of data in the frame works the same way. In the example below we \ngenerate a bar chart based on a  discrete  or  categorical  domain axis using the frame row axis of type  Year .\nGenerating bar charts based on  continuous  data is also possible and is demonstrated in the next example.   Range<Year> years = Range.of(2000, 2006).map(Year::of);\nDataFrame<Year,String> data = DataFrame.of(years, String.class, columns -> {\n    Stream.of(\"A\", \"B\", \"C\", \"D\").forEach(label -> {\n        columns.add(label, Array.of(Double.class, 6).applyDoubles(v -> Math.random()).cumSum());\n    });\n});\n\nChart.create().withBarPlot(data, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Year\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.title().withText(\"Bar Chart - Categorical Domain Axis\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Uniform Data\");\n    chart.legend().on();\n    chart.show();\n});  \n       Switching to a horizontal orientation can be done by simply calling the  orient().horizontal()  method as shown below.   Chart.create().withBarPlot(data, false, chart -> {\n    chart.plot().orient().horizontal();\n    chart.plot().axes().domain().label().withText(\"Year\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.title().withText(\"Bar Chart - Categorical Domain Axis\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Uniform Data\");\n    chart.legend().on();\n    chart.show();\n});",
            "title": "Discrete Domain Axis"
        },
        {
            "location": "/viz/charts/overview/#continuous-domain-axis",
            "text": "Bar charts are often used to display  categorical  or  discrete  data, however there are scenarios where a  continuous  variable in the  domain axis  is appropriate. In such cases, the question is how wide to make the bars? \nIn a discrete variable bar chart, the width of the bars is sized so that all content fits on the plot, so it is a \nfunction of how many discrete observations and series exist in the data. When creating a bar chart with a continuous \nvariable in the domain, we need to be explicit about how wide we want to make the bars, otherwise they will get \nrepresented as single vertical line.  Consider the  DataFrame  below with dimensions of  20x1  and a row axis of type  LocalDateTime  where the row key \ninterval is 10 minutes wide. The data series  A  is simply initialized to the sum of a randomly generated array of \nvalues, and the first 10 rows of this frame are shown below.   int rowCount = 20;\nLocalDateTime start = LocalDateTime.of(2014, 1, 1, 8, 30);\nRange<LocalDateTime> rowKeys = Range.of(0, rowCount).map(i -> start.plusMinutes(i * 10));\nDataFrame<LocalDateTime,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    columns.add(\"A\", Array.of(Double.class, rowCount).applyDoubles(v -> Math.random()).cumSum());\n});  \n        Index         |      A       |\n--------------------------------------\n 2014-01-01T08:30:00  |  0.27653233  |\n 2014-01-01T08:40:00  |  0.63501106  |\n 2014-01-01T08:50:00  |  1.19265518  |\n 2014-01-01T09:00:00  |   1.9021335  |\n 2014-01-01T09:10:00  |  2.32102833  |\n 2014-01-01T09:20:00  |  3.02188896  |\n 2014-01-01T09:30:00  |  3.08679679  |\n 2014-01-01T09:40:00  |  3.43408424  |\n 2014-01-01T09:50:00  |  4.35521882  |\n 2014-01-01T10:00:00  |  5.13252283  |  If we simply plot this as a bar chart in the usual fashion as per the code below, we end up with a plot where each\nobservation is represented by a vertical bar at each point in time that is 1 pixel wide. This may indeed be the result\none desires, but given that this  DataFrame  most likely represents some cumulative observations for each 10 minute\ninterval, it would be nice to explicitly make the bars 10 minutes wide.   Chart.create().withBarPlot(frame, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Year\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.title().withText(\"Bar Chart - Continuous Domain Axis\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Uniform Data\");\n    chart.legend().on();\n    chart.show();\n});  \n       Thankfully this can be achieved very easily through the API, namely via two methods on the  ChartModel  interface\ncalled  withLowerDomainInterval()  and  withUpperDomainInterval()  which each accept a lambda expression with an\nadjustment function. In this particular example, let us assume that the instantaneous times in the  DataFrame  row\naxis are the end of the measurement intervals, so the values in series  A  represent the aggregate values for the\nprior 10 minutes. We therefore bind an adjustment function that subtracts 10 minutes from row keys as shown in the\ncode example below, with a call to  withLowerDomainInterval() .   Chart.create().withBarPlot(frame, false, chart -> {\n    chart.plot().data().at(0).withLowerDomainInterval(t -> t.minusMinutes(10));\n    chart.plot().axes().domain().label().withText(\"Year\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.title().withText(\"Bar Chart - Continuous Domain Axis\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Uniform Data\");\n    chart.legend().on();\n    chart.show();\n});  \n       The flexibility of the  withLowerDomainInterval()  and  withUpperDomainInterval()  methods means lambdas with any\ndegree of complexity can be used to change the width of bars, and even make variable width bars on the same plot.",
            "title": "Continuous Domain Axis"
        },
        {
            "location": "/viz/charts/overview/#stacked-bar-charts",
            "text": "",
            "title": "Stacked Bar Charts"
        },
        {
            "location": "/viz/charts/overview/#discrete-domain-axis_1",
            "text": "Stacked bar charts are useful for visualizing the decomposition of some quantity into various sub-categories. For \nexample, it might be useful to decompose the total revenue of a company into various product and service categories\nto visualize how well balanced revenue sources are. Creating a stacked bar plot with the Morpheus API simply boils\ndown to passing  true  for the second argument of the  withBarPlot()  function. To illustrate, consider a  DataFrame \nwith 10 rows and 5 columns of randomly initialized values which can be generated as follows.   Range<Year> years = Range.of(2000, 2010).map(Year::of);\nDataFrame<Year,String> frame = DataFrame.of(years, String.class, columns -> {\n    Stream.of(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\").forEach(label -> {\n        columns.add(label, Array.randn(10).applyDoubles(v -> Math.abs(v.getDouble())).cumSum());\n    });\n});  \n Index  |      A       |       B       |       C       |      D       |      E       |      F       |      G       |\n--------------------------------------------------------------------------------------------------------------------\n  2000  |  1.31995055  |   0.53881463  |    0.3279245  |  0.11259494  |  1.67395423  |  0.18586162  |  0.87989445  |\n  2001  |  1.72056405  |   1.68922095  |   0.49067667  |  0.23015735  |  2.13037774  |  0.44835517  |  1.36713607  |\n  2002  |  2.79809076  |   3.94390632  |   3.30168445  |  1.29757044  |  3.06911301  |   0.9306128  |  1.80211665  |\n  2003  |  3.59071864  |   5.88855153  |   4.37231856  |  2.52348568  |   4.7653336  |  0.94635161  |  3.99710948  |\n  2004  |  6.08102384  |    7.7523729  |    4.5286778  |  2.59847512  |  4.85460383  |  2.42353332  |  5.06462412  |\n  2005  |  6.55934094  |   9.17683641  |   6.23641685  |  3.34769981  |  4.96006758  |   3.0262002  |  5.14085585  |\n  2006  |  7.60618601  |   9.59875411  |   6.42311354  |  3.58823615  |  5.81808152  |  3.59782465  |  6.08934143  |\n  2007  |  8.50669158  |  10.79619646  |   8.76745963  |  4.71219626  |  6.06259072  |   4.7197505  |  6.80808702  |\n  2008  |   9.3043879  |  10.98733186  |   9.91013585  |  5.78904589  |  7.63390658  |  6.01700338  |  8.27038276  |\n  2009  |  9.40761316  |  11.53533956  |  10.32051078  |  5.85462161  |  8.54673512  |   6.1083706  |  9.53989261  |  We can plot this data using stacked bars with the following code:    Chart.create().withBarPlot(frame, true, chart -> {\n    chart.plot().axes().domain().label().withText(\"Year\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.title().withText(\"Stacked Bar Chart - Categorical Domain Axis\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Uniform Data\");\n    chart.legend().on();\n    chart.show();\n});  \n       Switching to a horizontal orientation can be done by simply calling the  orient().horizontal()  method as shown below.   Chart.create().withBarPlot(frame, true, chart -> {\n    chart.plot().axes().domain().label().withText(\"Year\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.plot().orient().horizontal();\n    chart.title().withText(\"Stacked Bar Chart - Categorical Domain Axis\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Uniform Data\");\n    chart.legend().on();\n    chart.show();\n});",
            "title": "Discrete Domain Axis"
        },
        {
            "location": "/viz/charts/overview/#continuous-domain-axis_1",
            "text": "Stacked bar charts involving a continuous domain axis are also supported, much the same way as in the simple bar plot example\ndiscussed earlier. In order to control the  width of the bars , it is necessary to inject a function that expresses the \ndomain interval for each datum in the domain. The bar width can be tailored by applying either a lower internal, upper interval\nor both to each observation in the dataset. In the example below we create a  DataFrame  with datetimes to serve as the domain, \nand then proceed to make the bars 10 minutes wide by injecting a  lower  domain interval function:   int rowCount = 40;\nLocalDateTime start = LocalDateTime.of(2014, 1, 1, 8, 30);\nRange<LocalDateTime> rowKeys = Range.of(0, rowCount).map(i -> start.plusMinutes(i * 10));\nDataFrame<LocalDateTime,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    Stream.of(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\").forEach(label -> {\n        columns.add(label, Array.randn(40, 1, 5).cumSum());\n    });\n});\n\nChart.create().withBarPlot(frame, true, chart -> {\n    chart.plot().data().at(0).withLowerDomainInterval(t -> t.minusMinutes(10));\n    chart.plot().axes().domain().label().withText(\"Year\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.title().withText(\"Stacked Bar Chart - Continuous Domain Axis\");\n    chart.subtitle().withText(\"Random Uniform Data\");\n    chart.legend().on();\n    chart.show();\n});  \n       Compatibility Note : The Google Chart adapter  does not  support lower / upper domain interval functions as the underlying library\nonly supports the ability to specify inter-bar spacing. Bar plots created by the Google adapter that involve a continuous \ndomain axis, stacked or otherwise, will by default render bars with no spacing between them, which for the most part seems \nlike an appropriate strategy for most scenarios. Calling  withLowerDomainInterval()  or  withUpperDomainInterval()  with the\nGoogle adapter will not have any effect on the output. Future support for more customizable HTML charting libraries may \naddress this limitation.",
            "title": "Continuous Domain Axis"
        },
        {
            "location": "/viz/charts/overview/#histograms",
            "text": "",
            "title": "Histograms"
        },
        {
            "location": "/viz/charts/overview/#single-distribution",
            "text": "The Morpheus Charting API includes some convenience functions to plot frequency distributions of column data in a  DataFrame . In the example below we generate a single column  1000000x1  frame of data randomly generated from a \nstandard normal distribution. Calling  withHistPlot()  and indicating the number of bins, 50 in this case, yields\nthe plot below.   int recordCount = 1000000;\nDataFrame<Integer,String> frame = DataFrame.of(Range.of(0, recordCount), String.class, columns -> {\n    columns.add(\"A\", Array.randn(recordCount));\n});\n\nChart.create().withHistPlot(frame, 50, chart -> {\n    chart.title().withText(\"Normal Distribution\");\n    chart.subtitle().withText(\"Single Distribution\");\n    chart.show();\n});",
            "title": "Single Distribution"
        },
        {
            "location": "/viz/charts/overview/#multiple-distributions",
            "text": "A slight extension of the prior example is to consider a frequency distribution involving multiple series. In this case we \ncreate a  DataFrame  with 4 columns, where each column of 1 million observations is initialized from a normal distribution \nall with a mean of zero but different standard deviations. The different spreads of each series requires bins to be computed\non a per series basis, as it would not be optimal to create one set of bins across all series. We call the same  withHistPlot() \nfunction as before, and in this example we generate 100 bins per series to yield the plot below.   int recordCount = 1000000;\nDataFrame<Integer,String> frame = DataFrame.of(Range.of(0, recordCount), String.class, columns -> {\n    columns.add(\"A\", Array.randn(recordCount, 0d, 1d));\n    columns.add(\"B\", Array.randn(recordCount, 0d, 0.8d));\n    columns.add(\"C\", Array.randn(recordCount, 0d, 0.6d));\n    columns.add(\"D\", Array.randn(recordCount, 0d, 0.4d));\n});\n\nChart.create().withHistPlot(frame, 100, chart -> {\n    chart.title().withText(\"Normal Distribution\");\n    chart.subtitle().withText(\"Multiple Distributions\");\n    chart.legend().on();\n    chart.show();\n});",
            "title": "Multiple Distributions"
        },
        {
            "location": "/viz/charts/overview/#fitted-distribution",
            "text": "",
            "title": "Fitted Distribution"
        },
        {
            "location": "/viz/charts/overview/#scatter-charts",
            "text": "To demonstrate scatter plots, consider the random data generating function defined below, which yields an  nx2   DataFrame  with a column of  X  and  Y  values based on a linear model with noise. The method expects the intercept \n( alpha ), slope ( beta ), the standard deviation for the noise term ( sigma ), the step size for the domain interval\nand the number of data points to generate. The table below illustrates the basic structure of the data.   /**\n * Returns a 2D sample dataset given a slope and intercept while adding white noise based on sigma.\n * @param alpha     the intercept term for data\n * @param beta      the slope term for for data\n * @param sigma     the standard deviation for noise\n * @param stepSize  the step size for domain variable\n * @param n         the size of the sample to generate\n * @return          the frame of XY values\n */\nprivate DataFrame<Integer,String> sample(double alpha, double beta, double sigma, double stepSize, int n) {\n    final Array<Double> xValues = Array.of(Double.class, n).applyDoubles(v -> 0d + v.index() * stepSize);\n    final Array<Double> yValues = Array.of(Double.class, n).applyDoubles(v -> {\n        final double yfit = alpha + beta * xValues.getDouble(v.index());\n        return new NormalDistribution(yfit, sigma).sample();\n    });\n    final Array<Integer> rowKeys = Range.of(0, n).toArray();\n    return DataFrame.of(rowKeys, String.class, columns -> {\n        columns.add(\"X\", xValues);\n        columns.add(\"Y\", yValues);\n    }).cols().demean(true);\n}  \n Index  |      X      |        Y        |\n-----------------------------------------\n     0  |  -249.7500  |  -134.80224667  |\n     1  |  -249.2500  |  -120.77578676  |\n     2  |  -248.7500  |  -119.33607918  |\n     3  |  -248.2500  |  -126.59963476  |\n     4  |  -247.7500  |  -131.08470325  |\n     5  |  -247.2500  |  -120.08991786  |\n     6  |  -246.7500  |  -124.15182976  |\n     7  |  -246.2500  |   -115.3617302  |\n     8  |  -245.7500  |   -119.0303664  |\n     9  |  -245.2500  |  -141.49936933  |  The following examples leverage this data generating function to create various scatter plots.",
            "title": "Scatter Charts"
        },
        {
            "location": "/viz/charts/overview/#single-series_1",
            "text": "The example below generates a 1000 point sample dataset using our data generating function, and creates a scatter plot \nvia the  withScatterPlot()  method using the column labelled  X  for the domain axis.    DataFrame<Integer,String> frame = scatter(4d, 0.5d, 20d, 0.5, 1000);\n\nChart.create().withScatterPlot(frame, false, \"X\", chart -> {\n    chart.plot().axes().domain().label().withText(\"X-Value\");\n    chart.plot().axes().range(0).label().withText(\"Y-Value\");\n    chart.title().withText(\"Scatter Chart\");\n    chart.subtitle().withText(\"Single DataFrame, Single Series\");\n    chart.show();\n});",
            "title": "Single Series"
        },
        {
            "location": "/viz/charts/overview/#multiple-series_1",
            "text": "Below we combine 3  DataFrames  using our data generating function and re-label the various  Y  columns  A ,  B  and  C  respectively so that they are distinct in the combined frame. We plot the result using  withScatterPlot() \nin this case, and proceed to set specific colors for each series, and configure the  C  series to render with a  diamond  shape. Note that when we combine the frames, the 3 versions of the  X  column collapses into a single\ncolumn in the resulting frame based on a combine first rule. Since all 3 frames have identical x-values, this is \nimmaterial in this case.   DataFrame<Integer,String> frame = DataFrame.concatColumns(\n    scatter(4d, 1d, 80d, 0.5, 500).cols().replaceKey(\"Y\", \"A\"),\n    scatter(4d, 6d, 100d, 0.5, 500).cols().replaceKey(\"Y\", \"B\"),\n    scatter(4d, 12d, 180d, 0.5, 500).cols().replaceKey(\"Y\", \"C\")\n);\n\nChart.create().withScatterPlot(frame, false, \"X\", chart -> {\n    chart.plot().axes().domain().label().withText(\"X-Value\");\n    chart.plot().axes().range(0).label().withText(\"Y-Value\");\n    chart.plot().style(\"A\").withColor(new Color(255, 225, 25));\n    chart.plot().style(\"B\").withColor(new Color(0, 130, 200));\n    chart.plot().style(\"C\").withColor(new Color(245, 0, 48));\n    chart.title().withText(\"Scatter Chart\");\n    chart.subtitle().withText(\"Single DataFrame, Multiple Series, Custom Style\");\n    chart.legend().on();\n    chart.show();\n});",
            "title": "Multiple Series"
        },
        {
            "location": "/viz/charts/overview/#multiple-frames",
            "text": "Consider a scenario where you have results from multiple experiments which you would like to combine in a scatter\nchart, and while the measurements share the same domain (i.e. x-dimension), the actual observations of  x  are\nnot the same across the samples. It is possible to create a single sparse  DataFrame  with all the results combined, \nhowever it is most likely to be more convenient to keep them separate.   In the example below we generate two  DataFrames , each containing two series, and the x-values between the two frames \nare  not  coincident since we provide a different step size to our data generating function in each case. We can still \nplot both frames together with very little additional effort, namely by adding the second frame and instructing the chart \nto also render it with dots as follows.   DataFrame<Integer,String> frame1 = DataFrame.concatColumns(\n    sample(4d, 1d, 80d, 0.5, 500).cols().replaceKey(\"Y\", \"A\"),\n    sample(4d, 3d, 100d, 0.5, 500).cols().replaceKey(\"Y\", \"B\")\n);\n\nDataFrame<Integer,String> frame2 = DataFrame.concatColumns(\n    sample(4d, 7d, 80d, 0.55, 600).cols().replaceKey(\"Y\", \"C\"),\n    sample(4d, -10d, 100d, 0.55, 600).cols().replaceKey(\"Y\", \"D\")\n);\n\nChart.create().withScatterPlot(frame1, false, \"X\", chart -> {\n    chart.plot().data().add(frame2, \"X\");\n    chart.plot().render(1).withDots();\n    chart.plot().axes().domain().label().withText(\"X-Value\");\n    chart.plot().axes().range(0).label().withText(\"Y-Value\");\n    chart.title().withText(\"Scatter Chart\");\n    chart.subtitle().withText(\"Multiple DataFrames, Multiple Series\");\n    chart.legend().on();\n    chart.show();\n});",
            "title": "Multiple Frames"
        },
        {
            "location": "/viz/charts/overview/#regression-charts",
            "text": "",
            "title": "Regression Charts"
        },
        {
            "location": "/viz/charts/overview/#single-frame",
            "text": "The Morpheus charting API supports fitting a linear trendline to a dataset based on an  Ordinary Least Squares \nregression model. Both the JFreeChart and Google adapters expose the model equation as a tooltip when hovering over\nany of the data points that make up the trendline. By default, the trend line is rendered in black with a thicker\nline stroke, however this can easily be adjusted using the standard series  style()  controller.  In the example below we use the same data generating function introduced earlier, and the only additional call is to the  trend()  function which accepts the key of the series to which a linear model should be fitted. The resulting plot is shown \nbelow with the standard style applied to the regression line.   DataFrame<Integer,String> frame = scatter(4d, 1d, 80d, 0.5d, 1000);\nChart.create().withScatterPlot(frame, false, \"X\", chart -> {\n    chart.plot().axes().domain().label().withText(\"X-Value\");\n    chart.plot().axes().range(0).label().withText(\"Y-Value\");\n    chart.plot().trend(\"Y\");\n    chart.title().withText(\"Regression Chart\");\n    chart.subtitle().withText(\"Single DataFrame, Single Series\");\n    chart.show();\n});",
            "title": "Single Frame"
        },
        {
            "location": "/viz/charts/overview/#multiple-frames_1",
            "text": "Fitting linear trend lines to multiple series or frames follows naturally through the same API as the single model\nexample. Below we generate two  DataFrames  each of which has a non-intersecting set of domain values. By calling the  trend()  function on the plot and passing the relevant series keys across the two frames, trend lines are generated as \nexpected. Once again, the model equations are exposed via tooltips when hovering over data points that make up the \nfitted line.   DataFrame<Integer,String> frame1 = DataFrame.concatColumns(\n    scatter(4d, 1d, 80d, 0.5, 500).cols().replaceKey(\"Y\", \"A\"),\n    scatter(4d, 4d, 100d, 0.5, 500).cols().replaceKey(\"Y\", \"B\")\n);\n\nDataFrame<Integer,String> frame2 = DataFrame.concatColumns(\n    scatter(4d, -3d, 80d, 0.55, 600).cols().replaceKey(\"Y\", \"C\"),\n    scatter(4d, -10d, 100d, 0.45, 600).cols().replaceKey(\"Y\", \"D\")\n);\n\nChart.create().withScatterPlot(frame1, false, \"X\", chart -> {\n    chart.plot().<String>data().add(frame2, \"X\");\n    chart.plot().render(1).withDots();\n    chart.plot().axes().domain().label().withText(\"X-Value\");\n    chart.plot().axes().range(0).label().withText(\"Y-Value\");\n    chart.plot().trend(\"A\");\n    chart.plot().trend(\"B\");\n    chart.plot().trend(\"C\");\n    chart.title().withText(\"Regression Chart\");\n    chart.subtitle().withText(\"Multiple DataFrame, Multiple Series\");\n    chart.legend().on();\n    chart.show();\n});",
            "title": "Multiple Frames"
        },
        {
            "location": "/viz/charts/overview/#area-charts",
            "text": "An area chart is basically the same as a line chart however the area between the domain axis and the line is filled\nwith a series specific color. There is one variation on this to note, which is that more often than not, area charts \nare generated as stacked areas, which is useful to visualize the decomposition of some aggregate quantity. The two\nexamples below show a stacked and non-stacked area chart for the identical dataset.   int rowCount = 100;\nRange<Integer> rowKeys = Range.of(0, rowCount);\nDataFrame<Integer,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    Stream.of(\"A\", \"B\", \"C\", \"D\", \"E\").forEach(label -> {\n        columns.add(label, Array.randn(rowCount, 10d, 100d).cumSum());\n    });\n});\n\nChart.create().withAreaPlot(frame, true, chart -> {\n    chart.plot().axes().domain().label().withText(\"X-Value\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.title().withText(\"Stacked Area Chart\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Normal Data\");\n    chart.legend().on();\n    chart.show();\n});  \n       The following code plots the identical dataset but with overlapping areas rather than stacked (which is simply achieved by\npassing  stacked=false  to the  withAreaPlot()  method) . Notice the different scale in the y-axis between the two plots, the \nformer being of a larger scale given the aggregate nature of that visualization.   int rowCount = 100;\nRange<Integer> rowKeys = Range.of(0, rowCount);\nDataFrame<Integer,String> frame = DataFrame.of(rowKeys, String.class, columns -> {\n    Stream.of(\"A\", \"B\", \"C\", \"D\", \"E\").forEach(label -> {\n        columns.add(label, Array.randn(rowCount, 10d, 100d).cumSum());\n    });\n});\n\nChart.create().withAreaPlot(frame, false, chart -> {\n    chart.plot().render(0).withArea(false);\n    chart.plot().axes().domain().label().withText(\"X-Value\");\n    chart.plot().axes().range(0).label().withText(\"Random Value\");\n    chart.title().withText(\"Overlapping Area Chart\");\n    chart.subtitle().withText(\"Cumulative Sum of Random Normal Data\");\n    chart.legend().on();\n    chart.show();\n});",
            "title": "Area Charts"
        },
        {
            "location": "/viz/charts/overview/#pie-charts",
            "text": "Pie charts are perhaps the simplest of all the plots, although there are a few customizations that are worth being\naware of. Generally speaking pie plots are used to illustrate the decomposition of some measurement into different\ncategories, and the Morpheus API allows a specific column of a  DataFrame  to be presented in this way. Pie section \nlabels can be customized to show the actual value of the quantity, its percentage of the total or simply the name of \nthe category (which can also be shown via an optional legend).  Consider a single column  DataFrame  of random values that we will assume represent some measurement for different\ncountries which we have defined as the row keys in terms of their 3-character ISO codes.     DataFrame<String,String> frame = DataFrame.ofDoubles(\n    Array.of(\"AUS\", \"GBR\", \"USA\", \"DEU\", \"ITA\", \"ESP\", \"ZAF\"),\n    Array.of(\"Random\"),\n    value -> Math.random() * 10d\n);  \n Index  |    Random    |\n------------------------\n   AUS  |  4.05807605  |\n   GBR  |  2.80847587  |\n   USA  |  2.92350137  |\n   DEU  |  3.39330077  |\n   ITA  |  7.52006927  |\n   ESP  |  1.86101073  |\n   ZAF  |  4.03358896  |",
            "title": "Pie Charts"
        },
        {
            "location": "/viz/charts/overview/#default-style",
            "text": "The simplest pie chart can be generated via the code below, which by default labels the pie sections with the percentage \nof the total for the column being plotted. In this example we not indicate a specific column in the  DataFrame  to plot, \nin which case the  first numeric column  is chosen. Both the Swing version and Google version of this plot includes \ninformative tooltips with additional data as you mouse over each section.   Chart.create().withPiePlot(frame, false, chart -> {\n    chart.title().withText(\"Pie Chart of Random Data\");\n    chart.subtitle().withText(\"Labels with Section Percent\");\n    chart.legend().on().right();\n    chart.show();\n});",
            "title": "Default Style"
        },
        {
            "location": "/viz/charts/overview/#donut-pie-plot",
            "text": "A slight adjustment to the pie plot is to include a hole in order to create a donut chart. In this example we also\nexplicitly pass the name of the column to plot, although given that there is only one numeric column in the  DataFrame \nthis is superfluous.   Chart.create().withPiePlot(frame, false, \"Random\", chart -> {\n    chart.title().withText(\"Donut Pie Chart of Random Data\");\n    chart.subtitle().withText(\"Labels with Section Value\");\n    chart.plot().withPieHole(0.4);\n    chart.plot().labels().on().withValue();\n    chart.legend().on().right();\n    chart.show();\n});",
            "title": "Donut Pie Plot"
        },
        {
            "location": "/viz/charts/overview/#3d-pie-plot",
            "text": "Turning this into a 3D plot is as easy as passing in  true  as the second argument to the  withPiePlot()  method.    Chart.create().withPiePlot(frame, true, chart -> {\n    chart.title().withText(\"3D Pie Chart of Random Data\");\n    chart.subtitle().withText(\"Labels with Section Name\");\n    chart.plot().labels().on().withName();\n    chart.legend().on().right();\n    chart.show();\n});",
            "title": "3D Pie Plot"
        },
        {
            "location": "/viz/charts/overview/#exploded-section",
            "text": "Calling out one or more pie sections is possible by imposing an offset from the center as shown below. In this case\nwe explicitly access the section controller for  AUS  and apply an offset which must be a value between 0 and 1. In\naddition, we have rotated the plot clockwise by 90 degrees by calling the  withStartAngle()  method as illustrated.    Chart.create().withPiePlot(frame, false, chart -> {\n    chart.title().withText(\"Pie Chart of Random Data\");\n    chart.subtitle().withText(\"Custom Label Style with Exploded Pie Section\");\n    chart.plot().labels().withBackgroundColor(Color.WHITE).withFont(new Font(\"Arial\", Font.BOLD, 11));\n    chart.plot().section(\"AUS\").withOffset(0.2);\n    chart.plot().withStartAngle(90);\n    chart.legend().on().right();\n    chart.show();\n});",
            "title": "Exploded Section"
        },
        {
            "location": "/viz/charts/embed/",
            "text": "Embedding in HTML\n\n\nIt is often useful to be able to embed a chart into a web page, and the Morpheus charting API makes this\neasy. Charts can either be presented as static image files or alternatively as interactive HTML 5 charts\nthat support panning, zooming and dynamic tooltips. This section illustrates some examples of how to use \nthe Morpheus API to programmatically include charts in web based content.\n\n\nJavascript Approach\n\n\nThe Morpheus adapters for \nJFreeChart\n and \nGoogle Charts\n support Javascript \ncode generation\n to \nfacilitate embedding charts in a web page. In the former case, the charts are integrated as PNG images \nencoded as base-64 \ndata urls\n inside the Javascript. In\nthe latter case, appropriate Javascript code is generated in order to produce an interactive HTML5 based\nchart leveraging the \nGoogle charting API\n.\n\n\nExample\n\n\nThe code below generates 4 charts, 2 based on the Google adapter and 2 based on the JFreeChart adapter.\nWe then convert these 4 charts into Javascript code which we embed programmatically in the \nscript\n element\nof an HTML page. More likely than not, some sort of template framework like \nFreemarker\n \nwould be used to create the HTML, but below we use a simple code generation API that ships with the Morpheus\nvisualization library.\n\n\nOnce we have the Javascript, all that remains is to place the charts in the page. By design, the web page\nneeds to include \ndiv\n elements for each of the 4 charts, and each \ndiv\n needs to be given an appropriate \nid\n \nattribute so that it can be uniquely identified. You will notice that in the chart creation function below \nwe make a call to \nChart.options.withId()\n where we assign a unique \nid\n to a chart. This unique \nid\n is used \ninside the code generated Javascript to bind the chart to the relevant \ndiv\n.\n\n\nFor the image based charts generated by the JFreeChart adapter, we also call \nwithPreferredSize()\n on the\nchart options as this will define the size of the image created. For the Google HTML5 charts, this is not\nnecessary as the plot will fill the size of the \ndiv\n to which it is bound. Below we force the heigth of\nour \ndiv\n elements to be 400 pixels, and half the width of the browser window so we create a 2x2 grid of\ncharts.\n\n\n\n\n\npublic static void main(String[] args) throws Exception {\n    //Create some random data to plot\n    final DataFrame<Year,String> frame = randomData();\n    //Create the list of 4 charts, 2 JFree based, 2 Google based \n    final List<Chart<?>> charts = createCharts(frame);\n    //Generate Javascript code which can be embedded in an HTML page\n    final String javascript = Chart.create().javascript(charts);\n\n    //Code generate some HTML, embed the Javascript and create appropriate divs\n    final HtmlCode htmlCode = new HtmlCode();\n    htmlCode.newElement(\"html\", html -> {\n        html.newElement(\"head\", head -> {\n            head.newElement(\"script\", script -> {\n                script.newAttribute(\"type\", \"text/javascript\");\n                script.newAttribute(\"src\", \"https://www.gstatic.com/charts/loader.js\");\n            });\n            head.newElement(\"script\", script -> {\n                script.newAttribute(\"type\", \"text/javascript\");\n                script.text(javascript);\n            });\n        });\n        html.newElement(\"body\", body -> {\n            IntStream.range(0, 4).forEach(id -> {\n                body.newElement(\"div\", div -> {\n                    div.newAttribute(\"id\", String.format(\"chart_%s\", id));\n                    div.newAttribute(\"style\", \"float:left;width:50%;height:400px;\");\n                });\n            });\n        });\n    });\n    htmlCode.browse();\n}\n\n\n/**\n * Returns a list of 4 charts, 2 swing based and 2 html based\n * @param frame     the data to plot in different forms\n * @return          the list of 4 charts\n */\nprivate static List<Chart<?>> createCharts(DataFrame<Year,String> frame) {\n    return Collect.asList(\n        Chart.create().asHtml().withLinePlot(frame, chart -> {\n            chart.options().withId(\"chart_0\");\n            chart.title().withText(\"Single DataFrame Line Plot (Google)\");\n            chart.plot().axes().domain().label().withText(\"Year\");\n            chart.plot().axes().range(0).label().withText(\"Random Value\");\n            chart.legend().on().bottom();\n        }),\n        Chart.create().asHtml().withBarPlot(frame, true, chart -> {\n            chart.options().withId(\"chart_1\");\n            chart.title().withText(\"Single DataFrame Bar Plot (Google)\");\n            chart.plot().axes().domain().label().withText(\"Year\");\n            chart.plot().axes().range(0).label().withText(\"Random Value\");\n            chart.legend().on().bottom();\n        }),\n        Chart.create().asSwing().withLinePlot(frame, chart -> {\n            chart.options().withPreferredSize(600, 400).withId(\"chart_2\");\n            chart.title().withText(\"Single DataFrame Line Plot (JFree)\");\n            chart.plot().axes().domain().label().withText(\"Year\");\n            chart.plot().axes().range(0).label().withText(\"Random Value\");\n            chart.legend().on().bottom();\n        }),\n        Chart.create().asSwing().withBarPlot(frame, true, chart -> {\n            chart.options().withPreferredSize(600, 400).withId(\"chart_3\");\n            chart.title().withText(\"Single DataFrame Bar Plot (JFree)\");\n            chart.plot().axes().domain().label().withText(\"Year\");\n            chart.plot().axes().range(0).label().withText(\"Random Value\");\n            chart.legend().on().bottom();\n        })\n    );\n}\n\n/**\n * Creates a DataFrame of random data with 4 series\n * @return      the DataFrame with chart data\n */\nprivate static DataFrame<Year,String> randomData() {\n    final Array<Year> years = Range.of(2000, 2016).map(Year::of).toArray();\n    return DataFrame.of(years, String.class, columns -> {\n        Stream.of(\"A\", \"B\", \"C\", \"D\").forEach(label -> {\n            columns.add(label, Array.randn(years.length()).cumSum());\n        });\n    });\n}\n\n\n\n\nSample Output\n\n\nThe HTML output generate by the above example is show below. The base64 images have been truncated for legibility,\nbut hopefully this provides a good insight as to the fairly simple and unified solution for using Javascript to\nrender charts from each adapter.\n\n\n\n\n<html>\n    <head>\n        <script type=\"text/javascript\" src=\"https://www.gstatic.com/charts/loader.js\"></script>\n        <script type=\"text/javascript\">    \n\n            google.charts.load('current', {'packages':['corechart']});\n            google.charts.setOnLoadCallback(drawCharts);\n\n            function drawCharts() {\n                drawChart_0()\n                drawChart_1()\n                drawChart_2()\n                drawChart_3()\n            }\n\n            /** This is code generation by the Morpheus Visualization library */\n            function drawChart_0() {\n                var data = google.visualization.arrayToDataTable([\n                    [\n                        {id: \"domain\", label: \"Domain\", type: \"string\"},\n                        {id: \"A\", label: \"A\", type: \"number\"},\n                        {id: \"B\", label: \"B\", type: \"number\"},\n                        {id: \"C\", label: \"C\", type: \"number\"},\n                        {id: \"D\", label: \"D\", type: \"number\"}\n                    ],\n                    ['2000',0.5649231377923367,0.5453100986562593,-0.2030931021259246,-0.48360739892318605],\n                    ['2001',-0.23182829941568417,-1.03605735883078,0.09068337378223748,-2.116960106087407],\n                    ['2002',-1.208044022835305,-0.23036814946864737,0.4240705851894213,-1.616996062133938],\n                    ['2003',-0.7428178334708951,1.9563008036267904,1.3123458436616149,-0.7981998224236828],\n                    ['2004',1.0567387300679987,2.3968672877362533,1.0239596059901945,-1.6096974640778379],\n                    ['2005',1.4294955012985076,0.2901319385475669,0.9915597443573548,-1.0894259330202014],\n                    ['2006',1.2901648209824153,-2.076620989957175,0.08628099969449832,0.41435190883770856],\n                    ['2007',0.582294762560585,-1.795857876481523,0.2595097868637274,0.9955095602428286],\n                    ['2008',-0.23808622783335387,-1.5403271477485165,-1.7707040758630166,2.8379995610627615],\n                    ['2009',0.23614781485157388,-1.8756421596020807,0.20119674267818088,2.6158469831886735],\n                    ['2010',-0.17183433628278583,-0.8911557982977236,-1.0555322149236557,4.0443132884329485],\n                    ['2011',-1.6399147529641436,-0.11443828597796613,-1.5943434857459011,5.189000772984133],\n                    ['2012',-0.5298301520679034,0.32141819425165835,-0.6173563177411512,5.228795645597423],\n                    ['2013',-0.34320688750777756,1.7963170143536078,0.08061128454844213,4.96370354438183],\n                    ['2014',-1.3678140181362104,3.8156318180939715,0.26431218483382923,4.0556816321141635],\n                    ['2015',-2.1179730104550756,3.778589363871307,-1.057630438418794,3.3585122043768374]\n                ]);\n\n                var options = {    \n                    fontSize: \"automatic\",\n                    fontName: \"Arial\",\n                    title: \"Single DataFrame Line Plot (Google)\",\n                    titlePosition: \"out\",\n                    titleTextStyle: {    \n                        color: \"#000000\",\n                        fontName: \"Arial\",\n                        fontSize: 16,\n                        bold: true,\n                        italic: false\n                    },\n                    backgroundColor: {    \n                        strokeWidth: 0\n                    },\n                    chartArea: {    \n                        left: \"auto\",\n                        top: \"auto\",\n                        width: \"80%\",\n                        height: \"auto\"\n                    },\n                    legend: {    \n                        position: \"bottom\",\n                        alignment: \"start\",\n                        textStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 10,\n                            bold: false,\n                            italic: false\n                        }\n                    },\n                    dataOpacity: 1.0,\n                    axisTitlesPosition: \"out\",\n                    hAxis: {    \n                        baselineColor: \"#000000\",\n                        textPosition: \"out\",\n                        title: \"Year\",\n                        viewWindowMode: \"maximized\",\n                        format: \"auto\",\n                        titleTextStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 12,\n                            bold: false,\n                            italic: false\n                        },\n                        textStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 12,\n                            bold: false,\n                            italic: false\n                        },\n                        gridLines: \"\"\n                    },\n                    vAxis: {    \n                        baselineColor: \"#000000\",\n                        textPosition: \"out\",\n                        title: \"Random Value\",\n                        viewWindowMode: \"maximized\",\n                        format: \"decimal\",\n                        titleTextStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 12,\n                            bold: false,\n                            italic: false\n                        },\n                        textStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 12,\n                            bold: false,\n                            italic: false\n                        },\n                        gridLines: \"\"\n                    },\n                    series: {    \n                        0: {    \n                            targetAxisIndex: 0,\n                            visibleInLegend: true,\n                            color: \"#e6194b\",\n                            lineWidth: 1.0,\n                            curveType: \"none\"\n                        },\n                        1: {    \n                            targetAxisIndex: 0,\n                            visibleInLegend: true,\n                            color: \"#3cb44b\",\n                            lineWidth: 1.0,\n                            curveType: \"none\"\n                        },\n                        2: {    \n                            targetAxisIndex: 0,\n                            visibleInLegend: true,\n                            color: \"#ffe119\",\n                            lineWidth: 1.0,\n                            curveType: \"none\"\n                        },\n                        3: {    \n                            targetAxisIndex: 0,\n                            visibleInLegend: true,\n                            color: \"#0082c8\",\n                            lineWidth: 1.0,\n                            curveType: \"none\"\n                        }\n                    },\n                    lineWidth: 1,\n                    curveType: \"none\",\n                    orientation: \"horizontal\",\n                    explorer: {    \n                        keepInBounds: true,\n                        actions: [\"dragToZoom\",\"rightClickToReset\"]\n                    }\n                };\n\n                var target = document.getElementById('chart_0');\n                var chart = new google.visualization.LineChart(target);\n                chart.draw(data, options);\n            }\n\n            /** This is code generation by the Morpheus Visualization library */\n            function drawChart_1() {\n                var data = google.visualization.arrayToDataTable([\n                    [\n                        {id: \"domain\", label: \"Domain\", type: \"string\"},\n                        {id: \"A\", label: \"A\", type: \"number\"},\n                        {id: \"B\", label: \"B\", type: \"number\"},\n                        {id: \"C\", label: \"C\", type: \"number\"},\n                        {id: \"D\", label: \"D\", type: \"number\"}\n                    ],\n                    ['2000',0.5649231377923367,0.5453100986562593,-0.2030931021259246,-0.48360739892318605],\n                    ['2001',-0.23182829941568417,-1.03605735883078,0.09068337378223748,-2.116960106087407],\n                    ['2002',-1.208044022835305,-0.23036814946864737,0.4240705851894213,-1.616996062133938],\n                    ['2003',-0.7428178334708951,1.9563008036267904,1.3123458436616149,-0.7981998224236828],\n                    ['2004',1.0567387300679987,2.3968672877362533,1.0239596059901945,-1.6096974640778379],\n                    ['2005',1.4294955012985076,0.2901319385475669,0.9915597443573548,-1.0894259330202014],\n                    ['2006',1.2901648209824153,-2.076620989957175,0.08628099969449832,0.41435190883770856],\n                    ['2007',0.582294762560585,-1.795857876481523,0.2595097868637274,0.9955095602428286],\n                    ['2008',-0.23808622783335387,-1.5403271477485165,-1.7707040758630166,2.8379995610627615],\n                    ['2009',0.23614781485157388,-1.8756421596020807,0.20119674267818088,2.6158469831886735],\n                    ['2010',-0.17183433628278583,-0.8911557982977236,-1.0555322149236557,4.0443132884329485],\n                    ['2011',-1.6399147529641436,-0.11443828597796613,-1.5943434857459011,5.189000772984133],\n                    ['2012',-0.5298301520679034,0.32141819425165835,-0.6173563177411512,5.228795645597423],\n                    ['2013',-0.34320688750777756,1.7963170143536078,0.08061128454844213,4.96370354438183],\n                    ['2014',-1.3678140181362104,3.8156318180939715,0.26431218483382923,4.0556816321141635],\n                    ['2015',-2.1179730104550756,3.778589363871307,-1.057630438418794,3.3585122043768374]\n                ]);\n\n                var options = {    \n                    fontSize: \"automatic\",\n                    fontName: \"Arial\",\n                    title: \"Single DataFrame Bar Plot (Google)\",\n                    titlePosition: \"out\",\n                    titleTextStyle: {    \n                        color: \"#000000\",\n                        fontName: \"Arial\",\n                        fontSize: 16,\n                        bold: true,\n                        italic: false\n                    },\n                    backgroundColor: {    \n                        strokeWidth: 0\n                    },\n                    chartArea: {    \n                        left: \"auto\",\n                        top: \"auto\",\n                        width: \"80%\",\n                        height: \"auto\"\n                    },\n                    legend: {    \n                        position: \"bottom\",\n                        alignment: \"start\",\n                        textStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 10,\n                            bold: false,\n                            italic: false\n                        }\n                    },\n                    dataOpacity: 1.0,\n                    axisTitlesPosition: \"out\",\n                    hAxis: {    \n                        baselineColor: \"#000000\",\n                        textPosition: \"out\",\n                        title: \"Year\",\n                        viewWindowMode: \"maximized\",\n                        format: \"auto\",\n                        titleTextStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 12,\n                            bold: false,\n                            italic: false\n                        },\n                        textStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 12,\n                            bold: false,\n                            italic: false\n                        },\n                        gridLines: \"\"\n                    },\n                    vAxis: {    \n                        baselineColor: \"#000000\",\n                        textPosition: \"out\",\n                        title: \"Random Value\",\n                        viewWindowMode: \"maximized\",\n                        format: \"decimal\",\n                        titleTextStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 12,\n                            bold: false,\n                            italic: false\n                        },\n                        textStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 12,\n                            bold: false,\n                            italic: false\n                        },\n                        gridLines: \"\"\n                    },\n                    series: {    \n                        0: {    \n                            targetAxisIndex: 0,\n                            visibleInLegend: true,\n                            color: \"#e6194b\"\n                        },\n                        1: {    \n                            targetAxisIndex: 0,\n                            visibleInLegend: true,\n                            color: \"#3cb44b\"\n                        },\n                        2: {    \n                            targetAxisIndex: 0,\n                            visibleInLegend: true,\n                            color: \"#ffe119\"\n                        },\n                        3: {    \n                            targetAxisIndex: 0,\n                            visibleInLegend: true,\n                            color: \"#0082c8\"\n                        }\n                    },\n                    isStacked: true,\n                    bar: {    \n                        groupWidth: \"70%\"\n                    },\n                    explorer: {    \n                        keepInBounds: true,\n                        actions: [\"dragToZoom\",\"rightClickToReset\"]\n                    }\n                };\n\n                var target = document.getElementById('chart_1');\n                var chart = new google.visualization.ColumnChart(target);\n                chart.draw(data, options);\n            }\n\n\n            function drawChart_2() {\n                var divElement = document.getElementById('chart_2');\n                var imageElement = document.createElement('img');\n                imageElement.setAttribute('src', 'data:image/png;base64, iVBORw0KGgoAAAANSUhEUgA...........');\n                imageElement.setAttribute('alt', 'Embedded Chart');\n                imageElement.setAttribute('class', 'chart');\n                divElement.appendChild(imageElement);\n            }\n\n\n            function drawChart_3() {\n                var divElement = document.getElementById('chart_3');\n                var imageElement = document.createElement('img');\n                imageElement.setAttribute('src', 'data:image/png;base64, iVBORw0KGgoAAAANS...............');\n                imageElement.setAttribute('alt', 'Embedded Chart');\n                imageElement.setAttribute('class', 'chart');\n                divElement.appendChild(imageElement);\n            }\n        </script>\n    </head>\n<body>\n    <div id=\"chart_0\" style=\"float:left;width:50%;height:400px;\"></div>\n    <div id=\"chart_1\" style=\"float:left;width:50%;height:400px;\"></div>\n    <div id=\"chart_2\" style=\"float:left;width:50%;height:400px;\"></div>\n    <div id=\"chart_3\" style=\"float:left;width:50%;height:400px;\"></div>\n</body>\n</html>\n\n\n\n\nImage File Approach\n\n\nThe Morpheus \nChart\n interface declares overloaded \nwritePng()\n methods which can be used to programmatically \ngenerate \nPortable Network Graphics\n image content \nto a \nFile\n or an \nOutputStream\n. Note that these methods are only supported by the \nJFreeChart adapter\n, \ninvoking them on an instance of a Google chart will be a \nsilent operation\n.\n\n\nThe code below demonstrates how to generate a chart image via a \nJava Servlet\n as a response to some hypothetical\nHTTP GET request. The chart itself is based on some random data, and we output an image of a size defined by\nparameters in the GET request, and also impose that the chart background be transparent if the user requests it.\n\n\n\n\n\n/**\n * A basic Java Servlet example that demonstrates how to generate a PNG image of a chart\n */\npublic class ChartServlet extends javax.servlet.http.HttpServlet {\n\n    @Override\n    public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException {\n        try {\n            final String title = request.getParameter(\"title\");\n            final Optional<String> widthParam = Optional.ofNullable(request.getParameter(\"width\"));\n            final Optional<String> heightParam = Optional.ofNullable(request.getParameter(\"height\"));\n            final Optional<String> transParam = Optional.ofNullable(request.getParameter(\"transparent\"));\n            final DataFrame<Integer,String> frame = loadData();\n            //Only the swing based charts support PNG generation\n            Chart.create().asSwing().withLinePlot(frame, \"DataDate\", chart -> {\n                try {\n                    final int width = widthParam.map(Integer::parseInt).orElse(700);\n                    final int height = heightParam.map(Integer::parseInt).orElse(400);\n                    final boolean transparent = transParam.map(Boolean::parseBoolean).orElse(true);\n                    response.setContentType(\"image/png\");\n                    chart.title().withText(title);\n                    chart.legend().on().bottom();\n                    chart.plot().axes().domain().label().withText(\"Data Date\");\n                    chart.plot().axes().range(0).label().withText(\"Temperature\");\n                    chart.writerPng(response.getOutputStream(), width, height, transparent);\n                } catch (Exception ex) {\n                    throw new RuntimeException(ex.getMessage(), ex);\n                }\n            });\n        } catch (Exception ex) {\n            response.sendError(500, ex.getMessage());\n        }\n    }\n\n    /**\n     * Loads the data for the chart to plot\n     * @return      the DataFrame with chart data\n     */\n    private DataFrame<Integer,String> loadData() {\n        final int rowCount = 1000;\n        final LocalDate startDate = LocalDate.of(2013, 1, 1);\n        final Range<Integer> rowKeys = Range.of(0, rowCount);\n        final Range<LocalDate> dates = rowKeys.map(startDate::plusDays);\n        return DataFrame.of(rowKeys, String.class, columns -> {\n            columns.add(\"DataDate\", dates);\n            Stream.of(\"A\", \"B\", \"C\", \"D\").forEach(label -> {\n                columns.add(label, Array.randn(rowCount).cumSum());\n            });\n        });\n    }\n}",
            "title": "Embedding (HTML)"
        },
        {
            "location": "/viz/charts/embed/#embedding-in-html",
            "text": "It is often useful to be able to embed a chart into a web page, and the Morpheus charting API makes this\neasy. Charts can either be presented as static image files or alternatively as interactive HTML 5 charts\nthat support panning, zooming and dynamic tooltips. This section illustrates some examples of how to use \nthe Morpheus API to programmatically include charts in web based content.",
            "title": "Embedding in HTML"
        },
        {
            "location": "/viz/charts/embed/#javascript-approach",
            "text": "The Morpheus adapters for  JFreeChart  and  Google Charts  support Javascript  code generation  to \nfacilitate embedding charts in a web page. In the former case, the charts are integrated as PNG images \nencoded as base-64  data urls  inside the Javascript. In\nthe latter case, appropriate Javascript code is generated in order to produce an interactive HTML5 based\nchart leveraging the  Google charting API .",
            "title": "Javascript Approach"
        },
        {
            "location": "/viz/charts/embed/#example",
            "text": "The code below generates 4 charts, 2 based on the Google adapter and 2 based on the JFreeChart adapter.\nWe then convert these 4 charts into Javascript code which we embed programmatically in the  script  element\nof an HTML page. More likely than not, some sort of template framework like  Freemarker  \nwould be used to create the HTML, but below we use a simple code generation API that ships with the Morpheus\nvisualization library.  Once we have the Javascript, all that remains is to place the charts in the page. By design, the web page\nneeds to include  div  elements for each of the 4 charts, and each  div  needs to be given an appropriate  id  \nattribute so that it can be uniquely identified. You will notice that in the chart creation function below \nwe make a call to  Chart.options.withId()  where we assign a unique  id  to a chart. This unique  id  is used \ninside the code generated Javascript to bind the chart to the relevant  div .  For the image based charts generated by the JFreeChart adapter, we also call  withPreferredSize()  on the\nchart options as this will define the size of the image created. For the Google HTML5 charts, this is not\nnecessary as the plot will fill the size of the  div  to which it is bound. Below we force the heigth of\nour  div  elements to be 400 pixels, and half the width of the browser window so we create a 2x2 grid of\ncharts.   public static void main(String[] args) throws Exception {\n    //Create some random data to plot\n    final DataFrame<Year,String> frame = randomData();\n    //Create the list of 4 charts, 2 JFree based, 2 Google based \n    final List<Chart<?>> charts = createCharts(frame);\n    //Generate Javascript code which can be embedded in an HTML page\n    final String javascript = Chart.create().javascript(charts);\n\n    //Code generate some HTML, embed the Javascript and create appropriate divs\n    final HtmlCode htmlCode = new HtmlCode();\n    htmlCode.newElement(\"html\", html -> {\n        html.newElement(\"head\", head -> {\n            head.newElement(\"script\", script -> {\n                script.newAttribute(\"type\", \"text/javascript\");\n                script.newAttribute(\"src\", \"https://www.gstatic.com/charts/loader.js\");\n            });\n            head.newElement(\"script\", script -> {\n                script.newAttribute(\"type\", \"text/javascript\");\n                script.text(javascript);\n            });\n        });\n        html.newElement(\"body\", body -> {\n            IntStream.range(0, 4).forEach(id -> {\n                body.newElement(\"div\", div -> {\n                    div.newAttribute(\"id\", String.format(\"chart_%s\", id));\n                    div.newAttribute(\"style\", \"float:left;width:50%;height:400px;\");\n                });\n            });\n        });\n    });\n    htmlCode.browse();\n}\n\n\n/**\n * Returns a list of 4 charts, 2 swing based and 2 html based\n * @param frame     the data to plot in different forms\n * @return          the list of 4 charts\n */\nprivate static List<Chart<?>> createCharts(DataFrame<Year,String> frame) {\n    return Collect.asList(\n        Chart.create().asHtml().withLinePlot(frame, chart -> {\n            chart.options().withId(\"chart_0\");\n            chart.title().withText(\"Single DataFrame Line Plot (Google)\");\n            chart.plot().axes().domain().label().withText(\"Year\");\n            chart.plot().axes().range(0).label().withText(\"Random Value\");\n            chart.legend().on().bottom();\n        }),\n        Chart.create().asHtml().withBarPlot(frame, true, chart -> {\n            chart.options().withId(\"chart_1\");\n            chart.title().withText(\"Single DataFrame Bar Plot (Google)\");\n            chart.plot().axes().domain().label().withText(\"Year\");\n            chart.plot().axes().range(0).label().withText(\"Random Value\");\n            chart.legend().on().bottom();\n        }),\n        Chart.create().asSwing().withLinePlot(frame, chart -> {\n            chart.options().withPreferredSize(600, 400).withId(\"chart_2\");\n            chart.title().withText(\"Single DataFrame Line Plot (JFree)\");\n            chart.plot().axes().domain().label().withText(\"Year\");\n            chart.plot().axes().range(0).label().withText(\"Random Value\");\n            chart.legend().on().bottom();\n        }),\n        Chart.create().asSwing().withBarPlot(frame, true, chart -> {\n            chart.options().withPreferredSize(600, 400).withId(\"chart_3\");\n            chart.title().withText(\"Single DataFrame Bar Plot (JFree)\");\n            chart.plot().axes().domain().label().withText(\"Year\");\n            chart.plot().axes().range(0).label().withText(\"Random Value\");\n            chart.legend().on().bottom();\n        })\n    );\n}\n\n/**\n * Creates a DataFrame of random data with 4 series\n * @return      the DataFrame with chart data\n */\nprivate static DataFrame<Year,String> randomData() {\n    final Array<Year> years = Range.of(2000, 2016).map(Year::of).toArray();\n    return DataFrame.of(years, String.class, columns -> {\n        Stream.of(\"A\", \"B\", \"C\", \"D\").forEach(label -> {\n            columns.add(label, Array.randn(years.length()).cumSum());\n        });\n    });\n}",
            "title": "Example"
        },
        {
            "location": "/viz/charts/embed/#sample-output",
            "text": "The HTML output generate by the above example is show below. The base64 images have been truncated for legibility,\nbut hopefully this provides a good insight as to the fairly simple and unified solution for using Javascript to\nrender charts from each adapter.  \n\n<html>\n    <head>\n        <script type=\"text/javascript\" src=\"https://www.gstatic.com/charts/loader.js\"></script>\n        <script type=\"text/javascript\">    \n\n            google.charts.load('current', {'packages':['corechart']});\n            google.charts.setOnLoadCallback(drawCharts);\n\n            function drawCharts() {\n                drawChart_0()\n                drawChart_1()\n                drawChart_2()\n                drawChart_3()\n            }\n\n            /** This is code generation by the Morpheus Visualization library */\n            function drawChart_0() {\n                var data = google.visualization.arrayToDataTable([\n                    [\n                        {id: \"domain\", label: \"Domain\", type: \"string\"},\n                        {id: \"A\", label: \"A\", type: \"number\"},\n                        {id: \"B\", label: \"B\", type: \"number\"},\n                        {id: \"C\", label: \"C\", type: \"number\"},\n                        {id: \"D\", label: \"D\", type: \"number\"}\n                    ],\n                    ['2000',0.5649231377923367,0.5453100986562593,-0.2030931021259246,-0.48360739892318605],\n                    ['2001',-0.23182829941568417,-1.03605735883078,0.09068337378223748,-2.116960106087407],\n                    ['2002',-1.208044022835305,-0.23036814946864737,0.4240705851894213,-1.616996062133938],\n                    ['2003',-0.7428178334708951,1.9563008036267904,1.3123458436616149,-0.7981998224236828],\n                    ['2004',1.0567387300679987,2.3968672877362533,1.0239596059901945,-1.6096974640778379],\n                    ['2005',1.4294955012985076,0.2901319385475669,0.9915597443573548,-1.0894259330202014],\n                    ['2006',1.2901648209824153,-2.076620989957175,0.08628099969449832,0.41435190883770856],\n                    ['2007',0.582294762560585,-1.795857876481523,0.2595097868637274,0.9955095602428286],\n                    ['2008',-0.23808622783335387,-1.5403271477485165,-1.7707040758630166,2.8379995610627615],\n                    ['2009',0.23614781485157388,-1.8756421596020807,0.20119674267818088,2.6158469831886735],\n                    ['2010',-0.17183433628278583,-0.8911557982977236,-1.0555322149236557,4.0443132884329485],\n                    ['2011',-1.6399147529641436,-0.11443828597796613,-1.5943434857459011,5.189000772984133],\n                    ['2012',-0.5298301520679034,0.32141819425165835,-0.6173563177411512,5.228795645597423],\n                    ['2013',-0.34320688750777756,1.7963170143536078,0.08061128454844213,4.96370354438183],\n                    ['2014',-1.3678140181362104,3.8156318180939715,0.26431218483382923,4.0556816321141635],\n                    ['2015',-2.1179730104550756,3.778589363871307,-1.057630438418794,3.3585122043768374]\n                ]);\n\n                var options = {    \n                    fontSize: \"automatic\",\n                    fontName: \"Arial\",\n                    title: \"Single DataFrame Line Plot (Google)\",\n                    titlePosition: \"out\",\n                    titleTextStyle: {    \n                        color: \"#000000\",\n                        fontName: \"Arial\",\n                        fontSize: 16,\n                        bold: true,\n                        italic: false\n                    },\n                    backgroundColor: {    \n                        strokeWidth: 0\n                    },\n                    chartArea: {    \n                        left: \"auto\",\n                        top: \"auto\",\n                        width: \"80%\",\n                        height: \"auto\"\n                    },\n                    legend: {    \n                        position: \"bottom\",\n                        alignment: \"start\",\n                        textStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 10,\n                            bold: false,\n                            italic: false\n                        }\n                    },\n                    dataOpacity: 1.0,\n                    axisTitlesPosition: \"out\",\n                    hAxis: {    \n                        baselineColor: \"#000000\",\n                        textPosition: \"out\",\n                        title: \"Year\",\n                        viewWindowMode: \"maximized\",\n                        format: \"auto\",\n                        titleTextStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 12,\n                            bold: false,\n                            italic: false\n                        },\n                        textStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 12,\n                            bold: false,\n                            italic: false\n                        },\n                        gridLines: \"\"\n                    },\n                    vAxis: {    \n                        baselineColor: \"#000000\",\n                        textPosition: \"out\",\n                        title: \"Random Value\",\n                        viewWindowMode: \"maximized\",\n                        format: \"decimal\",\n                        titleTextStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 12,\n                            bold: false,\n                            italic: false\n                        },\n                        textStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 12,\n                            bold: false,\n                            italic: false\n                        },\n                        gridLines: \"\"\n                    },\n                    series: {    \n                        0: {    \n                            targetAxisIndex: 0,\n                            visibleInLegend: true,\n                            color: \"#e6194b\",\n                            lineWidth: 1.0,\n                            curveType: \"none\"\n                        },\n                        1: {    \n                            targetAxisIndex: 0,\n                            visibleInLegend: true,\n                            color: \"#3cb44b\",\n                            lineWidth: 1.0,\n                            curveType: \"none\"\n                        },\n                        2: {    \n                            targetAxisIndex: 0,\n                            visibleInLegend: true,\n                            color: \"#ffe119\",\n                            lineWidth: 1.0,\n                            curveType: \"none\"\n                        },\n                        3: {    \n                            targetAxisIndex: 0,\n                            visibleInLegend: true,\n                            color: \"#0082c8\",\n                            lineWidth: 1.0,\n                            curveType: \"none\"\n                        }\n                    },\n                    lineWidth: 1,\n                    curveType: \"none\",\n                    orientation: \"horizontal\",\n                    explorer: {    \n                        keepInBounds: true,\n                        actions: [\"dragToZoom\",\"rightClickToReset\"]\n                    }\n                };\n\n                var target = document.getElementById('chart_0');\n                var chart = new google.visualization.LineChart(target);\n                chart.draw(data, options);\n            }\n\n            /** This is code generation by the Morpheus Visualization library */\n            function drawChart_1() {\n                var data = google.visualization.arrayToDataTable([\n                    [\n                        {id: \"domain\", label: \"Domain\", type: \"string\"},\n                        {id: \"A\", label: \"A\", type: \"number\"},\n                        {id: \"B\", label: \"B\", type: \"number\"},\n                        {id: \"C\", label: \"C\", type: \"number\"},\n                        {id: \"D\", label: \"D\", type: \"number\"}\n                    ],\n                    ['2000',0.5649231377923367,0.5453100986562593,-0.2030931021259246,-0.48360739892318605],\n                    ['2001',-0.23182829941568417,-1.03605735883078,0.09068337378223748,-2.116960106087407],\n                    ['2002',-1.208044022835305,-0.23036814946864737,0.4240705851894213,-1.616996062133938],\n                    ['2003',-0.7428178334708951,1.9563008036267904,1.3123458436616149,-0.7981998224236828],\n                    ['2004',1.0567387300679987,2.3968672877362533,1.0239596059901945,-1.6096974640778379],\n                    ['2005',1.4294955012985076,0.2901319385475669,0.9915597443573548,-1.0894259330202014],\n                    ['2006',1.2901648209824153,-2.076620989957175,0.08628099969449832,0.41435190883770856],\n                    ['2007',0.582294762560585,-1.795857876481523,0.2595097868637274,0.9955095602428286],\n                    ['2008',-0.23808622783335387,-1.5403271477485165,-1.7707040758630166,2.8379995610627615],\n                    ['2009',0.23614781485157388,-1.8756421596020807,0.20119674267818088,2.6158469831886735],\n                    ['2010',-0.17183433628278583,-0.8911557982977236,-1.0555322149236557,4.0443132884329485],\n                    ['2011',-1.6399147529641436,-0.11443828597796613,-1.5943434857459011,5.189000772984133],\n                    ['2012',-0.5298301520679034,0.32141819425165835,-0.6173563177411512,5.228795645597423],\n                    ['2013',-0.34320688750777756,1.7963170143536078,0.08061128454844213,4.96370354438183],\n                    ['2014',-1.3678140181362104,3.8156318180939715,0.26431218483382923,4.0556816321141635],\n                    ['2015',-2.1179730104550756,3.778589363871307,-1.057630438418794,3.3585122043768374]\n                ]);\n\n                var options = {    \n                    fontSize: \"automatic\",\n                    fontName: \"Arial\",\n                    title: \"Single DataFrame Bar Plot (Google)\",\n                    titlePosition: \"out\",\n                    titleTextStyle: {    \n                        color: \"#000000\",\n                        fontName: \"Arial\",\n                        fontSize: 16,\n                        bold: true,\n                        italic: false\n                    },\n                    backgroundColor: {    \n                        strokeWidth: 0\n                    },\n                    chartArea: {    \n                        left: \"auto\",\n                        top: \"auto\",\n                        width: \"80%\",\n                        height: \"auto\"\n                    },\n                    legend: {    \n                        position: \"bottom\",\n                        alignment: \"start\",\n                        textStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 10,\n                            bold: false,\n                            italic: false\n                        }\n                    },\n                    dataOpacity: 1.0,\n                    axisTitlesPosition: \"out\",\n                    hAxis: {    \n                        baselineColor: \"#000000\",\n                        textPosition: \"out\",\n                        title: \"Year\",\n                        viewWindowMode: \"maximized\",\n                        format: \"auto\",\n                        titleTextStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 12,\n                            bold: false,\n                            italic: false\n                        },\n                        textStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 12,\n                            bold: false,\n                            italic: false\n                        },\n                        gridLines: \"\"\n                    },\n                    vAxis: {    \n                        baselineColor: \"#000000\",\n                        textPosition: \"out\",\n                        title: \"Random Value\",\n                        viewWindowMode: \"maximized\",\n                        format: \"decimal\",\n                        titleTextStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 12,\n                            bold: false,\n                            italic: false\n                        },\n                        textStyle: {    \n                            color: \"#000000\",\n                            fontName: \"Arial\",\n                            fontSize: 12,\n                            bold: false,\n                            italic: false\n                        },\n                        gridLines: \"\"\n                    },\n                    series: {    \n                        0: {    \n                            targetAxisIndex: 0,\n                            visibleInLegend: true,\n                            color: \"#e6194b\"\n                        },\n                        1: {    \n                            targetAxisIndex: 0,\n                            visibleInLegend: true,\n                            color: \"#3cb44b\"\n                        },\n                        2: {    \n                            targetAxisIndex: 0,\n                            visibleInLegend: true,\n                            color: \"#ffe119\"\n                        },\n                        3: {    \n                            targetAxisIndex: 0,\n                            visibleInLegend: true,\n                            color: \"#0082c8\"\n                        }\n                    },\n                    isStacked: true,\n                    bar: {    \n                        groupWidth: \"70%\"\n                    },\n                    explorer: {    \n                        keepInBounds: true,\n                        actions: [\"dragToZoom\",\"rightClickToReset\"]\n                    }\n                };\n\n                var target = document.getElementById('chart_1');\n                var chart = new google.visualization.ColumnChart(target);\n                chart.draw(data, options);\n            }\n\n\n            function drawChart_2() {\n                var divElement = document.getElementById('chart_2');\n                var imageElement = document.createElement('img');\n                imageElement.setAttribute('src', 'data:image/png;base64, iVBORw0KGgoAAAANSUhEUgA...........');\n                imageElement.setAttribute('alt', 'Embedded Chart');\n                imageElement.setAttribute('class', 'chart');\n                divElement.appendChild(imageElement);\n            }\n\n\n            function drawChart_3() {\n                var divElement = document.getElementById('chart_3');\n                var imageElement = document.createElement('img');\n                imageElement.setAttribute('src', 'data:image/png;base64, iVBORw0KGgoAAAANS...............');\n                imageElement.setAttribute('alt', 'Embedded Chart');\n                imageElement.setAttribute('class', 'chart');\n                divElement.appendChild(imageElement);\n            }\n        </script>\n    </head>\n<body>\n    <div id=\"chart_0\" style=\"float:left;width:50%;height:400px;\"></div>\n    <div id=\"chart_1\" style=\"float:left;width:50%;height:400px;\"></div>\n    <div id=\"chart_2\" style=\"float:left;width:50%;height:400px;\"></div>\n    <div id=\"chart_3\" style=\"float:left;width:50%;height:400px;\"></div>\n</body>\n</html>",
            "title": "Sample Output"
        },
        {
            "location": "/viz/charts/embed/#image-file-approach",
            "text": "The Morpheus  Chart  interface declares overloaded  writePng()  methods which can be used to programmatically \ngenerate  Portable Network Graphics  image content \nto a  File  or an  OutputStream . Note that these methods are only supported by the  JFreeChart adapter , \ninvoking them on an instance of a Google chart will be a  silent operation .  The code below demonstrates how to generate a chart image via a  Java Servlet  as a response to some hypothetical\nHTTP GET request. The chart itself is based on some random data, and we output an image of a size defined by\nparameters in the GET request, and also impose that the chart background be transparent if the user requests it.   /**\n * A basic Java Servlet example that demonstrates how to generate a PNG image of a chart\n */\npublic class ChartServlet extends javax.servlet.http.HttpServlet {\n\n    @Override\n    public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException {\n        try {\n            final String title = request.getParameter(\"title\");\n            final Optional<String> widthParam = Optional.ofNullable(request.getParameter(\"width\"));\n            final Optional<String> heightParam = Optional.ofNullable(request.getParameter(\"height\"));\n            final Optional<String> transParam = Optional.ofNullable(request.getParameter(\"transparent\"));\n            final DataFrame<Integer,String> frame = loadData();\n            //Only the swing based charts support PNG generation\n            Chart.create().asSwing().withLinePlot(frame, \"DataDate\", chart -> {\n                try {\n                    final int width = widthParam.map(Integer::parseInt).orElse(700);\n                    final int height = heightParam.map(Integer::parseInt).orElse(400);\n                    final boolean transparent = transParam.map(Boolean::parseBoolean).orElse(true);\n                    response.setContentType(\"image/png\");\n                    chart.title().withText(title);\n                    chart.legend().on().bottom();\n                    chart.plot().axes().domain().label().withText(\"Data Date\");\n                    chart.plot().axes().range(0).label().withText(\"Temperature\");\n                    chart.writerPng(response.getOutputStream(), width, height, transparent);\n                } catch (Exception ex) {\n                    throw new RuntimeException(ex.getMessage(), ex);\n                }\n            });\n        } catch (Exception ex) {\n            response.sendError(500, ex.getMessage());\n        }\n    }\n\n    /**\n     * Loads the data for the chart to plot\n     * @return      the DataFrame with chart data\n     */\n    private DataFrame<Integer,String> loadData() {\n        final int rowCount = 1000;\n        final LocalDate startDate = LocalDate.of(2013, 1, 1);\n        final Range<Integer> rowKeys = Range.of(0, rowCount);\n        final Range<LocalDate> dates = rowKeys.map(startDate::plusDays);\n        return DataFrame.of(rowKeys, String.class, columns -> {\n            columns.add(\"DataDate\", dates);\n            Stream.of(\"A\", \"B\", \"C\", \"D\").forEach(label -> {\n                columns.add(label, Array.randn(rowCount).cumSum());\n            });\n        });\n    }\n}",
            "title": "Image File Approach"
        },
        {
            "location": "/viz/charts/gallery1/",
            "text": "Google Chart Adapter Gallery\n\n\n\n\n\n\nThe following charts show the output generated by the Morpheus Google charting adapter based on the examples presented in the \n\noverview\n. Note that this page takes a fairly long time to load as it imposes a fairly significant burden on the browser\nto render this many charts in a single page. The Javascript that contains the charting data and instructions for these plots is code \ngenerated by the Morpheus adapter and can be viewed \nhere\n.\n\n\nNote how these charts are interactive in that they support click & drag to zoom and also support informative tooltips and other animations\nwhen you mouse over data points or the legend in the chart.\n\n\nLine Charts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Bar Plots\n\n\n\n\n\n\n\n\nStacked Bar Plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatter Plots\n\n\n\n\n\n\n\n\n\n\n\nRegression Plots\n\n\n\n\n\n\n\n\nArea Plots\n\n\n\n\n\n\n\n\nPie Plots",
            "title": "Gallery (Google)"
        },
        {
            "location": "/viz/charts/gallery1/#google-chart-adapter-gallery",
            "text": "The following charts show the output generated by the Morpheus Google charting adapter based on the examples presented in the  overview . Note that this page takes a fairly long time to load as it imposes a fairly significant burden on the browser\nto render this many charts in a single page. The Javascript that contains the charting data and instructions for these plots is code \ngenerated by the Morpheus adapter and can be viewed  here .  Note how these charts are interactive in that they support click & drag to zoom and also support informative tooltips and other animations\nwhen you mouse over data points or the legend in the chart.",
            "title": "Google Chart Adapter Gallery"
        },
        {
            "location": "/viz/charts/gallery1/#line-charts",
            "text": "",
            "title": "Line Charts"
        },
        {
            "location": "/viz/charts/gallery1/#simple-bar-plots",
            "text": "",
            "title": "Simple Bar Plots"
        },
        {
            "location": "/viz/charts/gallery1/#stacked-bar-plots",
            "text": "",
            "title": "Stacked Bar Plots"
        },
        {
            "location": "/viz/charts/gallery1/#scatter-plots",
            "text": "",
            "title": "Scatter Plots"
        },
        {
            "location": "/viz/charts/gallery1/#regression-plots",
            "text": "",
            "title": "Regression Plots"
        },
        {
            "location": "/viz/charts/gallery1/#area-plots",
            "text": "",
            "title": "Area Plots"
        },
        {
            "location": "/viz/charts/gallery1/#pie-plots",
            "text": "",
            "title": "Pie Plots"
        },
        {
            "location": "/viz/charts/gallery2/",
            "text": "JFreeChart Adapter Gallery\n\n\n\n\nThe following charts show the output generated by the Morpheus JFreeChart adapter based on the examples presented in the \n\noverview\n. While this adapter is intended to be used to build interactive Java Swing applications, it also supports \ngenerating Javascript that plots PNG images of the charts. \n\n\nIn order to achieve this, the chart images are base-64 encoded as \ndata urls\n and\nembedded inside the Javascript which avoids the need to generate separate image files that are then linked into the HTML. This \napproach makes it possible to create a single file HTML page with the images being self-contained, thus enabling the content \nto be shared in an email for example, without any dependencies on external links. The Javascript generated by the JFreeChart \nMorpheus adapter for this page can be viewed \nhere\n.\n\n\nLine Charts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Bar Plots\n\n\n\n\n\n\n\n\nStacked Bar Plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatter Plots\n\n\n\n\n\n\n\n\n\n\n\nRegression Plots\n\n\n\n\n\n\n\n\nArea Plots\n\n\n\n\n\n\n\n\nPie Plots",
            "title": "Gallery (JFree)"
        },
        {
            "location": "/viz/charts/gallery2/#jfreechart-adapter-gallery",
            "text": "The following charts show the output generated by the Morpheus JFreeChart adapter based on the examples presented in the  overview . While this adapter is intended to be used to build interactive Java Swing applications, it also supports \ngenerating Javascript that plots PNG images of the charts.   In order to achieve this, the chart images are base-64 encoded as  data urls  and\nembedded inside the Javascript which avoids the need to generate separate image files that are then linked into the HTML. This \napproach makes it possible to create a single file HTML page with the images being self-contained, thus enabling the content \nto be shared in an email for example, without any dependencies on external links. The Javascript generated by the JFreeChart \nMorpheus adapter for this page can be viewed  here .",
            "title": "JFreeChart Adapter Gallery"
        },
        {
            "location": "/viz/charts/gallery2/#line-charts",
            "text": "",
            "title": "Line Charts"
        },
        {
            "location": "/viz/charts/gallery2/#simple-bar-plots",
            "text": "",
            "title": "Simple Bar Plots"
        },
        {
            "location": "/viz/charts/gallery2/#stacked-bar-plots",
            "text": "",
            "title": "Stacked Bar Plots"
        },
        {
            "location": "/viz/charts/gallery2/#scatter-plots",
            "text": "",
            "title": "Scatter Plots"
        },
        {
            "location": "/viz/charts/gallery2/#regression-plots",
            "text": "",
            "title": "Regression Plots"
        },
        {
            "location": "/viz/charts/gallery2/#area-plots",
            "text": "",
            "title": "Area Plots"
        },
        {
            "location": "/viz/charts/gallery2/#pie-plots",
            "text": "",
            "title": "Pie Plots"
        },
        {
            "location": "/viz/tables/overview/",
            "text": "Tables\n\n\nSwing\n\n\nHTML",
            "title": "Tables"
        },
        {
            "location": "/viz/tables/overview/#tables",
            "text": "",
            "title": "Tables"
        },
        {
            "location": "/viz/tables/overview/#swing",
            "text": "",
            "title": "Swing"
        },
        {
            "location": "/viz/tables/overview/#html",
            "text": "",
            "title": "HTML"
        },
        {
            "location": "/providers/quandl/",
            "text": "Quandl\n\n\nTo be completed...",
            "title": "Quandl"
        },
        {
            "location": "/providers/quandl/#quandl",
            "text": "To be completed...",
            "title": "Quandl"
        },
        {
            "location": "/providers/fred/",
            "text": "Federal Reserve\n\n\nTo be completed...",
            "title": "Federal Reserve"
        },
        {
            "location": "/providers/fred/#federal-reserve",
            "text": "To be completed...",
            "title": "Federal Reserve"
        },
        {
            "location": "/providers/google/",
            "text": "Google Finance\n\n\nTo be completed...",
            "title": "Google Finance"
        },
        {
            "location": "/providers/google/#google-finance",
            "text": "To be completed...",
            "title": "Google Finance"
        },
        {
            "location": "/providers/yahoo/",
            "text": "Disclaimer\n\n\nTHIS PROJECT IS NOT AFFILIATED OR ENDORSED BY YAHOO AND IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, \nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR \nPURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES \nOR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION \nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. DATA EXTRACTED USING THIS LIBRARY IS FOR PERSONAL \nNON-PROFESSIONAL USE AND MUST CONFORM TO YAHOO'S TERMS AND CONDITIONS\n\n\nYahoo Finance\n\n\nIntroduction\n\n\nYahoo Finance\n is an excellent source of free market data and analytics covering both \ndomestic and international markets. While not aimed at a professional user, it does offer a rich source of information \nthat is invaluable to the \nindividual investor\n at a price that cannot be beat, namely free! The Morpheus adapter \nfor Yahoo Finance provides users a programmatic mechanism to capture some of this data for further analysis which\nmay be helpful for informing your own investment decisions. \n\n\nYahoo Finance does not provide any officially supported APIs, so this adapter is essentially reverse engineered \nfrom the actions one can affect on the website itself. For that reason, the adapter could break at anytime, and \ncould take some time to fix or perhaps become unsupportable altogether. Accordingly, the suggestion is to only \nrely on this code for personal non critical research projects.\n\n\nData Access\n\n\nThe Morpheus Yahoo Finance adapter includes various \nDataFrameSource\n implementations that provide access to\n\nasset prices, asset returns, equity fundamentals, option prices and FX rates\n using \nDataFrames\n. In particular, \ndaily \nhistorical quote\n bars (Open, High, Low, Close, Volume) in \nboth \ndividend adjusted\n and \nunadjusted\n formats are available, as well as \nintraday delayed quotes\n while markets \nare trading. For those of you interested in \noption\n markets, \nexpiry dates given an underlying ticker are accessible along with intraday delayed quotes which include an \nimplied \nvolatility estimate\n. Finally, equity fundamental data can be accessed programmatically from the key \nstatistics page\n\nof any security accessible on the site.\n\n\nThe following sections provide some useful examples of how to leverage the API and illustrate some commonly used \ntechniques for analyzing portfolios of investment securities. Firstly, to leverage the library in your project, the \nfollowing dependency should be included in your build tool of choice.\n\n\n<dependency>\n    <groupId>com.zavtech</groupId>\n    <artifactId>morpheus-yahoo</artifactId>\n    <version>${VERSION}</version>\n</dependency>\n\n\n\n\nThere are two ways to integrate with the library, either via a convenience API which involves instantiating an \ninstance of the \nYahooFinance\n class or by using the various \nDataFrameSource\n implementations directly. In the \nlatter case, the sources should be pre-registered in some initializer as follows.\n\n\n\n\n\nstatic {\n    DataFrameSource.register(new YahooOptionSource());\n    DataFrameSource.register(new YahooQuoteHistorySource());\n    DataFrameSource.register(new YahooQuoteLiveSource());\n    DataFrameSource.register(new YahooReturnSource());\n    DataFrameSource.register(new YahooStatsSource());\n}\n\n\n\n\nHistorical Quotes\n\n\nFor most securities represented on Yahoo Finance, it is possible to download historical quote bars into\na CSV file, and Morpheus leverages this same interface to capture \nOpen, High, Low, Close and Volume\n data \nin a \nDataFrame\n. Historical price quotes are most often used to compute asset returns, so Yahoo presents these \nhistorical prices in both \ndividend and split adjusted\n terms making it easy to compute returns that incorporate \nthese affects. The code below downloads split adjusted end of day quotes over the past 10 years for \nBlackRock\n, \nand plots the close price series.  \n\n\n\n\n\nString ticker = \"BLK\";\nYahooQuoteHistorySource source = DataFrameSource.lookup(YahooQuoteHistorySource.class);\n\n//Load end-of-day quote bars and select close price series\nDataFrame<LocalDate,YahooField> closePrices = source.read(options -> {\n    options.withStartDate(LocalDate.now().minusYears(10));\n    options.withEndDate(LocalDate.now());\n    options.withDividendAdjusted(true);\n    options.withTicker(ticker);\n}).cols().select(YahooField.PX_CLOSE);\n\n//Create area plot of close prices\nChart.create().asSwing().withAreaPlot(closePrices, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Date\");\n    chart.plot().axes().range(0).label().withText(\"Close Price\");\n    chart.title().withText(ticker + \": Close Prices (Last 10 Years)\");\n    chart.legend().off();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nIn the example we requested dividend adjusted prices which are convenient for computing returns, however\nthese are artificially changed and do not reflect actual observed prices at the time. To query for \nunadjusted\n \nprices simply pass \nfalse\n as the last parameter to \ngetQuoteBars()\n. The code below pulls both adjusted and \nunadjusted prices, then selects the close prices from each frame and plots them for comparison. BlackRock is a \nhigh paying dividend stock, so we would expect to see material differences in the two price series.\n\n\n\n\n\nString ticker = \"BLK\";\nYahooQuoteHistorySource source = DataFrameSource.lookup(YahooQuoteHistorySource.class);\n\n//Load both adjusted and unadjusted quotes\nDataFrame<LocalDate,String> frame = DataFrame.combineFirst(\n    Stream.of(\"Adjusted\", \"Unadjusted\").map(style -> source.read(options -> {\n        options.withStartDate(LocalDate.now().minusYears(10));\n        options.withEndDate(LocalDate.now());\n        options.withDividendAdjusted(style.equals(\"Adjusted\"));\n        options.withTicker(ticker);\n    }).cols().select(YahooField.PX_CLOSE).cols().mapKeys(column -> style))\n);\n\n//Plot close prices from both these series\nChart.create().asSwing().withLinePlot(frame, chart -> {\n    chart.plot().axes().domain().label().withText(\"Date\");\n    chart.plot().axes().range(0).label().withText(\"Close Price\");\n    chart.title().withText(ticker + \": Adjusted & Unadjusted Close Prices\");\n    chart.legend().bottom();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nNetflix\n currently does not pay a dividend, so if we ran this code for NFLX, \nthe two series should be coincident.\n\n\n\n    \n\n\n\n\n\nAsset Returns\n\n\nInvestment decisions fundamentally boil down to making a judgement call regarding future asset returns, and the likely\nvolatility of those returns over the investment horizon. While Yahoo Finance does not provide an API to directly download\nasset returns, the ability to capture dividend and split adjusted prices makes it very easy to compute returns. The Morpheus\nadapter for Yahoo Finance provides an API to calculate \ndaily\n, \nweekly\n, \nmonthly\n and \ncumulative\n returns as \ndemonstrated in the following sections.\n\n\nCumulative Returns\n\n\nConsider 6 major asset classes, namely Emerging Market Equities, Real-Estate, International Equities (outside US), Commodities,\nLarge Blend and Fixed Income. The returns for these asset classes can be proxied using well known liquid ETFs from the likes\nof \nVanguard\n, \nBlackRock\n or \nState Street Global Advisors\n \namong others. For this example, let's use the following tickers.\n\n\n\n\n\n\n\n\nTicker\n\n\nName\n\n\nProvider\n\n\n\n\n\n\n\n\n\n\n\n\nVWO\n\n\nVanguard FTSE Emerging Markets ETF\n\n\nVanguard\n\n\nDetails\n\n\n\n\n\n\nVNQ\n\n\nVanguard REIT ETF\n\n\nVanguard\n\n\nDetails\n\n\n\n\n\n\nVEA\n\n\nVanguard FTSE Developed Markets ETF\n\n\nVanguard\n\n\nDetails\n\n\n\n\n\n\nDBC\n\n\nPowerShares DB Commodity Tracking ETF\n\n\nPowershares\n\n\nDetails\n\n\n\n\n\n\nVTI\n\n\nVanguard Total Stock Market ETF\n\n\nVanguard\n\n\nDetails\n\n\n\n\n\n\nBND\n\n\nVanguard Total Bond Market ETF\n\n\nVanguard\n\n\nDetails\n\n\n\n\n\n\n\n\nThe code below demonstrates how to use the \nYahooReturnSource\n to calculate \ncumulative returns\n for these tickers including \nthe effect of any splits and dividend payments. To make the plot more user friendly, we re-label the columns as asset class \nnames rather than the ticker symbols.\n\n\n\n\n\nYahooReturnSource source = DataFrameSource.lookup(YahooReturnSource.class);\nDataFrame<LocalDate,String> returns = source.read(options -> {\n    options.withStartDate(LocalDate.now().minusYears(10));\n    options.withEndDate(LocalDate.now());\n    options.withTickers(\"VWO\", \"VNQ\", \"VEA\", \"DBC\", \"VTI\", \"BND\");\n    options.cumulative();\n}).cols().mapKeys(column -> {\n    switch (column.key()) {\n        case \"VWO\": return \"EM Equities\";\n        case \"VNQ\": return \"Real-estate\";\n        case \"VEA\": return \"Foreign Equity\";\n        case \"DBC\": return \"Commodities\";\n        case \"VTI\": return \"Large Blend\";\n        case \"BND\": return \"Fixed Income\";\n        default:    return column.key();\n    }\n}).applyDoubles(v -> {\n    return v.getDouble() * 100d;\n});\n\nChart.create().asSwing().withLinePlot(returns, chart -> {\n    chart.title().withText(\"Major Asset Class Cumulative Returns Last 10 Years (ETF Proxies)\");\n    chart.plot().axes().domain().label().withText(\"Date\");\n    chart.plot().axes().range(0).label().withText(\"Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.##'%';-0.##'%'\");\n    chart.legend().on().bottom();\n    chart.show();\n});\n\n\n\n\nSome of the potential take aways from this chart are as follows:\n\n\n\n\nFixed Income\n returns are much less volatile than the other asset classes in the plot.\n\n\nEverything except Fixed Income got crushed in the 2007/2008 \nGlobal Financial Crisis (GFC)\n\n\nCommodities\n have been a terrible investment over the past 10 years.\n\n\nEM\n & \nInternational\n Equities have only recently gone positive over the past 10 years.\n\n\nVTI, the \nbroadest\n of all the funds, has out performed over the past 10 years.\n\n\n\n\n\n    \n\n\n\n\n\nSmoothed Returns\n\n\nOn a day-to-day basis, market prices are extremely noisy and for the most part completely random. It is often useful to \nremove some of this noise by smoothing the returns, which in turn develops a more stable signal which can be consumed by \na systematic investment process. The Morpheus adapter supports smoothing returns based on an \nExponential Weighted Moving Average\n \n(EWMA) which is a commonly used technique in signal processing.\n\n\nThe code below generates cumulative returns for the \nS&P 500 Index\n tracker with ticker symbol \nSPY\n, \nand smooths the data with various half-lives, namely 0, 5, 10, 30 and 60 business days. A 0 day half-life is interpreted as \nno smoothing, so this simply yields the raw data. The smoothed signal is computed as follows:\n\n\n$$ S_{t} = \\alpha x_{t} + (1 - \\alpha) S_{t-1} \\ for \\ t > 0 \\ and \\ where \\ S_{0} = x_{0} $$\n\n\nThere are various ways to compute \\( \\alpha \\), but in Morpheus we calculate it based on a half-life as follows:\n\n\n$$ \\alpha = 1 - e^{log(0.5) / halfLife} $$\n\n\n\n\n\nString ticker = \"SPY\";\nYahooReturnSource source = DataFrameSource.lookup(YahooReturnSource.class);\nArray<Integer> halfLives = Array.of(0, 5, 10, 30, 60);\nDataFrame<LocalDate,String> frame = DataFrame.combineFirst(halfLives.map(halfLife -> {\n    return source.read(options -> {\n        options.withStartDate(LocalDate.now().minusYears(5));\n        options.withEndDate(LocalDate.now());\n        options.withTickers(ticker);\n        options.withEmaHalfLife(halfLife.getInt());\n        options.cumulative();\n    }).cols().replaceKey(ticker, String.format(\"%s(%s)\", ticker, halfLife.getInt()));\n}));\n\nChart.create().withLinePlot(frame.applyDoubles(v -> v.getDouble() * 100d), chart -> {\n    chart.title().withText(String.format(\"%s EWMA Smoothed Returns With Various Half-Lives\", ticker));\n    chart.plot().axes().domain().label().withText(\"Date\");\n    chart.plot().axes().range(0).label().withText(\"Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.##'%';-0.##'%'\");\n    chart.legend().on().bottom();\n    chart.show();\n});\n\n\n\n\nIt's clear from the plot below that the larger the half-life the more smoothing is applied, which makes sense\nbased on the expression for half-life above.\n\n\n\n    \n\n\n\n\n\nReturn Distribution\n\n\nMany of the statistical techniques in modern finance are predicated on the assumption that asset returns are \nnormally \ndistributed\n. However, it is often the case that returns are not normal, which is why during the GFC many funds experienced \ngreater than -6 sigma events one day after the next. While returns are not normal, the statistical techniques that are commonly \nused are still a reasonable engineering approximation when markets are \nnot\n under stress. When they are under stress, all bets \nare off (no pun intended), which can lead to bad outcomes for investors.\n\n\nThe code below computes daily returns for the S&P 500 ETF tracker with ticker symbol SPY over the past 20 years, and then generates \na histogram of these returns using 200 bins. In addition, the mean and standard deviation of these daily returns are calculated. \nThey are then used to compute a normal probability density function which is scaled appropriately and then overlaid on the chart. \n\n\n\n\n\nString ticker = \"SPY\";\nYahooReturnSource source = DataFrameSource.lookup(YahooReturnSource.class);\nDataFrame<LocalDate,String> returns = source.read(options -> {\n    options.withStartDate(LocalDate.now().minusYears(20));\n    options.withEndDate(LocalDate.now());\n    options.withTickers(ticker);\n}).applyDoubles(v -> {\n    return v.getDouble() * 1d;\n});\n\ndouble binCount = 200;\ndouble min = returns.stats().min();\ndouble max = returns.stats().max();\ndouble mean = returns.stats().mean();\ndouble stdDev = returns.stats().stdDev();\ndouble bound = Math.max(Math.abs(min), Math.abs(max));\ndouble scale = ((max - min) / binCount) * returns.stats().count();\nDataFrame<Double,String> normDist = normal(\"NDIST\", -bound, bound, (int)binCount, mean, stdDev, scale);\n\nChart.create().withHistPlot(returns, (int)binCount, chart -> {\n    chart.title().withText(ticker + \": Daily Return Frequency Distribution\");\n    chart.subtitle().withText(ticker + \" Returns over past 20 years\");\n    chart.plot().<String>data().add(normDist);\n    chart.plot().style(\"NDIST\").withColor(Color.BLACK).withLineWidth(2f);\n    chart.plot().render(1).withLines(false, false);\n    chart.plot().axes().domain().label().withText(\"Return\");\n    chart.plot().axes().domain().format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.plot().axes().range(0).label().withText(\"Frequency\");\n    chart.show();\n});\n\n\n\n\nA couple of features regarding the fit standout immediately, namely that the actual distribution has a \nhigher central peak\n and\nthe \nshoulders are a bit narrower\n. The standard deviation of SPY daily returns over the past 20 years comes out to be 1.239%, and\nthe plot suggests that the fit is a reasonable approximation between +/-3%, or say a little less than \n+/-2.5 standard deviations \n(2.5 sigma)\n. The fitted distribution predicts that a -2.5 sigma event, or a one day drop in the S&P 500 of approximately -3.1% has \na probability of 0.5733%, which on the assumption of \n252 trading days a year\n is likely to happen once in 175 days or about 1.44 \ntimes per year. This is not inconsistent with the actual frequency distribution as shown in the second plot below, which is zoomed \ninto the left side of the distribution inorder to magnify this part of the plot.\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\nWhere the normal distribution is a wholly \ninadequate model\n is in the space beyond +/- 2.5 sigma. As mentioned earlier, many funds \nexperienced -6 sigma events or greater during the global financial crisis, day after day. The S&P 500 fitted distribution presented\nabove suggests that such an extreme event is likely to occur with a probability of 8.2934E-8% implying it \nshould only happen once\nin 4,784,799 years!\n  The plot below zooms into the left tail of the distribution demonstrating that extreme events occur far more\nfrequently than a normal distribution would suggest.\n\n\n\n    \n\n\n\n\n\nIn the above example we leverage a function called \nnormal()\n to generate a scaled normal distribution that we can superimpose on \nthe plot to get a sense of how well the return histogram fits such a model. The code below simply generates a normal curve given \nthe mean, standard deviation of the daily returns, with an appropriate scale factor to fit the histogram.\n\n\n$$ f(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}} $$\n\n\n\n\n\n/**\n * Returns a single column DataFrame containing values generated from a normal distribution probability density function\n * @param label     the column key for the PDF series\n * @param lower     the lower bound for PDF\n * @param upper     the upper bound for PDF\n * @param count     the number of values to include\n * @param mean      the mean for the distribution\n * @param sigma     the standard deviation for the distribution\n * @return          the DataFrame of Normal PDF values\n */\n@SuppressWarnings(\"unchecked\")\n<C> DataFrame<Double,C> normal(C label, double lower, double upper, int count, double mean, double sigma, double scale) {\n    final double stepSize = (upper - lower) / (double)count;\n    final Range<Double> xValues = Range.of(lower, upper, stepSize);\n    return DataFrame.of(xValues, (Class<C>)label.getClass(), columns -> {\n        columns.add(label, xValues.map(x -> {\n            final double part1 = 1d / (sigma * Math.sqrt(2d * Math.PI));\n            final double part2 = Math.exp(-Math.pow(x - mean, 2d) / (2d * Math.pow(sigma, 2d)));\n            return part1 * part2;\n        }));\n    }).applyDoubles(v -> {\n        return v.getDouble() * scale;\n    });\n}\n\n\n\n\nReturn Distribution (Smoothed)\n\n\nConsider the same setup as in the prior example, but in this case we look at the frequency distribution of daily returns after they \nhave been \nexponentially smoothed\n with a 20 day half-life. De-noising the returns does not yield a better fit, and the positive \nskew in the distribution becomes more apparent. In addition, while there is a positive skew, it is also clear that smoothing the\nreturns highlights far \nmore negative extreme events\n than positive events.\n\n\n\n    \n\n\n\n\n\n\n\n\nString ticker = \"SPY\";\nint halfLife = 20;\nYahooReturnSource source = DataFrameSource.lookup(YahooReturnSource.class);\nDataFrame<LocalDate,String> returns = source.read(options -> {\n    options.withStartDate(LocalDate.now().minusYears(10));\n    options.withEndDate(LocalDate.now());\n    options.withTickers(ticker);\n    options.withEmaHalfLife(halfLife);\n}).applyDoubles(v -> {\n    return v.getDouble() * 100d;\n});\n\ndouble binCount = 200;\ndouble min = returns.stats().min();\ndouble max = returns.stats().max();\ndouble mean = returns.stats().mean();\ndouble stdDev = returns.stats().stdDev();\ndouble bound = Math.max(Math.abs(min), Math.abs(max));\ndouble scale = ((max - min) / binCount) * returns.stats().count();\nDataFrame<Double,String> normDist = normal(\"NDIST\", -bound, bound, (int)binCount, mean, stdDev, scale);\n\nChart.create().withHistPlot(returns, (int)binCount, chart -> {\n    chart.title().withText(String.format(\"%s: Smoothed Daily Return Distribution (HL: %s)\", ticker, halfLife));\n    chart.plot().<String>data().add(normDist);\n    chart.plot().style(\"NDIST\").withColor(Color.BLACK).withLineWidth(2f);\n    chart.plot().render(1).withLines(false, false);\n    chart.plot().axes().domain().label().withText(\"Return\");\n    chart.plot().axes().domain().format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.plot().axes().range(0).label().withText(\"Frequency\");\n    chart.show();\n});\n\n\n\n\nLatest Quotes\n\n\nAccessing a snapshot of the most recent data for one or more securities can be done via a single call using the \nYahooQuoteLiveSource\n\nas shown below. In this example, we request the most recent data for several tickers, and we are presented with a \nDataFrame\n \ncontaining all available fields as columns. When accessing this data outside of market hours, fields such as the bid/ask price \nand size will often show NaN values. The resulting \nDataFrame\n is keyed by the security ticker along the row axis and \nYahooField\n \nalong the column axis allowing fast access to elements of the frame. \n\n\n\n\n\nYahooQuoteLiveSource source = DataFrameSource.lookup(YahooQuoteLiveSource.class);\nDataFrame<String, YahooField> quotes = source.read(options -> {\n    options.withTickers(\"AAPL\", \"BLK\", \"NFLX\", \"ORCL\", \"GS\", \"C\", \"GOOGL\", \"MSFT\", \"AMZN\");\n});\n\n\n\n\n\n Index  |  TICKER  |               NAME                |   PX_BID   |  PX_BID_SIZE  |   PX_ASK   |  PX_ASK_SIZE  |    PX_VOLUME    |  PX_CHANGE  |  PX_CHANGE_PERCENT  |  PX_LAST_DATE  |  PX_LAST_TIME  |  PX_LAST   |  PX_LAST_SIZE  |   PX_LOW   |  PX_HIGH   |  PX_PREVIOUS_CLOSE  |  PX_OPEN   |  EXCHANGE  |  AVG_DAILY_VOLUME  |  TRADE_DATE  |  DIVIDEND_PER_SHARE  |    EPS    |  EPS_ESTIMATE  |  EPS_NEXT_YEAR  |  EPS_NEXT_QUARTER  |   FLOAT_SHARES    |  FIFTY_TWO_WEEK_LOW  |  ANNUALISED_GAIN  |     MARKET_CAP      |       EBITDA       |  PRICE_SALES_RATIO  |  PRICE_BOOK_RATIO  |  EX_DIVIDEND_DATE  |  PRICE_EARNINGS_RATIO  |  DIVIDEND_PAY_DATE  |  PEG_RATIO  |  PRICE_EPS_RATIO_CURRENT_YEAR  |  PRICE_EPS_RATIO_NEXT_YEAR  |  SHORT_RATIO  |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  AAPL  |    AAPL  |                       Apple Inc.  |  160.9400  |    1500.0000  |  161.0000  |     800.0000  |  71714046.0000  |    -0.6400  |            -0.0040  |    2017-09-12  |      16:00:00  |  160.8600  |      100.0000  |  158.7700  |  163.9600  |           161.5000  |  162.6100  |       NMS  |     26754500.0000  |        null  |              2.5200  |   8.8100  |        9.0200  |        10.8900  |            3.8400  |  5031913000.0000  |            104.0800  |              NaN  |  830880000000.0000  |  70210000000.0000  |             3.7300  |            6.3000  |        2017-08-10  |               18.2600  |         2017-08-17  |     1.5900  |                       17.8300  |                    14.7700  |       1.9000  |\n   BLK  |     BLK  |                  BlackRock, Inc.  |       NaN  |          NaN  |       NaN  |          NaN  |    333885.0000  |     4.5300  |             0.0107  |    2017-09-12  |      16:02:00  |  428.5700  |    38358.0000  |  424.6900  |  428.7000  |           424.0400  |  426.1400  |       NYQ  |       513127.0000  |        null  |             10.0000  |  20.8300  |       21.8200  |        24.9100  |            5.8200  |   123755000.0000  |            336.8400  |              NaN  |   69520000000.0000  |   5060000000.0000  |             5.9700  |            2.3300  |        2017-08-31  |               20.5700  |         2017-09-22  |     1.3700  |                       19.6400  |                    17.2000  |       3.1800  |\n  NFLX  |    NFLX  |                    Netflix, Inc.  |  185.0600  |     500.0000  |  185.6700  |     200.0000  |   6689568.0000  |     3.4100  |             0.0188  |    2017-09-12  |      16:00:00  |  185.1500  |   199365.0000  |  180.6400  |  185.3300  |           181.7400  |  182.5500  |       NMS  |      7144270.0000  |        null  |                 NaN  |   0.8200  |        1.1900  |         2.0200  |            0.3100  |   424846000.0000  |             93.2600  |              NaN  |   79940000000.0000  |    706920000.0000  |             7.7000  |           25.2100  |              null  |              225.2400  |               null  |     1.9700  |                      155.5900  |                    91.6600  |       3.0700  |\n  ORCL  |    ORCL  |               Oracle Corporation  |       NaN  |          NaN  |       NaN  |          NaN  |  13352387.0000  |     0.2800  |             0.0053  |    2017-09-12  |      16:01:00  |   52.7700  |   839129.0000  |   52.4700  |   52.8900  |            52.4900  |   52.6400  |       NYQ  |     13043800.0000  |        null  |              0.7600  |   2.2100  |        2.9500  |         3.1900  |            0.6900  |  3009105000.0000  |             37.6200  |              NaN  |  218290000000.0000  |  14670000000.0000  |             5.7600  |            4.0300  |        2017-07-17  |               23.8800  |         2017-08-02  |     1.9400  |                       17.8900  |                    16.5400  |       1.8400  |\n    GS  |      GS  |  Goldman Sachs Group, Inc. (The)  |       NaN  |          NaN  |       NaN  |          NaN  |   3745841.0000  |     4.8900  |             0.0221  |    2017-09-12  |      16:00:00  |  225.9500  |   160193.0000  |  222.0200  |  227.6900  |           221.0600  |  222.5400  |       NYQ  |      3043750.0000  |        null  |              3.0000  |  19.0700  |       18.2300  |        19.9700  |            5.0400  |   350537000.0000  |            157.7700  |              NaN  |   87720000000.0000  |            0.0000  |             2.6600  |            1.1400  |        2017-08-29  |               11.8500  |         2017-09-28  |     1.0600  |                       12.3900  |                    11.3100  |       1.2700  |\n     C  |       C  |                  Citigroup, Inc.  |       NaN  |          NaN  |       NaN  |          NaN  |  15495439.0000  |     1.0800  |             0.0160  |    2017-09-12  |      16:00:00  |   68.7900  |  1090112.0000  |   68.1000  |   69.2500  |            67.7100  |   68.2200  |       NYQ  |     16707800.0000  |        null  |              0.8000  |   4.9900  |        5.2200  |         5.9600  |            1.2900  |  2721749000.0000  |             45.1600  |              NaN  |  187420000000.0000  |            0.0000  |             2.8700  |            0.8800  |        2017-08-03  |               13.7700  |         2017-08-25  |     1.2700  |                       13.1800  |                    11.5400  |       0.0200  |\n GOOGL  |   GOOGL  |                    Alphabet Inc.  |  946.0100  |     100.0000  |  947.2300  |     100.0000  |   1284787.0000  |     3.3600  |             0.0036  |    2017-09-12  |      16:00:00  |  946.6500  |   113174.0000  |  937.5000  |  948.0900  |           943.2900  |  946.9200  |       NMS  |      1773140.0000  |        null  |                 NaN  |  27.5900  |       30.5900  |        39.9900  |            9.6000  |   600581000.0000  |            743.5900  |              NaN  |  655910000000.0000  |  32250000000.0000  |             6.5800  |            4.4100  |              null  |               34.3100  |               null  |     1.6200  |                       30.9500  |                    23.6700  |       1.4500  |\n  MSFT  |    MSFT  |            Microsoft Corporation  |   74.6800  |     500.0000  |   74.9300  |     100.0000  |  14394850.0000  |    -0.0800  |            -0.0011  |    2017-09-12  |      16:00:00  |   74.6800  |  1682458.0000  |   74.3700  |   75.2400  |            74.7600  |   74.7600  |       NMS  |     22427700.0000  |        null  |              1.5600  |   2.7100  |        3.1700  |         3.5600  |            0.8300  |  7595028000.0000  |             55.9800  |              NaN  |  575200000000.0000  |  30430000000.0000  |             6.4000  |            7.9600  |        2017-08-15  |               27.5600  |         2017-09-14  |     2.3500  |                       23.5600  |                    20.9800  |       1.9500  |\n  AMZN  |    AMZN  |                 Amazon.com, Inc.  |  982.8700  |     100.0000  |  985.0000  |     200.0000  |   2481066.0000  |     4.6200  |             0.0047  |    2017-09-12  |      16:00:00  |  982.5800  |    99104.0000  |  975.5200  |  984.6700  |           977.9600  |  983.2700  |       NMS  |      3773170.0000  |        null  |                 NaN  |   3.9300  |        3.9900  |         8.1800  |            1.7700  |   400088000.0000  |            710.1000  |              NaN  |  472010000000.0000  |  12300000000.0000  |             3.1300  |           20.2200  |              null  |              249.8900  |               null  |     6.3700  |                      246.2600  |                   120.1200  |       1.2100  |\n\n\n\n\nData from this frame can be accessed in a \ntype safe\n manner as follows:\n\n\n\n\n\nString name = quotes.data().getValue(\"AAPL\", YahooField.NAME);\ndouble closePrice = quotes.data().getDouble(\"AAPL\", YahooField.PX_LAST);\nLocalDate date = quotes.data().getValue(\"AAPL\", YahooField.PX_LAST_DATE);\n\n\n\n\nUsually not all these fields are desired so specific fields can be requested as follows:\n\n\n\n\n\nYahooQuoteLiveSource source = DataFrameSource.lookup(YahooQuoteLiveSource.class);\nDataFrame<String, YahooField> quotes = source.read(options -> {\n    options.withTickers(\"AAPL\", \"BLK\", \"NFLX\", \"ORCL\", \"GS\", \"C\", \"GOOGL\", \"MSFT\", \"AMZN\");\n    options.withFields(\n        YahooField.PX_LAST,\n        YahooField.PX_BID,\n        YahooField.PX_ASK,\n        YahooField.PX_VOLUME,\n        YahooField.PX_CHANGE,\n        YahooField.PX_LAST_DATE,\n        YahooField.PX_LAST_TIME\n    );\n});\n\n\n\n\n\n Index  |  PX_LAST   |   PX_BID   |   PX_ASK   |    PX_VOLUME    |  PX_CHANGE  |  PX_LAST_DATE  |  PX_LAST_TIME  |\n------------------------------------------------------------------------------------------------------------------\n  AAPL  |  160.8600  |  160.9400  |  161.0000  |  71714046.0000  |    -0.6400  |    2017-09-12  |      16:00:00  |\n   BLK  |  428.5700  |       NaN  |       NaN  |    333885.0000  |     4.5300  |    2017-09-12  |      16:02:00  |\n  NFLX  |  185.1500  |  185.0600  |  185.6700  |   6689568.0000  |     3.4100  |    2017-09-12  |      16:00:00  |\n  ORCL  |   52.7700  |       NaN  |       NaN  |  13352387.0000  |     0.2800  |    2017-09-12  |      16:01:00  |\n    GS  |  225.9500  |       NaN  |       NaN  |   3745841.0000  |     4.8900  |    2017-09-12  |      16:00:00  |\n     C  |   68.7900  |       NaN  |       NaN  |  15495439.0000  |     1.0800  |    2017-09-12  |      16:00:00  |\n GOOGL  |  946.6500  |  946.0100  |  947.2300  |   1284787.0000  |     3.3600  |    2017-09-12  |      16:00:00  |\n  MSFT  |   74.6800  |   74.6800  |   74.9300  |  14394850.0000  |    -0.0800  |    2017-09-12  |      16:00:00  |\n  AMZN  |  982.5800  |  982.8700  |  985.0000  |   2481066.0000  |     4.6200  |    2017-09-12  |      16:00:00  |\n\n\n\n\nOption Quotes\n\n\nMarket data for listed \nOptions\n on equities and ETFs is also accessible from \nYahoo Finance (for example, listed options on Apple can be accessed \nhere\n). \nIt is not clear that Yahoo provides a CSV API for this data, so the Morpheus Adapter screen scrapes the relevant information from \nthe HTML page, which obviously makes it somewhat sensitive to changes in the page style.\n\n\nThe code below demonstrates how to query for the \nexpiry dates\n on options given the underlying security symbol. It then\nselects the next upcoming expiry date, and queries for all options, including \ncalls and puts\n for this expiry.\n\n\n\n\n\nString ticker = \"SPY\";\nYahooFinance yahoo = new YahooFinance();\nSet<LocalDate> expiryDates = yahoo.getOptionExpiryDates(ticker);\nLocalDate nextExpiry = expiryDates.iterator().next();\nDataFrame<String,YahooField> optionQuotes = yahoo.getOptionQuotes(ticker, nextExpiry);\n\n\n\n\nCalls and puts\n can easily be separated by filtering the Morpheus \nDataFrame\n in the usual fashion as shown below.\n\n\n\n\n\n//Select rows representing CALL options\nDataFrame<String,YahooField> calls = optionQuotes.rows().select(row -> {\n    final String type = row.getValue(YahooField.OPTION_TYPE);\n    return type.equalsIgnoreCase(\"Call\");\n});\n\n\n\n\n\n       Index         |  TICKER  |  OPTION_TYPE  |  EXPIRY_DATE  |  PX_STRIKE  |  PX_LAST   |  PX_CHANGE  |  PX_CHANGE_PERCENT  |   PX_BID   |   PX_ASK   |  PX_VOLUME   |  OPEN_INTEREST  |  IMPLIED_VOLATILITY  |\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n SPY170915C00050000  |     SPY  |         CALL  |   2017-09-15  |    50.0000  |  187.5000  |     0.0000  |                NaN  |  186.0900  |  189.2100  |      1.0000  |         0.0000  |              0.0000  |\n SPY170915C00055000  |     SPY  |         CALL  |   2017-09-15  |    55.0000  |  195.0100  |     3.0300  |             0.0158  |  193.1200  |  197.1800  |     56.0000  |       686.0000  |             10.7500  |\n SPY170915C00060000  |     SPY  |         CALL  |   2017-09-15  |    60.0000  |  190.0900  |     3.1200  |             0.0167  |  188.1200  |  192.1300  |     38.0000  |       617.0000  |              9.7188  |\n SPY170915C00065000  |     SPY  |         CALL  |   2017-09-15  |    65.0000  |  185.1500  |     5.5800  |             0.0311  |  183.1200  |  187.2600  |     52.0000  |       390.0000  |             10.0625  |\n SPY170915C00070000  |     SPY  |         CALL  |   2017-09-15  |    70.0000  |  180.1100  |     2.2700  |             0.0128  |  178.0000  |  182.2000  |     67.0000  |       142.0000  |              8.0625  |\n SPY170915C00075000  |     SPY  |         CALL  |   2017-09-15  |    75.0000  |  175.0800  |     6.3200  |             0.0374  |  173.0000  |  177.0300  |     18.0000  |       138.0000  |             13.9258  |\n SPY170915C00080000  |     SPY  |         CALL  |   2017-09-15  |    80.0000  |  170.0800  |     6.1700  |             0.0376  |  168.0000  |  172.2500  |     25.0000  |        59.0000  |              7.8125  |\n SPY170915C00085000  |     SPY  |         CALL  |   2017-09-15  |    85.0000  |  165.1500  |     6.0600  |             0.0381  |  163.0000  |  167.0900  |     13.0000  |       182.0000  |             12.6523  |\n SPY170915C00090000  |     SPY  |         CALL  |   2017-09-15  |    90.0000  |  160.0800  |     8.3800  |             0.0552  |  158.0000  |  162.2200  |      8.0000  |       180.0000  |              6.7500  |\n SPY170915C00095000  |     SPY  |         CALL  |   2017-09-15  |    95.0000  |  155.0800  |     6.1700  |             0.0414  |  153.0000  |  157.2000  |      4.0000  |        55.0000  |              6.1875  |\n\n\n\n\n\n\n\n//Select rows representing PUT options\nDataFrame<String,YahooField> puts = optionQuotes.rows().select(row -> {\n    final String type = row.getValue(YahooField.OPTION_TYPE);\n    return type.equalsIgnoreCase(\"Put\");\n});\n\n\n\n\n\n       Index         |  TICKER  |  OPTION_TYPE  |  EXPIRY_DATE  |  PX_STRIKE  |  PX_LAST  |  PX_CHANGE  |  PX_CHANGE_PERCENT  |  PX_BID  |  PX_ASK  |  PX_VOLUME   |  OPEN_INTEREST  |  IMPLIED_VOLATILITY  |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n SPY170915P00055000  |     SPY  |          PUT  |   2017-09-15  |    55.0000  |   0.0200  |     0.0000  |                NaN  |  0.0000  |  0.0100  |     10.0000  |      2250.0000  |              8.5000  |\n SPY170915P00060000  |     SPY  |          PUT  |   2017-09-15  |    60.0000  |   0.0200  |     0.0000  |                NaN  |  0.0000  |  0.0100  |     50.0000  |      1912.0000  |              8.0000  |\n SPY170915P00065000  |     SPY  |          PUT  |   2017-09-15  |    65.0000  |   0.0100  |     0.0000  |                NaN  |  0.0000  |  0.0100  |     10.0000  |      4117.0000  |              7.7500  |\n SPY170915P00070000  |     SPY  |          PUT  |   2017-09-15  |    70.0000  |   0.0100  |     0.0000  |                NaN  |  0.0000  |  0.0100  |      5.0000  |      8582.0000  |              7.2500  |\n SPY170915P00075000  |     SPY  |          PUT  |   2017-09-15  |    75.0000  |   0.0100  |     0.0000  |                NaN  |  0.0000  |  0.0100  |      1.0000  |      5598.0000  |              6.8750  |\n SPY170915P00080000  |     SPY  |          PUT  |   2017-09-15  |    80.0000  |   0.0100  |     0.0000  |                NaN  |  0.0000  |  0.0100  |    150.0000  |      3061.0000  |              6.5000  |\n SPY170915P00085000  |     SPY  |          PUT  |   2017-09-15  |    85.0000  |   0.0100  |     0.0000  |                NaN  |  0.0000  |  0.0100  |     14.0000  |      5525.0000  |              6.1250  |\n SPY170915P00090000  |     SPY  |          PUT  |   2017-09-15  |    90.0000  |   0.0100  |     0.0000  |                NaN  |  0.0000  |  0.0100  |      2.0000  |      5638.0000  |              5.8750  |\n SPY170915P00095000  |     SPY  |          PUT  |   2017-09-15  |    95.0000  |   0.0100  |     0.0000  |                NaN  |  0.0000  |  0.0100  |     50.0000  |     22290.0000  |              5.5000  |\n SPY170915P00100000  |     SPY  |          PUT  |   2017-09-15  |   100.0000  |   0.0100  |     0.0000  |                NaN  |  0.0000  |  0.0100  |     99.0000  |     19382.0000  |              5.2500  |\n\n\n\n\nVolatility Smile\n\n\nOne of the most influential inputs to the famous \nBlack-Scholes-Merton\n \noption pricing model concerns the future volatility of the returns on the underlying asset. In fact, volatility is so fundamental to \noption pricing that certain types of option contracts are quoted in volatility units rather than price. Black-Scholes assumes that \nthe volatility of the underlying returns are constant over the term in question, and it also assumes that other characteristics of \nthe option such as the moneyness (how far in or out-of-the-money the option might be), do not influence estimates of future volatility.\n\n\nIn reality, this does not appear to be the case based on observed market prices. A commonly used visualization in option trading is \ncalled the \nOption Volatility Smile\n, which plots \nimplied volatilities\n of options with different \nstrike prices\n for the same \nexpiry date. According to Black-Scholes, these curves should be flat, but in reality they form more of a curve in the shape of a \nperson smiling. That is, implied volatilities for \nat-the-money\n options tend to be lower than for deep \nin-the-money\n or deep \n\nout-the-money\n options.\n\n\nThe plot below illustrates the volatility smiles for options on the S&P 500 ETF with ticker symbol SPY which has been generated\nusing the Morpheus Yahoo Finance adapter. While various \noutliers\n appear to exist, the smile pattern is pretty clear. These outliers \nare likely to be explained by low volume at the strike prices in question, so the calculated implied volatilities are stale at those\npoints. The other possibility is a mis-pricing of some options at these strikes, but that is fairly unlikely as such differences would \nbe quickly arbitraged away. The code to generate this plot is also shown below.\n\n\n\n    \n\n\n\n\n\n\n\n\nString ticker = \"SPY\";\n//Instantiate Yahoo convenience adapter\nYahooFinance yahoo = new YahooFinance();\n//Select last price for underlying\ndouble lastPrice = yahoo.getLiveQuotes(Array.of(ticker)).data().getDouble(0, YahooField.PX_LAST);\n//Select call options with strike price within 10% of current market price and non zero vol\nDataFrame<String,YahooField> options = yahoo.getOptionQuotes(ticker).rows().select(row -> {\n    final String type = row.getValue(YahooField.OPTION_TYPE);\n    if (!type.equalsIgnoreCase(\"CALL\")) {\n        return false;\n    } else {\n        final double strike = row.getDouble(YahooField.PX_STRIKE);\n        final double impliedVol = row.getDouble(YahooField.IMPLIED_VOLATILITY);\n        return impliedVol > 0 && strike > lastPrice * 0.9d && strike < lastPrice * 1.1d;\n    }\n});\n\n//Select all distinct expiry dates for quotes\nArray<LocalDate> expiryDates = options.colAt(YahooField.EXPIRY_DATE).distinct();\n//Creates frames for each expiry including only strike price and implied-vol columns\nArray<DataFrame<Integer,String>> frames = expiryDates.map(v -> {\n    final LocalDate expiry = v.getValue();\n    final DataFrame<String,YahooField> calls = options.rows().select(row -> {\n        final LocalDate date = row.getValue(YahooField.EXPIRY_DATE);\n        return expiry.equals(date);\n    });\n    return DataFrame.of(Range.of(0, calls.rowCount()), String.class, columns -> {\n        columns.add(\"Strike\", calls.colAt(YahooField.PX_STRIKE).toArray());\n        columns.add(expiry.toString(), calls.colAt(YahooField.IMPLIED_VOLATILITY).toArray().applyDoubles(x -> {\n            return x.getDouble() * 100d;\n        }));\n    });\n});\n\n//Create plot of N frames each for a different expiry\nChart.create().asSwing().withLinePlot(frames.getValue(0), \"Strike\", chart -> {\n    for (int i=1; i<frames.length(); ++i) {\n        DataFrame<Integer,String> data = frames.getValue(i);\n        chart.plot().<String>data().add(data, \"Strike\");\n        chart.plot().render(i).withLines(true, false);\n    }\n    chart.plot().render(0).withLines(true, false);\n    chart.plot().axes().domain().label().withText(\"Strike Price\");\n    chart.plot().axes().range(0).label().withText(\"Implied Volatility\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.title().withText(\"SPY Call Option Implied Volatility Smiles\");\n    chart.legend().on().right();\n    chart.show();\n});\n\n\n\n\nWe can use a slightly modified version of this code to run the analysis for all \nPUT options\n, the results of \nwhich are plotted below. The same smile pattern is evident, as are the existence of outliers, which is most likely due \nto the lack of transactions at the strikes in question. Another take-away from these plots is that deep \nin-the-money\n\noptions are more expensive from an implied volatility perspective than deep \nout-the-money\n options as the smile\ndoes not appear to be symmetric in nature.\n\n\n\n    \n\n\n\n\n\nKey Statistics\n\n\nVarious \nfinancial, trading\n and \nvaluation statistics\n are made available by Yahoo Finance on the \nStatistics\n\npage for each applicable security. The Morpheus adapter provides a programmatic mechanism to extract this data into\na \nDataFrame\n which allows for easy cross sectional comparisons between companies. Only current values are available\nvia this interface, historical values are not supported. The code below demonstrates how to extract these statistics\nfor various companies, and then transposes and prints the \nDataFrame\n to standard out.\n\n\n\n\n\nYahooFinance yahoo = new YahooFinance();\nArray<String> tickers = Array.of(\"AAPL\", \"ORCL\", \"BLK\", \"GS\", \"NFLX\", \"AMZN\", \"FB\");\nDataFrame<String,YahooField> stats = yahoo.getStatistics(tickers);\nstats.transpose().out().print(200, formats -> {\n    final SmartFormat smartFormat = new SmartFormat();\n    formats.setPrinter(Object.class, Printer.forObject(smartFormat::format));\n});\n\n\n\n\n\n           Index            |     BLK      |     NFLX     |     AAPL     |     ORCL     |      GS      |      FB      |     AMZN     |\n--------------------------------------------------------------------------------------------------------------------------------------\n                MARKET_CAP  |    69.4700B  |    78.7300B  |   825.8200B  |   201.6200B  |    87.4300B  |   498.4800B  |   474.0300B  |\n               PE_TRAILING  |     20.5800  |    221.8400  |     18.1500  |     22.0500  |     11.8100  |     38.4200  |    250.9600  |\n                PE_FORWARD  |      1.4000  |      1.9800  |      1.4500  |      1.9800  |      1.0900  |      1.2100  |      6.6200  |\n         PRICE_SALES_RATIO  |      6.0300  |      7.7300  |      3.6900  |      5.3400  |      2.7100  |     15.0300  |      3.1600  |\n          PRICE_BOOK_RATIO  |      2.3500  |     25.2900  |      6.2400  |      3.7400  |      1.1600  |      7.4900  |     20.4000  |\n           FISCAL_YEAR_END  |  2016-12-31  |  2016-12-31  |  2016-09-24  |  2017-05-31  |  2016-12-31  |  2016-12-31  |  2016-12-31  |\n       MOST_RECENT_QUARTER  |  2017-06-30  |  2017-06-30  |  2017-07-01  |  2017-05-31  |  2017-06-30  |  2017-06-30  |  2017-06-30  |\n             PROFIT_MARGIN  |      0.2992  |      0.0355  |      0.2087  |      0.2474  |      0.2644  |      0.3966  |      0.0128  |\n          OPERATING_MARGIN  |      0.4205  |      0.0633  |      0.2684  |      0.3519  |      0.3618  |      0.4666  |      0.0231  |\n          RETURN_ON_ASSETS  |      0.0138  |      0.0287  |      0.1152  |      0.0671  |      0.0095  |      0.1493  |      0.0283  |\n          RETURN_ON_EQUITY  |      0.1174  |      0.1310  |      0.3603  |      0.1830  |      0.0980  |      0.2251  |      0.0967  |\n               REVENUE_TTM  |    11.5200B  |    10.1900B  |   223.5100B  |    37.7300B  |    32.2500B  |    33.1700B  |   150.1200B  |\n         REVENUE_PER_SHARE  |     70.5300  |     23.6900  |     42.4000  |      9.1700  |     77.9100  |     11.5000  |    314.7300  |\n       REVENUE_GROWTH_QTLY  |      0.0570  |      0.3230  |      0.0720  |      0.0280  |     -0.0060  |      0.4480  |      0.2480  |\n              GROSS_PROFIT  |    10.9500B  |     2.8000B  |    84.2600B  |    30.2600B  |    30.6100B  |    23.8500B  |    47.7200B  |\n                EBITDA_TTM  |     5.0600B  |   706.9200M  |    70.2100B  |    14.6700B  |         NaN  |    18.0800B  |    12.3000B  |\n               EPS_DILUTED  |     20.8300  |      0.8200  |      8.8100  |      2.2100  |     19.0700  |      4.4700  |      3.9300  |\n      EARNINGS_GRWOTH_QTLY  |      0.0860  |      0.6100  |      0.1180  |      0.1490  |      0.0050  |      0.7060  |     -0.7700  |\n                      BETA  |      1.6800  |      0.6300  |      1.4300  |      1.1400  |      1.4400  |      0.5400  |      1.3800  |\n                  CASH_MRQ  |     6.2500B  |     2.1600B  |    77.0100B  |    66.0800B  |   732.4800B  |    35.4500B  |    21.4500B  |\n            CASH_PER_SHARE  |     38.5300  |      5.0100  |     14.9100  |     15.9700  |     1.8868K  |     12.2100  |     44.6500  |\n                  DEBT_MRQ  |     4.9700B  |     4.8400B  |   108.6000B  |    57.9100B  |   404.4600B  |         NaN  |    23.6200B  |\n      DEBT_OVER_EQUITY_MRQ  |     16.6300  |    155.3900  |     82.0100  |    106.7500  |    463.8500  |         NaN  |    101.7500  |\n             CURRENT_RATIO  |      1.2300  |      1.3100  |      1.3900  |      3.0800  |      1.8000  |     12.3100  |      1.0100  |\n      BOOK_VALUE_PER_SHARE  |    182.2100  |      7.2100  |     25.6100  |     13.0200  |    194.4100  |     22.9200  |     48.3600  |\n       OPERATING_CASH_FLOW  |     3.3900B  |    -1.9000B  |    64.0700B  |    14.1300B  |   -15.8300B  |    19.3800B  |    17.0600B  |\n    LEVERED_FREE_CASH_FLOW  |     3.4500B  |     5.8300B  |    40.6200B  |     9.6300B  |         NaN  |    10.2200B  |    11.1500B  |\n                ADV_3MONTH  |   505.7900K  |     6.9300M  |    26.7900M  |    13.1800M  |     3.0200M  |    16.4700M  |     3.6400M  |\n                 ADV_10DAY  |   435.9500K  |     5.6800M  |    34.1000M  |    14.5700M  |     3.3800M  |    13.0400M  |     2.7500M  |\n        SHARES_OUTSTANDING  |   160.9800M  |   431.7500M  |     5.1700B  |     4.1400B  |   388.2100M  |     2.3700B  |   480.3800M  |\n              SHARES_FLOAT  |   123.7600M  |   424.8500M  |     5.0300B  |     3.0100B  |   350.5400M  |     2.3400B  |   400.0900M  |\n     OWNER_PERCENT_INSIDER  |      0.0361  |      0.0182  |      0.0008  |      0.2725  |      0.0179  |      0.0175  |      0.1677  |\n OWNER_PERCENT_INSTITUTION  |      0.8775  |      0.8284  |      0.6247  |      0.5984  |      0.7931  |      0.7244  |      0.6234  |\n              SHARES_SHORT  |     2.0600M  |    27.8500M  |    40.3100M  |    31.8300M  |     4.5400M  |    20.2500M  |     4.7200M  |\n        SHARES_SHORT_RATIO  |      3.1800  |      3.0700  |      1.9000  |      1.8400  |      1.2700  |      0.9700  |      1.2100  |\n        SHARES_SHORT_PRIOR  |     2.3100M  |    25.7400M  |    39.1500M  |    35.7800M  |     4.3500M  |    21.8400M  |     5.0600M  |\n              DIVIDEND_FWD  |     10.0000  |         NaN  |      2.5200  |      0.7600  |      3.0000  |         NaN  |         NaN  |\n        DIVIDEND_FWD_YIELD  |      0.0235  |         NaN  |      0.0158  |      0.0144  |      0.0139  |         NaN  |         NaN  |\n         DIVIDEND_TRAILING  |      9.5800  |         NaN  |      2.3400  |      0.6400  |      2.7000  |         NaN  |         NaN  |\n   DIVIDEND_TRAILING_YIELD  |      0.0225  |         NaN  |      0.0148  |      0.0121  |      0.0119  |         NaN  |         NaN  |\n     DIVIDEND_PAYOUT_RATIO  |      0.4599  |         NaN  |      0.2650  |      0.2896  |      0.1421  |         NaN  |         NaN  |\n         DIVIDEND_PAY_DATE  |  2017-09-22  |        null  |  2017-08-17  |  2017-08-02  |  2017-09-28  |        null  |        null  |\n          DIVIDEND_EX_DATE  |  2017-08-31  |        null  |  2017-08-10  |  2017-07-17  |  2017-05-30  |        null  |        null  |\n           LAST_SPLIT_DATE  |  2007-06-05  |  2015-07-15  |  2014-06-09  |  2000-10-13  |        null  |        null  |  1999-09-02  |\n\n\n\n\nThe code below demonstrates how to generate a plot of a small subset of the profitability and return metrics for a number\nof prominent financial services and technology firms.\n\n\n\n\n\nYahooFinance yahoo = new YahooFinance();\nArray<String> tickers = Array.of(\"BLK\", \"GS\", \"MS\", \"JPM\", \"C\", \"BAC\", \"AAPL\", \"NVDA\", \"GOOGL\");\nDataFrame<String,YahooField> data = yahoo.getStatistics(tickers);\nDataFrame<String,YahooField> stats = data.cols().select(\n    YahooField.PROFIT_MARGIN,\n    YahooField.OPERATING_MARGIN,\n    YahooField.RETURN_ON_ASSETS,\n    YahooField.RETURN_ON_EQUITY\n).applyDoubles(v -> {\n    return v.getDouble() * 100d;\n});\n\nChart.create().withBarPlot(stats.transpose(), false, chart -> {\n    chart.title().withText(\"Profitability & Return Metrics\");\n    chart.plot().axes().domain().label().withText(\"Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Value\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.legend().right().on();\n    chart.show();\n});\n\n\n\n\n\n    \n\n\n\n\n\nPortfolio Analysis\n\n\nThere are several examples of how to use the Yahoo Finance adapter to analyze and construct portfolios of risky assets\nin the section on \nModern Portfolio Theory\n.",
            "title": "Yahoo Finance"
        },
        {
            "location": "/providers/yahoo/#disclaimer",
            "text": "THIS PROJECT IS NOT AFFILIATED OR ENDORSED BY YAHOO AND IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, \nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR \nPURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES \nOR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION \nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. DATA EXTRACTED USING THIS LIBRARY IS FOR PERSONAL \nNON-PROFESSIONAL USE AND MUST CONFORM TO YAHOO'S TERMS AND CONDITIONS",
            "title": "Disclaimer"
        },
        {
            "location": "/providers/yahoo/#yahoo-finance",
            "text": "",
            "title": "Yahoo Finance"
        },
        {
            "location": "/providers/yahoo/#introduction",
            "text": "Yahoo Finance  is an excellent source of free market data and analytics covering both \ndomestic and international markets. While not aimed at a professional user, it does offer a rich source of information \nthat is invaluable to the  individual investor  at a price that cannot be beat, namely free! The Morpheus adapter \nfor Yahoo Finance provides users a programmatic mechanism to capture some of this data for further analysis which\nmay be helpful for informing your own investment decisions.   Yahoo Finance does not provide any officially supported APIs, so this adapter is essentially reverse engineered \nfrom the actions one can affect on the website itself. For that reason, the adapter could break at anytime, and \ncould take some time to fix or perhaps become unsupportable altogether. Accordingly, the suggestion is to only \nrely on this code for personal non critical research projects.",
            "title": "Introduction"
        },
        {
            "location": "/providers/yahoo/#data-access",
            "text": "The Morpheus Yahoo Finance adapter includes various  DataFrameSource  implementations that provide access to asset prices, asset returns, equity fundamentals, option prices and FX rates  using  DataFrames . In particular, \ndaily  historical quote  bars (Open, High, Low, Close, Volume) in \nboth  dividend adjusted  and  unadjusted  formats are available, as well as  intraday delayed quotes  while markets \nare trading. For those of you interested in  option  markets, \nexpiry dates given an underlying ticker are accessible along with intraday delayed quotes which include an  implied \nvolatility estimate . Finally, equity fundamental data can be accessed programmatically from the key  statistics page \nof any security accessible on the site.  The following sections provide some useful examples of how to leverage the API and illustrate some commonly used \ntechniques for analyzing portfolios of investment securities. Firstly, to leverage the library in your project, the \nfollowing dependency should be included in your build tool of choice.  <dependency>\n    <groupId>com.zavtech</groupId>\n    <artifactId>morpheus-yahoo</artifactId>\n    <version>${VERSION}</version>\n</dependency>  There are two ways to integrate with the library, either via a convenience API which involves instantiating an \ninstance of the  YahooFinance  class or by using the various  DataFrameSource  implementations directly. In the \nlatter case, the sources should be pre-registered in some initializer as follows.   static {\n    DataFrameSource.register(new YahooOptionSource());\n    DataFrameSource.register(new YahooQuoteHistorySource());\n    DataFrameSource.register(new YahooQuoteLiveSource());\n    DataFrameSource.register(new YahooReturnSource());\n    DataFrameSource.register(new YahooStatsSource());\n}",
            "title": "Data Access"
        },
        {
            "location": "/providers/yahoo/#historical-quotes",
            "text": "For most securities represented on Yahoo Finance, it is possible to download historical quote bars into\na CSV file, and Morpheus leverages this same interface to capture  Open, High, Low, Close and Volume  data \nin a  DataFrame . Historical price quotes are most often used to compute asset returns, so Yahoo presents these \nhistorical prices in both  dividend and split adjusted  terms making it easy to compute returns that incorporate \nthese affects. The code below downloads split adjusted end of day quotes over the past 10 years for  BlackRock , \nand plots the close price series.     String ticker = \"BLK\";\nYahooQuoteHistorySource source = DataFrameSource.lookup(YahooQuoteHistorySource.class);\n\n//Load end-of-day quote bars and select close price series\nDataFrame<LocalDate,YahooField> closePrices = source.read(options -> {\n    options.withStartDate(LocalDate.now().minusYears(10));\n    options.withEndDate(LocalDate.now());\n    options.withDividendAdjusted(true);\n    options.withTicker(ticker);\n}).cols().select(YahooField.PX_CLOSE);\n\n//Create area plot of close prices\nChart.create().asSwing().withAreaPlot(closePrices, false, chart -> {\n    chart.plot().axes().domain().label().withText(\"Date\");\n    chart.plot().axes().range(0).label().withText(\"Close Price\");\n    chart.title().withText(ticker + \": Close Prices (Last 10 Years)\");\n    chart.legend().off();\n    chart.show();\n});  \n       In the example we requested dividend adjusted prices which are convenient for computing returns, however\nthese are artificially changed and do not reflect actual observed prices at the time. To query for  unadjusted  \nprices simply pass  false  as the last parameter to  getQuoteBars() . The code below pulls both adjusted and \nunadjusted prices, then selects the close prices from each frame and plots them for comparison. BlackRock is a \nhigh paying dividend stock, so we would expect to see material differences in the two price series.   String ticker = \"BLK\";\nYahooQuoteHistorySource source = DataFrameSource.lookup(YahooQuoteHistorySource.class);\n\n//Load both adjusted and unadjusted quotes\nDataFrame<LocalDate,String> frame = DataFrame.combineFirst(\n    Stream.of(\"Adjusted\", \"Unadjusted\").map(style -> source.read(options -> {\n        options.withStartDate(LocalDate.now().minusYears(10));\n        options.withEndDate(LocalDate.now());\n        options.withDividendAdjusted(style.equals(\"Adjusted\"));\n        options.withTicker(ticker);\n    }).cols().select(YahooField.PX_CLOSE).cols().mapKeys(column -> style))\n);\n\n//Plot close prices from both these series\nChart.create().asSwing().withLinePlot(frame, chart -> {\n    chart.plot().axes().domain().label().withText(\"Date\");\n    chart.plot().axes().range(0).label().withText(\"Close Price\");\n    chart.title().withText(ticker + \": Adjusted & Unadjusted Close Prices\");\n    chart.legend().bottom();\n    chart.show();\n});  \n       Netflix  currently does not pay a dividend, so if we ran this code for NFLX, \nthe two series should be coincident.",
            "title": "Historical Quotes"
        },
        {
            "location": "/providers/yahoo/#asset-returns",
            "text": "Investment decisions fundamentally boil down to making a judgement call regarding future asset returns, and the likely\nvolatility of those returns over the investment horizon. While Yahoo Finance does not provide an API to directly download\nasset returns, the ability to capture dividend and split adjusted prices makes it very easy to compute returns. The Morpheus\nadapter for Yahoo Finance provides an API to calculate  daily ,  weekly ,  monthly  and  cumulative  returns as \ndemonstrated in the following sections.",
            "title": "Asset Returns"
        },
        {
            "location": "/providers/yahoo/#cumulative-returns",
            "text": "Consider 6 major asset classes, namely Emerging Market Equities, Real-Estate, International Equities (outside US), Commodities,\nLarge Blend and Fixed Income. The returns for these asset classes can be proxied using well known liquid ETFs from the likes\nof  Vanguard ,  BlackRock  or  State Street Global Advisors  \namong others. For this example, let's use the following tickers.     Ticker  Name  Provider       VWO  Vanguard FTSE Emerging Markets ETF  Vanguard  Details    VNQ  Vanguard REIT ETF  Vanguard  Details    VEA  Vanguard FTSE Developed Markets ETF  Vanguard  Details    DBC  PowerShares DB Commodity Tracking ETF  Powershares  Details    VTI  Vanguard Total Stock Market ETF  Vanguard  Details    BND  Vanguard Total Bond Market ETF  Vanguard  Details     The code below demonstrates how to use the  YahooReturnSource  to calculate  cumulative returns  for these tickers including \nthe effect of any splits and dividend payments. To make the plot more user friendly, we re-label the columns as asset class \nnames rather than the ticker symbols.   YahooReturnSource source = DataFrameSource.lookup(YahooReturnSource.class);\nDataFrame<LocalDate,String> returns = source.read(options -> {\n    options.withStartDate(LocalDate.now().minusYears(10));\n    options.withEndDate(LocalDate.now());\n    options.withTickers(\"VWO\", \"VNQ\", \"VEA\", \"DBC\", \"VTI\", \"BND\");\n    options.cumulative();\n}).cols().mapKeys(column -> {\n    switch (column.key()) {\n        case \"VWO\": return \"EM Equities\";\n        case \"VNQ\": return \"Real-estate\";\n        case \"VEA\": return \"Foreign Equity\";\n        case \"DBC\": return \"Commodities\";\n        case \"VTI\": return \"Large Blend\";\n        case \"BND\": return \"Fixed Income\";\n        default:    return column.key();\n    }\n}).applyDoubles(v -> {\n    return v.getDouble() * 100d;\n});\n\nChart.create().asSwing().withLinePlot(returns, chart -> {\n    chart.title().withText(\"Major Asset Class Cumulative Returns Last 10 Years (ETF Proxies)\");\n    chart.plot().axes().domain().label().withText(\"Date\");\n    chart.plot().axes().range(0).label().withText(\"Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.##'%';-0.##'%'\");\n    chart.legend().on().bottom();\n    chart.show();\n});  Some of the potential take aways from this chart are as follows:   Fixed Income  returns are much less volatile than the other asset classes in the plot.  Everything except Fixed Income got crushed in the 2007/2008  Global Financial Crisis (GFC)  Commodities  have been a terrible investment over the past 10 years.  EM  &  International  Equities have only recently gone positive over the past 10 years.  VTI, the  broadest  of all the funds, has out performed over the past 10 years.",
            "title": "Cumulative Returns"
        },
        {
            "location": "/providers/yahoo/#smoothed-returns",
            "text": "On a day-to-day basis, market prices are extremely noisy and for the most part completely random. It is often useful to \nremove some of this noise by smoothing the returns, which in turn develops a more stable signal which can be consumed by \na systematic investment process. The Morpheus adapter supports smoothing returns based on an  Exponential Weighted Moving Average  \n(EWMA) which is a commonly used technique in signal processing.  The code below generates cumulative returns for the  S&P 500 Index  tracker with ticker symbol  SPY , \nand smooths the data with various half-lives, namely 0, 5, 10, 30 and 60 business days. A 0 day half-life is interpreted as \nno smoothing, so this simply yields the raw data. The smoothed signal is computed as follows:  $$ S_{t} = \\alpha x_{t} + (1 - \\alpha) S_{t-1} \\ for \\ t > 0 \\ and \\ where \\ S_{0} = x_{0} $$  There are various ways to compute \\( \\alpha \\), but in Morpheus we calculate it based on a half-life as follows:  $$ \\alpha = 1 - e^{log(0.5) / halfLife} $$   String ticker = \"SPY\";\nYahooReturnSource source = DataFrameSource.lookup(YahooReturnSource.class);\nArray<Integer> halfLives = Array.of(0, 5, 10, 30, 60);\nDataFrame<LocalDate,String> frame = DataFrame.combineFirst(halfLives.map(halfLife -> {\n    return source.read(options -> {\n        options.withStartDate(LocalDate.now().minusYears(5));\n        options.withEndDate(LocalDate.now());\n        options.withTickers(ticker);\n        options.withEmaHalfLife(halfLife.getInt());\n        options.cumulative();\n    }).cols().replaceKey(ticker, String.format(\"%s(%s)\", ticker, halfLife.getInt()));\n}));\n\nChart.create().withLinePlot(frame.applyDoubles(v -> v.getDouble() * 100d), chart -> {\n    chart.title().withText(String.format(\"%s EWMA Smoothed Returns With Various Half-Lives\", ticker));\n    chart.plot().axes().domain().label().withText(\"Date\");\n    chart.plot().axes().range(0).label().withText(\"Return\");\n    chart.plot().axes().range(0).format().withPattern(\"0.##'%';-0.##'%'\");\n    chart.legend().on().bottom();\n    chart.show();\n});  It's clear from the plot below that the larger the half-life the more smoothing is applied, which makes sense\nbased on the expression for half-life above.",
            "title": "Smoothed Returns"
        },
        {
            "location": "/providers/yahoo/#return-distribution",
            "text": "Many of the statistical techniques in modern finance are predicated on the assumption that asset returns are  normally \ndistributed . However, it is often the case that returns are not normal, which is why during the GFC many funds experienced \ngreater than -6 sigma events one day after the next. While returns are not normal, the statistical techniques that are commonly \nused are still a reasonable engineering approximation when markets are  not  under stress. When they are under stress, all bets \nare off (no pun intended), which can lead to bad outcomes for investors.  The code below computes daily returns for the S&P 500 ETF tracker with ticker symbol SPY over the past 20 years, and then generates \na histogram of these returns using 200 bins. In addition, the mean and standard deviation of these daily returns are calculated. \nThey are then used to compute a normal probability density function which is scaled appropriately and then overlaid on the chart.    String ticker = \"SPY\";\nYahooReturnSource source = DataFrameSource.lookup(YahooReturnSource.class);\nDataFrame<LocalDate,String> returns = source.read(options -> {\n    options.withStartDate(LocalDate.now().minusYears(20));\n    options.withEndDate(LocalDate.now());\n    options.withTickers(ticker);\n}).applyDoubles(v -> {\n    return v.getDouble() * 1d;\n});\n\ndouble binCount = 200;\ndouble min = returns.stats().min();\ndouble max = returns.stats().max();\ndouble mean = returns.stats().mean();\ndouble stdDev = returns.stats().stdDev();\ndouble bound = Math.max(Math.abs(min), Math.abs(max));\ndouble scale = ((max - min) / binCount) * returns.stats().count();\nDataFrame<Double,String> normDist = normal(\"NDIST\", -bound, bound, (int)binCount, mean, stdDev, scale);\n\nChart.create().withHistPlot(returns, (int)binCount, chart -> {\n    chart.title().withText(ticker + \": Daily Return Frequency Distribution\");\n    chart.subtitle().withText(ticker + \" Returns over past 20 years\");\n    chart.plot().<String>data().add(normDist);\n    chart.plot().style(\"NDIST\").withColor(Color.BLACK).withLineWidth(2f);\n    chart.plot().render(1).withLines(false, false);\n    chart.plot().axes().domain().label().withText(\"Return\");\n    chart.plot().axes().domain().format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.plot().axes().range(0).label().withText(\"Frequency\");\n    chart.show();\n});  A couple of features regarding the fit standout immediately, namely that the actual distribution has a  higher central peak  and\nthe  shoulders are a bit narrower . The standard deviation of SPY daily returns over the past 20 years comes out to be 1.239%, and\nthe plot suggests that the fit is a reasonable approximation between +/-3%, or say a little less than  +/-2.5 standard deviations \n(2.5 sigma) . The fitted distribution predicts that a -2.5 sigma event, or a one day drop in the S&P 500 of approximately -3.1% has \na probability of 0.5733%, which on the assumption of  252 trading days a year  is likely to happen once in 175 days or about 1.44 \ntimes per year. This is not inconsistent with the actual frequency distribution as shown in the second plot below, which is zoomed \ninto the left side of the distribution inorder to magnify this part of the plot.  \n       \n       Where the normal distribution is a wholly  inadequate model  is in the space beyond +/- 2.5 sigma. As mentioned earlier, many funds \nexperienced -6 sigma events or greater during the global financial crisis, day after day. The S&P 500 fitted distribution presented\nabove suggests that such an extreme event is likely to occur with a probability of 8.2934E-8% implying it  should only happen once\nin 4,784,799 years!   The plot below zooms into the left tail of the distribution demonstrating that extreme events occur far more\nfrequently than a normal distribution would suggest.  \n       In the above example we leverage a function called  normal()  to generate a scaled normal distribution that we can superimpose on \nthe plot to get a sense of how well the return histogram fits such a model. The code below simply generates a normal curve given \nthe mean, standard deviation of the daily returns, with an appropriate scale factor to fit the histogram.  $$ f(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}} $$   /**\n * Returns a single column DataFrame containing values generated from a normal distribution probability density function\n * @param label     the column key for the PDF series\n * @param lower     the lower bound for PDF\n * @param upper     the upper bound for PDF\n * @param count     the number of values to include\n * @param mean      the mean for the distribution\n * @param sigma     the standard deviation for the distribution\n * @return          the DataFrame of Normal PDF values\n */\n@SuppressWarnings(\"unchecked\")\n<C> DataFrame<Double,C> normal(C label, double lower, double upper, int count, double mean, double sigma, double scale) {\n    final double stepSize = (upper - lower) / (double)count;\n    final Range<Double> xValues = Range.of(lower, upper, stepSize);\n    return DataFrame.of(xValues, (Class<C>)label.getClass(), columns -> {\n        columns.add(label, xValues.map(x -> {\n            final double part1 = 1d / (sigma * Math.sqrt(2d * Math.PI));\n            final double part2 = Math.exp(-Math.pow(x - mean, 2d) / (2d * Math.pow(sigma, 2d)));\n            return part1 * part2;\n        }));\n    }).applyDoubles(v -> {\n        return v.getDouble() * scale;\n    });\n}",
            "title": "Return Distribution"
        },
        {
            "location": "/providers/yahoo/#return-distribution-smoothed",
            "text": "Consider the same setup as in the prior example, but in this case we look at the frequency distribution of daily returns after they \nhave been  exponentially smoothed  with a 20 day half-life. De-noising the returns does not yield a better fit, and the positive \nskew in the distribution becomes more apparent. In addition, while there is a positive skew, it is also clear that smoothing the\nreturns highlights far  more negative extreme events  than positive events.  \n        String ticker = \"SPY\";\nint halfLife = 20;\nYahooReturnSource source = DataFrameSource.lookup(YahooReturnSource.class);\nDataFrame<LocalDate,String> returns = source.read(options -> {\n    options.withStartDate(LocalDate.now().minusYears(10));\n    options.withEndDate(LocalDate.now());\n    options.withTickers(ticker);\n    options.withEmaHalfLife(halfLife);\n}).applyDoubles(v -> {\n    return v.getDouble() * 100d;\n});\n\ndouble binCount = 200;\ndouble min = returns.stats().min();\ndouble max = returns.stats().max();\ndouble mean = returns.stats().mean();\ndouble stdDev = returns.stats().stdDev();\ndouble bound = Math.max(Math.abs(min), Math.abs(max));\ndouble scale = ((max - min) / binCount) * returns.stats().count();\nDataFrame<Double,String> normDist = normal(\"NDIST\", -bound, bound, (int)binCount, mean, stdDev, scale);\n\nChart.create().withHistPlot(returns, (int)binCount, chart -> {\n    chart.title().withText(String.format(\"%s: Smoothed Daily Return Distribution (HL: %s)\", ticker, halfLife));\n    chart.plot().<String>data().add(normDist);\n    chart.plot().style(\"NDIST\").withColor(Color.BLACK).withLineWidth(2f);\n    chart.plot().render(1).withLines(false, false);\n    chart.plot().axes().domain().label().withText(\"Return\");\n    chart.plot().axes().domain().format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.plot().axes().range(0).label().withText(\"Frequency\");\n    chart.show();\n});",
            "title": "Return Distribution (Smoothed)"
        },
        {
            "location": "/providers/yahoo/#latest-quotes",
            "text": "Accessing a snapshot of the most recent data for one or more securities can be done via a single call using the  YahooQuoteLiveSource \nas shown below. In this example, we request the most recent data for several tickers, and we are presented with a  DataFrame  \ncontaining all available fields as columns. When accessing this data outside of market hours, fields such as the bid/ask price \nand size will often show NaN values. The resulting  DataFrame  is keyed by the security ticker along the row axis and  YahooField  \nalong the column axis allowing fast access to elements of the frame.    YahooQuoteLiveSource source = DataFrameSource.lookup(YahooQuoteLiveSource.class);\nDataFrame<String, YahooField> quotes = source.read(options -> {\n    options.withTickers(\"AAPL\", \"BLK\", \"NFLX\", \"ORCL\", \"GS\", \"C\", \"GOOGL\", \"MSFT\", \"AMZN\");\n});  \n Index  |  TICKER  |               NAME                |   PX_BID   |  PX_BID_SIZE  |   PX_ASK   |  PX_ASK_SIZE  |    PX_VOLUME    |  PX_CHANGE  |  PX_CHANGE_PERCENT  |  PX_LAST_DATE  |  PX_LAST_TIME  |  PX_LAST   |  PX_LAST_SIZE  |   PX_LOW   |  PX_HIGH   |  PX_PREVIOUS_CLOSE  |  PX_OPEN   |  EXCHANGE  |  AVG_DAILY_VOLUME  |  TRADE_DATE  |  DIVIDEND_PER_SHARE  |    EPS    |  EPS_ESTIMATE  |  EPS_NEXT_YEAR  |  EPS_NEXT_QUARTER  |   FLOAT_SHARES    |  FIFTY_TWO_WEEK_LOW  |  ANNUALISED_GAIN  |     MARKET_CAP      |       EBITDA       |  PRICE_SALES_RATIO  |  PRICE_BOOK_RATIO  |  EX_DIVIDEND_DATE  |  PRICE_EARNINGS_RATIO  |  DIVIDEND_PAY_DATE  |  PEG_RATIO  |  PRICE_EPS_RATIO_CURRENT_YEAR  |  PRICE_EPS_RATIO_NEXT_YEAR  |  SHORT_RATIO  |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  AAPL  |    AAPL  |                       Apple Inc.  |  160.9400  |    1500.0000  |  161.0000  |     800.0000  |  71714046.0000  |    -0.6400  |            -0.0040  |    2017-09-12  |      16:00:00  |  160.8600  |      100.0000  |  158.7700  |  163.9600  |           161.5000  |  162.6100  |       NMS  |     26754500.0000  |        null  |              2.5200  |   8.8100  |        9.0200  |        10.8900  |            3.8400  |  5031913000.0000  |            104.0800  |              NaN  |  830880000000.0000  |  70210000000.0000  |             3.7300  |            6.3000  |        2017-08-10  |               18.2600  |         2017-08-17  |     1.5900  |                       17.8300  |                    14.7700  |       1.9000  |\n   BLK  |     BLK  |                  BlackRock, Inc.  |       NaN  |          NaN  |       NaN  |          NaN  |    333885.0000  |     4.5300  |             0.0107  |    2017-09-12  |      16:02:00  |  428.5700  |    38358.0000  |  424.6900  |  428.7000  |           424.0400  |  426.1400  |       NYQ  |       513127.0000  |        null  |             10.0000  |  20.8300  |       21.8200  |        24.9100  |            5.8200  |   123755000.0000  |            336.8400  |              NaN  |   69520000000.0000  |   5060000000.0000  |             5.9700  |            2.3300  |        2017-08-31  |               20.5700  |         2017-09-22  |     1.3700  |                       19.6400  |                    17.2000  |       3.1800  |\n  NFLX  |    NFLX  |                    Netflix, Inc.  |  185.0600  |     500.0000  |  185.6700  |     200.0000  |   6689568.0000  |     3.4100  |             0.0188  |    2017-09-12  |      16:00:00  |  185.1500  |   199365.0000  |  180.6400  |  185.3300  |           181.7400  |  182.5500  |       NMS  |      7144270.0000  |        null  |                 NaN  |   0.8200  |        1.1900  |         2.0200  |            0.3100  |   424846000.0000  |             93.2600  |              NaN  |   79940000000.0000  |    706920000.0000  |             7.7000  |           25.2100  |              null  |              225.2400  |               null  |     1.9700  |                      155.5900  |                    91.6600  |       3.0700  |\n  ORCL  |    ORCL  |               Oracle Corporation  |       NaN  |          NaN  |       NaN  |          NaN  |  13352387.0000  |     0.2800  |             0.0053  |    2017-09-12  |      16:01:00  |   52.7700  |   839129.0000  |   52.4700  |   52.8900  |            52.4900  |   52.6400  |       NYQ  |     13043800.0000  |        null  |              0.7600  |   2.2100  |        2.9500  |         3.1900  |            0.6900  |  3009105000.0000  |             37.6200  |              NaN  |  218290000000.0000  |  14670000000.0000  |             5.7600  |            4.0300  |        2017-07-17  |               23.8800  |         2017-08-02  |     1.9400  |                       17.8900  |                    16.5400  |       1.8400  |\n    GS  |      GS  |  Goldman Sachs Group, Inc. (The)  |       NaN  |          NaN  |       NaN  |          NaN  |   3745841.0000  |     4.8900  |             0.0221  |    2017-09-12  |      16:00:00  |  225.9500  |   160193.0000  |  222.0200  |  227.6900  |           221.0600  |  222.5400  |       NYQ  |      3043750.0000  |        null  |              3.0000  |  19.0700  |       18.2300  |        19.9700  |            5.0400  |   350537000.0000  |            157.7700  |              NaN  |   87720000000.0000  |            0.0000  |             2.6600  |            1.1400  |        2017-08-29  |               11.8500  |         2017-09-28  |     1.0600  |                       12.3900  |                    11.3100  |       1.2700  |\n     C  |       C  |                  Citigroup, Inc.  |       NaN  |          NaN  |       NaN  |          NaN  |  15495439.0000  |     1.0800  |             0.0160  |    2017-09-12  |      16:00:00  |   68.7900  |  1090112.0000  |   68.1000  |   69.2500  |            67.7100  |   68.2200  |       NYQ  |     16707800.0000  |        null  |              0.8000  |   4.9900  |        5.2200  |         5.9600  |            1.2900  |  2721749000.0000  |             45.1600  |              NaN  |  187420000000.0000  |            0.0000  |             2.8700  |            0.8800  |        2017-08-03  |               13.7700  |         2017-08-25  |     1.2700  |                       13.1800  |                    11.5400  |       0.0200  |\n GOOGL  |   GOOGL  |                    Alphabet Inc.  |  946.0100  |     100.0000  |  947.2300  |     100.0000  |   1284787.0000  |     3.3600  |             0.0036  |    2017-09-12  |      16:00:00  |  946.6500  |   113174.0000  |  937.5000  |  948.0900  |           943.2900  |  946.9200  |       NMS  |      1773140.0000  |        null  |                 NaN  |  27.5900  |       30.5900  |        39.9900  |            9.6000  |   600581000.0000  |            743.5900  |              NaN  |  655910000000.0000  |  32250000000.0000  |             6.5800  |            4.4100  |              null  |               34.3100  |               null  |     1.6200  |                       30.9500  |                    23.6700  |       1.4500  |\n  MSFT  |    MSFT  |            Microsoft Corporation  |   74.6800  |     500.0000  |   74.9300  |     100.0000  |  14394850.0000  |    -0.0800  |            -0.0011  |    2017-09-12  |      16:00:00  |   74.6800  |  1682458.0000  |   74.3700  |   75.2400  |            74.7600  |   74.7600  |       NMS  |     22427700.0000  |        null  |              1.5600  |   2.7100  |        3.1700  |         3.5600  |            0.8300  |  7595028000.0000  |             55.9800  |              NaN  |  575200000000.0000  |  30430000000.0000  |             6.4000  |            7.9600  |        2017-08-15  |               27.5600  |         2017-09-14  |     2.3500  |                       23.5600  |                    20.9800  |       1.9500  |\n  AMZN  |    AMZN  |                 Amazon.com, Inc.  |  982.8700  |     100.0000  |  985.0000  |     200.0000  |   2481066.0000  |     4.6200  |             0.0047  |    2017-09-12  |      16:00:00  |  982.5800  |    99104.0000  |  975.5200  |  984.6700  |           977.9600  |  983.2700  |       NMS  |      3773170.0000  |        null  |                 NaN  |   3.9300  |        3.9900  |         8.1800  |            1.7700  |   400088000.0000  |            710.1000  |              NaN  |  472010000000.0000  |  12300000000.0000  |             3.1300  |           20.2200  |              null  |              249.8900  |               null  |     6.3700  |                      246.2600  |                   120.1200  |       1.2100  |  Data from this frame can be accessed in a  type safe  manner as follows:   String name = quotes.data().getValue(\"AAPL\", YahooField.NAME);\ndouble closePrice = quotes.data().getDouble(\"AAPL\", YahooField.PX_LAST);\nLocalDate date = quotes.data().getValue(\"AAPL\", YahooField.PX_LAST_DATE);  Usually not all these fields are desired so specific fields can be requested as follows:   YahooQuoteLiveSource source = DataFrameSource.lookup(YahooQuoteLiveSource.class);\nDataFrame<String, YahooField> quotes = source.read(options -> {\n    options.withTickers(\"AAPL\", \"BLK\", \"NFLX\", \"ORCL\", \"GS\", \"C\", \"GOOGL\", \"MSFT\", \"AMZN\");\n    options.withFields(\n        YahooField.PX_LAST,\n        YahooField.PX_BID,\n        YahooField.PX_ASK,\n        YahooField.PX_VOLUME,\n        YahooField.PX_CHANGE,\n        YahooField.PX_LAST_DATE,\n        YahooField.PX_LAST_TIME\n    );\n});  \n Index  |  PX_LAST   |   PX_BID   |   PX_ASK   |    PX_VOLUME    |  PX_CHANGE  |  PX_LAST_DATE  |  PX_LAST_TIME  |\n------------------------------------------------------------------------------------------------------------------\n  AAPL  |  160.8600  |  160.9400  |  161.0000  |  71714046.0000  |    -0.6400  |    2017-09-12  |      16:00:00  |\n   BLK  |  428.5700  |       NaN  |       NaN  |    333885.0000  |     4.5300  |    2017-09-12  |      16:02:00  |\n  NFLX  |  185.1500  |  185.0600  |  185.6700  |   6689568.0000  |     3.4100  |    2017-09-12  |      16:00:00  |\n  ORCL  |   52.7700  |       NaN  |       NaN  |  13352387.0000  |     0.2800  |    2017-09-12  |      16:01:00  |\n    GS  |  225.9500  |       NaN  |       NaN  |   3745841.0000  |     4.8900  |    2017-09-12  |      16:00:00  |\n     C  |   68.7900  |       NaN  |       NaN  |  15495439.0000  |     1.0800  |    2017-09-12  |      16:00:00  |\n GOOGL  |  946.6500  |  946.0100  |  947.2300  |   1284787.0000  |     3.3600  |    2017-09-12  |      16:00:00  |\n  MSFT  |   74.6800  |   74.6800  |   74.9300  |  14394850.0000  |    -0.0800  |    2017-09-12  |      16:00:00  |\n  AMZN  |  982.5800  |  982.8700  |  985.0000  |   2481066.0000  |     4.6200  |    2017-09-12  |      16:00:00  |",
            "title": "Latest Quotes"
        },
        {
            "location": "/providers/yahoo/#option-quotes",
            "text": "Market data for listed  Options  on equities and ETFs is also accessible from \nYahoo Finance (for example, listed options on Apple can be accessed  here ). \nIt is not clear that Yahoo provides a CSV API for this data, so the Morpheus Adapter screen scrapes the relevant information from \nthe HTML page, which obviously makes it somewhat sensitive to changes in the page style.  The code below demonstrates how to query for the  expiry dates  on options given the underlying security symbol. It then\nselects the next upcoming expiry date, and queries for all options, including  calls and puts  for this expiry.   String ticker = \"SPY\";\nYahooFinance yahoo = new YahooFinance();\nSet<LocalDate> expiryDates = yahoo.getOptionExpiryDates(ticker);\nLocalDate nextExpiry = expiryDates.iterator().next();\nDataFrame<String,YahooField> optionQuotes = yahoo.getOptionQuotes(ticker, nextExpiry);  Calls and puts  can easily be separated by filtering the Morpheus  DataFrame  in the usual fashion as shown below.   //Select rows representing CALL options\nDataFrame<String,YahooField> calls = optionQuotes.rows().select(row -> {\n    final String type = row.getValue(YahooField.OPTION_TYPE);\n    return type.equalsIgnoreCase(\"Call\");\n});  \n       Index         |  TICKER  |  OPTION_TYPE  |  EXPIRY_DATE  |  PX_STRIKE  |  PX_LAST   |  PX_CHANGE  |  PX_CHANGE_PERCENT  |   PX_BID   |   PX_ASK   |  PX_VOLUME   |  OPEN_INTEREST  |  IMPLIED_VOLATILITY  |\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n SPY170915C00050000  |     SPY  |         CALL  |   2017-09-15  |    50.0000  |  187.5000  |     0.0000  |                NaN  |  186.0900  |  189.2100  |      1.0000  |         0.0000  |              0.0000  |\n SPY170915C00055000  |     SPY  |         CALL  |   2017-09-15  |    55.0000  |  195.0100  |     3.0300  |             0.0158  |  193.1200  |  197.1800  |     56.0000  |       686.0000  |             10.7500  |\n SPY170915C00060000  |     SPY  |         CALL  |   2017-09-15  |    60.0000  |  190.0900  |     3.1200  |             0.0167  |  188.1200  |  192.1300  |     38.0000  |       617.0000  |              9.7188  |\n SPY170915C00065000  |     SPY  |         CALL  |   2017-09-15  |    65.0000  |  185.1500  |     5.5800  |             0.0311  |  183.1200  |  187.2600  |     52.0000  |       390.0000  |             10.0625  |\n SPY170915C00070000  |     SPY  |         CALL  |   2017-09-15  |    70.0000  |  180.1100  |     2.2700  |             0.0128  |  178.0000  |  182.2000  |     67.0000  |       142.0000  |              8.0625  |\n SPY170915C00075000  |     SPY  |         CALL  |   2017-09-15  |    75.0000  |  175.0800  |     6.3200  |             0.0374  |  173.0000  |  177.0300  |     18.0000  |       138.0000  |             13.9258  |\n SPY170915C00080000  |     SPY  |         CALL  |   2017-09-15  |    80.0000  |  170.0800  |     6.1700  |             0.0376  |  168.0000  |  172.2500  |     25.0000  |        59.0000  |              7.8125  |\n SPY170915C00085000  |     SPY  |         CALL  |   2017-09-15  |    85.0000  |  165.1500  |     6.0600  |             0.0381  |  163.0000  |  167.0900  |     13.0000  |       182.0000  |             12.6523  |\n SPY170915C00090000  |     SPY  |         CALL  |   2017-09-15  |    90.0000  |  160.0800  |     8.3800  |             0.0552  |  158.0000  |  162.2200  |      8.0000  |       180.0000  |              6.7500  |\n SPY170915C00095000  |     SPY  |         CALL  |   2017-09-15  |    95.0000  |  155.0800  |     6.1700  |             0.0414  |  153.0000  |  157.2000  |      4.0000  |        55.0000  |              6.1875  |   //Select rows representing PUT options\nDataFrame<String,YahooField> puts = optionQuotes.rows().select(row -> {\n    final String type = row.getValue(YahooField.OPTION_TYPE);\n    return type.equalsIgnoreCase(\"Put\");\n});  \n       Index         |  TICKER  |  OPTION_TYPE  |  EXPIRY_DATE  |  PX_STRIKE  |  PX_LAST  |  PX_CHANGE  |  PX_CHANGE_PERCENT  |  PX_BID  |  PX_ASK  |  PX_VOLUME   |  OPEN_INTEREST  |  IMPLIED_VOLATILITY  |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n SPY170915P00055000  |     SPY  |          PUT  |   2017-09-15  |    55.0000  |   0.0200  |     0.0000  |                NaN  |  0.0000  |  0.0100  |     10.0000  |      2250.0000  |              8.5000  |\n SPY170915P00060000  |     SPY  |          PUT  |   2017-09-15  |    60.0000  |   0.0200  |     0.0000  |                NaN  |  0.0000  |  0.0100  |     50.0000  |      1912.0000  |              8.0000  |\n SPY170915P00065000  |     SPY  |          PUT  |   2017-09-15  |    65.0000  |   0.0100  |     0.0000  |                NaN  |  0.0000  |  0.0100  |     10.0000  |      4117.0000  |              7.7500  |\n SPY170915P00070000  |     SPY  |          PUT  |   2017-09-15  |    70.0000  |   0.0100  |     0.0000  |                NaN  |  0.0000  |  0.0100  |      5.0000  |      8582.0000  |              7.2500  |\n SPY170915P00075000  |     SPY  |          PUT  |   2017-09-15  |    75.0000  |   0.0100  |     0.0000  |                NaN  |  0.0000  |  0.0100  |      1.0000  |      5598.0000  |              6.8750  |\n SPY170915P00080000  |     SPY  |          PUT  |   2017-09-15  |    80.0000  |   0.0100  |     0.0000  |                NaN  |  0.0000  |  0.0100  |    150.0000  |      3061.0000  |              6.5000  |\n SPY170915P00085000  |     SPY  |          PUT  |   2017-09-15  |    85.0000  |   0.0100  |     0.0000  |                NaN  |  0.0000  |  0.0100  |     14.0000  |      5525.0000  |              6.1250  |\n SPY170915P00090000  |     SPY  |          PUT  |   2017-09-15  |    90.0000  |   0.0100  |     0.0000  |                NaN  |  0.0000  |  0.0100  |      2.0000  |      5638.0000  |              5.8750  |\n SPY170915P00095000  |     SPY  |          PUT  |   2017-09-15  |    95.0000  |   0.0100  |     0.0000  |                NaN  |  0.0000  |  0.0100  |     50.0000  |     22290.0000  |              5.5000  |\n SPY170915P00100000  |     SPY  |          PUT  |   2017-09-15  |   100.0000  |   0.0100  |     0.0000  |                NaN  |  0.0000  |  0.0100  |     99.0000  |     19382.0000  |              5.2500  |",
            "title": "Option Quotes"
        },
        {
            "location": "/providers/yahoo/#volatility-smile",
            "text": "One of the most influential inputs to the famous  Black-Scholes-Merton  \noption pricing model concerns the future volatility of the returns on the underlying asset. In fact, volatility is so fundamental to \noption pricing that certain types of option contracts are quoted in volatility units rather than price. Black-Scholes assumes that \nthe volatility of the underlying returns are constant over the term in question, and it also assumes that other characteristics of \nthe option such as the moneyness (how far in or out-of-the-money the option might be), do not influence estimates of future volatility.  In reality, this does not appear to be the case based on observed market prices. A commonly used visualization in option trading is \ncalled the  Option Volatility Smile , which plots  implied volatilities  of options with different  strike prices  for the same \nexpiry date. According to Black-Scholes, these curves should be flat, but in reality they form more of a curve in the shape of a \nperson smiling. That is, implied volatilities for  at-the-money  options tend to be lower than for deep  in-the-money  or deep  out-the-money  options.  The plot below illustrates the volatility smiles for options on the S&P 500 ETF with ticker symbol SPY which has been generated\nusing the Morpheus Yahoo Finance adapter. While various  outliers  appear to exist, the smile pattern is pretty clear. These outliers \nare likely to be explained by low volume at the strike prices in question, so the calculated implied volatilities are stale at those\npoints. The other possibility is a mis-pricing of some options at these strikes, but that is fairly unlikely as such differences would \nbe quickly arbitraged away. The code to generate this plot is also shown below.  \n        String ticker = \"SPY\";\n//Instantiate Yahoo convenience adapter\nYahooFinance yahoo = new YahooFinance();\n//Select last price for underlying\ndouble lastPrice = yahoo.getLiveQuotes(Array.of(ticker)).data().getDouble(0, YahooField.PX_LAST);\n//Select call options with strike price within 10% of current market price and non zero vol\nDataFrame<String,YahooField> options = yahoo.getOptionQuotes(ticker).rows().select(row -> {\n    final String type = row.getValue(YahooField.OPTION_TYPE);\n    if (!type.equalsIgnoreCase(\"CALL\")) {\n        return false;\n    } else {\n        final double strike = row.getDouble(YahooField.PX_STRIKE);\n        final double impliedVol = row.getDouble(YahooField.IMPLIED_VOLATILITY);\n        return impliedVol > 0 && strike > lastPrice * 0.9d && strike < lastPrice * 1.1d;\n    }\n});\n\n//Select all distinct expiry dates for quotes\nArray<LocalDate> expiryDates = options.colAt(YahooField.EXPIRY_DATE).distinct();\n//Creates frames for each expiry including only strike price and implied-vol columns\nArray<DataFrame<Integer,String>> frames = expiryDates.map(v -> {\n    final LocalDate expiry = v.getValue();\n    final DataFrame<String,YahooField> calls = options.rows().select(row -> {\n        final LocalDate date = row.getValue(YahooField.EXPIRY_DATE);\n        return expiry.equals(date);\n    });\n    return DataFrame.of(Range.of(0, calls.rowCount()), String.class, columns -> {\n        columns.add(\"Strike\", calls.colAt(YahooField.PX_STRIKE).toArray());\n        columns.add(expiry.toString(), calls.colAt(YahooField.IMPLIED_VOLATILITY).toArray().applyDoubles(x -> {\n            return x.getDouble() * 100d;\n        }));\n    });\n});\n\n//Create plot of N frames each for a different expiry\nChart.create().asSwing().withLinePlot(frames.getValue(0), \"Strike\", chart -> {\n    for (int i=1; i<frames.length(); ++i) {\n        DataFrame<Integer,String> data = frames.getValue(i);\n        chart.plot().<String>data().add(data, \"Strike\");\n        chart.plot().render(i).withLines(true, false);\n    }\n    chart.plot().render(0).withLines(true, false);\n    chart.plot().axes().domain().label().withText(\"Strike Price\");\n    chart.plot().axes().range(0).label().withText(\"Implied Volatility\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.title().withText(\"SPY Call Option Implied Volatility Smiles\");\n    chart.legend().on().right();\n    chart.show();\n});  We can use a slightly modified version of this code to run the analysis for all  PUT options , the results of \nwhich are plotted below. The same smile pattern is evident, as are the existence of outliers, which is most likely due \nto the lack of transactions at the strikes in question. Another take-away from these plots is that deep  in-the-money \noptions are more expensive from an implied volatility perspective than deep  out-the-money  options as the smile\ndoes not appear to be symmetric in nature.",
            "title": "Volatility Smile"
        },
        {
            "location": "/providers/yahoo/#key-statistics",
            "text": "Various  financial, trading  and  valuation statistics  are made available by Yahoo Finance on the  Statistics \npage for each applicable security. The Morpheus adapter provides a programmatic mechanism to extract this data into\na  DataFrame  which allows for easy cross sectional comparisons between companies. Only current values are available\nvia this interface, historical values are not supported. The code below demonstrates how to extract these statistics\nfor various companies, and then transposes and prints the  DataFrame  to standard out.   YahooFinance yahoo = new YahooFinance();\nArray<String> tickers = Array.of(\"AAPL\", \"ORCL\", \"BLK\", \"GS\", \"NFLX\", \"AMZN\", \"FB\");\nDataFrame<String,YahooField> stats = yahoo.getStatistics(tickers);\nstats.transpose().out().print(200, formats -> {\n    final SmartFormat smartFormat = new SmartFormat();\n    formats.setPrinter(Object.class, Printer.forObject(smartFormat::format));\n});  \n           Index            |     BLK      |     NFLX     |     AAPL     |     ORCL     |      GS      |      FB      |     AMZN     |\n--------------------------------------------------------------------------------------------------------------------------------------\n                MARKET_CAP  |    69.4700B  |    78.7300B  |   825.8200B  |   201.6200B  |    87.4300B  |   498.4800B  |   474.0300B  |\n               PE_TRAILING  |     20.5800  |    221.8400  |     18.1500  |     22.0500  |     11.8100  |     38.4200  |    250.9600  |\n                PE_FORWARD  |      1.4000  |      1.9800  |      1.4500  |      1.9800  |      1.0900  |      1.2100  |      6.6200  |\n         PRICE_SALES_RATIO  |      6.0300  |      7.7300  |      3.6900  |      5.3400  |      2.7100  |     15.0300  |      3.1600  |\n          PRICE_BOOK_RATIO  |      2.3500  |     25.2900  |      6.2400  |      3.7400  |      1.1600  |      7.4900  |     20.4000  |\n           FISCAL_YEAR_END  |  2016-12-31  |  2016-12-31  |  2016-09-24  |  2017-05-31  |  2016-12-31  |  2016-12-31  |  2016-12-31  |\n       MOST_RECENT_QUARTER  |  2017-06-30  |  2017-06-30  |  2017-07-01  |  2017-05-31  |  2017-06-30  |  2017-06-30  |  2017-06-30  |\n             PROFIT_MARGIN  |      0.2992  |      0.0355  |      0.2087  |      0.2474  |      0.2644  |      0.3966  |      0.0128  |\n          OPERATING_MARGIN  |      0.4205  |      0.0633  |      0.2684  |      0.3519  |      0.3618  |      0.4666  |      0.0231  |\n          RETURN_ON_ASSETS  |      0.0138  |      0.0287  |      0.1152  |      0.0671  |      0.0095  |      0.1493  |      0.0283  |\n          RETURN_ON_EQUITY  |      0.1174  |      0.1310  |      0.3603  |      0.1830  |      0.0980  |      0.2251  |      0.0967  |\n               REVENUE_TTM  |    11.5200B  |    10.1900B  |   223.5100B  |    37.7300B  |    32.2500B  |    33.1700B  |   150.1200B  |\n         REVENUE_PER_SHARE  |     70.5300  |     23.6900  |     42.4000  |      9.1700  |     77.9100  |     11.5000  |    314.7300  |\n       REVENUE_GROWTH_QTLY  |      0.0570  |      0.3230  |      0.0720  |      0.0280  |     -0.0060  |      0.4480  |      0.2480  |\n              GROSS_PROFIT  |    10.9500B  |     2.8000B  |    84.2600B  |    30.2600B  |    30.6100B  |    23.8500B  |    47.7200B  |\n                EBITDA_TTM  |     5.0600B  |   706.9200M  |    70.2100B  |    14.6700B  |         NaN  |    18.0800B  |    12.3000B  |\n               EPS_DILUTED  |     20.8300  |      0.8200  |      8.8100  |      2.2100  |     19.0700  |      4.4700  |      3.9300  |\n      EARNINGS_GRWOTH_QTLY  |      0.0860  |      0.6100  |      0.1180  |      0.1490  |      0.0050  |      0.7060  |     -0.7700  |\n                      BETA  |      1.6800  |      0.6300  |      1.4300  |      1.1400  |      1.4400  |      0.5400  |      1.3800  |\n                  CASH_MRQ  |     6.2500B  |     2.1600B  |    77.0100B  |    66.0800B  |   732.4800B  |    35.4500B  |    21.4500B  |\n            CASH_PER_SHARE  |     38.5300  |      5.0100  |     14.9100  |     15.9700  |     1.8868K  |     12.2100  |     44.6500  |\n                  DEBT_MRQ  |     4.9700B  |     4.8400B  |   108.6000B  |    57.9100B  |   404.4600B  |         NaN  |    23.6200B  |\n      DEBT_OVER_EQUITY_MRQ  |     16.6300  |    155.3900  |     82.0100  |    106.7500  |    463.8500  |         NaN  |    101.7500  |\n             CURRENT_RATIO  |      1.2300  |      1.3100  |      1.3900  |      3.0800  |      1.8000  |     12.3100  |      1.0100  |\n      BOOK_VALUE_PER_SHARE  |    182.2100  |      7.2100  |     25.6100  |     13.0200  |    194.4100  |     22.9200  |     48.3600  |\n       OPERATING_CASH_FLOW  |     3.3900B  |    -1.9000B  |    64.0700B  |    14.1300B  |   -15.8300B  |    19.3800B  |    17.0600B  |\n    LEVERED_FREE_CASH_FLOW  |     3.4500B  |     5.8300B  |    40.6200B  |     9.6300B  |         NaN  |    10.2200B  |    11.1500B  |\n                ADV_3MONTH  |   505.7900K  |     6.9300M  |    26.7900M  |    13.1800M  |     3.0200M  |    16.4700M  |     3.6400M  |\n                 ADV_10DAY  |   435.9500K  |     5.6800M  |    34.1000M  |    14.5700M  |     3.3800M  |    13.0400M  |     2.7500M  |\n        SHARES_OUTSTANDING  |   160.9800M  |   431.7500M  |     5.1700B  |     4.1400B  |   388.2100M  |     2.3700B  |   480.3800M  |\n              SHARES_FLOAT  |   123.7600M  |   424.8500M  |     5.0300B  |     3.0100B  |   350.5400M  |     2.3400B  |   400.0900M  |\n     OWNER_PERCENT_INSIDER  |      0.0361  |      0.0182  |      0.0008  |      0.2725  |      0.0179  |      0.0175  |      0.1677  |\n OWNER_PERCENT_INSTITUTION  |      0.8775  |      0.8284  |      0.6247  |      0.5984  |      0.7931  |      0.7244  |      0.6234  |\n              SHARES_SHORT  |     2.0600M  |    27.8500M  |    40.3100M  |    31.8300M  |     4.5400M  |    20.2500M  |     4.7200M  |\n        SHARES_SHORT_RATIO  |      3.1800  |      3.0700  |      1.9000  |      1.8400  |      1.2700  |      0.9700  |      1.2100  |\n        SHARES_SHORT_PRIOR  |     2.3100M  |    25.7400M  |    39.1500M  |    35.7800M  |     4.3500M  |    21.8400M  |     5.0600M  |\n              DIVIDEND_FWD  |     10.0000  |         NaN  |      2.5200  |      0.7600  |      3.0000  |         NaN  |         NaN  |\n        DIVIDEND_FWD_YIELD  |      0.0235  |         NaN  |      0.0158  |      0.0144  |      0.0139  |         NaN  |         NaN  |\n         DIVIDEND_TRAILING  |      9.5800  |         NaN  |      2.3400  |      0.6400  |      2.7000  |         NaN  |         NaN  |\n   DIVIDEND_TRAILING_YIELD  |      0.0225  |         NaN  |      0.0148  |      0.0121  |      0.0119  |         NaN  |         NaN  |\n     DIVIDEND_PAYOUT_RATIO  |      0.4599  |         NaN  |      0.2650  |      0.2896  |      0.1421  |         NaN  |         NaN  |\n         DIVIDEND_PAY_DATE  |  2017-09-22  |        null  |  2017-08-17  |  2017-08-02  |  2017-09-28  |        null  |        null  |\n          DIVIDEND_EX_DATE  |  2017-08-31  |        null  |  2017-08-10  |  2017-07-17  |  2017-05-30  |        null  |        null  |\n           LAST_SPLIT_DATE  |  2007-06-05  |  2015-07-15  |  2014-06-09  |  2000-10-13  |        null  |        null  |  1999-09-02  |  The code below demonstrates how to generate a plot of a small subset of the profitability and return metrics for a number\nof prominent financial services and technology firms.   YahooFinance yahoo = new YahooFinance();\nArray<String> tickers = Array.of(\"BLK\", \"GS\", \"MS\", \"JPM\", \"C\", \"BAC\", \"AAPL\", \"NVDA\", \"GOOGL\");\nDataFrame<String,YahooField> data = yahoo.getStatistics(tickers);\nDataFrame<String,YahooField> stats = data.cols().select(\n    YahooField.PROFIT_MARGIN,\n    YahooField.OPERATING_MARGIN,\n    YahooField.RETURN_ON_ASSETS,\n    YahooField.RETURN_ON_EQUITY\n).applyDoubles(v -> {\n    return v.getDouble() * 100d;\n});\n\nChart.create().withBarPlot(stats.transpose(), false, chart -> {\n    chart.title().withText(\"Profitability & Return Metrics\");\n    chart.plot().axes().domain().label().withText(\"Statistic\");\n    chart.plot().axes().range(0).label().withText(\"Value\");\n    chart.plot().axes().range(0).format().withPattern(\"0.00'%';-0.00'%'\");\n    chart.legend().right().on();\n    chart.show();\n});",
            "title": "Key Statistics"
        },
        {
            "location": "/providers/yahoo/#portfolio-analysis",
            "text": "There are several examples of how to use the Yahoo Finance adapter to analyze and construct portfolios of risky assets\nin the section on  Modern Portfolio Theory .",
            "title": "Portfolio Analysis"
        },
        {
            "location": "/providers/world-bank/",
            "text": "World Bank\n\n\nTo be completed...",
            "title": "World Bank"
        },
        {
            "location": "/providers/world-bank/#world-bank",
            "text": "To be completed...",
            "title": "World Bank"
        }
    ]
}