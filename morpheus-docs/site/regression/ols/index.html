<!--
  ~ Copyright (C) 2014-2018 D3X Systems - All Rights Reserved
  ~
  ~ Licensed under the Apache License, Version 2.0 (the "License");
  ~ you may not use this file except in compliance with the License.
  ~ You may obtain a copy of the License at
  ~
  ~ http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->

<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <meta name="author" content="Xavier Witdouck">

        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>Ordinary Least Squares - Morpheus</title>
        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.5.0.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <link href="../.././css/morpheus.css" rel="stylesheet">
        <link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/styles/default.min.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

	<script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script>
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="../..">Morpheus</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                    <li >
                        <a href="../..">Overview</a>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Array <b class="caret"></b></a>
                        <ul class="dropdown-menu">

<li >
    <a href="../../array/overview/">Overview</a>
</li>

<li >
    <a href="../../array/performance/">Performance</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">DataFrame <b class="caret"></b></a>
                        <ul class="dropdown-menu">

<li >
    <a href="../../frame/construction/">Construction</a>
</li>

<li >
    <a href="../../frame/access/">Accessing</a>
</li>

<li >
    <a href="../../frame/reshaping/">Reshaping</a>
</li>

<li >
    <a href="../../frame/filtering/">Filtering</a>
</li>

<li >
    <a href="../../frame/sorting/">Sorting</a>
</li>

<li >
    <a href="../../frame/grouping/">Grouping</a>
</li>

<li >
    <a href="../../frame/finding/">Finding</a>
</li>

<li >
    <a href="../../frame/writing/">Writing</a>
</li>

<li >
    <a href="../../frame/performance/">Performance</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Analysis <b class="caret"></b></a>
                        <ul class="dropdown-menu">

<li >
    <a href="../../analysis/statistics/">Descriptive Statistics</a>
</li>

  <li class="dropdown-submenu">
    <a href="#">Linear Regression</a>
    <ul class="dropdown-menu">

<li class="active">
    <a href="./">Ordinary Least Squares</a>
</li>

<li >
    <a href="../wls/">Weighted Least Squares</a>
</li>

<li >
    <a href="../gls/">Generalized Least Squares</a>
</li>
    </ul>
  </li>

<li >
    <a href="../../analysis/pca/">Principal Component Analysis</a>
</li>

<li >
    <a href="../../analysis/linearalgebra/">Linear Algebra</a>
</li>

<li >
    <a href="../../analysis/timeseries/">Time Series Analysis</a>
</li>

  <li class="dropdown-submenu">
    <a href="#">Examples</a>
    <ul class="dropdown-menu">

<li >
    <a href="../../examples/mpt/">Modern Portfolio Theory</a>
</li>
    </ul>
  </li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Visualization <b class="caret"></b></a>
                        <ul class="dropdown-menu">

  <li class="dropdown-submenu">
    <a href="#">Charts</a>
    <ul class="dropdown-menu">

<li >
    <a href="../../viz/charts/overview/">Overview</a>
</li>

<li >
    <a href="../../viz/charts/embed/">Embedding (HTML)</a>
</li>

<li >
    <a href="../../viz/charts/gallery1/">Gallery (Google)</a>
</li>

<li >
    <a href="../../viz/charts/gallery2/">Gallery (JFree)</a>
</li>
    </ul>
  </li>

<li >
    <a href="../../viz/tables/overview/">Tables</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Data Sources <b class="caret"></b></a>
                        <ul class="dropdown-menu">

<li >
    <a href="../../providers/quandl/">Quandl</a>
</li>

<li >
    <a href="../../providers/fred/">Federal Reserve</a>
</li>

<li >
    <a href="../../providers/google/">Google Finance</a>
</li>

<li >
    <a href="../../providers/yahoo/">Yahoo Finance</a>
</li>

<li >
    <a href="../../providers/world-bank/">World Bank</a>
</li>
                        </ul>
                    </li>
                </ul>

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                    <li >
                        <a rel="next" href="../../analysis/statistics/">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../wls/">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/zavtech/morpheus-core">
                                <i class="fa fa-github"></i>GitHub
                        </a>
                    </li>
            </ul>
        </div>
    </div>
</div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#ordinary-least-squares-ols">Ordinary Least Squares (OLS)</a></li>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#theory">Theory</a></li>
            <li><a href="#solving">Solving</a></li>
            <li><a href="#diagnostics">Diagnostics</a></li>
            <li><a href="#gauss-markov-assumptions">Gauss Markov Assumptions</a></li>
            <li><a href="#example">Example</a></li>
            <li><a href="#unbiasedness">Unbiasedness</a></li>
            <li><a href="#consistency">Consistency</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h2 id="ordinary-least-squares-ols">Ordinary Least Squares (OLS)</h2>
<h3 id="introduction">Introduction</h3>
<p><a href="https://en.wikipedia.org/wiki/Regression_analysis">Regression analysis</a> is a statistical technique used to fit a model expressed in
terms of one or more variables to some data. In particular, it allows one to analyze the relationship of a dependent variable (also
referred to as the regressand) on one or more independent or predictor variables (also referred to as regressors), and assess how
influential each of these are.  </p>
<p>There are many types of regression analysis techniques, however one of the most commonly used is based on fitting data to a linear model,
and in particular using an approach called <a href="https://en.wikipedia.org/wiki/Least_squares">Least Squares</a>. The Morpheus API currently supports
3 variations of Linear Least Squares regression, namely <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">Ordinary Least Squares</a> (OLS),
<a href="https://en.wikipedia.org/wiki/Least_squares#Weighted_least_squares">Weighted Least Squares</a> (WLS) and <a href="https://en.wikipedia.org/wiki/Generalized_least_squares">Generalized Least Squares</a>
(GLS). The following section reviews some OLS regression theory, and provides an example of how to use the Morhpeus API to apply this technique.</p>
<h3 id="theory">Theory</h3>
<p>A linear regression model in matrix form can be expressed as:</p>
<p>$$ Y = X \beta + \epsilon \ \ \ \ where \ E[\epsilon]=0 \ \ \ \ Var[\epsilon] = \sigma^2 I  $$</p>
<p>Y represents an <code>nx1</code> vector of regressands, and X represents an <code>nxp</code> design matrix, where <code>n</code> is the number of observations in the data,
and <code>p</code> represents the number of parameters to estimate. If the model includes an intercept term, the first column in the design matrix is
populated with 1's and therefore the first entry in the <code>px1</code> \(\beta\) vector would represent the intercept value. The <code>nx1</code> \(\epsilon\)
vector represents the error or disturbance term, which is assumed to have a conditional mean of 0 and also to be free of any serial correlation
(see section below on the <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">Gauss Markov</a> assumptions).</p>
<p>A least squares regression model estimates \(\beta\) so as to minimize the sum of the squared error, or \(\epsilon^T\epsilon\). We
can use the model equation above to express \(\epsilon^T\epsilon\) in terms of Y and X\(\beta\), differentiate this by \(\beta\),
set the result to zero since we wish to minimize the squared error, and solve for \(\beta\) as follows:  </p>
<p>$$ \begin{align}
\epsilon^T\epsilon &amp;= (Y - X \beta)^T(Y - X \beta) \\
 &amp;= Y^TY - \beta^TX^TY - Y^TX\beta + \beta^T X^T X \beta \\
 &amp;= Y^TY - 2\beta^T X^T Y + \beta^T X^T X \beta
\end{align} $$</p>
<p>We now differentiate this expression with respect to \(\beta\) and set it to zero which yields the following:</p>
<p>$$ \frac{d\epsilon^T\epsilon}{d\beta} = -2X^T Y + 2X^T X \beta = 0 $$</p>
<p>This expression can be re-arranged to solve for \(\beta\) and yields the following equation: </p>
<p>$$ \beta = (X^T X)^{-1} X^T Y $$ </p>
<p>The value of \(\beta\) can only be <strong>estimated</strong> given some sample data drawn from a population or data generating process, the <em>true</em> value is
unknown. We usually refer to the estimate as \(\hat{\beta}\) (beta "hat") in order to distinguish it from the true population value. In addition,
when expressing the model in terms of \(\hat{\beta}\), the stochastic term is referred to as <em>residuals</em> rather than <em>errors</em>, and while they
are conceptually related, they are <a href="https://en.wikipedia.org/wiki/Errors_and_residuals">not the same thing</a>. Errors represent the deviation of
observations from the unknown population line, while residuals represent the deviations from the estimation line, a subtle difference and easily
confused. In terms of \(\hat{\beta}\) and residuals <strong>u</strong>, the model is expressed in the usual way. </p>
<p>$$ Y = X \hat{\beta} + u \ \ \ \ where \ E[u]=0 \ \ \ \ Var[u] = \sigma^2 I  $$</p>
<p>The residuals are of course still assumed to be consistent with the <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">Gauss Markov</a>
assumptions, and so the estimator for \(\hat{\beta}\) is: </p>
<p>$$ \hat{\beta} = (X^T X)^{-1} X^T Y $$ </p>
<h3 id="solving">Solving</h3>
<p>We can solve for \(\hat{\beta}\) directly by calculating the right hand side of the above equation, which is what the Morpheus library does
by default. There are situations in which this can present numerical stability issues, in which case it may be preferable to solve for \(\hat{\beta}\)
by factorizing the design matrix X using a <a href="https://en.wikipedia.org/wiki/QR_decomposition">QR decomposition</a> where Q represents an orthogonal matrix
such that \(Q^T Q = I\) and thus \(Q^T = Q^{-1}\), and where R is an upper triangular matrix. The QR decomposition approach is based on the
following reasoning.</p>
<p>$$ \begin{align}
X^T Y &amp; = X^T X \hat{\beta}  \\
\big( QR \big)^T Y &amp; = \big( QR \big)^T \big( QR \big) \hat{\beta} \\
R^T Q^T Y &amp; = R^T Q^T Q R \hat{\beta} \\
R^T Q^T Y &amp; = R^T R \hat{\beta} \\
(R^T)^{-1} R^T Q^T Y &amp; = \big( R^T \big)^{-1} R^T R \hat{\beta} \\
Q^T Y &amp; = R \hat{\beta}
\end{align} $$</p>
<p>This can be solved efficiently by <a href="https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution">backward substitution</a> because the R
matrix is upper <a href="https://en.wikipedia.org/wiki/Triangular_matrix">triangular</a>, and therefore no inversion of the design matrix is required. The Morpheus
library also supports estimating \(\hat{\beta}\) using this technique, which can be configured on a case by case basis via the <code>withSolver()</code> method
on the <code>DataFrameLeastSquares</code> interface. </p>
<h3 id="diagnostics">Diagnostics</h3>
<p>Just like any other statistical technique, regression analysis is susceptible to <a href="https://en.wikipedia.org/wiki/Sampling_error">sampling error</a>, and
it is therefore common to compute the variance of the parameter estimates, as well as their <a href="https://en.wikipedia.org/wiki/Standard_error">standard error</a>,
which can then be used for <a href="https://en.wikipedia.org/wiki/Statistical_inference">inferential</a> purposes. In the context of a linear regression, the
<a href="https://en.wikipedia.org/wiki/Null_hypothesis">null hypotheses</a> \(H_{0}\), is generally that the model parameters are zero. The parameter standard
errors can be used to calculate a <a href="https://en.wikipedia.org/wiki/T-statistic">t-statistic</a> and corresponding <a href="https://en.wikipedia.org/wiki/P-value">p-value</a>
in order to decide if \(H_{0}\) can be rejected in favour of the alternative hypothesis, and thus assert that the estimates are statistically
significantly different from zero.</p>
<p>The variance of \(\hat{\beta}\) with respect to the true population value can be expressed as follows:</p>
<p>$$ Var(\hat{\beta}) = E[(\hat{\beta} - \beta)(\hat{\beta} - \beta)^T] $$</p>
<p>We substitute our population model \( Y = X \beta + \epsilon \) in our sample estimator \( \hat{\beta} = (X^T X)^{-1} X^T Y \) as follows:  </p>
<p>$$ \begin{align}
\hat{\beta} &amp;= (X^T X)^{-1} X^T ( X \beta + \epsilon ) \\
\hat{\beta} &amp;= (X^T X)^{-1} X^T X \beta + (X^T X)^{-1} X^T \epsilon  \\
\hat{\beta} &amp;= \beta + (X^T X)^{-1} X^T \epsilon  \\
\hat{\beta} - \beta &amp;= (X^T X)^{-1} X^T \epsilon  \\
\end{align} $$</p>
<p>With the above expression for \( \hat{\beta} - \beta \) we can now solve for the variance of the OLS estimator as follows: </p>
<p>$$ \begin{align}
Var(\hat{\beta}) &amp;= E[(\hat{\beta} - \beta)(\hat{\beta} - \beta)^T] \\
 &amp; = E[((X^T X)^{-1} X^T \epsilon) ((X^T X)^{-1} X^T \epsilon)^T] \\
 &amp; = E[(X^T X)^{-1} X^T \epsilon \epsilon^T X (X^T X)^{-1} ]
\end{align} $$</p>
<p>Given that the design matrix X is non-stochastic and the \( E[\epsilon \epsilon^T] = \sigma^2 I \):</p>
<p>$$ \begin{align}
Var(\hat{\beta}) &amp; = (X^T X)^{-1} X^T E[\epsilon \epsilon^T] X (X^T X)^{-1} \\
 &amp; = (X^T X)^{-1} X^T (\sigma^2 I) X (X^T X)^{-1} \\
 &amp; = \sigma^2 I (X^T X)^{-1} X^T X (X^T X)^{-1} \\
 &amp; = \sigma^2 I (X^T X)^{-1} \\
 &amp; = \sigma^2 (X^T X)^{-1}
\end{align} $$</p>
<p>Other regression diagnostics that are calculated include the <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">coefficient of determination</a> or
\(R^2\) which is a number that indicates the proportion of variance in the dependent variable that is explained by the independent variables, and is
documented in the table below. The parameter variance estimates as calculated above are used to compute the standard errors and a corresponding
<a href="https://en.wikipedia.org/wiki/T-statistic">t-statistic</a> which can be used for statistical inference.</p>
<table>
<thead>
<tr>
<th>Quantity</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Residual Sum of Squares (RSS)</td>
<td align="left">$$ RSS = \sum_{i=1}^n \big(y_{i} - \hat{y_{i}} \big)^2 = \sum_{i=1}^n \epsilon_{i}^2 = \epsilon^T \epsilon $$</td>
</tr>
<tr>
<td>Total Sum of Squares (TSS)</td>
<td align="left">$$ TSS = \sum_{i=1}^{n} \big(y_{i} - \overline{y}\big)^2 \, \textrm{where} \; \overline{y} = \frac{1}{n} \sum_{i=1}^n y_{i} $$</td>
</tr>
<tr>
<td>Explained Sum of Squares (ESS)</td>
<td align="left">$$ ESS = \sum_{i=1}^{n} \big(\hat{y_{i}} - \overline{y}\big)^2 \, \textrm{where} \; \overline{y} = \frac{1}{n} \sum_{i=1}^n y_{i} $$</td>
</tr>
<tr>
<td>R-Squared</td>
<td align="left">$$ R^2 = 1 - \frac{RSS}{TSS}  $$</td>
</tr>
<tr>
<td>R-Squared (Adjusted)</td>
<td align="left">$$ R^2_{adj} = 1 - \frac{ RSS * \big( n - 1 \big) }{ TSS * \big( n - p \big)}  $$</td>
</tr>
<tr>
<td>Regression Standard Error</td>
<td align="left">$$ SE = \sqrt{ \frac{RSS}{ n - p }}  $$</td>
</tr>
<tr>
<td>Parameter Variance</td>
<td align="left">$$ Var(\hat{\beta}) = SE^2( X^T X )^{-1}  $$</td>
</tr>
<tr>
<td>Parameter Std Errors</td>
<td align="left">$$ SE(\hat{\beta_{i}}) = \sqrt{ Var(\hat{\beta_{i}})} $$</td>
</tr>
<tr>
<td>Parameter T-statistics</td>
<td align="left">$$ T_{\beta_{i}} = \frac{\hat{ \beta_{i}}}{ SE( \hat{ \beta_{i} } ) }    $$</td>
</tr>
</tbody>
</table>
<h3 id="gauss-markov-assumptions">Gauss Markov Assumptions</h3>
<p>The <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">Gauss Markov Theorem</a> states that Ordinary Least Squares is the Best Linear Unbiased
and Efficient (BLUE) estimator of \(\beta\), conditional on a certain set of assumptions being met. In this context, "best" means that there are
no other unbiased estimators with a smaller sampling variance than OLS. Unbiased means that that the expectation of \( \hat{\beta}\) is equal to the
population \(\beta\), or otherwise stated \( E[ \hat{\beta} ] = \beta \). The assumptions that must hold for OLS to be BLUE are as follows:</p>
<table>
<thead>
<tr>
<th>Assumptions</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Assumption 1</td>
<td>The regression model is linear in parameters, and therefore well specified</td>
</tr>
<tr>
<td>Assumption 2</td>
<td>The regressors are linearly independent, and therefore do not exhibit perfect <a href="https://en.wikipedia.org/wiki/Multicollinearity">multicollinearity</a></td>
</tr>
<tr>
<td>Assumption 3</td>
<td>The errors in the regression have a conditional mean of zero</td>
</tr>
<tr>
<td>Assumption 4</td>
<td>The errors are <a href="https://en.wikipedia.org/wiki/Homoscedasticity">homoscedastic</a>, which means they exhibit constant variance</td>
</tr>
<tr>
<td>Assumption 5</td>
<td>The errors are uncorrelated between observations</td>
</tr>
<tr>
<td>Assumption 6</td>
<td>The errors are normally distributed, and independent and identically distributed (iid)</td>
</tr>
</tbody>
</table>
<h4 id="linear-in-parameters"><strong> Linear in Parameters </strong></h4>
<p>The first assumption regarding linearity suggests that the dependent variable is a linear function of the independent variables. This does not imply that there
is a linear relationship between the independent and dependent variables, it only states the the model is linear in parameters. For example, a model of the form
\(y = \alpha + \beta x^2 \) qualifies as being linear in parameters, while \(y = \alpha + \beta^2 x \) does not. If the functional form of a  model under
investigation is not linear in parameters, it can often be transformed so as to render it linear.</p>
<h4 id="linearly-independent"><strong> Linearly Independent </strong></h4>
<p>The second assumption that there is no perfect <a href="https://en.wikipedia.org/wiki/Multicollinearity">multicollinearity</a> between the regressors is important, as if
it exists, the OLS estimator cannot be calculated. Another way of expressing this condition is that one of the independent variables cannot be a function of any
of the other independent variables, and therefore the design matrix X must be non-singular, and therefore have full rank.</p>
<h4 id="strict-exogeneity"><strong> Strict exogeneity </strong></h4>
<p>The third assumption above states that the disturbance term averages out to zero for any given instance of X, which implies that no observations of the independent
variables convey any information about the error. Mathematically this is stated as \( E[ \epsilon | X ] = 0 \). This assumption is violated if the independent
variables are <a href="https://en.wikipedia.org/wiki/Stochastic_process">stochastic</a> in nature, which can arise as a result of <a href="https://en.wikipedia.org/wiki/Errors-in-variables_models">measurement error</a>, or if there is
<a href="https://en.wikipedia.org/wiki/Endogeneity_(econometrics)">endogeneity</a> in the model.</p>
<h4 id="spherical-errors"><strong> Spherical Errors </strong></h4>
<p>The fourth and fifth assumptions relate to the <a href="https://en.wikipedia.org/wiki/Covariance_matrix">covariance matrix</a> of the error term, and specifically states
that \(E[ \epsilon \epsilon^T | X] = \sigma^2 I \). There are two key concepts embedded in this statement, the first is that the disturbance term has uniform
variance of \(\sigma^2\) regardless of the values of the independent variables, and is referred to as homoscedasticity. In addition, the off diagonal terms of
the covariance matrix are assumed to be zero, which suggests there is no serial correlation between errors. Either or both of these assumptions may not hold in
the real world, in which case a WLS or GLS estimator may prove to be a better, unbiased linear estimator. </p>
<h3 id="example">Example</h3>
<p>The Morpheus API defines an interface called <code>DataFrameRegression</code> which exposes a number of methods that support different linear regression techniques, namely
OLS, WLS and GLS. There are overloaded methods that take one or more regressors in order to conveniently support simple and multiple linear regression. </p>
<p>The regression interface, which operates on the column data in a <code>DataFrame</code>, can be accessed by calling the <code>regress()</code> method on the frame. If  a regression in
the row dimension is required, simply call <code>transpose()</code> on the frame before calling <code>regress()</code>.</p>
<p>To illustrate an example, consider the same motor vehicle dataset introduced earlier, which can be loaded with the code below. The first 10 rows of this <code>DataFrame</code>
is also included for inspection, and in this exercise we are going to be interested in the <strong>EngineSize</strong> and <strong>Horsepower</strong> columns. </p>
<?prettify?>

<pre><code class="java">static DataFrame&lt;Integer,String&gt; loadCarDataset() {
    return DataFrame.read().csv(options -&gt; {
        options.setResource(&quot;https://www.d3xsystems.com/public/data/samples/cars93.csv&quot;);
        options.setExcludeColumnIndexes(0);
    });
}
</code></pre>

<div class="frame"><pre class="frame">
 Index  |  Manufacturer  |    Model     |   Type    |  Min.Price  |   Price   |  Max.Price  |  MPG.city  |  MPG.highway  |       AirBags        |  DriveTrain  |  Cylinders  |  EngineSize  |  Horsepower  |  RPM   |  Rev.per.mile  |  Man.trans.avail  |  Fuel.tank.capacity  |  Passengers  |  Length  |  Wheelbase  |  Width  |  Turn.circle  |  Rear.seat.room  |  Luggage.room  |  Weight  |  Origin   |        Make        |
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
     0  |         Acura  |     Integra  |    Small  |    12.9000  |  15.9000  |    18.8000  |        25  |           31  |                None  |       Front  |          4  |      1.8000  |         140  |  6300  |          2890  |              Yes  |             13.2000  |           5  |     177  |        102  |     68  |           37  |            26.5  |            11  |    2705  |  non-USA  |     Acura Integra  |
     1  |         Acura  |      Legend  |  Midsize  |    29.2000  |  33.9000  |    38.7000  |        18  |           25  |  Driver & Passenger  |       Front  |          6  |      3.2000  |         200  |  5500  |          2335  |              Yes  |             18.0000  |           5  |     195  |        115  |     71  |           38  |              30  |            15  |    3560  |  non-USA  |      Acura Legend  |
     2  |          Audi  |          90  |  Compact  |    25.9000  |  29.1000  |    32.3000  |        20  |           26  |         Driver only  |       Front  |          6  |      2.8000  |         172  |  5500  |          2280  |              Yes  |             16.9000  |           5  |     180  |        102  |     67  |           37  |              28  |            14  |    3375  |  non-USA  |           Audi 90  |
     3  |          Audi  |         100  |  Midsize  |    30.8000  |  37.7000  |    44.6000  |        19  |           26  |  Driver & Passenger  |       Front  |          6  |      2.8000  |         172  |  5500  |          2535  |              Yes  |             21.1000  |           6  |     193  |        106  |     70  |           37  |              31  |            17  |    3405  |  non-USA  |          Audi 100  |
     4  |           BMW  |        535i  |  Midsize  |    23.7000  |  30.0000  |    36.2000  |        22  |           30  |         Driver only  |        Rear  |          4  |      3.5000  |         208  |  5700  |          2545  |              Yes  |             21.1000  |           4  |     186  |        109  |     69  |           39  |              27  |            13  |    3640  |  non-USA  |          BMW 535i  |
     5  |         Buick  |     Century  |  Midsize  |    14.2000  |  15.7000  |    17.3000  |        22  |           31  |         Driver only  |       Front  |          4  |      2.2000  |         110  |  5200  |          2565  |               No  |             16.4000  |           6  |     189  |        105  |     69  |           41  |              28  |            16  |    2880  |      USA  |     Buick Century  |
     6  |         Buick  |     LeSabre  |    Large  |    19.9000  |  20.8000  |    21.7000  |        19  |           28  |         Driver only  |       Front  |          6  |      3.8000  |         170  |  4800  |          1570  |               No  |             18.0000  |           6  |     200  |        111  |     74  |           42  |            30.5  |            17  |    3470  |      USA  |     Buick LeSabre  |
     7  |         Buick  |  Roadmaster  |    Large  |    22.6000  |  23.7000  |    24.9000  |        16  |           25  |         Driver only  |        Rear  |          6  |      5.7000  |         180  |  4000  |          1320  |               No  |             23.0000  |           6  |     216  |        116  |     78  |           45  |            30.5  |            21  |    4105  |      USA  |  Buick Roadmaster  |
     8  |         Buick  |     Riviera  |  Midsize  |    26.3000  |  26.3000  |    26.3000  |        19  |           27  |         Driver only  |       Front  |          6  |      3.8000  |         170  |  4800  |          1690  |               No  |             18.8000  |           5  |     198  |        108  |     73  |           41  |            26.5  |            14  |    3495  |      USA  |     Buick Riviera  |
</pre></div>

<p>Based on our understanding of the factors that influence the power of an internal combustion engine, one might hypothesize that there is a positive and linear
relationship between the size of an engine and how much horsepower it produces. . We can use the car dataset to test this hypothesis. First we can use the Morpheus
library to generate a scatter plot of the data, with <strong>EngineSize</strong> on the x-axis and <strong>Horsepower</strong> on the y-axis as follows:</p>
<?prettify?>

<pre><code class="java">final String y = &quot;Horsepower&quot;;
final String x = &quot;EngineSize&quot;;
final DataFrame&lt;Integer,String&gt; frame = loadCarDataset();
final DataFrame&lt;Integer,String&gt; xy = frame.cols().select(y, x);
Chart.create().withScatterPlot(xy, false, x, chart -&gt; {
    chart.title().withText(y + &quot; vs &quot; + x);
    chart.plot().style(y).withColor(Color.RED);
    chart.plot().style(y).withPointsVisible(true).withPointShape(ChartShape.DIAMOND);
    chart.plot().axes().domain().label().withText(x);
    chart.plot().axes().domain().format().withPattern(&quot;0.00;-0.00&quot;);
    chart.plot().axes().range(0).label().withText(y);
    chart.plot().axes().range(0).format().withPattern(&quot;0;-0&quot;);
    chart.show(845, 450);
});
</code></pre>

<p align="center">
    <img class="chart" src="../../images/ols/data-frame-ols1.png"/>
</p>

<p>The scatter plot certainly appears to suggest that there is a positive relationship between <strong>EngineSize</strong> and <strong>Horsepower</strong>. In addition, it seems somewhat
linear, however the dispersion appears to get more significant for larger engine sizes, which would be a violation of one of the <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">Gauss Markov</a>
assumptions, namely of <a href="https://en.wikipedia.org/wiki/Homoscedasticity">homoscedastic</a> errors. Nevertheless, let us proceed to regress <strong>Horsepower</strong> on
<strong>EngineSize</strong> and see what the results look like. The code below runs a single variable regression and simply prints the model results to standard-out
for inspection.</p>
<?prettify?>

<pre><code class="java">DataFrame&lt;Integer,String&gt; frame = loadCarDataset();
String regressand = &quot;Horsepower&quot;;
String regressor = &quot;EngineSize&quot;;
frame.regress().ols(regressand, regressor, true, model -&gt; {
    System.out.println(model);
    return Optional.empty();
});
</code></pre>

<div class="frame"><pre class="frame">
==============================================================================================
                                   Linear Regression Results
==============================================================================================
Model:                                   OLS    R-Squared:                            0.5360
Observations:                             93    R-Squared(adjusted):                  0.5309
DF Model:                                  1    F-Statistic:                        105.1204
DF Residuals:                             91    F-Statistic(Prob):                  1.11E-16
Standard Error:                      35.8717    Runtime(millis)                           48
Durbin-Watson:                        1.9591
==============================================================================================
   Index     |  PARAMETER  |  STD_ERROR  |  T_STAT   |   P_VALUE   |  CI_LOWER  |  CI_UPPER  |
----------------------------------------------------------------------------------------------
  Intercept  |    45.2195  |    10.3119  |   4.3852  |   3.107E-5  |    24.736  |   65.7029  |
 EngineSize  |    36.9633  |     3.6052  |  10.2528  |  7.573E-17  |    29.802  |   44.1245  |
==============================================================================================
</pre></div>

<p>The regression results yield a slope coefficient of <strong>36.96</strong>, suggesting that for every additional litre of engine capacity, we can expect to add another
36.96 horsepower. While the <a href="https://en.wikipedia.org/wiki/P-value">p-value</a> associated with the slope coefficient suggests that it is statistically
significantly different from zero, it does not tell us about how relevant the parameter is in the regression. In this case we can reasonably surmise
that engine size is relevant given our understanding of how an internal combustion engine works and what factors affect output power. Having said that,
the coefficient of determination is perhaps lower than one might expect, and the heteroscedasticity of the residuals provides some hint that the model
may be incomplete. In particular, <a href="https://en.wikipedia.org/wiki/Omitted-variable_bias">omitted-variable bias</a> may be at play here, in the sense that
there are other important factors that influence an engine's horsepower that we have not included.</p>
<p>While the code example above simply prints the model results to standard out, the illustration below demonstrates how to access all the relevant model
outputs via the API. The <code>ols()</code> method takes a lambda parameter that consumes the regression model, which is an instance of the <code>DataFrameLeastSquares</code>
interface, and provides all the relevant hooks to access the model inputs and outputs.</p>
<?prettify?>

<pre><code class="java">final String regressand = &quot;Horsepower&quot;;
final String regressor = &quot;EngineSize&quot;;
final DataFrame&lt;Integer,String&gt; frame = loadCarDataset();
frame.regress().ols(regressand, regressor, true, model -&gt; {
    assert (model.getRegressand().equals(regressand));
    assert (model.getRegressors().size() == 1);
    assertEquals(model.getRSquared(), 0.5359992996664269, 0.00001);
    assertEquals(model.getRSquaredAdj(), 0.5309003908715525, 0.000001);
    assertEquals(model.getStdError(), 35.87167658782274, 0.00001);
    assertEquals(model.getFValue(), 105.120393642, 0.00001);
    assertEquals(model.getFValueProbability(), 0, 0.00001);
    assertEquals(model.getBetaValue(&quot;EngineSize&quot;, Field.PARAMETER), 36.96327914, 0.0000001);
    assertEquals(model.getBetaValue(&quot;EngineSize&quot;, Field.STD_ERROR), 3.60518041, 0.0000001);
    assertEquals(model.getBetaValue(&quot;EngineSize&quot;, Field.T_STAT), 10.25282369, 0.0000001);
    assertEquals(model.getBetaValue(&quot;EngineSize&quot;, Field.P_VALUE), 0.0000, 0.0000001);
    assertEquals(model.getBetaValue(&quot;EngineSize&quot;, Field.CI_LOWER), 29.80203113, 0.0000001);
    assertEquals(model.getBetaValue(&quot;EngineSize&quot;, Field.CI_UPPER), 44.12452714, 0.0000001);
    assertEquals(model.getInterceptValue(Field.PARAMETER), 45.21946716, 0.0000001);
    assertEquals(model.getInterceptValue(Field.STD_ERROR), 10.31194906, 0.0000001);
    assertEquals(model.getInterceptValue(Field.T_STAT), 4.3851523, 0.0000001);
    assertEquals(model.getInterceptValue(Field.P_VALUE), 0.00003107, 0.0000001);
    assertEquals(model.getInterceptValue(Field.CI_LOWER), 24.73604714, 0.0000001);
    assertEquals(model.getInterceptValue(Field.CI_UPPER), 65.70288719, 0.0000001);
    System.out.println(model);
    return Optional.of(model);
});
</code></pre>

<p>Finally, the chart below adds the OLS trendline to the initial scatter plot to get a better sense of how the solution fits the data. </p>
<p align="center">
    <img class="chart" src="../../images/ols/data-frame-ols2.png"/>
</p>

<p>The code to generate this chart is as follows:</p>
<?prettify?>

<pre><code class="java">final String regressand = &quot;Horsepower&quot;;
final String regressor = &quot;EngineSize&quot;;
final DataFrame&lt;Integer,String&gt; frame = loadCarDataset();
final DataFrame&lt;Integer,String&gt; xy = frame.cols().select(regressand, regressor);
Chart.create().withScatterPlot(xy, false, regressor, chart -&gt; {
    chart.title().withFont(new Font(&quot;Verdana&quot;, Font.BOLD, 16));
    chart.title().withText(regressand + &quot; regressed on &quot; + regressor);
    chart.subtitle().withText(&quot;Single Variable Linear Regression&quot;);
    chart.plot().style(regressand).withColor(Color.RED);
    chart.plot().trend(regressand).withColor(Color.BLACK);
    chart.plot().axes().domain().label().withText(regressor);
    chart.plot().axes().domain().format().withPattern(&quot;0.00;-0.00&quot;);
    chart.plot().axes().range(0).label().withText(regressand);
    chart.plot().axes().range(0).format().withPattern(&quot;0;-0&quot;);
    chart.show();
});
</code></pre>

<h3 id="unbiasedness">Unbiasedness</h3>
<p>An Ordinary Least Squares estimator is said to be <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">unbiased</a> in the sense that
if you run regressions on many samples of data generated from the same population process, the coefficient estimates from all these samples
would be centered on the true population values. To demonstrate this empirically, we can define a population process in 2D space with a known
slope and intercept coefficient, and then proceed to generate many samples from this process while adding Gaussian noise to the dependent variable
in order to simulate the error term. The code below defines a data generating function that returns a <code>DataFrame</code> of X and Y values initialized
from the population coefficients, while adding white noise scaled according to the standard deviation specified by the <code>sigma</code> parameter.</p>
<?prettify?>

<pre><code class="java">/**
 * Returns a 2D sample dataset based on a population process using the coefficients provided
 * @param alpha     the intercept term for population process
 * @param beta      the slope term for population process
 * @param startX    the start value for independent variable
 * @param stepX     the step size for independent variable
 * @param sigma     the variance to add noise to dependent variable
 * @param n         the size of the sample to generate
 * @return          the frame of XY values
 */
DataFrame&lt;Integer,String&gt; sample(double alpha, double beta, double startX, double stepX, double sigma, int n) {
    final Array&lt;Double&gt; xValues = Array.of(Double.class, n).applyDoubles(v -&gt; startX + v.index() * stepX);
    final Array&lt;Double&gt; yValues = Array.of(Double.class, n).applyDoubles(v -&gt; {
        final double yfit = alpha + beta * xValues.getDouble(v.index());
        return new NormalDistribution(yfit, sigma).sample();
    });
    final Array&lt;Integer&gt; rowKeys = Range.of(0, n).toArray();
    return DataFrame.of(rowKeys, String.class, columns -&gt; {
        columns.add(&quot;X&quot;, xValues);
        columns.add(&quot;Y&quot;, yValues);
    });
}
</code></pre>

<p>To get a sense of the nature of the dataset generated by this function for some chosen set of parameters, we can plot a number
of samples. The code below plots 4 random samples of this population process with beta = 1.45, alpha = 4.15 and a sigma value of 20.</p>
<?prettify?>

<pre><code class="java">final double beta = 1.45d;
final double alpha = 4.15d;
final double sigma = 20d;
Chart.show(2, IntStream.range(0, 4).mapToObj(i -&gt; {
    DataFrame&lt;Integer,String&gt; frame = sample(alpha, beta, 0, 1, sigma, 100);
    String title = &quot;Sample %s Dataset, Beta: %.2f Alpha: %.2f&quot;;
    String subtitle = &quot;Parameter estimates, Beta^: %.3f, Alpha^: %.3f&quot;;
    DataFrameLeastSquares&lt;Integer,String&gt; ols = frame.regress().ols(&quot;Y&quot;, &quot;X&quot;, true, Optional::of).get();
    double betaHat = ols.getBetaValue(&quot;X&quot;, DataFrameLeastSquares.Field.PARAMETER);
    double alphaHat = ols.getInterceptValue(DataFrameLeastSquares.Field.PARAMETER);
    return Chart.create().withScatterPlot(frame, false, &quot;X&quot;, chart -&gt; {
        chart.title().withText(String.format(title, i, beta, alpha));
        chart.title().withFont(new Font(&quot;Arial&quot;, Font.BOLD, 14));
        chart.subtitle().withText(String.format(subtitle, betaHat, alphaHat));
        chart.plot().style(&quot;Y&quot;).withColor(Color.RED).withPointsVisible(true);
        chart.plot().trend(&quot;Y&quot;);
    });
}));
</code></pre>

<div style="float:left;width:50%;">
    <img class="chart" src="../../images/ols/ols-sample-0.png"/>
</div>

<div style="float:left;width:50%;">
    <img class="chart" src="../../images/ols/ols-sample-1.png"/>
</div>

<div style="float:left;width:50%;">
    <img class="chart" src="../../images/ols/ols-sample-2.png"/>
</div>

<div style="float:left;width:50%;">
    <img class="chart" src="../../images/ols/ols-sample-3.png"/>
</div>

<p>Given this data generating function, we can produce many samples from a known population process and then proceed to run OLS regressions on
these samples. For each run we capture the coefficient estimates, and then plot a histogram of all the recorded estimates to confirm that the
coefficients are indeed centered on the known population values. The following code performs this procedure for 100,000 regressions, and is
followed by the resulting plots.</p>
<?prettify?>

<pre><code class="java">final int n = 100;
final double actAlpha = 4.15d;
final double actBeta = 1.45d;
final double sigma = 20d;
final int regressionCount = 100000;
final Range&lt;Integer&gt; rows = Range.of(0, regressionCount);
final Array&lt;String&gt; columns = Array.of(&quot;Beta&quot;, &quot;Alpha&quot;);
final DataFrame&lt;Integer,String&gt; results = DataFrame.ofDoubles(rows, columns);

//Run 100K regressions in parallel
results.rows().parallel().forEach(row -&gt; {
    final DataFrame&lt;Integer,String&gt; frame = sample(actAlpha, actBeta, 0, 1, sigma, n);
    frame.regress().ols(&quot;Y&quot;, &quot;X&quot;, true, model -&gt; {
        final double alpha = model.getInterceptValue(DataFrameLeastSquares.Field.PARAMETER);
        final double beta = model.getBetaValue(&quot;X&quot;, DataFrameLeastSquares.Field.PARAMETER);
        row.setDouble(&quot;Alpha&quot;, alpha);
        row.setDouble(&quot;Beta&quot;, beta);
        return Optional.empty();
    });
});

Array.of(&quot;Beta&quot;, &quot;Alpha&quot;).forEach(coefficient -&gt; {
    Chart.create().withHistPlot(results, 250, coefficient, chart -&gt; {
        final double mean = results.colAt(coefficient).stats().mean();
        final double stdDev = results.colAt(coefficient).stats().stdDev();
        final double actual = coefficient.equals(&quot;Beta&quot;) ? actBeta : actAlpha;
        final String title = &quot;%s Histogram from %s Regressions (n=%s)&quot;;
        final String subtitle = &quot;Actual: %.4f, Mean: %.4f, StdDev: %.4f&quot;;
        chart.title().withText(String.format(title, coefficient, regressionCount, n));
        chart.subtitle().withText(String.format(subtitle, actual, mean, stdDev));
        chart.show(700, 400);
    });
});
</code></pre>

<p align="center">
    <img class="chart" src="../../images/ols/ols-Alpha-unbiased.png"/>
    <img class="chart" src="../../images/ols/ols-Beta-unbiased.png"/>
</p>

<p>The alpha and beta histogram plots above clearly show that the distribution of the 100000 estimates of each coefficient are centered on the
known population values. In the case of the slope coefficient, the known population value is 1.45 and the mean value over the 100000 estimates
is a good match. Similarly, the intercept estimate mean 4.1266 is very close to the known population value of 4.15.</p>
<h3 id="consistency">Consistency</h3>
<p>An OLS estimator is said to be <a href="https://en.wikipedia.org/wiki/Consistent_estimator">consistent</a> in the sense that as the sample size increases,
the variance in the coefficient estimates should decrease. This can be demonstrated empirically once again using the data generation function introduced
earlier. In this experiment we run a certain number of regressions based on samples generated from a known population process, but we would need to do
this multiple times with increasing sample sizes. The code below implements this by running 100,000 regressions for sample sizes ranging from 100 to 500
in steps of 100, and captures the coefficient estimates for each run. It then plots histograms for the beta and intercept estimates to illustrate the
narrowing variance as sample size increases.</p>
<?prettify?>

<pre><code class="java">final double actAlpha = 4.15d;
final double actBeta = 1.45d;
final double sigma = 20d;
final int regressionCount = 100000;
final Range&lt;Integer&gt; sampleSizes = Range.of(100, 600, 100);
final Range&lt;Integer&gt; rows = Range.of(0, regressionCount);
final DataFrame&lt;Integer,String&gt; results = DataFrame.of(rows, String.class, columns -&gt; {
    sampleSizes.forEach(n -&gt; {
        columns.add(String.format(&quot;Beta(n=%s)&quot;, n), Double.class);
        columns.add(String.format(&quot;Alpha(n=%s)&quot;, n), Double.class);
    });
});

sampleSizes.forEach(n -&gt; {
    System.out.println(&quot;Running &quot; + regressionCount + &quot; regressions for n=&quot; + n);
    final String betaKey = String.format(&quot;Beta(n=%s)&quot;, n);
    final String alphaKey = String.format(&quot;Alpha(n=%s)&quot;, n);
    results.rows().parallel().forEach(row -&gt; {
        final DataFrame&lt;Integer,String&gt; frame = sample(actAlpha, actBeta, 0, 1, sigma, n);
        frame.regress().ols(&quot;Y&quot;, &quot;X&quot;, true, model -&gt; {
            final double alpha = model.getInterceptValue(DataFrameLeastSquares.Field.PARAMETER);
            final double beta = model.getBetaValue(&quot;X&quot;, DataFrameLeastSquares.Field.PARAMETER);
            row.setDouble(alphaKey, alpha);
            row.setDouble(betaKey, beta);
            return Optional.empty();
        });
    });
});

Array.of(&quot;Beta&quot;, &quot;Alpha&quot;).forEach(coeff -&gt; {
    final DataFrame&lt;Integer,String&gt; coeffResults = results.cols().select(col -&gt; col.key().startsWith(coeff));
    Chart.create().withHistPlot(coeffResults, 250, true, chart -&gt; {
        chart.plot().axes().domain().label().withText(&quot;Coefficient Estimate&quot;);
        chart.title().withText(coeff + &quot; Histograms of &quot; + regressionCount + &quot; Regressions&quot;);
        chart.subtitle().withText(coeff + &quot; Variance decreases as sample size increases&quot;);
        chart.legend().on().bottom();
        chart.show(700, 400);
    });
});
</code></pre>

<p align="center">
    <img class="chart" src="../../images/ols/ols-Beta-consistency.png"/>
    <img class="chart" src="../../images/ols/ols-Alpha-consistency.png"/>
</p>

<p>It is clear from the above plots that as sample size increases, the variance in the estimates decreases, which is what we expect if the
estimator is consistent. The bar charts below summarize the change in variance for each of the coefficients, and is follow by the code
that generates these plots.</p>
<div style="float:left;width:50%;">
    <img class="chart" src="../../images/ols/ols-beta-variance.png"/>
</div>

<div style="float:left;width:50%;">
    <img class="chart" src="../../images/ols/ols-alpha-variance.png"/>
</div>

<?prettify?>

<pre><code class="java">Array&lt;DataFrame&lt;String,StatType&gt;&gt; variances = Array.of(&quot;Beta&quot;, &quot;Alpha&quot;).map(value -&gt; {
    final String coefficient = value.getValue();
    final Matcher matcher = Pattern.compile(coefficient + &quot;\\(n=(\\d+)\\)&quot;).matcher(&quot;&quot;);
    return results.cols().select(column -&gt; {
        final String name = column.key();
        return matcher.reset(name).matches();
    }).cols().mapKeys(column -&gt; {
        final String name = column.key();
        if (matcher.reset(name).matches()) return matcher.group(1);
        throw new IllegalArgumentException(&quot;Unexpected column name: &quot; + column.key());
    }).cols().stats().variance();
});

Chart.show(2, Collect.asList(
    Chart.create().withBarPlot(variances.getValue(0), false, chart -&gt; {
        chart.title().withText(&quot;Beta variance with sample size&quot;);
        chart.plot().style(StatType.VARIANCE).withColor(new Color(255, 100, 100));
        chart.plot().axes().range(0).label().withText(&quot;Beta Variance&quot;);
        chart.plot().axes().domain().label().withText(&quot;Sample Size&quot;);
    }),
    Chart.create().withBarPlot(variances.getValue(1), false, chart -&gt; {
        chart.title().withText(&quot;Alpha variance with sample size&quot;);
        chart.plot().style(StatType.VARIANCE).withColor(new Color(102, 204, 255));
        chart.plot().axes().range(0).label().withText(&quot;Alpha Variance&quot;);
        chart.plot().axes().domain().label().withText(&quot;Sample Size&quot;);
    })
));
</code></pre></div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright (C) 2014-2017 Xavier Witdouck</p>
            <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>var base_url = '../..';</script>
        <script data-main="../../mkdocs/js/search.js" src="../../mkdocs/js/require.js"></script>
        <script src="../../js/base.js"></script>
        <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/highlight.min.js"></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <script src="http://www.zavtech.com/morpheus/docs/javascript/analytics.js"></script><div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
